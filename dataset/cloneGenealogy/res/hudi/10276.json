[{"authorTime":"2020-05-14 06:37:03","codes":[{"authorDate":"2020-05-14 19:15:49","commitOrder":2,"curCode":"  private void verifyResultData(List<GenericRecord> expectData) {\n    Dataset<Row> ds = HoodieClientTestUtils.read(jsc, tablePath, sqlContext, fs, tablePath + \"/*/*/*/*\");\n\n    List<Row> readData = ds.select(\"timestamp\", \"_row_key\", \"rider\", \"driver\", \"begin_lat\", \"begin_lon\", \"end_lat\", \"end_lon\").collectAsList();\n    List<HoodieTripModel> result = readData.stream().map(row ->\n        new HoodieTripModel(row.getDouble(0), row.getString(1), row.getString(2), row.getString(3), row.getDouble(4),\n            row.getDouble(5), row.getDouble(6), row.getDouble(7)))\n        .collect(Collectors.toList());\n\n    List<HoodieTripModel> expected = expectData.stream().map(g ->\n        new HoodieTripModel(Double.parseDouble(g.get(\"timestamp\").toString()),\n            g.get(\"_row_key\").toString(),\n            g.get(\"rider\").toString(),\n            g.get(\"driver\").toString(),\n            Double.parseDouble(g.get(\"begin_lat\").toString()),\n            Double.parseDouble(g.get(\"begin_lon\").toString()),\n            Double.parseDouble(g.get(\"end_lat\").toString()),\n            Double.parseDouble(g.get(\"end_lon\").toString())))\n        .collect(Collectors.toList());\n\n    assertAll(\"Result list equals\",\n        () -> assertEquals(expected.size(), result.size()),\n        () -> assertTrue(result.containsAll(expected) && expected.containsAll(result)));\n  }\n","date":"2020-05-14 19:15:49","endLine":185,"groupId":"4295","id":1,"instanceNumber":1,"isCurCommit":0,"methodName":"verifyResultData","params":"(List<GenericRecord>expectData)","path":"/mnt/clonedata/CloneManagementServer/ManagementServer/consistResult/result-hudi-10-0.7/blobInfo/CC_OUT/blobs/34/7396b23074554f6eecf7dcff1e190c4068c946.src","preCode":"  private void verifyResultData(List<GenericRecord> expectData) {\n    Dataset<Row> ds = HoodieClientTestUtils.read(jsc, tablePath, sqlContext, fs, tablePath + \"/*/*/*/*\");\n\n    List<Row> readData = ds.select(\"timestamp\", \"_row_key\", \"rider\", \"driver\", \"begin_lat\", \"begin_lon\", \"end_lat\", \"end_lon\").collectAsList();\n    List<HoodieTripModel> result = readData.stream().map(row ->\n        new HoodieTripModel(row.getDouble(0), row.getString(1), row.getString(2), row.getString(3), row.getDouble(4),\n            row.getDouble(5), row.getDouble(6), row.getDouble(7)))\n        .collect(Collectors.toList());\n\n    List<HoodieTripModel> expected = expectData.stream().map(g ->\n        new HoodieTripModel(Double.parseDouble(g.get(\"timestamp\").toString()),\n            g.get(\"_row_key\").toString(),\n            g.get(\"rider\").toString(),\n            g.get(\"driver\").toString(),\n            Double.parseDouble(g.get(\"begin_lat\").toString()),\n            Double.parseDouble(g.get(\"begin_lon\").toString()),\n            Double.parseDouble(g.get(\"end_lat\").toString()),\n            Double.parseDouble(g.get(\"end_lon\").toString())))\n        .collect(Collectors.toList());\n\n    assertAll(\"Result list equals\",\n        () -> assertEquals(expected.size(), result.size()),\n        () -> assertTrue(result.containsAll(expected) && expected.containsAll(result)));\n  }\n","realPath":"hudi-cli/src/test/java/org/apache/hudi/cli/integ/ITTestHDFSParquetImportCommand.java","repoName":"hudi","snippetEndLine":0,"snippetStartLine":0,"startLine":162,"status":"B"},{"authorDate":"2020-05-14 06:37:03","commitOrder":2,"curCode":"  public void testImportWithUpsert() throws IOException, ParseException {\n    try (JavaSparkContext jsc = getJavaSparkContext()) {\n      insert(jsc);\n\n      \r\n      String schemaFile = new Path(basePath, \"file.schema\").toString();\n\n      Path upsertFolder = new Path(basePath, \"testUpsertSrc\");\n      List<GenericRecord> upsertData = createUpsertRecords(upsertFolder);\n\n      HDFSParquetImporter.Config cfg = getHDFSParquetImporterConfig(upsertFolder.toString(), hoodieFolder.toString(),\n          \"testTable\", \"COPY_ON_WRITE\", \"_row_key\", \"timestamp\", 1, schemaFile);\n      cfg.command = \"upsert\";\n      HDFSParquetImporter dataImporter = new HDFSParquetImporter(cfg);\n\n      dataImporter.dataImport(jsc, 0);\n\n      \r\n      List<GenericRecord> expectData = insertData.subList(11, 96);\n      expectData.addAll(upsertData);\n\n      \r\n      SQLContext sqlContext = new SQLContext(jsc);\n      Dataset<Row> ds = HoodieClientTestUtils.read(jsc, basePath + \"/testTarget\", sqlContext, dfs, basePath + \"/testTarget/*/*/*/*\");\n\n      List<Row> readData = ds.select(\"timestamp\", \"_row_key\", \"rider\", \"driver\", \"begin_lat\", \"begin_lon\", \"end_lat\", \"end_lon\").collectAsList();\n      List<HoodieTripModel> result = readData.stream().map(row ->\n          new HoodieTripModel(row.getDouble(0), row.getString(1), row.getString(2), row.getString(3), row.getDouble(4),\n              row.getDouble(5), row.getDouble(6), row.getDouble(7)))\n          .collect(Collectors.toList());\n\n      \r\n      List<HoodieTripModel> expected = expectData.stream().map(g ->\n          new HoodieTripModel(Double.parseDouble(g.get(\"timestamp\").toString()),\n              g.get(\"_row_key\").toString(),\n              g.get(\"rider\").toString(),\n              g.get(\"driver\").toString(),\n              Double.parseDouble(g.get(\"begin_lat\").toString()),\n              Double.parseDouble(g.get(\"begin_lon\").toString()),\n              Double.parseDouble(g.get(\"end_lat\").toString()),\n              Double.parseDouble(g.get(\"end_lon\").toString())))\n          .collect(Collectors.toList());\n\n      assertTrue(result.containsAll(expected) && expected.containsAll(result) && result.size() == expected.size());\n    }\n  }\n","date":"2020-05-14 06:37:03","endLine":265,"groupId":"4295","id":2,"instanceNumber":2,"isCurCommit":0,"methodName":"testImportWithUpsert","params":"()","path":"/mnt/clonedata/CloneManagementServer/ManagementServer/consistResult/result-hudi-10-0.7/blobInfo/CC_OUT/blobs/f7/38cdfdece690b977ec31850ebdf81d1d20a181.src","preCode":"  public void testImportWithUpsert() throws IOException, ParseException {\n    try (JavaSparkContext jsc = getJavaSparkContext()) {\n      insert(jsc);\n\n      \r\n      String schemaFile = new Path(basePath, \"file.schema\").toString();\n\n      Path upsertFolder = new Path(basePath, \"testUpsertSrc\");\n      List<GenericRecord> upsertData = createUpsertRecords(upsertFolder);\n\n      HDFSParquetImporter.Config cfg = getHDFSParquetImporterConfig(upsertFolder.toString(), hoodieFolder.toString(),\n          \"testTable\", \"COPY_ON_WRITE\", \"_row_key\", \"timestamp\", 1, schemaFile);\n      cfg.command = \"upsert\";\n      HDFSParquetImporter dataImporter = new HDFSParquetImporter(cfg);\n\n      dataImporter.dataImport(jsc, 0);\n\n      \r\n      List<GenericRecord> expectData = insertData.subList(11, 96);\n      expectData.addAll(upsertData);\n\n      \r\n      SQLContext sqlContext = new SQLContext(jsc);\n      Dataset<Row> ds = HoodieClientTestUtils.read(jsc, basePath + \"/testTarget\", sqlContext, dfs, basePath + \"/testTarget/*/*/*/*\");\n\n      List<Row> readData = ds.select(\"timestamp\", \"_row_key\", \"rider\", \"driver\", \"begin_lat\", \"begin_lon\", \"end_lat\", \"end_lon\").collectAsList();\n      List<HoodieTripModel> result = readData.stream().map(row ->\n          new HoodieTripModel(row.getDouble(0), row.getString(1), row.getString(2), row.getString(3), row.getDouble(4),\n              row.getDouble(5), row.getDouble(6), row.getDouble(7)))\n          .collect(Collectors.toList());\n\n      \r\n      List<HoodieTripModel> expected = expectData.stream().map(g ->\n          new HoodieTripModel(Double.parseDouble(g.get(\"timestamp\").toString()),\n              g.get(\"_row_key\").toString(),\n              g.get(\"rider\").toString(),\n              g.get(\"driver\").toString(),\n              Double.parseDouble(g.get(\"begin_lat\").toString()),\n              Double.parseDouble(g.get(\"begin_lon\").toString()),\n              Double.parseDouble(g.get(\"end_lat\").toString()),\n              Double.parseDouble(g.get(\"end_lon\").toString())))\n          .collect(Collectors.toList());\n\n      assertTrue(result.containsAll(expected) && expected.containsAll(result) && result.size() == expected.size());\n    }\n  }\n","realPath":"hudi-utilities/src/test/java/org/apache/hudi/utilities/functional/TestHDFSParquetImporter.java","repoName":"hudi","snippetEndLine":0,"snippetStartLine":0,"startLine":220,"status":"NB"}],"commitId":"3a2fe13fcb7c168f8ff023e3bdb6ae482b400316","commitMessage":"@@@[HUDI-701] Add unit test for HDFSParquetImportCommand (#1574)\n\n","date":"2020-05-14 19:15:49","modifiedFileCount":"3","status":"M","submitter":"hongdd"},{"authorTime":"2020-07-06 07:44:31","codes":[{"authorDate":"2020-05-14 19:15:49","commitOrder":3,"curCode":"  private void verifyResultData(List<GenericRecord> expectData) {\n    Dataset<Row> ds = HoodieClientTestUtils.read(jsc, tablePath, sqlContext, fs, tablePath + \"/*/*/*/*\");\n\n    List<Row> readData = ds.select(\"timestamp\", \"_row_key\", \"rider\", \"driver\", \"begin_lat\", \"begin_lon\", \"end_lat\", \"end_lon\").collectAsList();\n    List<HoodieTripModel> result = readData.stream().map(row ->\n        new HoodieTripModel(row.getDouble(0), row.getString(1), row.getString(2), row.getString(3), row.getDouble(4),\n            row.getDouble(5), row.getDouble(6), row.getDouble(7)))\n        .collect(Collectors.toList());\n\n    List<HoodieTripModel> expected = expectData.stream().map(g ->\n        new HoodieTripModel(Double.parseDouble(g.get(\"timestamp\").toString()),\n            g.get(\"_row_key\").toString(),\n            g.get(\"rider\").toString(),\n            g.get(\"driver\").toString(),\n            Double.parseDouble(g.get(\"begin_lat\").toString()),\n            Double.parseDouble(g.get(\"begin_lon\").toString()),\n            Double.parseDouble(g.get(\"end_lat\").toString()),\n            Double.parseDouble(g.get(\"end_lon\").toString())))\n        .collect(Collectors.toList());\n\n    assertAll(\"Result list equals\",\n        () -> assertEquals(expected.size(), result.size()),\n        () -> assertTrue(result.containsAll(expected) && expected.containsAll(result)));\n  }\n","date":"2020-05-14 19:15:49","endLine":185,"groupId":"4295","id":3,"instanceNumber":1,"isCurCommit":0,"methodName":"verifyResultData","params":"(List<GenericRecord>expectData)","path":"/mnt/clonedata/CloneManagementServer/ManagementServer/consistResult/result-hudi-10-0.7/blobInfo/CC_OUT/blobs/34/7396b23074554f6eecf7dcff1e190c4068c946.src","preCode":"  private void verifyResultData(List<GenericRecord> expectData) {\n    Dataset<Row> ds = HoodieClientTestUtils.read(jsc, tablePath, sqlContext, fs, tablePath + \"/*/*/*/*\");\n\n    List<Row> readData = ds.select(\"timestamp\", \"_row_key\", \"rider\", \"driver\", \"begin_lat\", \"begin_lon\", \"end_lat\", \"end_lon\").collectAsList();\n    List<HoodieTripModel> result = readData.stream().map(row ->\n        new HoodieTripModel(row.getDouble(0), row.getString(1), row.getString(2), row.getString(3), row.getDouble(4),\n            row.getDouble(5), row.getDouble(6), row.getDouble(7)))\n        .collect(Collectors.toList());\n\n    List<HoodieTripModel> expected = expectData.stream().map(g ->\n        new HoodieTripModel(Double.parseDouble(g.get(\"timestamp\").toString()),\n            g.get(\"_row_key\").toString(),\n            g.get(\"rider\").toString(),\n            g.get(\"driver\").toString(),\n            Double.parseDouble(g.get(\"begin_lat\").toString()),\n            Double.parseDouble(g.get(\"begin_lon\").toString()),\n            Double.parseDouble(g.get(\"end_lat\").toString()),\n            Double.parseDouble(g.get(\"end_lon\").toString())))\n        .collect(Collectors.toList());\n\n    assertAll(\"Result list equals\",\n        () -> assertEquals(expected.size(), result.size()),\n        () -> assertTrue(result.containsAll(expected) && expected.containsAll(result)));\n  }\n","realPath":"hudi-cli/src/test/java/org/apache/hudi/cli/integ/ITTestHDFSParquetImportCommand.java","repoName":"hudi","snippetEndLine":0,"snippetStartLine":0,"startLine":162,"status":"N"},{"authorDate":"2020-07-06 07:44:31","commitOrder":3,"curCode":"  public void testImportWithUpsert() throws IOException, ParseException {\n    insert(jsc());\n\n    \r\n    String schemaFile = new Path(basePath, \"file.schema\").toString();\n\n    Path upsertFolder = new Path(basePath, \"testUpsertSrc\");\n    List<GenericRecord> upsertData = createUpsertRecords(upsertFolder);\n\n    HDFSParquetImporter.Config cfg = getHDFSParquetImporterConfig(upsertFolder.toString(), hoodieFolder.toString(),\n        \"testTable\", \"COPY_ON_WRITE\", \"_row_key\", \"timestamp\", 1, schemaFile);\n    cfg.command = \"upsert\";\n    HDFSParquetImporter dataImporter = new HDFSParquetImporter(cfg);\n\n    dataImporter.dataImport(jsc(), 0);\n\n    \r\n    List<GenericRecord> expectData = insertData.subList(11, 96);\n    expectData.addAll(upsertData);\n\n    \r\n    Dataset<Row> ds = HoodieClientTestUtils.read(jsc(), basePath + \"/testTarget\", sqlContext(), dfs(), basePath + \"/testTarget/*/*/*/*\");\n\n    List<Row> readData = ds.select(\"timestamp\", \"_row_key\", \"rider\", \"driver\", \"begin_lat\", \"begin_lon\", \"end_lat\", \"end_lon\").collectAsList();\n    List<HoodieTripModel> result = readData.stream().map(row ->\n        new HoodieTripModel(row.getDouble(0), row.getString(1), row.getString(2), row.getString(3), row.getDouble(4),\n            row.getDouble(5), row.getDouble(6), row.getDouble(7)))\n        .collect(Collectors.toList());\n\n    \r\n    List<HoodieTripModel> expected = expectData.stream().map(g ->\n        new HoodieTripModel(Double.parseDouble(g.get(\"timestamp\").toString()),\n            g.get(\"_row_key\").toString(),\n            g.get(\"rider\").toString(),\n            g.get(\"driver\").toString(),\n            Double.parseDouble(g.get(\"begin_lat\").toString()),\n            Double.parseDouble(g.get(\"begin_lon\").toString()),\n            Double.parseDouble(g.get(\"end_lat\").toString()),\n            Double.parseDouble(g.get(\"end_lon\").toString())))\n        .collect(Collectors.toList());\n\n    assertTrue(result.containsAll(expected) && expected.containsAll(result) && result.size() == expected.size());\n  }\n","date":"2020-07-06 07:44:31","endLine":227,"groupId":"4295","id":4,"instanceNumber":2,"isCurCommit":0,"methodName":"testImportWithUpsert","params":"()","path":"/mnt/clonedata/CloneManagementServer/ManagementServer/consistResult/result-hudi-10-0.7/blobInfo/CC_OUT/blobs/90/fa31d0a221913499ac024a693662e815d2179d.src","preCode":"  public void testImportWithUpsert() throws IOException, ParseException {\n    try (JavaSparkContext jsc = getJavaSparkContext()) {\n      insert(jsc);\n\n      \r\n      String schemaFile = new Path(basePath, \"file.schema\").toString();\n\n      Path upsertFolder = new Path(basePath, \"testUpsertSrc\");\n      List<GenericRecord> upsertData = createUpsertRecords(upsertFolder);\n\n      HDFSParquetImporter.Config cfg = getHDFSParquetImporterConfig(upsertFolder.toString(), hoodieFolder.toString(),\n          \"testTable\", \"COPY_ON_WRITE\", \"_row_key\", \"timestamp\", 1, schemaFile);\n      cfg.command = \"upsert\";\n      HDFSParquetImporter dataImporter = new HDFSParquetImporter(cfg);\n\n      dataImporter.dataImport(jsc, 0);\n\n      \r\n      List<GenericRecord> expectData = insertData.subList(11, 96);\n      expectData.addAll(upsertData);\n\n      \r\n      SQLContext sqlContext = new SQLContext(jsc);\n      Dataset<Row> ds = HoodieClientTestUtils.read(jsc, basePath + \"/testTarget\", sqlContext, dfs, basePath + \"/testTarget/*/*/*/*\");\n\n      List<Row> readData = ds.select(\"timestamp\", \"_row_key\", \"rider\", \"driver\", \"begin_lat\", \"begin_lon\", \"end_lat\", \"end_lon\").collectAsList();\n      List<HoodieTripModel> result = readData.stream().map(row ->\n          new HoodieTripModel(row.getDouble(0), row.getString(1), row.getString(2), row.getString(3), row.getDouble(4),\n              row.getDouble(5), row.getDouble(6), row.getDouble(7)))\n          .collect(Collectors.toList());\n\n      \r\n      List<HoodieTripModel> expected = expectData.stream().map(g ->\n          new HoodieTripModel(Double.parseDouble(g.get(\"timestamp\").toString()),\n              g.get(\"_row_key\").toString(),\n              g.get(\"rider\").toString(),\n              g.get(\"driver\").toString(),\n              Double.parseDouble(g.get(\"begin_lat\").toString()),\n              Double.parseDouble(g.get(\"begin_lon\").toString()),\n              Double.parseDouble(g.get(\"end_lat\").toString()),\n              Double.parseDouble(g.get(\"end_lon\").toString())))\n          .collect(Collectors.toList());\n\n      assertTrue(result.containsAll(expected) && expected.containsAll(result) && result.size() == expected.size());\n    }\n  }\n","realPath":"hudi-utilities/src/test/java/org/apache/hudi/utilities/functional/TestHDFSParquetImporter.java","repoName":"hudi","snippetEndLine":0,"snippetStartLine":0,"startLine":185,"status":"M"}],"commitId":"3b9a30528bd6a6369181702303f3384162b04a7f","commitMessage":"@@@[HUDI-996] Add functional test suite for hudi-utilities (#1746)\n\n- Share resources for functional tests\n- Add suite for functional test classes from hudi-utilities\n","date":"2020-07-06 07:44:31","modifiedFileCount":"8","status":"M","submitter":"Raymond Xu"},{"authorTime":"2020-09-06 16:00:45","codes":[{"authorDate":"2020-09-06 16:00:45","commitOrder":4,"curCode":"  private void verifyResultData(List<GenericRecord> expectData) {\n    Dataset<Row> ds = HoodieClientTestUtils.read(jsc, tablePath, sqlContext, fs, tablePath + \"/*/*/*/*\");\n\n    List<Row> readData = ds.select(\"timestamp\", \"_row_key\", \"rider\", \"driver\", \"begin_lat\", \"begin_lon\", \"end_lat\", \"end_lon\").collectAsList();\n    List<HoodieTripModel> result = readData.stream().map(row ->\n        new HoodieTripModel(row.getLong(0), row.getString(1), row.getString(2), row.getString(3), row.getDouble(4),\n            row.getDouble(5), row.getDouble(6), row.getDouble(7)))\n        .collect(Collectors.toList());\n\n    List<HoodieTripModel> expected = expectData.stream().map(g ->\n        new HoodieTripModel(Long.parseLong(g.get(\"timestamp\").toString()),\n            g.get(\"_row_key\").toString(),\n            g.get(\"rider\").toString(),\n            g.get(\"driver\").toString(),\n            Double.parseDouble(g.get(\"begin_lat\").toString()),\n            Double.parseDouble(g.get(\"begin_lon\").toString()),\n            Double.parseDouble(g.get(\"end_lat\").toString()),\n            Double.parseDouble(g.get(\"end_lon\").toString())))\n        .collect(Collectors.toList());\n\n    assertAll(\"Result list equals\",\n        () -> assertEquals(expected.size(), result.size()),\n        () -> assertTrue(result.containsAll(expected) && expected.containsAll(result)));\n  }\n","date":"2020-09-16 11:58:29","endLine":184,"groupId":"10276","id":5,"instanceNumber":1,"isCurCommit":0,"methodName":"verifyResultData","params":"(List<GenericRecord>expectData)","path":"/mnt/clonedata/CloneManagementServer/ManagementServer/consistResult/result-hudi-10-0.7/blobInfo/CC_OUT/blobs/17/b997ac26f232d17707a219d646912772f41e90.src","preCode":"  private void verifyResultData(List<GenericRecord> expectData) {\n    Dataset<Row> ds = HoodieClientTestUtils.read(jsc, tablePath, sqlContext, fs, tablePath + \"/*/*/*/*\");\n\n    List<Row> readData = ds.select(\"timestamp\", \"_row_key\", \"rider\", \"driver\", \"begin_lat\", \"begin_lon\", \"end_lat\", \"end_lon\").collectAsList();\n    List<HoodieTripModel> result = readData.stream().map(row ->\n        new HoodieTripModel(row.getDouble(0), row.getString(1), row.getString(2), row.getString(3), row.getDouble(4),\n            row.getDouble(5), row.getDouble(6), row.getDouble(7)))\n        .collect(Collectors.toList());\n\n    List<HoodieTripModel> expected = expectData.stream().map(g ->\n        new HoodieTripModel(Double.parseDouble(g.get(\"timestamp\").toString()),\n            g.get(\"_row_key\").toString(),\n            g.get(\"rider\").toString(),\n            g.get(\"driver\").toString(),\n            Double.parseDouble(g.get(\"begin_lat\").toString()),\n            Double.parseDouble(g.get(\"begin_lon\").toString()),\n            Double.parseDouble(g.get(\"end_lat\").toString()),\n            Double.parseDouble(g.get(\"end_lon\").toString())))\n        .collect(Collectors.toList());\n\n    assertAll(\"Result list equals\",\n        () -> assertEquals(expected.size(), result.size()),\n        () -> assertTrue(result.containsAll(expected) && expected.containsAll(result)));\n  }\n","realPath":"hudi-cli/src/test/java/org/apache/hudi/cli/integ/ITTestHDFSParquetImportCommand.java","repoName":"hudi","snippetEndLine":0,"snippetStartLine":0,"startLine":161,"status":"M"},{"authorDate":"2020-09-06 16:00:45","commitOrder":4,"curCode":"  public void testImportWithUpsert() throws IOException, ParseException {\n    insert(jsc());\n\n    \r\n    String schemaFile = new Path(basePath, \"file.schema\").toString();\n\n    Path upsertFolder = new Path(basePath, \"testUpsertSrc\");\n    List<GenericRecord> upsertData = createUpsertRecords(upsertFolder);\n\n    HDFSParquetImporter.Config cfg = getHDFSParquetImporterConfig(upsertFolder.toString(), hoodieFolder.toString(),\n        \"testTable\", \"COPY_ON_WRITE\", \"_row_key\", \"timestamp\", 1, schemaFile);\n    cfg.command = \"upsert\";\n    HDFSParquetImporter dataImporter = new HDFSParquetImporter(cfg);\n\n    dataImporter.dataImport(jsc(), 0);\n\n    \r\n    List<GenericRecord> expectData = insertData.subList(11, 96);\n    expectData.addAll(upsertData);\n\n    \r\n    Dataset<Row> ds = HoodieClientTestUtils.read(jsc(), basePath + \"/testTarget\", sqlContext(), dfs(), basePath + \"/testTarget/*/*/*/*\");\n\n    List<Row> readData = ds.select(\"timestamp\", \"_row_key\", \"rider\", \"driver\", \"begin_lat\", \"begin_lon\", \"end_lat\", \"end_lon\").collectAsList();\n    List<HoodieTripModel> result = readData.stream().map(row ->\n        new HoodieTripModel(row.getLong(0), row.getString(1), row.getString(2), row.getString(3), row.getDouble(4),\n            row.getDouble(5), row.getDouble(6), row.getDouble(7)))\n        .collect(Collectors.toList());\n\n    \r\n    List<HoodieTripModel> expected = expectData.stream().map(g ->\n        new HoodieTripModel(Long.parseLong(g.get(\"timestamp\").toString()),\n            g.get(\"_row_key\").toString(),\n            g.get(\"rider\").toString(),\n            g.get(\"driver\").toString(),\n            Double.parseDouble(g.get(\"begin_lat\").toString()),\n            Double.parseDouble(g.get(\"begin_lon\").toString()),\n            Double.parseDouble(g.get(\"end_lat\").toString()),\n            Double.parseDouble(g.get(\"end_lon\").toString())))\n        .collect(Collectors.toList());\n\n    assertTrue(result.containsAll(expected) && expected.containsAll(result) && result.size() == expected.size());\n  }\n","date":"2020-09-16 11:58:29","endLine":227,"groupId":"10276","id":6,"instanceNumber":2,"isCurCommit":0,"methodName":"testImportWithUpsert","params":"()","path":"/mnt/clonedata/CloneManagementServer/ManagementServer/consistResult/result-hudi-10-0.7/blobInfo/CC_OUT/blobs/1f/e6800eb7e1e9beb143061a4b2d2da7013f6962.src","preCode":"  public void testImportWithUpsert() throws IOException, ParseException {\n    insert(jsc());\n\n    \r\n    String schemaFile = new Path(basePath, \"file.schema\").toString();\n\n    Path upsertFolder = new Path(basePath, \"testUpsertSrc\");\n    List<GenericRecord> upsertData = createUpsertRecords(upsertFolder);\n\n    HDFSParquetImporter.Config cfg = getHDFSParquetImporterConfig(upsertFolder.toString(), hoodieFolder.toString(),\n        \"testTable\", \"COPY_ON_WRITE\", \"_row_key\", \"timestamp\", 1, schemaFile);\n    cfg.command = \"upsert\";\n    HDFSParquetImporter dataImporter = new HDFSParquetImporter(cfg);\n\n    dataImporter.dataImport(jsc(), 0);\n\n    \r\n    List<GenericRecord> expectData = insertData.subList(11, 96);\n    expectData.addAll(upsertData);\n\n    \r\n    Dataset<Row> ds = HoodieClientTestUtils.read(jsc(), basePath + \"/testTarget\", sqlContext(), dfs(), basePath + \"/testTarget/*/*/*/*\");\n\n    List<Row> readData = ds.select(\"timestamp\", \"_row_key\", \"rider\", \"driver\", \"begin_lat\", \"begin_lon\", \"end_lat\", \"end_lon\").collectAsList();\n    List<HoodieTripModel> result = readData.stream().map(row ->\n        new HoodieTripModel(row.getDouble(0), row.getString(1), row.getString(2), row.getString(3), row.getDouble(4),\n            row.getDouble(5), row.getDouble(6), row.getDouble(7)))\n        .collect(Collectors.toList());\n\n    \r\n    List<HoodieTripModel> expected = expectData.stream().map(g ->\n        new HoodieTripModel(Double.parseDouble(g.get(\"timestamp\").toString()),\n            g.get(\"_row_key\").toString(),\n            g.get(\"rider\").toString(),\n            g.get(\"driver\").toString(),\n            Double.parseDouble(g.get(\"begin_lat\").toString()),\n            Double.parseDouble(g.get(\"begin_lon\").toString()),\n            Double.parseDouble(g.get(\"end_lat\").toString()),\n            Double.parseDouble(g.get(\"end_lon\").toString())))\n        .collect(Collectors.toList());\n\n    assertTrue(result.containsAll(expected) && expected.containsAll(result) && result.size() == expected.size());\n  }\n","realPath":"hudi-utilities/src/test/java/org/apache/hudi/utilities/functional/TestHDFSParquetImporter.java","repoName":"hudi","snippetEndLine":0,"snippetStartLine":0,"startLine":185,"status":"M"}],"commitId":"581d54097c89e5f9c04d8f606c9081dc17138bd5","commitMessage":"@@@[HUDI-1143] Change timestamp field in HoodieTestDataGenerator from double to long\n","date":"2020-09-16 11:58:29","modifiedFileCount":"8","status":"M","submitter":"shenh062326"}]
