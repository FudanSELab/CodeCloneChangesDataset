[{"authorTime":"2020-11-18 17:57:11","codes":[{"authorDate":"2020-10-12 09:38:42","commitOrder":3,"curCode":"  public void runMerge(HoodieTable<T, JavaRDD<HoodieRecord<T>>, JavaRDD<HoodieKey>, JavaRDD<WriteStatus>> table,\n                       HoodieMergeHandle<T, JavaRDD<HoodieRecord<T>>, JavaRDD<HoodieKey>, JavaRDD<WriteStatus>> upsertHandle) throws IOException {\n    final boolean externalSchemaTransformation = table.getConfig().shouldUseExternalSchemaTransformation();\n    Configuration cfgForHoodieFile = new Configuration(table.getHadoopConf());\n    HoodieMergeHandle<T, JavaRDD<HoodieRecord<T>>, JavaRDD<HoodieKey>, JavaRDD<WriteStatus>> mergeHandle = upsertHandle;\n    HoodieBaseFile baseFile = mergeHandle.baseFileForMerge();\n\n    final GenericDatumWriter<GenericRecord> gWriter;\n    final GenericDatumReader<GenericRecord> gReader;\n    Schema readSchema;\n    if (externalSchemaTransformation || baseFile.getBootstrapBaseFile().isPresent()) {\n      readSchema = HoodieFileReaderFactory.getFileReader(table.getHadoopConf(), mergeHandle.getOldFilePath()).getSchema();\n      gWriter = new GenericDatumWriter<>(readSchema);\n      gReader = new GenericDatumReader<>(readSchema, mergeHandle.getWriterSchemaWithMetafields());\n    } else {\n      gReader = null;\n      gWriter = null;\n      readSchema = mergeHandle.getWriterSchemaWithMetafields();\n    }\n\n    BoundedInMemoryExecutor<GenericRecord, GenericRecord, Void> wrapper = null;\n    HoodieFileReader<GenericRecord> reader = HoodieFileReaderFactory.<GenericRecord>getFileReader(cfgForHoodieFile, mergeHandle.getOldFilePath());\n    try {\n      final Iterator<GenericRecord> readerIterator;\n      if (baseFile.getBootstrapBaseFile().isPresent()) {\n        readerIterator = getMergingIterator(table, mergeHandle, baseFile, reader, readSchema, externalSchemaTransformation);\n      } else {\n        readerIterator = reader.getRecordIterator(readSchema);\n      }\n\n      ThreadLocal<BinaryEncoder> encoderCache = new ThreadLocal<>();\n      ThreadLocal<BinaryDecoder> decoderCache = new ThreadLocal<>();\n      wrapper = new SparkBoundedInMemoryExecutor(table.getConfig(), readerIterator,\n          new UpdateHandler(mergeHandle), record -> {\n        if (!externalSchemaTransformation) {\n          return record;\n        }\n        return transformRecordBasedOnNewSchema(gReader, gWriter, encoderCache, decoderCache, (GenericRecord) record);\n      });\n      wrapper.execute();\n    } catch (Exception e) {\n      throw new HoodieException(e);\n    } finally {\n      if (reader != null) {\n        reader.close();\n      }\n      mergeHandle.close();\n      if (null != wrapper) {\n        wrapper.shutdownNow();\n      }\n    }\n  }\n","date":"2020-10-12 09:38:42","endLine":109,"groupId":"2650","id":1,"instanceNumber":1,"isCurCommit":0,"methodName":"runMerge","params":"(HoodieTable<T@JavaRDD<HoodieRecord<T>>@JavaRDD<HoodieKey>@JavaRDD<WriteStatus>>table@HoodieMergeHandle<T@JavaRDD<HoodieRecord<T>>@JavaRDD<HoodieKey>@JavaRDD<WriteStatus>>upsertHandle)","path":"/mnt/clonedata/CloneManagementServer/ManagementServer/consistResult/result-hudi-10-0.7/blobInfo/CC_OUT/blobs/2d/130e35d70fb8fd9102f5576c733fc563f9f0d8.src","preCode":"  public void runMerge(HoodieTable<T, JavaRDD<HoodieRecord<T>>, JavaRDD<HoodieKey>, JavaRDD<WriteStatus>> table,\n                       HoodieMergeHandle<T, JavaRDD<HoodieRecord<T>>, JavaRDD<HoodieKey>, JavaRDD<WriteStatus>> upsertHandle) throws IOException {\n    final boolean externalSchemaTransformation = table.getConfig().shouldUseExternalSchemaTransformation();\n    Configuration cfgForHoodieFile = new Configuration(table.getHadoopConf());\n    HoodieMergeHandle<T, JavaRDD<HoodieRecord<T>>, JavaRDD<HoodieKey>, JavaRDD<WriteStatus>> mergeHandle = upsertHandle;\n    HoodieBaseFile baseFile = mergeHandle.baseFileForMerge();\n\n    final GenericDatumWriter<GenericRecord> gWriter;\n    final GenericDatumReader<GenericRecord> gReader;\n    Schema readSchema;\n    if (externalSchemaTransformation || baseFile.getBootstrapBaseFile().isPresent()) {\n      readSchema = HoodieFileReaderFactory.getFileReader(table.getHadoopConf(), mergeHandle.getOldFilePath()).getSchema();\n      gWriter = new GenericDatumWriter<>(readSchema);\n      gReader = new GenericDatumReader<>(readSchema, mergeHandle.getWriterSchemaWithMetafields());\n    } else {\n      gReader = null;\n      gWriter = null;\n      readSchema = mergeHandle.getWriterSchemaWithMetafields();\n    }\n\n    BoundedInMemoryExecutor<GenericRecord, GenericRecord, Void> wrapper = null;\n    HoodieFileReader<GenericRecord> reader = HoodieFileReaderFactory.<GenericRecord>getFileReader(cfgForHoodieFile, mergeHandle.getOldFilePath());\n    try {\n      final Iterator<GenericRecord> readerIterator;\n      if (baseFile.getBootstrapBaseFile().isPresent()) {\n        readerIterator = getMergingIterator(table, mergeHandle, baseFile, reader, readSchema, externalSchemaTransformation);\n      } else {\n        readerIterator = reader.getRecordIterator(readSchema);\n      }\n\n      ThreadLocal<BinaryEncoder> encoderCache = new ThreadLocal<>();\n      ThreadLocal<BinaryDecoder> decoderCache = new ThreadLocal<>();\n      wrapper = new SparkBoundedInMemoryExecutor(table.getConfig(), readerIterator,\n          new UpdateHandler(mergeHandle), record -> {\n        if (!externalSchemaTransformation) {\n          return record;\n        }\n        return transformRecordBasedOnNewSchema(gReader, gWriter, encoderCache, decoderCache, (GenericRecord) record);\n      });\n      wrapper.execute();\n    } catch (Exception e) {\n      throw new HoodieException(e);\n    } finally {\n      if (reader != null) {\n        reader.close();\n      }\n      mergeHandle.close();\n      if (null != wrapper) {\n        wrapper.shutdownNow();\n      }\n    }\n  }\n","realPath":"hudi-client/hudi-spark-client/src/main/java/org/apache/hudi/table/action/commit/SparkMergeHelper.java","repoName":"hudi","snippetEndLine":0,"snippetStartLine":0,"startLine":58,"status":"NB"},{"authorDate":"2020-11-18 17:57:11","commitOrder":3,"curCode":"  public void runMerge(HoodieTable<T, List<HoodieRecord<T>>, List<HoodieKey>, List<WriteStatus>> table,\n                       HoodieMergeHandle<T, List<HoodieRecord<T>>, List<HoodieKey>, List<WriteStatus>> upsertHandle) throws IOException {\n    final boolean externalSchemaTransformation = table.getConfig().shouldUseExternalSchemaTransformation();\n    Configuration cfgForHoodieFile = new Configuration(table.getHadoopConf());\n    HoodieMergeHandle<T, List<HoodieRecord<T>>, List<HoodieKey>, List<WriteStatus>> mergeHandle = upsertHandle;\n    HoodieBaseFile baseFile = mergeHandle.baseFileForMerge();\n\n    final GenericDatumWriter<GenericRecord> gWriter;\n    final GenericDatumReader<GenericRecord> gReader;\n    Schema readSchema;\n    if (externalSchemaTransformation || baseFile.getBootstrapBaseFile().isPresent()) {\n      readSchema = HoodieFileReaderFactory.getFileReader(table.getHadoopConf(), mergeHandle.getOldFilePath()).getSchema();\n      gWriter = new GenericDatumWriter<>(readSchema);\n      gReader = new GenericDatumReader<>(readSchema, mergeHandle.getWriterSchemaWithMetafields());\n    } else {\n      gReader = null;\n      gWriter = null;\n      readSchema = mergeHandle.getWriterSchemaWithMetafields();\n    }\n\n    BoundedInMemoryExecutor<GenericRecord, GenericRecord, Void> wrapper = null;\n    HoodieFileReader<GenericRecord> reader = HoodieFileReaderFactory.<GenericRecord>getFileReader(cfgForHoodieFile, mergeHandle.getOldFilePath());\n    try {\n      final Iterator<GenericRecord> readerIterator;\n      if (baseFile.getBootstrapBaseFile().isPresent()) {\n        readerIterator = getMergingIterator(table, mergeHandle, baseFile, reader, readSchema, externalSchemaTransformation);\n      } else {\n        readerIterator = reader.getRecordIterator(readSchema);\n      }\n\n      ThreadLocal<BinaryEncoder> encoderCache = new ThreadLocal<>();\n      ThreadLocal<BinaryDecoder> decoderCache = new ThreadLocal<>();\n      wrapper = new BoundedInMemoryExecutor(table.getConfig().getWriteBufferLimitBytes(), new IteratorBasedQueueProducer<>(readerIterator),\n          Option.of(new UpdateHandler(mergeHandle)), record -> {\n        if (!externalSchemaTransformation) {\n          return record;\n        }\n        return transformRecordBasedOnNewSchema(gReader, gWriter, encoderCache, decoderCache, (GenericRecord) record);\n      });\n      wrapper.execute();\n    } catch (Exception e) {\n      throw new HoodieException(e);\n    } finally {\n      if (reader != null) {\n        reader.close();\n      }\n      mergeHandle.close();\n      if (null != wrapper) {\n        wrapper.shutdownNow();\n      }\n    }\n  }\n","date":"2020-11-18 17:57:11","endLine":114,"groupId":"470","id":2,"instanceNumber":2,"isCurCommit":0,"methodName":"runMerge","params":"(HoodieTable<T@List<HoodieRecord<T>>@List<HoodieKey>@List<WriteStatus>>table@HoodieMergeHandle<T@List<HoodieRecord<T>>@List<HoodieKey>@List<WriteStatus>>upsertHandle)","path":"/mnt/clonedata/CloneManagementServer/ManagementServer/consistResult/result-hudi-10-0.7/blobInfo/CC_OUT/blobs/d3/4aca22f049c6480e3c75cb6a0afa4899ce2f7a.src","preCode":"  public void runMerge(HoodieTable<T, List<HoodieRecord<T>>, List<HoodieKey>, List<WriteStatus>> table,\n                       HoodieMergeHandle<T, List<HoodieRecord<T>>, List<HoodieKey>, List<WriteStatus>> upsertHandle) throws IOException {\n    final boolean externalSchemaTransformation = table.getConfig().shouldUseExternalSchemaTransformation();\n    Configuration cfgForHoodieFile = new Configuration(table.getHadoopConf());\n    HoodieMergeHandle<T, List<HoodieRecord<T>>, List<HoodieKey>, List<WriteStatus>> mergeHandle = upsertHandle;\n    HoodieBaseFile baseFile = mergeHandle.baseFileForMerge();\n\n    final GenericDatumWriter<GenericRecord> gWriter;\n    final GenericDatumReader<GenericRecord> gReader;\n    Schema readSchema;\n    if (externalSchemaTransformation || baseFile.getBootstrapBaseFile().isPresent()) {\n      readSchema = HoodieFileReaderFactory.getFileReader(table.getHadoopConf(), mergeHandle.getOldFilePath()).getSchema();\n      gWriter = new GenericDatumWriter<>(readSchema);\n      gReader = new GenericDatumReader<>(readSchema, mergeHandle.getWriterSchemaWithMetafields());\n    } else {\n      gReader = null;\n      gWriter = null;\n      readSchema = mergeHandle.getWriterSchemaWithMetafields();\n    }\n\n    BoundedInMemoryExecutor<GenericRecord, GenericRecord, Void> wrapper = null;\n    HoodieFileReader<GenericRecord> reader = HoodieFileReaderFactory.<GenericRecord>getFileReader(cfgForHoodieFile, mergeHandle.getOldFilePath());\n    try {\n      final Iterator<GenericRecord> readerIterator;\n      if (baseFile.getBootstrapBaseFile().isPresent()) {\n        readerIterator = getMergingIterator(table, mergeHandle, baseFile, reader, readSchema, externalSchemaTransformation);\n      } else {\n        readerIterator = reader.getRecordIterator(readSchema);\n      }\n\n      ThreadLocal<BinaryEncoder> encoderCache = new ThreadLocal<>();\n      ThreadLocal<BinaryDecoder> decoderCache = new ThreadLocal<>();\n      wrapper = new BoundedInMemoryExecutor(table.getConfig().getWriteBufferLimitBytes(), new IteratorBasedQueueProducer<>(readerIterator),\n          Option.of(new UpdateHandler(mergeHandle)), record -> {\n        if (!externalSchemaTransformation) {\n          return record;\n        }\n        return transformRecordBasedOnNewSchema(gReader, gWriter, encoderCache, decoderCache, (GenericRecord) record);\n      });\n      wrapper.execute();\n    } catch (Exception e) {\n      throw new HoodieException(e);\n    } finally {\n      if (reader != null) {\n        reader.close();\n      }\n      mergeHandle.close();\n      if (null != wrapper) {\n        wrapper.shutdownNow();\n      }\n    }\n  }\n","realPath":"hudi-client/hudi-flink-client/src/main/java/org/apache/hudi/table/action/commit/FlinkMergeHelper.java","repoName":"hudi","snippetEndLine":0,"snippetStartLine":0,"startLine":63,"status":"B"}],"commitId":"4d05680038752077ceaebef261b66a5afc761e10","commitMessage":"@@@[HUDI-1327] Introduce base implemetation of hudi-flink-client (#2176)\n\n","date":"2020-11-18 17:57:11","modifiedFileCount":"6","status":"M","submitter":"wangxianghu"},{"authorTime":"2021-02-17 15:24:50","codes":[{"authorDate":"2020-10-12 09:38:42","commitOrder":4,"curCode":"  public void runMerge(HoodieTable<T, JavaRDD<HoodieRecord<T>>, JavaRDD<HoodieKey>, JavaRDD<WriteStatus>> table,\n                       HoodieMergeHandle<T, JavaRDD<HoodieRecord<T>>, JavaRDD<HoodieKey>, JavaRDD<WriteStatus>> upsertHandle) throws IOException {\n    final boolean externalSchemaTransformation = table.getConfig().shouldUseExternalSchemaTransformation();\n    Configuration cfgForHoodieFile = new Configuration(table.getHadoopConf());\n    HoodieMergeHandle<T, JavaRDD<HoodieRecord<T>>, JavaRDD<HoodieKey>, JavaRDD<WriteStatus>> mergeHandle = upsertHandle;\n    HoodieBaseFile baseFile = mergeHandle.baseFileForMerge();\n\n    final GenericDatumWriter<GenericRecord> gWriter;\n    final GenericDatumReader<GenericRecord> gReader;\n    Schema readSchema;\n    if (externalSchemaTransformation || baseFile.getBootstrapBaseFile().isPresent()) {\n      readSchema = HoodieFileReaderFactory.getFileReader(table.getHadoopConf(), mergeHandle.getOldFilePath()).getSchema();\n      gWriter = new GenericDatumWriter<>(readSchema);\n      gReader = new GenericDatumReader<>(readSchema, mergeHandle.getWriterSchemaWithMetafields());\n    } else {\n      gReader = null;\n      gWriter = null;\n      readSchema = mergeHandle.getWriterSchemaWithMetafields();\n    }\n\n    BoundedInMemoryExecutor<GenericRecord, GenericRecord, Void> wrapper = null;\n    HoodieFileReader<GenericRecord> reader = HoodieFileReaderFactory.<GenericRecord>getFileReader(cfgForHoodieFile, mergeHandle.getOldFilePath());\n    try {\n      final Iterator<GenericRecord> readerIterator;\n      if (baseFile.getBootstrapBaseFile().isPresent()) {\n        readerIterator = getMergingIterator(table, mergeHandle, baseFile, reader, readSchema, externalSchemaTransformation);\n      } else {\n        readerIterator = reader.getRecordIterator(readSchema);\n      }\n\n      ThreadLocal<BinaryEncoder> encoderCache = new ThreadLocal<>();\n      ThreadLocal<BinaryDecoder> decoderCache = new ThreadLocal<>();\n      wrapper = new SparkBoundedInMemoryExecutor(table.getConfig(), readerIterator,\n          new UpdateHandler(mergeHandle), record -> {\n        if (!externalSchemaTransformation) {\n          return record;\n        }\n        return transformRecordBasedOnNewSchema(gReader, gWriter, encoderCache, decoderCache, (GenericRecord) record);\n      });\n      wrapper.execute();\n    } catch (Exception e) {\n      throw new HoodieException(e);\n    } finally {\n      if (reader != null) {\n        reader.close();\n      }\n      mergeHandle.close();\n      if (null != wrapper) {\n        wrapper.shutdownNow();\n      }\n    }\n  }\n","date":"2020-10-12 09:38:42","endLine":109,"groupId":"2650","id":3,"instanceNumber":1,"isCurCommit":0,"methodName":"runMerge","params":"(HoodieTable<T@JavaRDD<HoodieRecord<T>>@JavaRDD<HoodieKey>@JavaRDD<WriteStatus>>table@HoodieMergeHandle<T@JavaRDD<HoodieRecord<T>>@JavaRDD<HoodieKey>@JavaRDD<WriteStatus>>upsertHandle)","path":"/mnt/clonedata/CloneManagementServer/ManagementServer/consistResult/result-hudi-10-0.7/blobInfo/CC_OUT/blobs/2d/130e35d70fb8fd9102f5576c733fc563f9f0d8.src","preCode":"  public void runMerge(HoodieTable<T, JavaRDD<HoodieRecord<T>>, JavaRDD<HoodieKey>, JavaRDD<WriteStatus>> table,\n                       HoodieMergeHandle<T, JavaRDD<HoodieRecord<T>>, JavaRDD<HoodieKey>, JavaRDD<WriteStatus>> upsertHandle) throws IOException {\n    final boolean externalSchemaTransformation = table.getConfig().shouldUseExternalSchemaTransformation();\n    Configuration cfgForHoodieFile = new Configuration(table.getHadoopConf());\n    HoodieMergeHandle<T, JavaRDD<HoodieRecord<T>>, JavaRDD<HoodieKey>, JavaRDD<WriteStatus>> mergeHandle = upsertHandle;\n    HoodieBaseFile baseFile = mergeHandle.baseFileForMerge();\n\n    final GenericDatumWriter<GenericRecord> gWriter;\n    final GenericDatumReader<GenericRecord> gReader;\n    Schema readSchema;\n    if (externalSchemaTransformation || baseFile.getBootstrapBaseFile().isPresent()) {\n      readSchema = HoodieFileReaderFactory.getFileReader(table.getHadoopConf(), mergeHandle.getOldFilePath()).getSchema();\n      gWriter = new GenericDatumWriter<>(readSchema);\n      gReader = new GenericDatumReader<>(readSchema, mergeHandle.getWriterSchemaWithMetafields());\n    } else {\n      gReader = null;\n      gWriter = null;\n      readSchema = mergeHandle.getWriterSchemaWithMetafields();\n    }\n\n    BoundedInMemoryExecutor<GenericRecord, GenericRecord, Void> wrapper = null;\n    HoodieFileReader<GenericRecord> reader = HoodieFileReaderFactory.<GenericRecord>getFileReader(cfgForHoodieFile, mergeHandle.getOldFilePath());\n    try {\n      final Iterator<GenericRecord> readerIterator;\n      if (baseFile.getBootstrapBaseFile().isPresent()) {\n        readerIterator = getMergingIterator(table, mergeHandle, baseFile, reader, readSchema, externalSchemaTransformation);\n      } else {\n        readerIterator = reader.getRecordIterator(readSchema);\n      }\n\n      ThreadLocal<BinaryEncoder> encoderCache = new ThreadLocal<>();\n      ThreadLocal<BinaryDecoder> decoderCache = new ThreadLocal<>();\n      wrapper = new SparkBoundedInMemoryExecutor(table.getConfig(), readerIterator,\n          new UpdateHandler(mergeHandle), record -> {\n        if (!externalSchemaTransformation) {\n          return record;\n        }\n        return transformRecordBasedOnNewSchema(gReader, gWriter, encoderCache, decoderCache, (GenericRecord) record);\n      });\n      wrapper.execute();\n    } catch (Exception e) {\n      throw new HoodieException(e);\n    } finally {\n      if (reader != null) {\n        reader.close();\n      }\n      mergeHandle.close();\n      if (null != wrapper) {\n        wrapper.shutdownNow();\n      }\n    }\n  }\n","realPath":"hudi-client/hudi-spark-client/src/main/java/org/apache/hudi/table/action/commit/SparkMergeHelper.java","repoName":"hudi","snippetEndLine":0,"snippetStartLine":0,"startLine":58,"status":"N"},{"authorDate":"2021-02-17 15:24:50","commitOrder":4,"curCode":"  public void runMerge(HoodieTable<T, List<HoodieRecord<T>>, List<HoodieKey>, List<WriteStatus>> table,\n                       HoodieMergeHandle<T, List<HoodieRecord<T>>, List<HoodieKey>, List<WriteStatus>> upsertHandle) throws IOException {\n    final boolean externalSchemaTransformation = table.getConfig().shouldUseExternalSchemaTransformation();\n    Configuration cfgForHoodieFile = new Configuration(table.getHadoopConf());\n    FlinkMergeHandle<T, List<HoodieRecord<T>>, List<HoodieKey>, List<WriteStatus>> mergeHandle =\n        (FlinkMergeHandle<T, List<HoodieRecord<T>>, List<HoodieKey>, List<WriteStatus>>) upsertHandle;\n    HoodieBaseFile baseFile = mergeHandle.baseFileForMerge();\n\n    final GenericDatumWriter<GenericRecord> gWriter;\n    final GenericDatumReader<GenericRecord> gReader;\n    Schema readSchema;\n    if (mergeHandle.isNeedBootStrap()\n        && (externalSchemaTransformation || baseFile.getBootstrapBaseFile().isPresent())) {\n      readSchema = HoodieFileReaderFactory.getFileReader(table.getHadoopConf(), mergeHandle.getOldFilePath()).getSchema();\n      gWriter = new GenericDatumWriter<>(readSchema);\n      gReader = new GenericDatumReader<>(readSchema, mergeHandle.getWriterSchemaWithMetafields());\n    } else {\n      gReader = null;\n      gWriter = null;\n      readSchema = mergeHandle.getWriterSchemaWithMetafields();\n    }\n\n    BoundedInMemoryExecutor<GenericRecord, GenericRecord, Void> wrapper = null;\n    HoodieFileReader<GenericRecord> reader = HoodieFileReaderFactory.<GenericRecord>getFileReader(cfgForHoodieFile, mergeHandle.getOldFilePath());\n    try {\n      final Iterator<GenericRecord> readerIterator;\n      if (mergeHandle.isNeedBootStrap() && baseFile.getBootstrapBaseFile().isPresent()) {\n        readerIterator = getMergingIterator(table, mergeHandle, baseFile, reader, readSchema, externalSchemaTransformation);\n      } else {\n        readerIterator = reader.getRecordIterator(readSchema);\n      }\n\n      ThreadLocal<BinaryEncoder> encoderCache = new ThreadLocal<>();\n      ThreadLocal<BinaryDecoder> decoderCache = new ThreadLocal<>();\n      wrapper = new BoundedInMemoryExecutor(table.getConfig().getWriteBufferLimitBytes(), new IteratorBasedQueueProducer<>(readerIterator),\n          Option.of(new UpdateHandler(mergeHandle)), record -> {\n        if (!externalSchemaTransformation) {\n          return record;\n        }\n        return transformRecordBasedOnNewSchema(gReader, gWriter, encoderCache, decoderCache, (GenericRecord) record);\n      });\n      wrapper.execute();\n    } catch (Exception e) {\n      throw new HoodieException(e);\n    } finally {\n      if (reader != null) {\n        reader.close();\n      }\n      mergeHandle.close();\n      if (null != wrapper) {\n        wrapper.shutdownNow();\n      }\n    }\n  }\n","date":"2021-02-17 15:24:50","endLine":117,"groupId":"470","id":4,"instanceNumber":2,"isCurCommit":0,"methodName":"runMerge","params":"(HoodieTable<T@List<HoodieRecord<T>>@List<HoodieKey>@List<WriteStatus>>table@HoodieMergeHandle<T@List<HoodieRecord<T>>@List<HoodieKey>@List<WriteStatus>>upsertHandle)","path":"/mnt/clonedata/CloneManagementServer/ManagementServer/consistResult/result-hudi-10-0.7/blobInfo/CC_OUT/blobs/53/9f551c92585757366f34090bb57d07ca470ed2.src","preCode":"  public void runMerge(HoodieTable<T, List<HoodieRecord<T>>, List<HoodieKey>, List<WriteStatus>> table,\n                       HoodieMergeHandle<T, List<HoodieRecord<T>>, List<HoodieKey>, List<WriteStatus>> upsertHandle) throws IOException {\n    final boolean externalSchemaTransformation = table.getConfig().shouldUseExternalSchemaTransformation();\n    Configuration cfgForHoodieFile = new Configuration(table.getHadoopConf());\n    HoodieMergeHandle<T, List<HoodieRecord<T>>, List<HoodieKey>, List<WriteStatus>> mergeHandle = upsertHandle;\n    HoodieBaseFile baseFile = mergeHandle.baseFileForMerge();\n\n    final GenericDatumWriter<GenericRecord> gWriter;\n    final GenericDatumReader<GenericRecord> gReader;\n    Schema readSchema;\n    if (externalSchemaTransformation || baseFile.getBootstrapBaseFile().isPresent()) {\n      readSchema = HoodieFileReaderFactory.getFileReader(table.getHadoopConf(), mergeHandle.getOldFilePath()).getSchema();\n      gWriter = new GenericDatumWriter<>(readSchema);\n      gReader = new GenericDatumReader<>(readSchema, mergeHandle.getWriterSchemaWithMetafields());\n    } else {\n      gReader = null;\n      gWriter = null;\n      readSchema = mergeHandle.getWriterSchemaWithMetafields();\n    }\n\n    BoundedInMemoryExecutor<GenericRecord, GenericRecord, Void> wrapper = null;\n    HoodieFileReader<GenericRecord> reader = HoodieFileReaderFactory.<GenericRecord>getFileReader(cfgForHoodieFile, mergeHandle.getOldFilePath());\n    try {\n      final Iterator<GenericRecord> readerIterator;\n      if (baseFile.getBootstrapBaseFile().isPresent()) {\n        readerIterator = getMergingIterator(table, mergeHandle, baseFile, reader, readSchema, externalSchemaTransformation);\n      } else {\n        readerIterator = reader.getRecordIterator(readSchema);\n      }\n\n      ThreadLocal<BinaryEncoder> encoderCache = new ThreadLocal<>();\n      ThreadLocal<BinaryDecoder> decoderCache = new ThreadLocal<>();\n      wrapper = new BoundedInMemoryExecutor(table.getConfig().getWriteBufferLimitBytes(), new IteratorBasedQueueProducer<>(readerIterator),\n          Option.of(new UpdateHandler(mergeHandle)), record -> {\n        if (!externalSchemaTransformation) {\n          return record;\n        }\n        return transformRecordBasedOnNewSchema(gReader, gWriter, encoderCache, decoderCache, (GenericRecord) record);\n      });\n      wrapper.execute();\n    } catch (Exception e) {\n      throw new HoodieException(e);\n    } finally {\n      if (reader != null) {\n        reader.close();\n      }\n      mergeHandle.close();\n      if (null != wrapper) {\n        wrapper.shutdownNow();\n      }\n    }\n  }\n","realPath":"hudi-client/hudi-flink-client/src/main/java/org/apache/hudi/table/action/commit/FlinkMergeHelper.java","repoName":"hudi","snippetEndLine":0,"snippetStartLine":0,"startLine":64,"status":"M"}],"commitId":"5d2491d10c70e4e5fc9b7aeb62cc64bcaaf6043f","commitMessage":"@@@[HUDI-1598] Write as minor batches during one checkpoint interval for the new writer (#2553)\n\n","date":"2021-02-17 15:24:50","modifiedFileCount":"22","status":"M","submitter":"Danny Chan"},{"authorTime":"2021-03-01 12:29:41","codes":[{"authorDate":"2020-10-12 09:38:42","commitOrder":5,"curCode":"  public void runMerge(HoodieTable<T, JavaRDD<HoodieRecord<T>>, JavaRDD<HoodieKey>, JavaRDD<WriteStatus>> table,\n                       HoodieMergeHandle<T, JavaRDD<HoodieRecord<T>>, JavaRDD<HoodieKey>, JavaRDD<WriteStatus>> upsertHandle) throws IOException {\n    final boolean externalSchemaTransformation = table.getConfig().shouldUseExternalSchemaTransformation();\n    Configuration cfgForHoodieFile = new Configuration(table.getHadoopConf());\n    HoodieMergeHandle<T, JavaRDD<HoodieRecord<T>>, JavaRDD<HoodieKey>, JavaRDD<WriteStatus>> mergeHandle = upsertHandle;\n    HoodieBaseFile baseFile = mergeHandle.baseFileForMerge();\n\n    final GenericDatumWriter<GenericRecord> gWriter;\n    final GenericDatumReader<GenericRecord> gReader;\n    Schema readSchema;\n    if (externalSchemaTransformation || baseFile.getBootstrapBaseFile().isPresent()) {\n      readSchema = HoodieFileReaderFactory.getFileReader(table.getHadoopConf(), mergeHandle.getOldFilePath()).getSchema();\n      gWriter = new GenericDatumWriter<>(readSchema);\n      gReader = new GenericDatumReader<>(readSchema, mergeHandle.getWriterSchemaWithMetafields());\n    } else {\n      gReader = null;\n      gWriter = null;\n      readSchema = mergeHandle.getWriterSchemaWithMetafields();\n    }\n\n    BoundedInMemoryExecutor<GenericRecord, GenericRecord, Void> wrapper = null;\n    HoodieFileReader<GenericRecord> reader = HoodieFileReaderFactory.<GenericRecord>getFileReader(cfgForHoodieFile, mergeHandle.getOldFilePath());\n    try {\n      final Iterator<GenericRecord> readerIterator;\n      if (baseFile.getBootstrapBaseFile().isPresent()) {\n        readerIterator = getMergingIterator(table, mergeHandle, baseFile, reader, readSchema, externalSchemaTransformation);\n      } else {\n        readerIterator = reader.getRecordIterator(readSchema);\n      }\n\n      ThreadLocal<BinaryEncoder> encoderCache = new ThreadLocal<>();\n      ThreadLocal<BinaryDecoder> decoderCache = new ThreadLocal<>();\n      wrapper = new SparkBoundedInMemoryExecutor(table.getConfig(), readerIterator,\n          new UpdateHandler(mergeHandle), record -> {\n        if (!externalSchemaTransformation) {\n          return record;\n        }\n        return transformRecordBasedOnNewSchema(gReader, gWriter, encoderCache, decoderCache, (GenericRecord) record);\n      });\n      wrapper.execute();\n    } catch (Exception e) {\n      throw new HoodieException(e);\n    } finally {\n      if (reader != null) {\n        reader.close();\n      }\n      mergeHandle.close();\n      if (null != wrapper) {\n        wrapper.shutdownNow();\n      }\n    }\n  }\n","date":"2020-10-12 09:38:42","endLine":109,"groupId":"2650","id":5,"instanceNumber":1,"isCurCommit":0,"methodName":"runMerge","params":"(HoodieTable<T@JavaRDD<HoodieRecord<T>>@JavaRDD<HoodieKey>@JavaRDD<WriteStatus>>table@HoodieMergeHandle<T@JavaRDD<HoodieRecord<T>>@JavaRDD<HoodieKey>@JavaRDD<WriteStatus>>upsertHandle)","path":"/mnt/clonedata/CloneManagementServer/ManagementServer/consistResult/result-hudi-10-0.7/blobInfo/CC_OUT/blobs/2d/130e35d70fb8fd9102f5576c733fc563f9f0d8.src","preCode":"  public void runMerge(HoodieTable<T, JavaRDD<HoodieRecord<T>>, JavaRDD<HoodieKey>, JavaRDD<WriteStatus>> table,\n                       HoodieMergeHandle<T, JavaRDD<HoodieRecord<T>>, JavaRDD<HoodieKey>, JavaRDD<WriteStatus>> upsertHandle) throws IOException {\n    final boolean externalSchemaTransformation = table.getConfig().shouldUseExternalSchemaTransformation();\n    Configuration cfgForHoodieFile = new Configuration(table.getHadoopConf());\n    HoodieMergeHandle<T, JavaRDD<HoodieRecord<T>>, JavaRDD<HoodieKey>, JavaRDD<WriteStatus>> mergeHandle = upsertHandle;\n    HoodieBaseFile baseFile = mergeHandle.baseFileForMerge();\n\n    final GenericDatumWriter<GenericRecord> gWriter;\n    final GenericDatumReader<GenericRecord> gReader;\n    Schema readSchema;\n    if (externalSchemaTransformation || baseFile.getBootstrapBaseFile().isPresent()) {\n      readSchema = HoodieFileReaderFactory.getFileReader(table.getHadoopConf(), mergeHandle.getOldFilePath()).getSchema();\n      gWriter = new GenericDatumWriter<>(readSchema);\n      gReader = new GenericDatumReader<>(readSchema, mergeHandle.getWriterSchemaWithMetafields());\n    } else {\n      gReader = null;\n      gWriter = null;\n      readSchema = mergeHandle.getWriterSchemaWithMetafields();\n    }\n\n    BoundedInMemoryExecutor<GenericRecord, GenericRecord, Void> wrapper = null;\n    HoodieFileReader<GenericRecord> reader = HoodieFileReaderFactory.<GenericRecord>getFileReader(cfgForHoodieFile, mergeHandle.getOldFilePath());\n    try {\n      final Iterator<GenericRecord> readerIterator;\n      if (baseFile.getBootstrapBaseFile().isPresent()) {\n        readerIterator = getMergingIterator(table, mergeHandle, baseFile, reader, readSchema, externalSchemaTransformation);\n      } else {\n        readerIterator = reader.getRecordIterator(readSchema);\n      }\n\n      ThreadLocal<BinaryEncoder> encoderCache = new ThreadLocal<>();\n      ThreadLocal<BinaryDecoder> decoderCache = new ThreadLocal<>();\n      wrapper = new SparkBoundedInMemoryExecutor(table.getConfig(), readerIterator,\n          new UpdateHandler(mergeHandle), record -> {\n        if (!externalSchemaTransformation) {\n          return record;\n        }\n        return transformRecordBasedOnNewSchema(gReader, gWriter, encoderCache, decoderCache, (GenericRecord) record);\n      });\n      wrapper.execute();\n    } catch (Exception e) {\n      throw new HoodieException(e);\n    } finally {\n      if (reader != null) {\n        reader.close();\n      }\n      mergeHandle.close();\n      if (null != wrapper) {\n        wrapper.shutdownNow();\n      }\n    }\n  }\n","realPath":"hudi-client/hudi-spark-client/src/main/java/org/apache/hudi/table/action/commit/SparkMergeHelper.java","repoName":"hudi","snippetEndLine":0,"snippetStartLine":0,"startLine":58,"status":"N"},{"authorDate":"2021-03-01 12:29:41","commitOrder":5,"curCode":"  public void runMerge(HoodieTable<T, List<HoodieRecord<T>>, List<HoodieKey>, List<WriteStatus>> table,\n                       HoodieMergeHandle<T, List<HoodieRecord<T>>, List<HoodieKey>, List<WriteStatus>> upsertHandle) throws IOException {\n    final boolean externalSchemaTransformation = table.getConfig().shouldUseExternalSchemaTransformation();\n    Configuration cfgForHoodieFile = new Configuration(table.getHadoopConf());\n    HoodieMergeHandle<T, List<HoodieRecord<T>>, List<HoodieKey>, List<WriteStatus>> mergeHandle = upsertHandle;\n    HoodieBaseFile baseFile = mergeHandle.baseFileForMerge();\n\n    final GenericDatumWriter<GenericRecord> gWriter;\n    final GenericDatumReader<GenericRecord> gReader;\n    Schema readSchema;\n    if (isNeedBootStrap(mergeHandle)\n        && (externalSchemaTransformation || baseFile.getBootstrapBaseFile().isPresent())) {\n      readSchema = HoodieFileReaderFactory.getFileReader(table.getHadoopConf(), mergeHandle.getOldFilePath()).getSchema();\n      gWriter = new GenericDatumWriter<>(readSchema);\n      gReader = new GenericDatumReader<>(readSchema, mergeHandle.getWriterSchemaWithMetafields());\n    } else {\n      gReader = null;\n      gWriter = null;\n      readSchema = mergeHandle.getWriterSchemaWithMetafields();\n    }\n\n    BoundedInMemoryExecutor<GenericRecord, GenericRecord, Void> wrapper = null;\n    HoodieFileReader<GenericRecord> reader = HoodieFileReaderFactory.<GenericRecord>getFileReader(cfgForHoodieFile, mergeHandle.getOldFilePath());\n    try {\n      final Iterator<GenericRecord> readerIterator;\n      if (isNeedBootStrap(mergeHandle) && baseFile.getBootstrapBaseFile().isPresent()) {\n        readerIterator = getMergingIterator(table, mergeHandle, baseFile, reader, readSchema, externalSchemaTransformation);\n      } else {\n        readerIterator = reader.getRecordIterator(readSchema);\n      }\n\n      ThreadLocal<BinaryEncoder> encoderCache = new ThreadLocal<>();\n      ThreadLocal<BinaryDecoder> decoderCache = new ThreadLocal<>();\n      wrapper = new BoundedInMemoryExecutor(table.getConfig().getWriteBufferLimitBytes(), new IteratorBasedQueueProducer<>(readerIterator),\n          Option.of(new UpdateHandler(mergeHandle)), record -> {\n        if (!externalSchemaTransformation) {\n          return record;\n        }\n        return transformRecordBasedOnNewSchema(gReader, gWriter, encoderCache, decoderCache, (GenericRecord) record);\n      });\n      wrapper.execute();\n    } catch (Exception e) {\n      throw new HoodieException(e);\n    } finally {\n      if (reader != null) {\n        reader.close();\n      }\n      mergeHandle.close();\n      if (null != wrapper) {\n        wrapper.shutdownNow();\n      }\n    }\n  }\n","date":"2021-03-01 12:29:41","endLine":116,"groupId":"470","id":6,"instanceNumber":2,"isCurCommit":0,"methodName":"runMerge","params":"(HoodieTable<T@List<HoodieRecord<T>>@List<HoodieKey>@List<WriteStatus>>table@HoodieMergeHandle<T@List<HoodieRecord<T>>@List<HoodieKey>@List<WriteStatus>>upsertHandle)","path":"/mnt/clonedata/CloneManagementServer/ManagementServer/consistResult/result-hudi-10-0.7/blobInfo/CC_OUT/blobs/9f/9f865d51ae1dccb7886b3717e3858f74eb75f4.src","preCode":"  public void runMerge(HoodieTable<T, List<HoodieRecord<T>>, List<HoodieKey>, List<WriteStatus>> table,\n                       HoodieMergeHandle<T, List<HoodieRecord<T>>, List<HoodieKey>, List<WriteStatus>> upsertHandle) throws IOException {\n    final boolean externalSchemaTransformation = table.getConfig().shouldUseExternalSchemaTransformation();\n    Configuration cfgForHoodieFile = new Configuration(table.getHadoopConf());\n    FlinkMergeHandle<T, List<HoodieRecord<T>>, List<HoodieKey>, List<WriteStatus>> mergeHandle =\n        (FlinkMergeHandle<T, List<HoodieRecord<T>>, List<HoodieKey>, List<WriteStatus>>) upsertHandle;\n    HoodieBaseFile baseFile = mergeHandle.baseFileForMerge();\n\n    final GenericDatumWriter<GenericRecord> gWriter;\n    final GenericDatumReader<GenericRecord> gReader;\n    Schema readSchema;\n    if (mergeHandle.isNeedBootStrap()\n        && (externalSchemaTransformation || baseFile.getBootstrapBaseFile().isPresent())) {\n      readSchema = HoodieFileReaderFactory.getFileReader(table.getHadoopConf(), mergeHandle.getOldFilePath()).getSchema();\n      gWriter = new GenericDatumWriter<>(readSchema);\n      gReader = new GenericDatumReader<>(readSchema, mergeHandle.getWriterSchemaWithMetafields());\n    } else {\n      gReader = null;\n      gWriter = null;\n      readSchema = mergeHandle.getWriterSchemaWithMetafields();\n    }\n\n    BoundedInMemoryExecutor<GenericRecord, GenericRecord, Void> wrapper = null;\n    HoodieFileReader<GenericRecord> reader = HoodieFileReaderFactory.<GenericRecord>getFileReader(cfgForHoodieFile, mergeHandle.getOldFilePath());\n    try {\n      final Iterator<GenericRecord> readerIterator;\n      if (mergeHandle.isNeedBootStrap() && baseFile.getBootstrapBaseFile().isPresent()) {\n        readerIterator = getMergingIterator(table, mergeHandle, baseFile, reader, readSchema, externalSchemaTransformation);\n      } else {\n        readerIterator = reader.getRecordIterator(readSchema);\n      }\n\n      ThreadLocal<BinaryEncoder> encoderCache = new ThreadLocal<>();\n      ThreadLocal<BinaryDecoder> decoderCache = new ThreadLocal<>();\n      wrapper = new BoundedInMemoryExecutor(table.getConfig().getWriteBufferLimitBytes(), new IteratorBasedQueueProducer<>(readerIterator),\n          Option.of(new UpdateHandler(mergeHandle)), record -> {\n        if (!externalSchemaTransformation) {\n          return record;\n        }\n        return transformRecordBasedOnNewSchema(gReader, gWriter, encoderCache, decoderCache, (GenericRecord) record);\n      });\n      wrapper.execute();\n    } catch (Exception e) {\n      throw new HoodieException(e);\n    } finally {\n      if (reader != null) {\n        reader.close();\n      }\n      mergeHandle.close();\n      if (null != wrapper) {\n        wrapper.shutdownNow();\n      }\n    }\n  }\n","realPath":"hudi-client/hudi-flink-client/src/main/java/org/apache/hudi/table/action/commit/FlinkMergeHelper.java","repoName":"hudi","snippetEndLine":0,"snippetStartLine":0,"startLine":64,"status":"M"}],"commitId":"7a11de12764d8f68f296c6e68a22822318bfbefa","commitMessage":"@@@[HUDI-1632] Supports merge on read write mode for Flink writer (#2593)\n\nAlso supports async compaction with pluggable strategies.","date":"2021-03-01 12:29:41","modifiedFileCount":"20","status":"M","submitter":"Danny Chan"},{"authorTime":"2021-04-16 11:40:53","codes":[{"authorDate":"2020-10-12 09:38:42","commitOrder":6,"curCode":"  public void runMerge(HoodieTable<T, JavaRDD<HoodieRecord<T>>, JavaRDD<HoodieKey>, JavaRDD<WriteStatus>> table,\n                       HoodieMergeHandle<T, JavaRDD<HoodieRecord<T>>, JavaRDD<HoodieKey>, JavaRDD<WriteStatus>> upsertHandle) throws IOException {\n    final boolean externalSchemaTransformation = table.getConfig().shouldUseExternalSchemaTransformation();\n    Configuration cfgForHoodieFile = new Configuration(table.getHadoopConf());\n    HoodieMergeHandle<T, JavaRDD<HoodieRecord<T>>, JavaRDD<HoodieKey>, JavaRDD<WriteStatus>> mergeHandle = upsertHandle;\n    HoodieBaseFile baseFile = mergeHandle.baseFileForMerge();\n\n    final GenericDatumWriter<GenericRecord> gWriter;\n    final GenericDatumReader<GenericRecord> gReader;\n    Schema readSchema;\n    if (externalSchemaTransformation || baseFile.getBootstrapBaseFile().isPresent()) {\n      readSchema = HoodieFileReaderFactory.getFileReader(table.getHadoopConf(), mergeHandle.getOldFilePath()).getSchema();\n      gWriter = new GenericDatumWriter<>(readSchema);\n      gReader = new GenericDatumReader<>(readSchema, mergeHandle.getWriterSchemaWithMetafields());\n    } else {\n      gReader = null;\n      gWriter = null;\n      readSchema = mergeHandle.getWriterSchemaWithMetafields();\n    }\n\n    BoundedInMemoryExecutor<GenericRecord, GenericRecord, Void> wrapper = null;\n    HoodieFileReader<GenericRecord> reader = HoodieFileReaderFactory.<GenericRecord>getFileReader(cfgForHoodieFile, mergeHandle.getOldFilePath());\n    try {\n      final Iterator<GenericRecord> readerIterator;\n      if (baseFile.getBootstrapBaseFile().isPresent()) {\n        readerIterator = getMergingIterator(table, mergeHandle, baseFile, reader, readSchema, externalSchemaTransformation);\n      } else {\n        readerIterator = reader.getRecordIterator(readSchema);\n      }\n\n      ThreadLocal<BinaryEncoder> encoderCache = new ThreadLocal<>();\n      ThreadLocal<BinaryDecoder> decoderCache = new ThreadLocal<>();\n      wrapper = new SparkBoundedInMemoryExecutor(table.getConfig(), readerIterator,\n          new UpdateHandler(mergeHandle), record -> {\n        if (!externalSchemaTransformation) {\n          return record;\n        }\n        return transformRecordBasedOnNewSchema(gReader, gWriter, encoderCache, decoderCache, (GenericRecord) record);\n      });\n      wrapper.execute();\n    } catch (Exception e) {\n      throw new HoodieException(e);\n    } finally {\n      if (reader != null) {\n        reader.close();\n      }\n      mergeHandle.close();\n      if (null != wrapper) {\n        wrapper.shutdownNow();\n      }\n    }\n  }\n","date":"2020-10-12 09:38:42","endLine":109,"groupId":"2650","id":7,"instanceNumber":1,"isCurCommit":0,"methodName":"runMerge","params":"(HoodieTable<T@JavaRDD<HoodieRecord<T>>@JavaRDD<HoodieKey>@JavaRDD<WriteStatus>>table@HoodieMergeHandle<T@JavaRDD<HoodieRecord<T>>@JavaRDD<HoodieKey>@JavaRDD<WriteStatus>>upsertHandle)","path":"/mnt/clonedata/CloneManagementServer/ManagementServer/consistResult/result-hudi-10-0.7/blobInfo/CC_OUT/blobs/2d/130e35d70fb8fd9102f5576c733fc563f9f0d8.src","preCode":"  public void runMerge(HoodieTable<T, JavaRDD<HoodieRecord<T>>, JavaRDD<HoodieKey>, JavaRDD<WriteStatus>> table,\n                       HoodieMergeHandle<T, JavaRDD<HoodieRecord<T>>, JavaRDD<HoodieKey>, JavaRDD<WriteStatus>> upsertHandle) throws IOException {\n    final boolean externalSchemaTransformation = table.getConfig().shouldUseExternalSchemaTransformation();\n    Configuration cfgForHoodieFile = new Configuration(table.getHadoopConf());\n    HoodieMergeHandle<T, JavaRDD<HoodieRecord<T>>, JavaRDD<HoodieKey>, JavaRDD<WriteStatus>> mergeHandle = upsertHandle;\n    HoodieBaseFile baseFile = mergeHandle.baseFileForMerge();\n\n    final GenericDatumWriter<GenericRecord> gWriter;\n    final GenericDatumReader<GenericRecord> gReader;\n    Schema readSchema;\n    if (externalSchemaTransformation || baseFile.getBootstrapBaseFile().isPresent()) {\n      readSchema = HoodieFileReaderFactory.getFileReader(table.getHadoopConf(), mergeHandle.getOldFilePath()).getSchema();\n      gWriter = new GenericDatumWriter<>(readSchema);\n      gReader = new GenericDatumReader<>(readSchema, mergeHandle.getWriterSchemaWithMetafields());\n    } else {\n      gReader = null;\n      gWriter = null;\n      readSchema = mergeHandle.getWriterSchemaWithMetafields();\n    }\n\n    BoundedInMemoryExecutor<GenericRecord, GenericRecord, Void> wrapper = null;\n    HoodieFileReader<GenericRecord> reader = HoodieFileReaderFactory.<GenericRecord>getFileReader(cfgForHoodieFile, mergeHandle.getOldFilePath());\n    try {\n      final Iterator<GenericRecord> readerIterator;\n      if (baseFile.getBootstrapBaseFile().isPresent()) {\n        readerIterator = getMergingIterator(table, mergeHandle, baseFile, reader, readSchema, externalSchemaTransformation);\n      } else {\n        readerIterator = reader.getRecordIterator(readSchema);\n      }\n\n      ThreadLocal<BinaryEncoder> encoderCache = new ThreadLocal<>();\n      ThreadLocal<BinaryDecoder> decoderCache = new ThreadLocal<>();\n      wrapper = new SparkBoundedInMemoryExecutor(table.getConfig(), readerIterator,\n          new UpdateHandler(mergeHandle), record -> {\n        if (!externalSchemaTransformation) {\n          return record;\n        }\n        return transformRecordBasedOnNewSchema(gReader, gWriter, encoderCache, decoderCache, (GenericRecord) record);\n      });\n      wrapper.execute();\n    } catch (Exception e) {\n      throw new HoodieException(e);\n    } finally {\n      if (reader != null) {\n        reader.close();\n      }\n      mergeHandle.close();\n      if (null != wrapper) {\n        wrapper.shutdownNow();\n      }\n    }\n  }\n","realPath":"hudi-client/hudi-spark-client/src/main/java/org/apache/hudi/table/action/commit/SparkMergeHelper.java","repoName":"hudi","snippetEndLine":0,"snippetStartLine":0,"startLine":58,"status":"N"},{"authorDate":"2021-04-16 11:40:53","commitOrder":6,"curCode":"  public void runMerge(HoodieTable<T, List<HoodieRecord<T>>, List<HoodieKey>, List<WriteStatus>> table,\n                       HoodieMergeHandle<T, List<HoodieRecord<T>>, List<HoodieKey>, List<WriteStatus>> upsertHandle) throws IOException {\n    final boolean externalSchemaTransformation = table.getConfig().shouldUseExternalSchemaTransformation();\n    Configuration cfgForHoodieFile = new Configuration(table.getHadoopConf());\n    HoodieMergeHandle<T, List<HoodieRecord<T>>, List<HoodieKey>, List<WriteStatus>> mergeHandle = upsertHandle;\n    HoodieBaseFile baseFile = mergeHandle.baseFileForMerge();\n\n    final GenericDatumWriter<GenericRecord> gWriter;\n    final GenericDatumReader<GenericRecord> gReader;\n    Schema readSchema;\n    if (isFirstTimeMerge(mergeHandle)\n        && (externalSchemaTransformation || baseFile.getBootstrapBaseFile().isPresent())) {\n      readSchema = HoodieFileReaderFactory.getFileReader(table.getHadoopConf(), mergeHandle.getOldFilePath()).getSchema();\n      gWriter = new GenericDatumWriter<>(readSchema);\n      gReader = new GenericDatumReader<>(readSchema, mergeHandle.getWriterSchemaWithMetafields());\n    } else {\n      gReader = null;\n      gWriter = null;\n      readSchema = mergeHandle.getWriterSchemaWithMetafields();\n    }\n\n    BoundedInMemoryExecutor<GenericRecord, GenericRecord, Void> wrapper = null;\n    HoodieFileReader<GenericRecord> reader = HoodieFileReaderFactory.<GenericRecord>getFileReader(cfgForHoodieFile, mergeHandle.getOldFilePath());\n    try {\n      final Iterator<GenericRecord> readerIterator;\n      if (isFirstTimeMerge(mergeHandle) && baseFile.getBootstrapBaseFile().isPresent()) {\n        readerIterator = getMergingIterator(table, mergeHandle, baseFile, reader, readSchema, externalSchemaTransformation);\n      } else {\n        readerIterator = reader.getRecordIterator(readSchema);\n      }\n\n      ThreadLocal<BinaryEncoder> encoderCache = new ThreadLocal<>();\n      ThreadLocal<BinaryDecoder> decoderCache = new ThreadLocal<>();\n      wrapper = new BoundedInMemoryExecutor(table.getConfig().getWriteBufferLimitBytes(), new IteratorBasedQueueProducer<>(readerIterator),\n          Option.of(new UpdateHandler(mergeHandle)), record -> {\n        if (!externalSchemaTransformation) {\n          return record;\n        }\n        return transformRecordBasedOnNewSchema(gReader, gWriter, encoderCache, decoderCache, (GenericRecord) record);\n      });\n      wrapper.execute();\n    } catch (Exception e) {\n      throw new HoodieException(e);\n    } finally {\n      if (reader != null) {\n        reader.close();\n      }\n      mergeHandle.close();\n      if (null != wrapper) {\n        wrapper.shutdownNow();\n      }\n    }\n  }\n","date":"2021-04-16 11:40:53","endLine":116,"groupId":"470","id":8,"instanceNumber":2,"isCurCommit":0,"methodName":"runMerge","params":"(HoodieTable<T@List<HoodieRecord<T>>@List<HoodieKey>@List<WriteStatus>>table@HoodieMergeHandle<T@List<HoodieRecord<T>>@List<HoodieKey>@List<WriteStatus>>upsertHandle)","path":"/mnt/clonedata/CloneManagementServer/ManagementServer/consistResult/result-hudi-10-0.7/blobInfo/CC_OUT/blobs/5e/9d8cb19c688e003e7597866069f78131a59b6e.src","preCode":"  public void runMerge(HoodieTable<T, List<HoodieRecord<T>>, List<HoodieKey>, List<WriteStatus>> table,\n                       HoodieMergeHandle<T, List<HoodieRecord<T>>, List<HoodieKey>, List<WriteStatus>> upsertHandle) throws IOException {\n    final boolean externalSchemaTransformation = table.getConfig().shouldUseExternalSchemaTransformation();\n    Configuration cfgForHoodieFile = new Configuration(table.getHadoopConf());\n    HoodieMergeHandle<T, List<HoodieRecord<T>>, List<HoodieKey>, List<WriteStatus>> mergeHandle = upsertHandle;\n    HoodieBaseFile baseFile = mergeHandle.baseFileForMerge();\n\n    final GenericDatumWriter<GenericRecord> gWriter;\n    final GenericDatumReader<GenericRecord> gReader;\n    Schema readSchema;\n    if (isNeedBootStrap(mergeHandle)\n        && (externalSchemaTransformation || baseFile.getBootstrapBaseFile().isPresent())) {\n      readSchema = HoodieFileReaderFactory.getFileReader(table.getHadoopConf(), mergeHandle.getOldFilePath()).getSchema();\n      gWriter = new GenericDatumWriter<>(readSchema);\n      gReader = new GenericDatumReader<>(readSchema, mergeHandle.getWriterSchemaWithMetafields());\n    } else {\n      gReader = null;\n      gWriter = null;\n      readSchema = mergeHandle.getWriterSchemaWithMetafields();\n    }\n\n    BoundedInMemoryExecutor<GenericRecord, GenericRecord, Void> wrapper = null;\n    HoodieFileReader<GenericRecord> reader = HoodieFileReaderFactory.<GenericRecord>getFileReader(cfgForHoodieFile, mergeHandle.getOldFilePath());\n    try {\n      final Iterator<GenericRecord> readerIterator;\n      if (isNeedBootStrap(mergeHandle) && baseFile.getBootstrapBaseFile().isPresent()) {\n        readerIterator = getMergingIterator(table, mergeHandle, baseFile, reader, readSchema, externalSchemaTransformation);\n      } else {\n        readerIterator = reader.getRecordIterator(readSchema);\n      }\n\n      ThreadLocal<BinaryEncoder> encoderCache = new ThreadLocal<>();\n      ThreadLocal<BinaryDecoder> decoderCache = new ThreadLocal<>();\n      wrapper = new BoundedInMemoryExecutor(table.getConfig().getWriteBufferLimitBytes(), new IteratorBasedQueueProducer<>(readerIterator),\n          Option.of(new UpdateHandler(mergeHandle)), record -> {\n        if (!externalSchemaTransformation) {\n          return record;\n        }\n        return transformRecordBasedOnNewSchema(gReader, gWriter, encoderCache, decoderCache, (GenericRecord) record);\n      });\n      wrapper.execute();\n    } catch (Exception e) {\n      throw new HoodieException(e);\n    } finally {\n      if (reader != null) {\n        reader.close();\n      }\n      mergeHandle.close();\n      if (null != wrapper) {\n        wrapper.shutdownNow();\n      }\n    }\n  }\n","realPath":"hudi-client/hudi-flink-client/src/main/java/org/apache/hudi/table/action/commit/FlinkMergeHelper.java","repoName":"hudi","snippetEndLine":0,"snippetStartLine":0,"startLine":64,"status":"M"}],"commitId":"b6d949b48a649acac27d5d9b91677bf2e25e9342","commitMessage":"@@@[HUDI-1801] FlinkMergeHandle rolling over may miss to rename the latest file handle (#2831)\n\nThe FlinkMergeHandle may rename the N-1 th file handle instead of the\nlatest one.  thus to cause data duplication.","date":"2021-04-16 11:40:53","modifiedFileCount":"4","status":"M","submitter":"Danny Chan"},{"authorTime":"2021-05-14 10:25:18","codes":[{"authorDate":"2020-10-12 09:38:42","commitOrder":7,"curCode":"  public void runMerge(HoodieTable<T, JavaRDD<HoodieRecord<T>>, JavaRDD<HoodieKey>, JavaRDD<WriteStatus>> table,\n                       HoodieMergeHandle<T, JavaRDD<HoodieRecord<T>>, JavaRDD<HoodieKey>, JavaRDD<WriteStatus>> upsertHandle) throws IOException {\n    final boolean externalSchemaTransformation = table.getConfig().shouldUseExternalSchemaTransformation();\n    Configuration cfgForHoodieFile = new Configuration(table.getHadoopConf());\n    HoodieMergeHandle<T, JavaRDD<HoodieRecord<T>>, JavaRDD<HoodieKey>, JavaRDD<WriteStatus>> mergeHandle = upsertHandle;\n    HoodieBaseFile baseFile = mergeHandle.baseFileForMerge();\n\n    final GenericDatumWriter<GenericRecord> gWriter;\n    final GenericDatumReader<GenericRecord> gReader;\n    Schema readSchema;\n    if (externalSchemaTransformation || baseFile.getBootstrapBaseFile().isPresent()) {\n      readSchema = HoodieFileReaderFactory.getFileReader(table.getHadoopConf(), mergeHandle.getOldFilePath()).getSchema();\n      gWriter = new GenericDatumWriter<>(readSchema);\n      gReader = new GenericDatumReader<>(readSchema, mergeHandle.getWriterSchemaWithMetafields());\n    } else {\n      gReader = null;\n      gWriter = null;\n      readSchema = mergeHandle.getWriterSchemaWithMetafields();\n    }\n\n    BoundedInMemoryExecutor<GenericRecord, GenericRecord, Void> wrapper = null;\n    HoodieFileReader<GenericRecord> reader = HoodieFileReaderFactory.<GenericRecord>getFileReader(cfgForHoodieFile, mergeHandle.getOldFilePath());\n    try {\n      final Iterator<GenericRecord> readerIterator;\n      if (baseFile.getBootstrapBaseFile().isPresent()) {\n        readerIterator = getMergingIterator(table, mergeHandle, baseFile, reader, readSchema, externalSchemaTransformation);\n      } else {\n        readerIterator = reader.getRecordIterator(readSchema);\n      }\n\n      ThreadLocal<BinaryEncoder> encoderCache = new ThreadLocal<>();\n      ThreadLocal<BinaryDecoder> decoderCache = new ThreadLocal<>();\n      wrapper = new SparkBoundedInMemoryExecutor(table.getConfig(), readerIterator,\n          new UpdateHandler(mergeHandle), record -> {\n        if (!externalSchemaTransformation) {\n          return record;\n        }\n        return transformRecordBasedOnNewSchema(gReader, gWriter, encoderCache, decoderCache, (GenericRecord) record);\n      });\n      wrapper.execute();\n    } catch (Exception e) {\n      throw new HoodieException(e);\n    } finally {\n      if (reader != null) {\n        reader.close();\n      }\n      mergeHandle.close();\n      if (null != wrapper) {\n        wrapper.shutdownNow();\n      }\n    }\n  }\n","date":"2020-10-12 09:38:42","endLine":109,"groupId":"2650","id":9,"instanceNumber":1,"isCurCommit":0,"methodName":"runMerge","params":"(HoodieTable<T@JavaRDD<HoodieRecord<T>>@JavaRDD<HoodieKey>@JavaRDD<WriteStatus>>table@HoodieMergeHandle<T@JavaRDD<HoodieRecord<T>>@JavaRDD<HoodieKey>@JavaRDD<WriteStatus>>upsertHandle)","path":"/mnt/clonedata/CloneManagementServer/ManagementServer/consistResult/result-hudi-10-0.7/blobInfo/CC_OUT/blobs/2d/130e35d70fb8fd9102f5576c733fc563f9f0d8.src","preCode":"  public void runMerge(HoodieTable<T, JavaRDD<HoodieRecord<T>>, JavaRDD<HoodieKey>, JavaRDD<WriteStatus>> table,\n                       HoodieMergeHandle<T, JavaRDD<HoodieRecord<T>>, JavaRDD<HoodieKey>, JavaRDD<WriteStatus>> upsertHandle) throws IOException {\n    final boolean externalSchemaTransformation = table.getConfig().shouldUseExternalSchemaTransformation();\n    Configuration cfgForHoodieFile = new Configuration(table.getHadoopConf());\n    HoodieMergeHandle<T, JavaRDD<HoodieRecord<T>>, JavaRDD<HoodieKey>, JavaRDD<WriteStatus>> mergeHandle = upsertHandle;\n    HoodieBaseFile baseFile = mergeHandle.baseFileForMerge();\n\n    final GenericDatumWriter<GenericRecord> gWriter;\n    final GenericDatumReader<GenericRecord> gReader;\n    Schema readSchema;\n    if (externalSchemaTransformation || baseFile.getBootstrapBaseFile().isPresent()) {\n      readSchema = HoodieFileReaderFactory.getFileReader(table.getHadoopConf(), mergeHandle.getOldFilePath()).getSchema();\n      gWriter = new GenericDatumWriter<>(readSchema);\n      gReader = new GenericDatumReader<>(readSchema, mergeHandle.getWriterSchemaWithMetafields());\n    } else {\n      gReader = null;\n      gWriter = null;\n      readSchema = mergeHandle.getWriterSchemaWithMetafields();\n    }\n\n    BoundedInMemoryExecutor<GenericRecord, GenericRecord, Void> wrapper = null;\n    HoodieFileReader<GenericRecord> reader = HoodieFileReaderFactory.<GenericRecord>getFileReader(cfgForHoodieFile, mergeHandle.getOldFilePath());\n    try {\n      final Iterator<GenericRecord> readerIterator;\n      if (baseFile.getBootstrapBaseFile().isPresent()) {\n        readerIterator = getMergingIterator(table, mergeHandle, baseFile, reader, readSchema, externalSchemaTransformation);\n      } else {\n        readerIterator = reader.getRecordIterator(readSchema);\n      }\n\n      ThreadLocal<BinaryEncoder> encoderCache = new ThreadLocal<>();\n      ThreadLocal<BinaryDecoder> decoderCache = new ThreadLocal<>();\n      wrapper = new SparkBoundedInMemoryExecutor(table.getConfig(), readerIterator,\n          new UpdateHandler(mergeHandle), record -> {\n        if (!externalSchemaTransformation) {\n          return record;\n        }\n        return transformRecordBasedOnNewSchema(gReader, gWriter, encoderCache, decoderCache, (GenericRecord) record);\n      });\n      wrapper.execute();\n    } catch (Exception e) {\n      throw new HoodieException(e);\n    } finally {\n      if (reader != null) {\n        reader.close();\n      }\n      mergeHandle.close();\n      if (null != wrapper) {\n        wrapper.shutdownNow();\n      }\n    }\n  }\n","realPath":"hudi-client/hudi-spark-client/src/main/java/org/apache/hudi/table/action/commit/SparkMergeHelper.java","repoName":"hudi","snippetEndLine":0,"snippetStartLine":0,"startLine":58,"status":"N"},{"authorDate":"2021-05-14 10:25:18","commitOrder":7,"curCode":"  public void runMerge(HoodieTable<T, List<HoodieRecord<T>>, List<HoodieKey>, List<WriteStatus>> table,\n                       HoodieMergeHandle<T, List<HoodieRecord<T>>, List<HoodieKey>, List<WriteStatus>> mergeHandle) throws IOException {\n    final GenericDatumWriter<GenericRecord> gWriter;\n    final GenericDatumReader<GenericRecord> gReader;\n    Schema readSchema;\n\n    final boolean externalSchemaTransformation = table.getConfig().shouldUseExternalSchemaTransformation();\n    HoodieBaseFile baseFile = mergeHandle.baseFileForMerge();\n    if (externalSchemaTransformation || baseFile.getBootstrapBaseFile().isPresent()) {\n      readSchema = HoodieFileReaderFactory.getFileReader(table.getHadoopConf(), mergeHandle.getOldFilePath()).getSchema();\n      gWriter = new GenericDatumWriter<>(readSchema);\n      gReader = new GenericDatumReader<>(readSchema, mergeHandle.getWriterSchemaWithMetafields());\n    } else {\n      gReader = null;\n      gWriter = null;\n      readSchema = mergeHandle.getWriterSchemaWithMetafields();\n    }\n\n    BoundedInMemoryExecutor<GenericRecord, GenericRecord, Void> wrapper = null;\n    Configuration cfgForHoodieFile = new Configuration(table.getHadoopConf());\n    HoodieFileReader<GenericRecord> reader = HoodieFileReaderFactory.<GenericRecord>getFileReader(cfgForHoodieFile, mergeHandle.getOldFilePath());\n    try {\n      final Iterator<GenericRecord> readerIterator;\n      if (baseFile.getBootstrapBaseFile().isPresent()) {\n        readerIterator = getMergingIterator(table, mergeHandle, baseFile, reader, readSchema, externalSchemaTransformation);\n      } else {\n        readerIterator = reader.getRecordIterator(readSchema);\n      }\n\n      ThreadLocal<BinaryEncoder> encoderCache = new ThreadLocal<>();\n      ThreadLocal<BinaryDecoder> decoderCache = new ThreadLocal<>();\n      wrapper = new BoundedInMemoryExecutor(table.getConfig().getWriteBufferLimitBytes(), new IteratorBasedQueueProducer<>(readerIterator),\n          Option.of(new UpdateHandler(mergeHandle)), record -> {\n        if (!externalSchemaTransformation) {\n          return record;\n        }\n        return transformRecordBasedOnNewSchema(gReader, gWriter, encoderCache, decoderCache, (GenericRecord) record);\n      });\n      wrapper.execute();\n    } catch (Exception e) {\n      throw new HoodieException(e);\n    } finally {\n      if (reader != null) {\n        reader.close();\n      }\n      mergeHandle.close();\n      if (null != wrapper) {\n        wrapper.shutdownNow();\n      }\n    }\n  }\n","date":"2021-05-14 10:25:18","endLine":113,"groupId":"4381","id":10,"instanceNumber":2,"isCurCommit":0,"methodName":"runMerge","params":"(HoodieTable<T@List<HoodieRecord<T>>@List<HoodieKey>@List<WriteStatus>>table@HoodieMergeHandle<T@List<HoodieRecord<T>>@List<HoodieKey>@List<WriteStatus>>mergeHandle)","path":"/mnt/clonedata/CloneManagementServer/ManagementServer/consistResult/result-hudi-10-0.7/blobInfo/CC_OUT/blobs/63/91750fd70cfb3dfa9b7cf9090085f864cf5758.src","preCode":"  public void runMerge(HoodieTable<T, List<HoodieRecord<T>>, List<HoodieKey>, List<WriteStatus>> table,\n                       HoodieMergeHandle<T, List<HoodieRecord<T>>, List<HoodieKey>, List<WriteStatus>> upsertHandle) throws IOException {\n    final boolean externalSchemaTransformation = table.getConfig().shouldUseExternalSchemaTransformation();\n    Configuration cfgForHoodieFile = new Configuration(table.getHadoopConf());\n    HoodieMergeHandle<T, List<HoodieRecord<T>>, List<HoodieKey>, List<WriteStatus>> mergeHandle = upsertHandle;\n    HoodieBaseFile baseFile = mergeHandle.baseFileForMerge();\n\n    final GenericDatumWriter<GenericRecord> gWriter;\n    final GenericDatumReader<GenericRecord> gReader;\n    Schema readSchema;\n    if (isFirstTimeMerge(mergeHandle)\n        && (externalSchemaTransformation || baseFile.getBootstrapBaseFile().isPresent())) {\n      readSchema = HoodieFileReaderFactory.getFileReader(table.getHadoopConf(), mergeHandle.getOldFilePath()).getSchema();\n      gWriter = new GenericDatumWriter<>(readSchema);\n      gReader = new GenericDatumReader<>(readSchema, mergeHandle.getWriterSchemaWithMetafields());\n    } else {\n      gReader = null;\n      gWriter = null;\n      readSchema = mergeHandle.getWriterSchemaWithMetafields();\n    }\n\n    BoundedInMemoryExecutor<GenericRecord, GenericRecord, Void> wrapper = null;\n    HoodieFileReader<GenericRecord> reader = HoodieFileReaderFactory.<GenericRecord>getFileReader(cfgForHoodieFile, mergeHandle.getOldFilePath());\n    try {\n      final Iterator<GenericRecord> readerIterator;\n      if (isFirstTimeMerge(mergeHandle) && baseFile.getBootstrapBaseFile().isPresent()) {\n        readerIterator = getMergingIterator(table, mergeHandle, baseFile, reader, readSchema, externalSchemaTransformation);\n      } else {\n        readerIterator = reader.getRecordIterator(readSchema);\n      }\n\n      ThreadLocal<BinaryEncoder> encoderCache = new ThreadLocal<>();\n      ThreadLocal<BinaryDecoder> decoderCache = new ThreadLocal<>();\n      wrapper = new BoundedInMemoryExecutor(table.getConfig().getWriteBufferLimitBytes(), new IteratorBasedQueueProducer<>(readerIterator),\n          Option.of(new UpdateHandler(mergeHandle)), record -> {\n        if (!externalSchemaTransformation) {\n          return record;\n        }\n        return transformRecordBasedOnNewSchema(gReader, gWriter, encoderCache, decoderCache, (GenericRecord) record);\n      });\n      wrapper.execute();\n    } catch (Exception e) {\n      throw new HoodieException(e);\n    } finally {\n      if (reader != null) {\n        reader.close();\n      }\n      mergeHandle.close();\n      if (null != wrapper) {\n        wrapper.shutdownNow();\n      }\n    }\n  }\n","realPath":"hudi-client/hudi-flink-client/src/main/java/org/apache/hudi/table/action/commit/FlinkMergeHelper.java","repoName":"hudi","snippetEndLine":0,"snippetStartLine":0,"startLine":63,"status":"M"}],"commitId":"ad77cf42ba7d8eab086be1f6d91f465bf29164ed","commitMessage":"@@@[HUDI-1900] Always close the file handle for a flink mini-batch write (#2943)\n\nClose the file handle eagerly to avoid corrupted files as much as\npossible.","date":"2021-05-14 10:25:18","modifiedFileCount":"12","status":"M","submitter":"Danny Chan"},{"authorTime":"2021-06-08 14:24:32","codes":[{"authorDate":"2021-06-08 14:24:32","commitOrder":8,"curCode":"  public void runMerge(HoodieTable<T, JavaRDD<HoodieRecord<T>>, JavaRDD<HoodieKey>, JavaRDD<WriteStatus>> table,\n                       HoodieMergeHandle<T, JavaRDD<HoodieRecord<T>>, JavaRDD<HoodieKey>, JavaRDD<WriteStatus>> upsertHandle) throws IOException {\n    final boolean externalSchemaTransformation = table.getConfig().shouldUseExternalSchemaTransformation();\n    Configuration cfgForHoodieFile = new Configuration(table.getHadoopConf());\n    HoodieMergeHandle<T, JavaRDD<HoodieRecord<T>>, JavaRDD<HoodieKey>, JavaRDD<WriteStatus>> mergeHandle = upsertHandle;\n    HoodieBaseFile baseFile = mergeHandle.baseFileForMerge();\n\n    final GenericDatumWriter<GenericRecord> gWriter;\n    final GenericDatumReader<GenericRecord> gReader;\n    Schema readSchema;\n    if (externalSchemaTransformation || baseFile.getBootstrapBaseFile().isPresent()) {\n      readSchema = HoodieFileReaderFactory.getFileReader(table.getHadoopConf(), mergeHandle.getOldFilePath()).getSchema();\n      gWriter = new GenericDatumWriter<>(readSchema);\n      gReader = new GenericDatumReader<>(readSchema, mergeHandle.getWriterSchemaWithMetaFields());\n    } else {\n      gReader = null;\n      gWriter = null;\n      readSchema = mergeHandle.getWriterSchemaWithMetaFields();\n    }\n\n    BoundedInMemoryExecutor<GenericRecord, GenericRecord, Void> wrapper = null;\n    HoodieFileReader<GenericRecord> reader = HoodieFileReaderFactory.<GenericRecord>getFileReader(cfgForHoodieFile, mergeHandle.getOldFilePath());\n    try {\n      final Iterator<GenericRecord> readerIterator;\n      if (baseFile.getBootstrapBaseFile().isPresent()) {\n        readerIterator = getMergingIterator(table, mergeHandle, baseFile, reader, readSchema, externalSchemaTransformation);\n      } else {\n        readerIterator = reader.getRecordIterator(readSchema);\n      }\n\n      ThreadLocal<BinaryEncoder> encoderCache = new ThreadLocal<>();\n      ThreadLocal<BinaryDecoder> decoderCache = new ThreadLocal<>();\n      wrapper = new SparkBoundedInMemoryExecutor(table.getConfig(), readerIterator,\n          new UpdateHandler(mergeHandle), record -> {\n        if (!externalSchemaTransformation) {\n          return record;\n        }\n        return transformRecordBasedOnNewSchema(gReader, gWriter, encoderCache, decoderCache, (GenericRecord) record);\n      });\n      wrapper.execute();\n    } catch (Exception e) {\n      throw new HoodieException(e);\n    } finally {\n      if (reader != null) {\n        reader.close();\n      }\n      mergeHandle.close();\n      if (null != wrapper) {\n        wrapper.shutdownNow();\n      }\n    }\n  }\n","date":"2021-06-08 14:24:32","endLine":112,"groupId":"10486","id":11,"instanceNumber":1,"isCurCommit":1,"methodName":"runMerge","params":"(HoodieTable<T@JavaRDD<HoodieRecord<T>>@JavaRDD<HoodieKey>@JavaRDD<WriteStatus>>table@HoodieMergeHandle<T@JavaRDD<HoodieRecord<T>>@JavaRDD<HoodieKey>@JavaRDD<WriteStatus>>upsertHandle)","path":"/mnt/clonedata/CloneManagementServer/ManagementServer/consistResult/result-hudi-10-0.7/blobInfo/CC_OUT/blobs/5e/82dbd8c566da17bbb60fbfeb802ea290b4ecb1.src","preCode":"  public void runMerge(HoodieTable<T, JavaRDD<HoodieRecord<T>>, JavaRDD<HoodieKey>, JavaRDD<WriteStatus>> table,\n                       HoodieMergeHandle<T, JavaRDD<HoodieRecord<T>>, JavaRDD<HoodieKey>, JavaRDD<WriteStatus>> upsertHandle) throws IOException {\n    final boolean externalSchemaTransformation = table.getConfig().shouldUseExternalSchemaTransformation();\n    Configuration cfgForHoodieFile = new Configuration(table.getHadoopConf());\n    HoodieMergeHandle<T, JavaRDD<HoodieRecord<T>>, JavaRDD<HoodieKey>, JavaRDD<WriteStatus>> mergeHandle = upsertHandle;\n    HoodieBaseFile baseFile = mergeHandle.baseFileForMerge();\n\n    final GenericDatumWriter<GenericRecord> gWriter;\n    final GenericDatumReader<GenericRecord> gReader;\n    Schema readSchema;\n    if (externalSchemaTransformation || baseFile.getBootstrapBaseFile().isPresent()) {\n      readSchema = HoodieFileReaderFactory.getFileReader(table.getHadoopConf(), mergeHandle.getOldFilePath()).getSchema();\n      gWriter = new GenericDatumWriter<>(readSchema);\n      gReader = new GenericDatumReader<>(readSchema, mergeHandle.getWriterSchemaWithMetafields());\n    } else {\n      gReader = null;\n      gWriter = null;\n      readSchema = mergeHandle.getWriterSchemaWithMetafields();\n    }\n\n    BoundedInMemoryExecutor<GenericRecord, GenericRecord, Void> wrapper = null;\n    HoodieFileReader<GenericRecord> reader = HoodieFileReaderFactory.<GenericRecord>getFileReader(cfgForHoodieFile, mergeHandle.getOldFilePath());\n    try {\n      final Iterator<GenericRecord> readerIterator;\n      if (baseFile.getBootstrapBaseFile().isPresent()) {\n        readerIterator = getMergingIterator(table, mergeHandle, baseFile, reader, readSchema, externalSchemaTransformation);\n      } else {\n        readerIterator = reader.getRecordIterator(readSchema);\n      }\n\n      ThreadLocal<BinaryEncoder> encoderCache = new ThreadLocal<>();\n      ThreadLocal<BinaryDecoder> decoderCache = new ThreadLocal<>();\n      wrapper = new SparkBoundedInMemoryExecutor(table.getConfig(), readerIterator,\n          new UpdateHandler(mergeHandle), record -> {\n        if (!externalSchemaTransformation) {\n          return record;\n        }\n        return transformRecordBasedOnNewSchema(gReader, gWriter, encoderCache, decoderCache, (GenericRecord) record);\n      });\n      wrapper.execute();\n    } catch (Exception e) {\n      throw new HoodieException(e);\n    } finally {\n      if (reader != null) {\n        reader.close();\n      }\n      mergeHandle.close();\n      if (null != wrapper) {\n        wrapper.shutdownNow();\n      }\n    }\n  }\n","realPath":"hudi-client/hudi-spark-client/src/main/java/org/apache/hudi/table/action/commit/SparkMergeHelper.java","repoName":"hudi","snippetEndLine":0,"snippetStartLine":0,"startLine":61,"status":"M"},{"authorDate":"2021-06-08 14:24:32","commitOrder":8,"curCode":"  public void runMerge(HoodieTable<T, List<HoodieRecord<T>>, List<HoodieKey>, List<WriteStatus>> table,\n                       HoodieMergeHandle<T, List<HoodieRecord<T>>, List<HoodieKey>, List<WriteStatus>> mergeHandle) throws IOException {\n    final GenericDatumWriter<GenericRecord> gWriter;\n    final GenericDatumReader<GenericRecord> gReader;\n    Schema readSchema;\n\n    final boolean externalSchemaTransformation = table.getConfig().shouldUseExternalSchemaTransformation();\n    HoodieBaseFile baseFile = mergeHandle.baseFileForMerge();\n    if (externalSchemaTransformation || baseFile.getBootstrapBaseFile().isPresent()) {\n      readSchema = HoodieFileReaderFactory.getFileReader(table.getHadoopConf(), mergeHandle.getOldFilePath()).getSchema();\n      gWriter = new GenericDatumWriter<>(readSchema);\n      gReader = new GenericDatumReader<>(readSchema, mergeHandle.getWriterSchemaWithMetaFields());\n    } else {\n      gReader = null;\n      gWriter = null;\n      readSchema = mergeHandle.getWriterSchemaWithMetaFields();\n    }\n\n    BoundedInMemoryExecutor<GenericRecord, GenericRecord, Void> wrapper = null;\n    Configuration cfgForHoodieFile = new Configuration(table.getHadoopConf());\n    HoodieFileReader<GenericRecord> reader = HoodieFileReaderFactory.<GenericRecord>getFileReader(cfgForHoodieFile, mergeHandle.getOldFilePath());\n    try {\n      final Iterator<GenericRecord> readerIterator;\n      if (baseFile.getBootstrapBaseFile().isPresent()) {\n        readerIterator = getMergingIterator(table, mergeHandle, baseFile, reader, readSchema, externalSchemaTransformation);\n      } else {\n        readerIterator = reader.getRecordIterator(readSchema);\n      }\n\n      ThreadLocal<BinaryEncoder> encoderCache = new ThreadLocal<>();\n      ThreadLocal<BinaryDecoder> decoderCache = new ThreadLocal<>();\n      wrapper = new BoundedInMemoryExecutor(table.getConfig().getWriteBufferLimitBytes(), new IteratorBasedQueueProducer<>(readerIterator),\n          Option.of(new UpdateHandler(mergeHandle)), record -> {\n        if (!externalSchemaTransformation) {\n          return record;\n        }\n        return transformRecordBasedOnNewSchema(gReader, gWriter, encoderCache, decoderCache, (GenericRecord) record);\n      });\n      wrapper.execute();\n    } catch (Exception e) {\n      throw new HoodieException(e);\n    } finally {\n      if (reader != null) {\n        reader.close();\n      }\n      mergeHandle.close();\n      if (null != wrapper) {\n        wrapper.shutdownNow();\n      }\n    }\n  }\n","date":"2021-06-08 14:24:32","endLine":113,"groupId":"10486","id":12,"instanceNumber":2,"isCurCommit":1,"methodName":"runMerge","params":"(HoodieTable<T@List<HoodieRecord<T>>@List<HoodieKey>@List<WriteStatus>>table@HoodieMergeHandle<T@List<HoodieRecord<T>>@List<HoodieKey>@List<WriteStatus>>mergeHandle)","path":"/mnt/clonedata/CloneManagementServer/ManagementServer/consistResult/result-hudi-10-0.7/blobInfo/CC_OUT/blobs/5e/d6d5d529ba3d8fe4df010b1d97dfce2c3630e0.src","preCode":"  public void runMerge(HoodieTable<T, List<HoodieRecord<T>>, List<HoodieKey>, List<WriteStatus>> table,\n                       HoodieMergeHandle<T, List<HoodieRecord<T>>, List<HoodieKey>, List<WriteStatus>> mergeHandle) throws IOException {\n    final GenericDatumWriter<GenericRecord> gWriter;\n    final GenericDatumReader<GenericRecord> gReader;\n    Schema readSchema;\n\n    final boolean externalSchemaTransformation = table.getConfig().shouldUseExternalSchemaTransformation();\n    HoodieBaseFile baseFile = mergeHandle.baseFileForMerge();\n    if (externalSchemaTransformation || baseFile.getBootstrapBaseFile().isPresent()) {\n      readSchema = HoodieFileReaderFactory.getFileReader(table.getHadoopConf(), mergeHandle.getOldFilePath()).getSchema();\n      gWriter = new GenericDatumWriter<>(readSchema);\n      gReader = new GenericDatumReader<>(readSchema, mergeHandle.getWriterSchemaWithMetafields());\n    } else {\n      gReader = null;\n      gWriter = null;\n      readSchema = mergeHandle.getWriterSchemaWithMetafields();\n    }\n\n    BoundedInMemoryExecutor<GenericRecord, GenericRecord, Void> wrapper = null;\n    Configuration cfgForHoodieFile = new Configuration(table.getHadoopConf());\n    HoodieFileReader<GenericRecord> reader = HoodieFileReaderFactory.<GenericRecord>getFileReader(cfgForHoodieFile, mergeHandle.getOldFilePath());\n    try {\n      final Iterator<GenericRecord> readerIterator;\n      if (baseFile.getBootstrapBaseFile().isPresent()) {\n        readerIterator = getMergingIterator(table, mergeHandle, baseFile, reader, readSchema, externalSchemaTransformation);\n      } else {\n        readerIterator = reader.getRecordIterator(readSchema);\n      }\n\n      ThreadLocal<BinaryEncoder> encoderCache = new ThreadLocal<>();\n      ThreadLocal<BinaryDecoder> decoderCache = new ThreadLocal<>();\n      wrapper = new BoundedInMemoryExecutor(table.getConfig().getWriteBufferLimitBytes(), new IteratorBasedQueueProducer<>(readerIterator),\n          Option.of(new UpdateHandler(mergeHandle)), record -> {\n        if (!externalSchemaTransformation) {\n          return record;\n        }\n        return transformRecordBasedOnNewSchema(gReader, gWriter, encoderCache, decoderCache, (GenericRecord) record);\n      });\n      wrapper.execute();\n    } catch (Exception e) {\n      throw new HoodieException(e);\n    } finally {\n      if (reader != null) {\n        reader.close();\n      }\n      mergeHandle.close();\n      if (null != wrapper) {\n        wrapper.shutdownNow();\n      }\n    }\n  }\n","realPath":"hudi-client/hudi-flink-client/src/main/java/org/apache/hudi/table/action/commit/FlinkMergeHelper.java","repoName":"hudi","snippetEndLine":0,"snippetStartLine":0,"startLine":63,"status":"M"}],"commitId":"f760ec543ec9ea23b7d4c9f61c76a283bd737f27","commitMessage":"@@@[HUDI-1659] Basic Implement Of Spark Sql Support For Hoodie (#2645)\n\nMain functions:\nSupport create table for hoodie.\nSupport CTAS.\nSupport Insert for hoodie. Including dynamic partition and static partition insert.\nSupport MergeInto for hoodie.\nSupport DELETE\nSupport UPDATE\nBoth support spark2 & spark3 based on DataSourceV1.\n\nMain changes:\nAdd sql parser for spark2.\nAdd HoodieAnalysis for sql resolve and logical plan rewrite.\nAdd commands implementation for CREATE TABLE?INSERT?MERGE INTO & CTAS.\nIn order to push down the update&insert logical to the HoodieRecordPayload for MergeInto.  I make same change to the\nHoodieWriteHandler and other related classes.\n1?Add the inputSchema for parser the incoming record. This is because the inputSchema for MergeInto is different from writeSchema as there are some transforms in the update& insert expression.\n2?Add WRITE_SCHEMA to HoodieWriteConfig to pass the write schema for merge into.\n3?Pass properties to HoodieRecordPayload#getInsertValue to pass the insert expression and table schema.\n\n\nVerify this pull request\nAdd TestCreateTable for test create hoodie tables and CTAS.\nAdd TestInsertTable for test insert hoodie tables.\nAdd TestMergeIntoTable for test merge hoodie tables.\nAdd TestUpdateTable for test update hoodie tables.\nAdd TestDeleteTable for test delete hoodie tables.\nAdd TestSqlStatement for test supported ddl/dml currently.","date":"2021-06-08 14:24:32","modifiedFileCount":"28","status":"M","submitter":"pengzhiwei"}]
