[{"authorTime":"2021-03-17 07:43:53","codes":[{"authorDate":"2021-03-17 07:43:53","commitOrder":1,"curCode":"  private void testUpsertsContinuousModeWithMultipleWriters(HoodieTableType tableType, String tempDir) throws Exception {\n    \r\n    String tableBasePath = dfsBasePath + \"/\" + tempDir;\n    \r\n    TypedProperties props = prepareMultiWriterProps(PROPS_FILENAME_TEST_MULTI_WRITER);\n    props.setProperty(\"hoodie.writer.lock.provider\", \"org.apache.hudi.client.transaction.FileSystemBasedLockProviderTestClass\");\n    props.setProperty(\"hoodie.writer.lock.filesystem.path\", tableBasePath);\n    UtilitiesTestBase.Helpers.savePropsToDFS(props, dfs, dfsBasePath + \"/\" + PROPS_FILENAME_TEST_MULTI_WRITER);\n    \r\n    int totalRecords = 3000;\n\n    HoodieDeltaStreamer.Config cfgIngestionJob = TestHelpers.makeConfig(tableBasePath, WriteOperationType.UPSERT,\n        Arrays.asList(TripsWithDistanceTransformer.class.getName()), PROPS_FILENAME_TEST_MULTI_WRITER, false);\n    cfgIngestionJob.continuousMode = true;\n    cfgIngestionJob.tableType = tableType.name();\n    cfgIngestionJob.configs.add(String.format(\"%s=%d\", SourceConfigs.MAX_UNIQUE_RECORDS_PROP, totalRecords));\n    cfgIngestionJob.configs.add(String.format(\"%s=false\", HoodieCompactionConfig.AUTO_CLEAN_PROP));\n    HoodieDeltaStreamer ingestionJob = new HoodieDeltaStreamer(cfgIngestionJob, jsc);\n\n    \r\n    deltaStreamerTestRunner(ingestionJob, cfgIngestionJob, (r) -> {\n      if (tableType.equals(HoodieTableType.MERGE_ON_READ)) {\n        TestHelpers.assertAtleastNDeltaCommits(3, tableBasePath, dfs);\n        TestHelpers.assertAtleastNCompactionCommits(1, tableBasePath, dfs);\n      } else {\n        TestHelpers.assertAtleastNCompactionCommits(3, tableBasePath, dfs);\n      }\n      TestHelpers.assertRecordCount(totalRecords, tableBasePath + \"/*/*.parquet\", sqlContext);\n      TestHelpers.assertDistanceCount(totalRecords, tableBasePath + \"/*/*.parquet\", sqlContext);\n      return true;\n    });\n\n    \r\n    HoodieDeltaStreamer.Config cfgBackfillJob = TestHelpers.makeConfig(tableBasePath, WriteOperationType.UPSERT,\n        Arrays.asList(TripsWithDistanceTransformer.class.getName()), PROPS_FILENAME_TEST_MULTI_WRITER, false);\n    cfgBackfillJob.continuousMode = false;\n    cfgBackfillJob.tableType = tableType.name();\n    HoodieTableMetaClient meta = HoodieTableMetaClient.builder().setConf(dfs.getConf()).setBasePath(tableBasePath).build();\n    HoodieTimeline timeline = meta.getActiveTimeline().getCommitsTimeline().filterCompletedInstants();\n    HoodieCommitMetadata commitMetadata = HoodieCommitMetadata\n        .fromBytes(timeline.getInstantDetails(timeline.firstInstant().get()).get(), HoodieCommitMetadata.class);\n    cfgBackfillJob.checkpoint = commitMetadata.getMetadata(CHECKPOINT_KEY);\n    cfgBackfillJob.configs.add(String.format(\"%s=%d\", SourceConfigs.MAX_UNIQUE_RECORDS_PROP, totalRecords));\n    cfgBackfillJob.configs.add(String.format(\"%s=false\", HoodieCompactionConfig.AUTO_CLEAN_PROP));\n    HoodieDeltaStreamer backfillJob = new HoodieDeltaStreamer(cfgBackfillJob, jsc);\n\n    \r\n    HoodieDeltaStreamer ingestionJob2 = new HoodieDeltaStreamer(cfgIngestionJob, jsc);\n\n    \r\n    runJobsInParallel(tableBasePath, tableType, totalRecords, ingestionJob2,\n        cfgIngestionJob, backfillJob, cfgBackfillJob, true);\n\n    \r\n    props = prepareMultiWriterProps(PROPS_FILENAME_TEST_MULTI_WRITER);\n    props.setProperty(\"hoodie.writer.lock.provider\", \"org.apache.hudi.client.transaction.FileSystemBasedLockProviderTestClass\");\n    props.setProperty(\"hoodie.writer.lock.filesystem.path\", tableBasePath);\n    props.setProperty(\"hoodie.test.source.generate.inserts\", \"true\");\n    UtilitiesTestBase.Helpers.savePropsToDFS(props, dfs, dfsBasePath + \"/\" + PROPS_FILENAME_TEST_MULTI_WRITER);\n    cfgBackfillJob = TestHelpers.makeConfig(tableBasePath, WriteOperationType.INSERT,\n        Arrays.asList(TestIdentityTransformer.class.getName()), PROPS_FILENAME_TEST_MULTI_WRITER, false);\n    cfgBackfillJob.continuousMode = false;\n    cfgBackfillJob.tableType = tableType.name();\n    meta = HoodieTableMetaClient.builder().setConf(dfs.getConf()).setBasePath(tableBasePath).build();\n    timeline = meta.getActiveTimeline().getCommitsTimeline().filterCompletedInstants();\n    commitMetadata = HoodieCommitMetadata\n        .fromBytes(timeline.getInstantDetails(timeline.firstInstant().get()).get(), HoodieCommitMetadata.class);\n    cfgBackfillJob.checkpoint = commitMetadata.getMetadata(CHECKPOINT_KEY);\n    cfgBackfillJob.configs.add(String.format(\"%s=%d\", SourceConfigs.MAX_UNIQUE_RECORDS_PROP, totalRecords));\n    cfgBackfillJob.configs.add(String.format(\"%s=false\", HoodieCompactionConfig.AUTO_CLEAN_PROP));\n\n    cfgIngestionJob = TestHelpers.makeConfig(tableBasePath, WriteOperationType.UPSERT,\n        Arrays.asList(TestIdentityTransformer.class.getName()), PROPS_FILENAME_TEST_MULTI_WRITER, false);\n    cfgIngestionJob.continuousMode = true;\n    cfgIngestionJob.tableType = tableType.name();\n    cfgIngestionJob.configs.add(String.format(\"%s=%d\", SourceConfigs.MAX_UNIQUE_RECORDS_PROP, totalRecords));\n    cfgIngestionJob.configs.add(String.format(\"%s=false\", HoodieCompactionConfig.AUTO_CLEAN_PROP));\n    \r\n    HoodieDeltaStreamer ingestionJob3 = new HoodieDeltaStreamer(cfgIngestionJob, jsc);\n    \r\n    HoodieDeltaStreamer backfillJob2 = new HoodieDeltaStreamer(cfgBackfillJob, jsc);\n\n    \r\n    runJobsInParallel(tableBasePath, tableType, totalRecords, ingestionJob3,\n        cfgIngestionJob, backfillJob2, cfgBackfillJob, false);\n  }\n","date":"2021-03-17 07:43:53","endLine":825,"groupId":"4358","id":1,"instanceNumber":1,"isCurCommit":0,"methodName":"testUpsertsContinuousModeWithMultipleWriters","params":"(HoodieTableTypetableType@StringtempDir)","path":"/mnt/clonedata/CloneManagementServer/ManagementServer/consistResult/result-hudi-10-0.7/blobInfo/CC_OUT/blobs/8f/3e045217cd7c7f09d92f32e296bc40b344c0a5.src","preCode":"  private void testUpsertsContinuousModeWithMultipleWriters(HoodieTableType tableType, String tempDir) throws Exception {\n    \r\n    String tableBasePath = dfsBasePath + \"/\" + tempDir;\n    \r\n    TypedProperties props = prepareMultiWriterProps(PROPS_FILENAME_TEST_MULTI_WRITER);\n    props.setProperty(\"hoodie.writer.lock.provider\", \"org.apache.hudi.client.transaction.FileSystemBasedLockProviderTestClass\");\n    props.setProperty(\"hoodie.writer.lock.filesystem.path\", tableBasePath);\n    UtilitiesTestBase.Helpers.savePropsToDFS(props, dfs, dfsBasePath + \"/\" + PROPS_FILENAME_TEST_MULTI_WRITER);\n    \r\n    int totalRecords = 3000;\n\n    HoodieDeltaStreamer.Config cfgIngestionJob = TestHelpers.makeConfig(tableBasePath, WriteOperationType.UPSERT,\n        Arrays.asList(TripsWithDistanceTransformer.class.getName()), PROPS_FILENAME_TEST_MULTI_WRITER, false);\n    cfgIngestionJob.continuousMode = true;\n    cfgIngestionJob.tableType = tableType.name();\n    cfgIngestionJob.configs.add(String.format(\"%s=%d\", SourceConfigs.MAX_UNIQUE_RECORDS_PROP, totalRecords));\n    cfgIngestionJob.configs.add(String.format(\"%s=false\", HoodieCompactionConfig.AUTO_CLEAN_PROP));\n    HoodieDeltaStreamer ingestionJob = new HoodieDeltaStreamer(cfgIngestionJob, jsc);\n\n    \r\n    deltaStreamerTestRunner(ingestionJob, cfgIngestionJob, (r) -> {\n      if (tableType.equals(HoodieTableType.MERGE_ON_READ)) {\n        TestHelpers.assertAtleastNDeltaCommits(3, tableBasePath, dfs);\n        TestHelpers.assertAtleastNCompactionCommits(1, tableBasePath, dfs);\n      } else {\n        TestHelpers.assertAtleastNCompactionCommits(3, tableBasePath, dfs);\n      }\n      TestHelpers.assertRecordCount(totalRecords, tableBasePath + \"/*/*.parquet\", sqlContext);\n      TestHelpers.assertDistanceCount(totalRecords, tableBasePath + \"/*/*.parquet\", sqlContext);\n      return true;\n    });\n\n    \r\n    HoodieDeltaStreamer.Config cfgBackfillJob = TestHelpers.makeConfig(tableBasePath, WriteOperationType.UPSERT,\n        Arrays.asList(TripsWithDistanceTransformer.class.getName()), PROPS_FILENAME_TEST_MULTI_WRITER, false);\n    cfgBackfillJob.continuousMode = false;\n    cfgBackfillJob.tableType = tableType.name();\n    HoodieTableMetaClient meta = HoodieTableMetaClient.builder().setConf(dfs.getConf()).setBasePath(tableBasePath).build();\n    HoodieTimeline timeline = meta.getActiveTimeline().getCommitsTimeline().filterCompletedInstants();\n    HoodieCommitMetadata commitMetadata = HoodieCommitMetadata\n        .fromBytes(timeline.getInstantDetails(timeline.firstInstant().get()).get(), HoodieCommitMetadata.class);\n    cfgBackfillJob.checkpoint = commitMetadata.getMetadata(CHECKPOINT_KEY);\n    cfgBackfillJob.configs.add(String.format(\"%s=%d\", SourceConfigs.MAX_UNIQUE_RECORDS_PROP, totalRecords));\n    cfgBackfillJob.configs.add(String.format(\"%s=false\", HoodieCompactionConfig.AUTO_CLEAN_PROP));\n    HoodieDeltaStreamer backfillJob = new HoodieDeltaStreamer(cfgBackfillJob, jsc);\n\n    \r\n    HoodieDeltaStreamer ingestionJob2 = new HoodieDeltaStreamer(cfgIngestionJob, jsc);\n\n    \r\n    runJobsInParallel(tableBasePath, tableType, totalRecords, ingestionJob2,\n        cfgIngestionJob, backfillJob, cfgBackfillJob, true);\n\n    \r\n    props = prepareMultiWriterProps(PROPS_FILENAME_TEST_MULTI_WRITER);\n    props.setProperty(\"hoodie.writer.lock.provider\", \"org.apache.hudi.client.transaction.FileSystemBasedLockProviderTestClass\");\n    props.setProperty(\"hoodie.writer.lock.filesystem.path\", tableBasePath);\n    props.setProperty(\"hoodie.test.source.generate.inserts\", \"true\");\n    UtilitiesTestBase.Helpers.savePropsToDFS(props, dfs, dfsBasePath + \"/\" + PROPS_FILENAME_TEST_MULTI_WRITER);\n    cfgBackfillJob = TestHelpers.makeConfig(tableBasePath, WriteOperationType.INSERT,\n        Arrays.asList(TestIdentityTransformer.class.getName()), PROPS_FILENAME_TEST_MULTI_WRITER, false);\n    cfgBackfillJob.continuousMode = false;\n    cfgBackfillJob.tableType = tableType.name();\n    meta = HoodieTableMetaClient.builder().setConf(dfs.getConf()).setBasePath(tableBasePath).build();\n    timeline = meta.getActiveTimeline().getCommitsTimeline().filterCompletedInstants();\n    commitMetadata = HoodieCommitMetadata\n        .fromBytes(timeline.getInstantDetails(timeline.firstInstant().get()).get(), HoodieCommitMetadata.class);\n    cfgBackfillJob.checkpoint = commitMetadata.getMetadata(CHECKPOINT_KEY);\n    cfgBackfillJob.configs.add(String.format(\"%s=%d\", SourceConfigs.MAX_UNIQUE_RECORDS_PROP, totalRecords));\n    cfgBackfillJob.configs.add(String.format(\"%s=false\", HoodieCompactionConfig.AUTO_CLEAN_PROP));\n\n    cfgIngestionJob = TestHelpers.makeConfig(tableBasePath, WriteOperationType.UPSERT,\n        Arrays.asList(TestIdentityTransformer.class.getName()), PROPS_FILENAME_TEST_MULTI_WRITER, false);\n    cfgIngestionJob.continuousMode = true;\n    cfgIngestionJob.tableType = tableType.name();\n    cfgIngestionJob.configs.add(String.format(\"%s=%d\", SourceConfigs.MAX_UNIQUE_RECORDS_PROP, totalRecords));\n    cfgIngestionJob.configs.add(String.format(\"%s=false\", HoodieCompactionConfig.AUTO_CLEAN_PROP));\n    \r\n    HoodieDeltaStreamer ingestionJob3 = new HoodieDeltaStreamer(cfgIngestionJob, jsc);\n    \r\n    HoodieDeltaStreamer backfillJob2 = new HoodieDeltaStreamer(cfgBackfillJob, jsc);\n\n    \r\n    runJobsInParallel(tableBasePath, tableType, totalRecords, ingestionJob3,\n        cfgIngestionJob, backfillJob2, cfgBackfillJob, false);\n  }\n","realPath":"hudi-utilities/src/test/java/org/apache/hudi/utilities/functional/TestHoodieDeltaStreamer.java","repoName":"hudi","snippetEndLine":0,"snippetStartLine":0,"startLine":740,"status":"B"},{"authorDate":"2021-03-17 07:43:53","commitOrder":1,"curCode":"  private void testLatestCheckpointCarryOverWithMultipleWriters(HoodieTableType tableType, String tempDir) throws Exception {\n    \r\n    String tableBasePath = dfsBasePath + \"/\" + tempDir;\n    \r\n    TypedProperties props = prepareMultiWriterProps(PROPS_FILENAME_TEST_MULTI_WRITER);\n    props.setProperty(\"hoodie.writer.lock.provider\", \"org.apache.hudi.client.transaction.FileSystemBasedLockProviderTestClass\");\n    props.setProperty(\"hoodie.writer.lock.filesystem.path\", tableBasePath);\n    UtilitiesTestBase.Helpers.savePropsToDFS(props, dfs, dfsBasePath + \"/\" + PROPS_FILENAME_TEST_MULTI_WRITER);\n    \r\n    int totalRecords = 3000;\n\n    HoodieDeltaStreamer.Config cfgIngestionJob = TestHelpers.makeConfig(tableBasePath, WriteOperationType.UPSERT,\n        Arrays.asList(TripsWithDistanceTransformer.class.getName()), PROPS_FILENAME_TEST_MULTI_WRITER, false);\n    cfgIngestionJob.continuousMode = true;\n    cfgIngestionJob.tableType = tableType.name();\n    cfgIngestionJob.configs.add(String.format(\"%s=%d\", SourceConfigs.MAX_UNIQUE_RECORDS_PROP, totalRecords));\n    cfgIngestionJob.configs.add(String.format(\"%s=false\", HoodieCompactionConfig.AUTO_CLEAN_PROP));\n    HoodieDeltaStreamer ingestionJob = new HoodieDeltaStreamer(cfgIngestionJob, jsc);\n\n    \r\n    deltaStreamerTestRunner(ingestionJob, cfgIngestionJob, (r) -> {\n      if (tableType.equals(HoodieTableType.MERGE_ON_READ)) {\n        TestHelpers.assertAtleastNDeltaCommits(3, tableBasePath, dfs);\n        TestHelpers.assertAtleastNCompactionCommits(1, tableBasePath, dfs);\n      } else {\n        TestHelpers.assertAtleastNCompactionCommits(3, tableBasePath, dfs);\n      }\n      TestHelpers.assertRecordCount(totalRecords, tableBasePath + \"/*/*.parquet\", sqlContext);\n      TestHelpers.assertDistanceCount(totalRecords, tableBasePath + \"/*/*.parquet\", sqlContext);\n      return true;\n    });\n\n    \r\n    HoodieDeltaStreamer.Config cfgBackfillJob = TestHelpers.makeConfig(tableBasePath, WriteOperationType.UPSERT,\n        Arrays.asList(TripsWithDistanceTransformer.class.getName()), PROPS_FILENAME_TEST_MULTI_WRITER, false);\n    cfgBackfillJob.continuousMode = false;\n    cfgBackfillJob.tableType = tableType.name();\n    HoodieTableMetaClient meta = HoodieTableMetaClient.builder().setConf(dfs.getConf()).setBasePath(tableBasePath).build();\n    HoodieTimeline timeline = meta.getActiveTimeline().getCommitsTimeline().filterCompletedInstants();\n    HoodieCommitMetadata commitMetadataForFirstInstant = HoodieCommitMetadata\n        .fromBytes(timeline.getInstantDetails(timeline.firstInstant().get()).get(), HoodieCommitMetadata.class);\n\n    \r\n    HoodieCommitMetadata commitMetadataForLastInstant = HoodieCommitMetadata\n        .fromBytes(timeline.getInstantDetails(timeline.lastInstant().get()).get(), HoodieCommitMetadata.class);\n    String lastCheckpointBeforeParallelBackfill = commitMetadataForLastInstant.getMetadata(CHECKPOINT_KEY);\n\n    \r\n    props = prepareMultiWriterProps(PROPS_FILENAME_TEST_MULTI_WRITER);\n    props.setProperty(\"hoodie.writer.lock.provider\", \"org.apache.hudi.client.transaction.FileSystemBasedLockProviderTestClass\");\n    props.setProperty(\"hoodie.writer.lock.filesystem.path\", tableBasePath);\n    props.setProperty(\"hoodie.write.meta.key.prefixes\", CHECKPOINT_KEY);\n    UtilitiesTestBase.Helpers.savePropsToDFS(props, dfs, dfsBasePath + \"/\" + PROPS_FILENAME_TEST_MULTI_WRITER);\n\n    \r\n    \r\n    cfgBackfillJob.checkpoint = commitMetadataForFirstInstant.getMetadata(CHECKPOINT_KEY);\n    cfgBackfillJob.configs.add(String.format(\"%s=%d\", SourceConfigs.MAX_UNIQUE_RECORDS_PROP, totalRecords));\n    cfgBackfillJob.configs.add(String.format(\"%s=false\", HoodieCompactionConfig.AUTO_CLEAN_PROP));\n    HoodieDeltaStreamer backfillJob = new HoodieDeltaStreamer(cfgBackfillJob, jsc);\n    backfillJob.sync();\n\n    \r\n    timeline = meta.getActiveTimeline().reload().getCommitsTimeline().filterCompletedInstants();\n    commitMetadataForLastInstant = HoodieCommitMetadata\n        .fromBytes(timeline.getInstantDetails(timeline.lastInstant().get()).get(), HoodieCommitMetadata.class);\n    String lastCheckpointAfterParallelBackfill = commitMetadataForLastInstant.getMetadata(CHECKPOINT_KEY);\n    Assertions.assertEquals(lastCheckpointBeforeParallelBackfill, lastCheckpointAfterParallelBackfill);\n  }\n","date":"2021-03-17 07:43:53","endLine":895,"groupId":"1268","id":2,"instanceNumber":2,"isCurCommit":0,"methodName":"testLatestCheckpointCarryOverWithMultipleWriters","params":"(HoodieTableTypetableType@StringtempDir)","path":"/mnt/clonedata/CloneManagementServer/ManagementServer/consistResult/result-hudi-10-0.7/blobInfo/CC_OUT/blobs/8f/3e045217cd7c7f09d92f32e296bc40b344c0a5.src","preCode":"  private void testLatestCheckpointCarryOverWithMultipleWriters(HoodieTableType tableType, String tempDir) throws Exception {\n    \r\n    String tableBasePath = dfsBasePath + \"/\" + tempDir;\n    \r\n    TypedProperties props = prepareMultiWriterProps(PROPS_FILENAME_TEST_MULTI_WRITER);\n    props.setProperty(\"hoodie.writer.lock.provider\", \"org.apache.hudi.client.transaction.FileSystemBasedLockProviderTestClass\");\n    props.setProperty(\"hoodie.writer.lock.filesystem.path\", tableBasePath);\n    UtilitiesTestBase.Helpers.savePropsToDFS(props, dfs, dfsBasePath + \"/\" + PROPS_FILENAME_TEST_MULTI_WRITER);\n    \r\n    int totalRecords = 3000;\n\n    HoodieDeltaStreamer.Config cfgIngestionJob = TestHelpers.makeConfig(tableBasePath, WriteOperationType.UPSERT,\n        Arrays.asList(TripsWithDistanceTransformer.class.getName()), PROPS_FILENAME_TEST_MULTI_WRITER, false);\n    cfgIngestionJob.continuousMode = true;\n    cfgIngestionJob.tableType = tableType.name();\n    cfgIngestionJob.configs.add(String.format(\"%s=%d\", SourceConfigs.MAX_UNIQUE_RECORDS_PROP, totalRecords));\n    cfgIngestionJob.configs.add(String.format(\"%s=false\", HoodieCompactionConfig.AUTO_CLEAN_PROP));\n    HoodieDeltaStreamer ingestionJob = new HoodieDeltaStreamer(cfgIngestionJob, jsc);\n\n    \r\n    deltaStreamerTestRunner(ingestionJob, cfgIngestionJob, (r) -> {\n      if (tableType.equals(HoodieTableType.MERGE_ON_READ)) {\n        TestHelpers.assertAtleastNDeltaCommits(3, tableBasePath, dfs);\n        TestHelpers.assertAtleastNCompactionCommits(1, tableBasePath, dfs);\n      } else {\n        TestHelpers.assertAtleastNCompactionCommits(3, tableBasePath, dfs);\n      }\n      TestHelpers.assertRecordCount(totalRecords, tableBasePath + \"/*/*.parquet\", sqlContext);\n      TestHelpers.assertDistanceCount(totalRecords, tableBasePath + \"/*/*.parquet\", sqlContext);\n      return true;\n    });\n\n    \r\n    HoodieDeltaStreamer.Config cfgBackfillJob = TestHelpers.makeConfig(tableBasePath, WriteOperationType.UPSERT,\n        Arrays.asList(TripsWithDistanceTransformer.class.getName()), PROPS_FILENAME_TEST_MULTI_WRITER, false);\n    cfgBackfillJob.continuousMode = false;\n    cfgBackfillJob.tableType = tableType.name();\n    HoodieTableMetaClient meta = HoodieTableMetaClient.builder().setConf(dfs.getConf()).setBasePath(tableBasePath).build();\n    HoodieTimeline timeline = meta.getActiveTimeline().getCommitsTimeline().filterCompletedInstants();\n    HoodieCommitMetadata commitMetadataForFirstInstant = HoodieCommitMetadata\n        .fromBytes(timeline.getInstantDetails(timeline.firstInstant().get()).get(), HoodieCommitMetadata.class);\n\n    \r\n    HoodieCommitMetadata commitMetadataForLastInstant = HoodieCommitMetadata\n        .fromBytes(timeline.getInstantDetails(timeline.lastInstant().get()).get(), HoodieCommitMetadata.class);\n    String lastCheckpointBeforeParallelBackfill = commitMetadataForLastInstant.getMetadata(CHECKPOINT_KEY);\n\n    \r\n    props = prepareMultiWriterProps(PROPS_FILENAME_TEST_MULTI_WRITER);\n    props.setProperty(\"hoodie.writer.lock.provider\", \"org.apache.hudi.client.transaction.FileSystemBasedLockProviderTestClass\");\n    props.setProperty(\"hoodie.writer.lock.filesystem.path\", tableBasePath);\n    props.setProperty(\"hoodie.write.meta.key.prefixes\", CHECKPOINT_KEY);\n    UtilitiesTestBase.Helpers.savePropsToDFS(props, dfs, dfsBasePath + \"/\" + PROPS_FILENAME_TEST_MULTI_WRITER);\n\n    \r\n    \r\n    cfgBackfillJob.checkpoint = commitMetadataForFirstInstant.getMetadata(CHECKPOINT_KEY);\n    cfgBackfillJob.configs.add(String.format(\"%s=%d\", SourceConfigs.MAX_UNIQUE_RECORDS_PROP, totalRecords));\n    cfgBackfillJob.configs.add(String.format(\"%s=false\", HoodieCompactionConfig.AUTO_CLEAN_PROP));\n    HoodieDeltaStreamer backfillJob = new HoodieDeltaStreamer(cfgBackfillJob, jsc);\n    backfillJob.sync();\n\n    \r\n    timeline = meta.getActiveTimeline().reload().getCommitsTimeline().filterCompletedInstants();\n    commitMetadataForLastInstant = HoodieCommitMetadata\n        .fromBytes(timeline.getInstantDetails(timeline.lastInstant().get()).get(), HoodieCommitMetadata.class);\n    String lastCheckpointAfterParallelBackfill = commitMetadataForLastInstant.getMetadata(CHECKPOINT_KEY);\n    Assertions.assertEquals(lastCheckpointBeforeParallelBackfill, lastCheckpointAfterParallelBackfill);\n  }\n","realPath":"hudi-utilities/src/test/java/org/apache/hudi/utilities/functional/TestHoodieDeltaStreamer.java","repoName":"hudi","snippetEndLine":0,"snippetStartLine":0,"startLine":827,"status":"B"}],"commitId":"74241947c123c860a1b0344f25cef316440a70d6","commitMessage":"@@@[HUDI-845] Added locking capability to allow multiple writers (#2374)\n\n* [HUDI-845] Added locking capability to allow multiple writers\n1. Added LockProvider API for pluggable lock methodologies\n2. Added Resolution Strategy API to allow for pluggable conflict resolution\n3. Added TableService client API to schedule table services\n4. Added Transaction Manager for wrapping actions within transactions","date":"2021-03-17 07:43:53","modifiedFileCount":"48","status":"B","submitter":"n3nash"},{"authorTime":"2021-03-24 17:24:02","codes":[{"authorDate":"2021-03-24 17:24:02","commitOrder":2,"curCode":"  private void testUpsertsContinuousModeWithMultipleWriters(HoodieTableType tableType, String tempDir) throws Exception {\n    \r\n    String tableBasePath = dfsBasePath + \"/\" + tempDir;\n    \r\n    TypedProperties props = prepareMultiWriterProps(PROPS_FILENAME_TEST_MULTI_WRITER);\n    props.setProperty(\"hoodie.write.lock.provider\", \"org.apache.hudi.client.transaction.FileSystemBasedLockProviderTestClass\");\n    props.setProperty(\"hoodie.write.lock.filesystem.path\", tableBasePath);\n    UtilitiesTestBase.Helpers.savePropsToDFS(props, dfs, dfsBasePath + \"/\" + PROPS_FILENAME_TEST_MULTI_WRITER);\n    \r\n    int totalRecords = 3000;\n\n    HoodieDeltaStreamer.Config cfgIngestionJob = TestHelpers.makeConfig(tableBasePath, WriteOperationType.UPSERT,\n        Arrays.asList(TripsWithDistanceTransformer.class.getName()), PROPS_FILENAME_TEST_MULTI_WRITER, false);\n    cfgIngestionJob.continuousMode = true;\n    cfgIngestionJob.tableType = tableType.name();\n    cfgIngestionJob.configs.add(String.format(\"%s=%d\", SourceConfigs.MAX_UNIQUE_RECORDS_PROP, totalRecords));\n    cfgIngestionJob.configs.add(String.format(\"%s=false\", HoodieCompactionConfig.AUTO_CLEAN_PROP));\n    HoodieDeltaStreamer ingestionJob = new HoodieDeltaStreamer(cfgIngestionJob, jsc);\n\n    \r\n    deltaStreamerTestRunner(ingestionJob, cfgIngestionJob, (r) -> {\n      if (tableType.equals(HoodieTableType.MERGE_ON_READ)) {\n        TestHelpers.assertAtleastNDeltaCommits(3, tableBasePath, dfs);\n        TestHelpers.assertAtleastNCompactionCommits(1, tableBasePath, dfs);\n      } else {\n        TestHelpers.assertAtleastNCompactionCommits(3, tableBasePath, dfs);\n      }\n      TestHelpers.assertRecordCount(totalRecords, tableBasePath + \"/*/*.parquet\", sqlContext);\n      TestHelpers.assertDistanceCount(totalRecords, tableBasePath + \"/*/*.parquet\", sqlContext);\n      return true;\n    });\n\n    \r\n    HoodieDeltaStreamer.Config cfgBackfillJob = TestHelpers.makeConfig(tableBasePath, WriteOperationType.UPSERT,\n        Arrays.asList(TripsWithDistanceTransformer.class.getName()), PROPS_FILENAME_TEST_MULTI_WRITER, false);\n    cfgBackfillJob.continuousMode = false;\n    cfgBackfillJob.tableType = tableType.name();\n    HoodieTableMetaClient meta = HoodieTableMetaClient.builder().setConf(dfs.getConf()).setBasePath(tableBasePath).build();\n    HoodieTimeline timeline = meta.getActiveTimeline().getCommitsTimeline().filterCompletedInstants();\n    HoodieCommitMetadata commitMetadata = HoodieCommitMetadata\n        .fromBytes(timeline.getInstantDetails(timeline.firstInstant().get()).get(), HoodieCommitMetadata.class);\n    cfgBackfillJob.checkpoint = commitMetadata.getMetadata(CHECKPOINT_KEY);\n    cfgBackfillJob.configs.add(String.format(\"%s=%d\", SourceConfigs.MAX_UNIQUE_RECORDS_PROP, totalRecords));\n    cfgBackfillJob.configs.add(String.format(\"%s=false\", HoodieCompactionConfig.AUTO_CLEAN_PROP));\n    HoodieDeltaStreamer backfillJob = new HoodieDeltaStreamer(cfgBackfillJob, jsc);\n\n    \r\n    HoodieDeltaStreamer ingestionJob2 = new HoodieDeltaStreamer(cfgIngestionJob, jsc);\n\n    \r\n    runJobsInParallel(tableBasePath, tableType, totalRecords, ingestionJob2,\n        cfgIngestionJob, backfillJob, cfgBackfillJob, true);\n\n    \r\n    props = prepareMultiWriterProps(PROPS_FILENAME_TEST_MULTI_WRITER);\n    props.setProperty(\"hoodie.write.lock.provider\", \"org.apache.hudi.client.transaction.FileSystemBasedLockProviderTestClass\");\n    props.setProperty(\"hoodie.write.lock.filesystem.path\", tableBasePath);\n    props.setProperty(\"hoodie.test.source.generate.inserts\", \"true\");\n    UtilitiesTestBase.Helpers.savePropsToDFS(props, dfs, dfsBasePath + \"/\" + PROPS_FILENAME_TEST_MULTI_WRITER);\n    cfgBackfillJob = TestHelpers.makeConfig(tableBasePath, WriteOperationType.INSERT,\n        Arrays.asList(TestIdentityTransformer.class.getName()), PROPS_FILENAME_TEST_MULTI_WRITER, false);\n    cfgBackfillJob.continuousMode = false;\n    cfgBackfillJob.tableType = tableType.name();\n    meta = HoodieTableMetaClient.builder().setConf(dfs.getConf()).setBasePath(tableBasePath).build();\n    timeline = meta.getActiveTimeline().getCommitsTimeline().filterCompletedInstants();\n    commitMetadata = HoodieCommitMetadata\n        .fromBytes(timeline.getInstantDetails(timeline.firstInstant().get()).get(), HoodieCommitMetadata.class);\n    cfgBackfillJob.checkpoint = commitMetadata.getMetadata(CHECKPOINT_KEY);\n    cfgBackfillJob.configs.add(String.format(\"%s=%d\", SourceConfigs.MAX_UNIQUE_RECORDS_PROP, totalRecords));\n    cfgBackfillJob.configs.add(String.format(\"%s=false\", HoodieCompactionConfig.AUTO_CLEAN_PROP));\n\n    cfgIngestionJob = TestHelpers.makeConfig(tableBasePath, WriteOperationType.UPSERT,\n        Arrays.asList(TestIdentityTransformer.class.getName()), PROPS_FILENAME_TEST_MULTI_WRITER, false);\n    cfgIngestionJob.continuousMode = true;\n    cfgIngestionJob.tableType = tableType.name();\n    cfgIngestionJob.configs.add(String.format(\"%s=%d\", SourceConfigs.MAX_UNIQUE_RECORDS_PROP, totalRecords));\n    cfgIngestionJob.configs.add(String.format(\"%s=false\", HoodieCompactionConfig.AUTO_CLEAN_PROP));\n    \r\n    HoodieDeltaStreamer ingestionJob3 = new HoodieDeltaStreamer(cfgIngestionJob, jsc);\n    \r\n    HoodieDeltaStreamer backfillJob2 = new HoodieDeltaStreamer(cfgBackfillJob, jsc);\n\n    \r\n    runJobsInParallel(tableBasePath, tableType, totalRecords, ingestionJob3,\n        cfgIngestionJob, backfillJob2, cfgBackfillJob, false);\n  }\n","date":"2021-03-24 17:24:02","endLine":826,"groupId":"4358","id":3,"instanceNumber":1,"isCurCommit":0,"methodName":"testUpsertsContinuousModeWithMultipleWriters","params":"(HoodieTableTypetableType@StringtempDir)","path":"/mnt/clonedata/CloneManagementServer/ManagementServer/consistResult/result-hudi-10-0.7/blobInfo/CC_OUT/blobs/cb/59ce7b1d3804b8f705b994b23b300ba16a238a.src","preCode":"  private void testUpsertsContinuousModeWithMultipleWriters(HoodieTableType tableType, String tempDir) throws Exception {\n    \r\n    String tableBasePath = dfsBasePath + \"/\" + tempDir;\n    \r\n    TypedProperties props = prepareMultiWriterProps(PROPS_FILENAME_TEST_MULTI_WRITER);\n    props.setProperty(\"hoodie.writer.lock.provider\", \"org.apache.hudi.client.transaction.FileSystemBasedLockProviderTestClass\");\n    props.setProperty(\"hoodie.writer.lock.filesystem.path\", tableBasePath);\n    UtilitiesTestBase.Helpers.savePropsToDFS(props, dfs, dfsBasePath + \"/\" + PROPS_FILENAME_TEST_MULTI_WRITER);\n    \r\n    int totalRecords = 3000;\n\n    HoodieDeltaStreamer.Config cfgIngestionJob = TestHelpers.makeConfig(tableBasePath, WriteOperationType.UPSERT,\n        Arrays.asList(TripsWithDistanceTransformer.class.getName()), PROPS_FILENAME_TEST_MULTI_WRITER, false);\n    cfgIngestionJob.continuousMode = true;\n    cfgIngestionJob.tableType = tableType.name();\n    cfgIngestionJob.configs.add(String.format(\"%s=%d\", SourceConfigs.MAX_UNIQUE_RECORDS_PROP, totalRecords));\n    cfgIngestionJob.configs.add(String.format(\"%s=false\", HoodieCompactionConfig.AUTO_CLEAN_PROP));\n    HoodieDeltaStreamer ingestionJob = new HoodieDeltaStreamer(cfgIngestionJob, jsc);\n\n    \r\n    deltaStreamerTestRunner(ingestionJob, cfgIngestionJob, (r) -> {\n      if (tableType.equals(HoodieTableType.MERGE_ON_READ)) {\n        TestHelpers.assertAtleastNDeltaCommits(3, tableBasePath, dfs);\n        TestHelpers.assertAtleastNCompactionCommits(1, tableBasePath, dfs);\n      } else {\n        TestHelpers.assertAtleastNCompactionCommits(3, tableBasePath, dfs);\n      }\n      TestHelpers.assertRecordCount(totalRecords, tableBasePath + \"/*/*.parquet\", sqlContext);\n      TestHelpers.assertDistanceCount(totalRecords, tableBasePath + \"/*/*.parquet\", sqlContext);\n      return true;\n    });\n\n    \r\n    HoodieDeltaStreamer.Config cfgBackfillJob = TestHelpers.makeConfig(tableBasePath, WriteOperationType.UPSERT,\n        Arrays.asList(TripsWithDistanceTransformer.class.getName()), PROPS_FILENAME_TEST_MULTI_WRITER, false);\n    cfgBackfillJob.continuousMode = false;\n    cfgBackfillJob.tableType = tableType.name();\n    HoodieTableMetaClient meta = HoodieTableMetaClient.builder().setConf(dfs.getConf()).setBasePath(tableBasePath).build();\n    HoodieTimeline timeline = meta.getActiveTimeline().getCommitsTimeline().filterCompletedInstants();\n    HoodieCommitMetadata commitMetadata = HoodieCommitMetadata\n        .fromBytes(timeline.getInstantDetails(timeline.firstInstant().get()).get(), HoodieCommitMetadata.class);\n    cfgBackfillJob.checkpoint = commitMetadata.getMetadata(CHECKPOINT_KEY);\n    cfgBackfillJob.configs.add(String.format(\"%s=%d\", SourceConfigs.MAX_UNIQUE_RECORDS_PROP, totalRecords));\n    cfgBackfillJob.configs.add(String.format(\"%s=false\", HoodieCompactionConfig.AUTO_CLEAN_PROP));\n    HoodieDeltaStreamer backfillJob = new HoodieDeltaStreamer(cfgBackfillJob, jsc);\n\n    \r\n    HoodieDeltaStreamer ingestionJob2 = new HoodieDeltaStreamer(cfgIngestionJob, jsc);\n\n    \r\n    runJobsInParallel(tableBasePath, tableType, totalRecords, ingestionJob2,\n        cfgIngestionJob, backfillJob, cfgBackfillJob, true);\n\n    \r\n    props = prepareMultiWriterProps(PROPS_FILENAME_TEST_MULTI_WRITER);\n    props.setProperty(\"hoodie.writer.lock.provider\", \"org.apache.hudi.client.transaction.FileSystemBasedLockProviderTestClass\");\n    props.setProperty(\"hoodie.writer.lock.filesystem.path\", tableBasePath);\n    props.setProperty(\"hoodie.test.source.generate.inserts\", \"true\");\n    UtilitiesTestBase.Helpers.savePropsToDFS(props, dfs, dfsBasePath + \"/\" + PROPS_FILENAME_TEST_MULTI_WRITER);\n    cfgBackfillJob = TestHelpers.makeConfig(tableBasePath, WriteOperationType.INSERT,\n        Arrays.asList(TestIdentityTransformer.class.getName()), PROPS_FILENAME_TEST_MULTI_WRITER, false);\n    cfgBackfillJob.continuousMode = false;\n    cfgBackfillJob.tableType = tableType.name();\n    meta = HoodieTableMetaClient.builder().setConf(dfs.getConf()).setBasePath(tableBasePath).build();\n    timeline = meta.getActiveTimeline().getCommitsTimeline().filterCompletedInstants();\n    commitMetadata = HoodieCommitMetadata\n        .fromBytes(timeline.getInstantDetails(timeline.firstInstant().get()).get(), HoodieCommitMetadata.class);\n    cfgBackfillJob.checkpoint = commitMetadata.getMetadata(CHECKPOINT_KEY);\n    cfgBackfillJob.configs.add(String.format(\"%s=%d\", SourceConfigs.MAX_UNIQUE_RECORDS_PROP, totalRecords));\n    cfgBackfillJob.configs.add(String.format(\"%s=false\", HoodieCompactionConfig.AUTO_CLEAN_PROP));\n\n    cfgIngestionJob = TestHelpers.makeConfig(tableBasePath, WriteOperationType.UPSERT,\n        Arrays.asList(TestIdentityTransformer.class.getName()), PROPS_FILENAME_TEST_MULTI_WRITER, false);\n    cfgIngestionJob.continuousMode = true;\n    cfgIngestionJob.tableType = tableType.name();\n    cfgIngestionJob.configs.add(String.format(\"%s=%d\", SourceConfigs.MAX_UNIQUE_RECORDS_PROP, totalRecords));\n    cfgIngestionJob.configs.add(String.format(\"%s=false\", HoodieCompactionConfig.AUTO_CLEAN_PROP));\n    \r\n    HoodieDeltaStreamer ingestionJob3 = new HoodieDeltaStreamer(cfgIngestionJob, jsc);\n    \r\n    HoodieDeltaStreamer backfillJob2 = new HoodieDeltaStreamer(cfgBackfillJob, jsc);\n\n    \r\n    runJobsInParallel(tableBasePath, tableType, totalRecords, ingestionJob3,\n        cfgIngestionJob, backfillJob2, cfgBackfillJob, false);\n  }\n","realPath":"hudi-utilities/src/test/java/org/apache/hudi/utilities/functional/TestHoodieDeltaStreamer.java","repoName":"hudi","snippetEndLine":0,"snippetStartLine":0,"startLine":741,"status":"M"},{"authorDate":"2021-03-24 17:24:02","commitOrder":2,"curCode":"  private void testLatestCheckpointCarryOverWithMultipleWriters(HoodieTableType tableType, String tempDir) throws Exception {\n    \r\n    String tableBasePath = dfsBasePath + \"/\" + tempDir;\n    \r\n    TypedProperties props = prepareMultiWriterProps(PROPS_FILENAME_TEST_MULTI_WRITER);\n    props.setProperty(\"hoodie.write.lock.provider\", \"org.apache.hudi.client.transaction.FileSystemBasedLockProviderTestClass\");\n    props.setProperty(\"hoodie.write.lock.filesystem.path\", tableBasePath);\n    UtilitiesTestBase.Helpers.savePropsToDFS(props, dfs, dfsBasePath + \"/\" + PROPS_FILENAME_TEST_MULTI_WRITER);\n    \r\n    int totalRecords = 3000;\n\n    HoodieDeltaStreamer.Config cfgIngestionJob = TestHelpers.makeConfig(tableBasePath, WriteOperationType.UPSERT,\n        Arrays.asList(TripsWithDistanceTransformer.class.getName()), PROPS_FILENAME_TEST_MULTI_WRITER, false);\n    cfgIngestionJob.continuousMode = true;\n    cfgIngestionJob.tableType = tableType.name();\n    cfgIngestionJob.configs.add(String.format(\"%s=%d\", SourceConfigs.MAX_UNIQUE_RECORDS_PROP, totalRecords));\n    cfgIngestionJob.configs.add(String.format(\"%s=false\", HoodieCompactionConfig.AUTO_CLEAN_PROP));\n    HoodieDeltaStreamer ingestionJob = new HoodieDeltaStreamer(cfgIngestionJob, jsc);\n\n    \r\n    deltaStreamerTestRunner(ingestionJob, cfgIngestionJob, (r) -> {\n      if (tableType.equals(HoodieTableType.MERGE_ON_READ)) {\n        TestHelpers.assertAtleastNDeltaCommits(3, tableBasePath, dfs);\n        TestHelpers.assertAtleastNCompactionCommits(1, tableBasePath, dfs);\n      } else {\n        TestHelpers.assertAtleastNCompactionCommits(3, tableBasePath, dfs);\n      }\n      TestHelpers.assertRecordCount(totalRecords, tableBasePath + \"/*/*.parquet\", sqlContext);\n      TestHelpers.assertDistanceCount(totalRecords, tableBasePath + \"/*/*.parquet\", sqlContext);\n      return true;\n    });\n\n    \r\n    HoodieDeltaStreamer.Config cfgBackfillJob = TestHelpers.makeConfig(tableBasePath, WriteOperationType.UPSERT,\n        Arrays.asList(TripsWithDistanceTransformer.class.getName()), PROPS_FILENAME_TEST_MULTI_WRITER, false);\n    cfgBackfillJob.continuousMode = false;\n    cfgBackfillJob.tableType = tableType.name();\n    HoodieTableMetaClient meta = HoodieTableMetaClient.builder().setConf(dfs.getConf()).setBasePath(tableBasePath).build();\n    HoodieTimeline timeline = meta.getActiveTimeline().getCommitsTimeline().filterCompletedInstants();\n    HoodieCommitMetadata commitMetadataForFirstInstant = HoodieCommitMetadata\n        .fromBytes(timeline.getInstantDetails(timeline.firstInstant().get()).get(), HoodieCommitMetadata.class);\n\n    \r\n    HoodieCommitMetadata commitMetadataForLastInstant = HoodieCommitMetadata\n        .fromBytes(timeline.getInstantDetails(timeline.lastInstant().get()).get(), HoodieCommitMetadata.class);\n    String lastCheckpointBeforeParallelBackfill = commitMetadataForLastInstant.getMetadata(CHECKPOINT_KEY);\n\n    \r\n    props = prepareMultiWriterProps(PROPS_FILENAME_TEST_MULTI_WRITER);\n    props.setProperty(\"hoodie.write.lock.provider\", \"org.apache.hudi.client.transaction.FileSystemBasedLockProviderTestClass\");\n    props.setProperty(\"hoodie.write.lock.filesystem.path\", tableBasePath);\n    props.setProperty(\"hoodie.write.meta.key.prefixes\", CHECKPOINT_KEY);\n    UtilitiesTestBase.Helpers.savePropsToDFS(props, dfs, dfsBasePath + \"/\" + PROPS_FILENAME_TEST_MULTI_WRITER);\n\n    \r\n    \r\n    cfgBackfillJob.checkpoint = commitMetadataForFirstInstant.getMetadata(CHECKPOINT_KEY);\n    cfgBackfillJob.configs.add(String.format(\"%s=%d\", SourceConfigs.MAX_UNIQUE_RECORDS_PROP, totalRecords));\n    cfgBackfillJob.configs.add(String.format(\"%s=false\", HoodieCompactionConfig.AUTO_CLEAN_PROP));\n    HoodieDeltaStreamer backfillJob = new HoodieDeltaStreamer(cfgBackfillJob, jsc);\n    backfillJob.sync();\n\n    \r\n    timeline = meta.getActiveTimeline().reload().getCommitsTimeline().filterCompletedInstants();\n    commitMetadataForLastInstant = HoodieCommitMetadata\n        .fromBytes(timeline.getInstantDetails(timeline.lastInstant().get()).get(), HoodieCommitMetadata.class);\n    String lastCheckpointAfterParallelBackfill = commitMetadataForLastInstant.getMetadata(CHECKPOINT_KEY);\n    Assertions.assertEquals(lastCheckpointBeforeParallelBackfill, lastCheckpointAfterParallelBackfill);\n  }\n","date":"2021-03-24 17:24:02","endLine":896,"groupId":"1268","id":4,"instanceNumber":2,"isCurCommit":0,"methodName":"testLatestCheckpointCarryOverWithMultipleWriters","params":"(HoodieTableTypetableType@StringtempDir)","path":"/mnt/clonedata/CloneManagementServer/ManagementServer/consistResult/result-hudi-10-0.7/blobInfo/CC_OUT/blobs/cb/59ce7b1d3804b8f705b994b23b300ba16a238a.src","preCode":"  private void testLatestCheckpointCarryOverWithMultipleWriters(HoodieTableType tableType, String tempDir) throws Exception {\n    \r\n    String tableBasePath = dfsBasePath + \"/\" + tempDir;\n    \r\n    TypedProperties props = prepareMultiWriterProps(PROPS_FILENAME_TEST_MULTI_WRITER);\n    props.setProperty(\"hoodie.writer.lock.provider\", \"org.apache.hudi.client.transaction.FileSystemBasedLockProviderTestClass\");\n    props.setProperty(\"hoodie.writer.lock.filesystem.path\", tableBasePath);\n    UtilitiesTestBase.Helpers.savePropsToDFS(props, dfs, dfsBasePath + \"/\" + PROPS_FILENAME_TEST_MULTI_WRITER);\n    \r\n    int totalRecords = 3000;\n\n    HoodieDeltaStreamer.Config cfgIngestionJob = TestHelpers.makeConfig(tableBasePath, WriteOperationType.UPSERT,\n        Arrays.asList(TripsWithDistanceTransformer.class.getName()), PROPS_FILENAME_TEST_MULTI_WRITER, false);\n    cfgIngestionJob.continuousMode = true;\n    cfgIngestionJob.tableType = tableType.name();\n    cfgIngestionJob.configs.add(String.format(\"%s=%d\", SourceConfigs.MAX_UNIQUE_RECORDS_PROP, totalRecords));\n    cfgIngestionJob.configs.add(String.format(\"%s=false\", HoodieCompactionConfig.AUTO_CLEAN_PROP));\n    HoodieDeltaStreamer ingestionJob = new HoodieDeltaStreamer(cfgIngestionJob, jsc);\n\n    \r\n    deltaStreamerTestRunner(ingestionJob, cfgIngestionJob, (r) -> {\n      if (tableType.equals(HoodieTableType.MERGE_ON_READ)) {\n        TestHelpers.assertAtleastNDeltaCommits(3, tableBasePath, dfs);\n        TestHelpers.assertAtleastNCompactionCommits(1, tableBasePath, dfs);\n      } else {\n        TestHelpers.assertAtleastNCompactionCommits(3, tableBasePath, dfs);\n      }\n      TestHelpers.assertRecordCount(totalRecords, tableBasePath + \"/*/*.parquet\", sqlContext);\n      TestHelpers.assertDistanceCount(totalRecords, tableBasePath + \"/*/*.parquet\", sqlContext);\n      return true;\n    });\n\n    \r\n    HoodieDeltaStreamer.Config cfgBackfillJob = TestHelpers.makeConfig(tableBasePath, WriteOperationType.UPSERT,\n        Arrays.asList(TripsWithDistanceTransformer.class.getName()), PROPS_FILENAME_TEST_MULTI_WRITER, false);\n    cfgBackfillJob.continuousMode = false;\n    cfgBackfillJob.tableType = tableType.name();\n    HoodieTableMetaClient meta = HoodieTableMetaClient.builder().setConf(dfs.getConf()).setBasePath(tableBasePath).build();\n    HoodieTimeline timeline = meta.getActiveTimeline().getCommitsTimeline().filterCompletedInstants();\n    HoodieCommitMetadata commitMetadataForFirstInstant = HoodieCommitMetadata\n        .fromBytes(timeline.getInstantDetails(timeline.firstInstant().get()).get(), HoodieCommitMetadata.class);\n\n    \r\n    HoodieCommitMetadata commitMetadataForLastInstant = HoodieCommitMetadata\n        .fromBytes(timeline.getInstantDetails(timeline.lastInstant().get()).get(), HoodieCommitMetadata.class);\n    String lastCheckpointBeforeParallelBackfill = commitMetadataForLastInstant.getMetadata(CHECKPOINT_KEY);\n\n    \r\n    props = prepareMultiWriterProps(PROPS_FILENAME_TEST_MULTI_WRITER);\n    props.setProperty(\"hoodie.writer.lock.provider\", \"org.apache.hudi.client.transaction.FileSystemBasedLockProviderTestClass\");\n    props.setProperty(\"hoodie.writer.lock.filesystem.path\", tableBasePath);\n    props.setProperty(\"hoodie.write.meta.key.prefixes\", CHECKPOINT_KEY);\n    UtilitiesTestBase.Helpers.savePropsToDFS(props, dfs, dfsBasePath + \"/\" + PROPS_FILENAME_TEST_MULTI_WRITER);\n\n    \r\n    \r\n    cfgBackfillJob.checkpoint = commitMetadataForFirstInstant.getMetadata(CHECKPOINT_KEY);\n    cfgBackfillJob.configs.add(String.format(\"%s=%d\", SourceConfigs.MAX_UNIQUE_RECORDS_PROP, totalRecords));\n    cfgBackfillJob.configs.add(String.format(\"%s=false\", HoodieCompactionConfig.AUTO_CLEAN_PROP));\n    HoodieDeltaStreamer backfillJob = new HoodieDeltaStreamer(cfgBackfillJob, jsc);\n    backfillJob.sync();\n\n    \r\n    timeline = meta.getActiveTimeline().reload().getCommitsTimeline().filterCompletedInstants();\n    commitMetadataForLastInstant = HoodieCommitMetadata\n        .fromBytes(timeline.getInstantDetails(timeline.lastInstant().get()).get(), HoodieCommitMetadata.class);\n    String lastCheckpointAfterParallelBackfill = commitMetadataForLastInstant.getMetadata(CHECKPOINT_KEY);\n    Assertions.assertEquals(lastCheckpointBeforeParallelBackfill, lastCheckpointAfterParallelBackfill);\n  }\n","realPath":"hudi-utilities/src/test/java/org/apache/hudi/utilities/functional/TestHoodieDeltaStreamer.java","repoName":"hudi","snippetEndLine":0,"snippetStartLine":0,"startLine":828,"status":"M"}],"commitId":"01a1d7997bc8f3f82452ebf8e2e655143d00926c","commitMessage":"@@@[HUDI-1712] Rename & standardize config to match other configs (#2708)\n\n","date":"2021-03-24 17:24:02","modifiedFileCount":"3","status":"M","submitter":"n3nash"},{"authorTime":"2021-07-01 05:26:30","codes":[{"authorDate":"2021-07-01 05:26:30","commitOrder":3,"curCode":"  private void testUpsertsContinuousModeWithMultipleWriters(HoodieTableType tableType, String tempDir) throws Exception {\n    \r\n    String tableBasePath = dfsBasePath + \"/\" + tempDir;\n    \r\n    TypedProperties props = prepareMultiWriterProps(PROPS_FILENAME_TEST_MULTI_WRITER);\n    props.setProperty(\"hoodie.write.lock.provider\", \"org.apache.hudi.client.transaction.FileSystemBasedLockProviderTestClass\");\n    props.setProperty(\"hoodie.write.lock.filesystem.path\", tableBasePath);\n    UtilitiesTestBase.Helpers.savePropsToDFS(props, dfs, dfsBasePath + \"/\" + PROPS_FILENAME_TEST_MULTI_WRITER);\n    \r\n    int totalRecords = 3000;\n\n    HoodieDeltaStreamer.Config cfgIngestionJob = TestHelpers.makeConfig(tableBasePath, WriteOperationType.UPSERT,\n        Arrays.asList(TripsWithDistanceTransformer.class.getName()), PROPS_FILENAME_TEST_MULTI_WRITER, false);\n    cfgIngestionJob.continuousMode = true;\n    cfgIngestionJob.tableType = tableType.name();\n    cfgIngestionJob.configs.add(String.format(\"%s=%d\", SourceConfigs.MAX_UNIQUE_RECORDS_PROP, totalRecords));\n    cfgIngestionJob.configs.add(String.format(\"%s=false\", HoodieCompactionConfig.AUTO_CLEAN_PROP.key()));\n    HoodieDeltaStreamer ingestionJob = new HoodieDeltaStreamer(cfgIngestionJob, jsc);\n\n    \r\n    deltaStreamerTestRunner(ingestionJob, cfgIngestionJob, (r) -> {\n      if (tableType.equals(HoodieTableType.MERGE_ON_READ)) {\n        TestHelpers.assertAtleastNDeltaCommits(3, tableBasePath, dfs);\n        TestHelpers.assertAtleastNCompactionCommits(1, tableBasePath, dfs);\n      } else {\n        TestHelpers.assertAtleastNCompactionCommits(3, tableBasePath, dfs);\n      }\n      TestHelpers.assertRecordCount(totalRecords, tableBasePath + \"/*/*.parquet\", sqlContext);\n      TestHelpers.assertDistanceCount(totalRecords, tableBasePath + \"/*/*.parquet\", sqlContext);\n      return true;\n    });\n\n    \r\n    HoodieDeltaStreamer.Config cfgBackfillJob = TestHelpers.makeConfig(tableBasePath, WriteOperationType.UPSERT,\n        Arrays.asList(TripsWithDistanceTransformer.class.getName()), PROPS_FILENAME_TEST_MULTI_WRITER, false);\n    cfgBackfillJob.continuousMode = false;\n    cfgBackfillJob.tableType = tableType.name();\n    HoodieTableMetaClient meta = HoodieTableMetaClient.builder().setConf(dfs.getConf()).setBasePath(tableBasePath).build();\n    HoodieTimeline timeline = meta.getActiveTimeline().getCommitsTimeline().filterCompletedInstants();\n    HoodieCommitMetadata commitMetadata = HoodieCommitMetadata\n        .fromBytes(timeline.getInstantDetails(timeline.firstInstant().get()).get(), HoodieCommitMetadata.class);\n    cfgBackfillJob.checkpoint = commitMetadata.getMetadata(CHECKPOINT_KEY);\n    cfgBackfillJob.configs.add(String.format(\"%s=%d\", SourceConfigs.MAX_UNIQUE_RECORDS_PROP, totalRecords));\n    cfgBackfillJob.configs.add(String.format(\"%s=false\", HoodieCompactionConfig.AUTO_CLEAN_PROP.key()));\n    HoodieDeltaStreamer backfillJob = new HoodieDeltaStreamer(cfgBackfillJob, jsc);\n\n    \r\n    HoodieDeltaStreamer ingestionJob2 = new HoodieDeltaStreamer(cfgIngestionJob, jsc);\n\n    \r\n    runJobsInParallel(tableBasePath, tableType, totalRecords, ingestionJob2,\n        cfgIngestionJob, backfillJob, cfgBackfillJob, true);\n\n    \r\n    props = prepareMultiWriterProps(PROPS_FILENAME_TEST_MULTI_WRITER);\n    props.setProperty(\"hoodie.write.lock.provider\", \"org.apache.hudi.client.transaction.FileSystemBasedLockProviderTestClass\");\n    props.setProperty(\"hoodie.write.lock.filesystem.path\", tableBasePath);\n    props.setProperty(\"hoodie.test.source.generate.inserts\", \"true\");\n    UtilitiesTestBase.Helpers.savePropsToDFS(props, dfs, dfsBasePath + \"/\" + PROPS_FILENAME_TEST_MULTI_WRITER);\n    cfgBackfillJob = TestHelpers.makeConfig(tableBasePath, WriteOperationType.INSERT,\n        Arrays.asList(TestIdentityTransformer.class.getName()), PROPS_FILENAME_TEST_MULTI_WRITER, false);\n    cfgBackfillJob.continuousMode = false;\n    cfgBackfillJob.tableType = tableType.name();\n    meta = HoodieTableMetaClient.builder().setConf(dfs.getConf()).setBasePath(tableBasePath).build();\n    timeline = meta.getActiveTimeline().getCommitsTimeline().filterCompletedInstants();\n    commitMetadata = HoodieCommitMetadata\n        .fromBytes(timeline.getInstantDetails(timeline.firstInstant().get()).get(), HoodieCommitMetadata.class);\n    cfgBackfillJob.checkpoint = commitMetadata.getMetadata(CHECKPOINT_KEY);\n    cfgBackfillJob.configs.add(String.format(\"%s=%d\", SourceConfigs.MAX_UNIQUE_RECORDS_PROP, totalRecords));\n    cfgBackfillJob.configs.add(String.format(\"%s=false\", HoodieCompactionConfig.AUTO_CLEAN_PROP.key()));\n\n    cfgIngestionJob = TestHelpers.makeConfig(tableBasePath, WriteOperationType.UPSERT,\n        Arrays.asList(TestIdentityTransformer.class.getName()), PROPS_FILENAME_TEST_MULTI_WRITER, false);\n    cfgIngestionJob.continuousMode = true;\n    cfgIngestionJob.tableType = tableType.name();\n    cfgIngestionJob.configs.add(String.format(\"%s=%d\", SourceConfigs.MAX_UNIQUE_RECORDS_PROP, totalRecords));\n    cfgIngestionJob.configs.add(String.format(\"%s=false\", HoodieCompactionConfig.AUTO_CLEAN_PROP.key()));\n    \r\n    HoodieDeltaStreamer ingestionJob3 = new HoodieDeltaStreamer(cfgIngestionJob, jsc);\n    \r\n    HoodieDeltaStreamer backfillJob2 = new HoodieDeltaStreamer(cfgBackfillJob, jsc);\n\n    \r\n    runJobsInParallel(tableBasePath, tableType, totalRecords, ingestionJob3,\n        cfgIngestionJob, backfillJob2, cfgBackfillJob, false);\n  }\n","date":"2021-07-01 05:26:30","endLine":842,"groupId":"4358","id":5,"instanceNumber":1,"isCurCommit":0,"methodName":"testUpsertsContinuousModeWithMultipleWriters","params":"(HoodieTableTypetableType@StringtempDir)","path":"/mnt/clonedata/CloneManagementServer/ManagementServer/consistResult/result-hudi-10-0.7/blobInfo/CC_OUT/blobs/8a/2648dc4073b890af70b626df8d8a006f1e1490.src","preCode":"  private void testUpsertsContinuousModeWithMultipleWriters(HoodieTableType tableType, String tempDir) throws Exception {\n    \r\n    String tableBasePath = dfsBasePath + \"/\" + tempDir;\n    \r\n    TypedProperties props = prepareMultiWriterProps(PROPS_FILENAME_TEST_MULTI_WRITER);\n    props.setProperty(\"hoodie.write.lock.provider\", \"org.apache.hudi.client.transaction.FileSystemBasedLockProviderTestClass\");\n    props.setProperty(\"hoodie.write.lock.filesystem.path\", tableBasePath);\n    UtilitiesTestBase.Helpers.savePropsToDFS(props, dfs, dfsBasePath + \"/\" + PROPS_FILENAME_TEST_MULTI_WRITER);\n    \r\n    int totalRecords = 3000;\n\n    HoodieDeltaStreamer.Config cfgIngestionJob = TestHelpers.makeConfig(tableBasePath, WriteOperationType.UPSERT,\n        Arrays.asList(TripsWithDistanceTransformer.class.getName()), PROPS_FILENAME_TEST_MULTI_WRITER, false);\n    cfgIngestionJob.continuousMode = true;\n    cfgIngestionJob.tableType = tableType.name();\n    cfgIngestionJob.configs.add(String.format(\"%s=%d\", SourceConfigs.MAX_UNIQUE_RECORDS_PROP, totalRecords));\n    cfgIngestionJob.configs.add(String.format(\"%s=false\", HoodieCompactionConfig.AUTO_CLEAN_PROP));\n    HoodieDeltaStreamer ingestionJob = new HoodieDeltaStreamer(cfgIngestionJob, jsc);\n\n    \r\n    deltaStreamerTestRunner(ingestionJob, cfgIngestionJob, (r) -> {\n      if (tableType.equals(HoodieTableType.MERGE_ON_READ)) {\n        TestHelpers.assertAtleastNDeltaCommits(3, tableBasePath, dfs);\n        TestHelpers.assertAtleastNCompactionCommits(1, tableBasePath, dfs);\n      } else {\n        TestHelpers.assertAtleastNCompactionCommits(3, tableBasePath, dfs);\n      }\n      TestHelpers.assertRecordCount(totalRecords, tableBasePath + \"/*/*.parquet\", sqlContext);\n      TestHelpers.assertDistanceCount(totalRecords, tableBasePath + \"/*/*.parquet\", sqlContext);\n      return true;\n    });\n\n    \r\n    HoodieDeltaStreamer.Config cfgBackfillJob = TestHelpers.makeConfig(tableBasePath, WriteOperationType.UPSERT,\n        Arrays.asList(TripsWithDistanceTransformer.class.getName()), PROPS_FILENAME_TEST_MULTI_WRITER, false);\n    cfgBackfillJob.continuousMode = false;\n    cfgBackfillJob.tableType = tableType.name();\n    HoodieTableMetaClient meta = HoodieTableMetaClient.builder().setConf(dfs.getConf()).setBasePath(tableBasePath).build();\n    HoodieTimeline timeline = meta.getActiveTimeline().getCommitsTimeline().filterCompletedInstants();\n    HoodieCommitMetadata commitMetadata = HoodieCommitMetadata\n        .fromBytes(timeline.getInstantDetails(timeline.firstInstant().get()).get(), HoodieCommitMetadata.class);\n    cfgBackfillJob.checkpoint = commitMetadata.getMetadata(CHECKPOINT_KEY);\n    cfgBackfillJob.configs.add(String.format(\"%s=%d\", SourceConfigs.MAX_UNIQUE_RECORDS_PROP, totalRecords));\n    cfgBackfillJob.configs.add(String.format(\"%s=false\", HoodieCompactionConfig.AUTO_CLEAN_PROP));\n    HoodieDeltaStreamer backfillJob = new HoodieDeltaStreamer(cfgBackfillJob, jsc);\n\n    \r\n    HoodieDeltaStreamer ingestionJob2 = new HoodieDeltaStreamer(cfgIngestionJob, jsc);\n\n    \r\n    runJobsInParallel(tableBasePath, tableType, totalRecords, ingestionJob2,\n        cfgIngestionJob, backfillJob, cfgBackfillJob, true);\n\n    \r\n    props = prepareMultiWriterProps(PROPS_FILENAME_TEST_MULTI_WRITER);\n    props.setProperty(\"hoodie.write.lock.provider\", \"org.apache.hudi.client.transaction.FileSystemBasedLockProviderTestClass\");\n    props.setProperty(\"hoodie.write.lock.filesystem.path\", tableBasePath);\n    props.setProperty(\"hoodie.test.source.generate.inserts\", \"true\");\n    UtilitiesTestBase.Helpers.savePropsToDFS(props, dfs, dfsBasePath + \"/\" + PROPS_FILENAME_TEST_MULTI_WRITER);\n    cfgBackfillJob = TestHelpers.makeConfig(tableBasePath, WriteOperationType.INSERT,\n        Arrays.asList(TestIdentityTransformer.class.getName()), PROPS_FILENAME_TEST_MULTI_WRITER, false);\n    cfgBackfillJob.continuousMode = false;\n    cfgBackfillJob.tableType = tableType.name();\n    meta = HoodieTableMetaClient.builder().setConf(dfs.getConf()).setBasePath(tableBasePath).build();\n    timeline = meta.getActiveTimeline().getCommitsTimeline().filterCompletedInstants();\n    commitMetadata = HoodieCommitMetadata\n        .fromBytes(timeline.getInstantDetails(timeline.firstInstant().get()).get(), HoodieCommitMetadata.class);\n    cfgBackfillJob.checkpoint = commitMetadata.getMetadata(CHECKPOINT_KEY);\n    cfgBackfillJob.configs.add(String.format(\"%s=%d\", SourceConfigs.MAX_UNIQUE_RECORDS_PROP, totalRecords));\n    cfgBackfillJob.configs.add(String.format(\"%s=false\", HoodieCompactionConfig.AUTO_CLEAN_PROP));\n\n    cfgIngestionJob = TestHelpers.makeConfig(tableBasePath, WriteOperationType.UPSERT,\n        Arrays.asList(TestIdentityTransformer.class.getName()), PROPS_FILENAME_TEST_MULTI_WRITER, false);\n    cfgIngestionJob.continuousMode = true;\n    cfgIngestionJob.tableType = tableType.name();\n    cfgIngestionJob.configs.add(String.format(\"%s=%d\", SourceConfigs.MAX_UNIQUE_RECORDS_PROP, totalRecords));\n    cfgIngestionJob.configs.add(String.format(\"%s=false\", HoodieCompactionConfig.AUTO_CLEAN_PROP));\n    \r\n    HoodieDeltaStreamer ingestionJob3 = new HoodieDeltaStreamer(cfgIngestionJob, jsc);\n    \r\n    HoodieDeltaStreamer backfillJob2 = new HoodieDeltaStreamer(cfgBackfillJob, jsc);\n\n    \r\n    runJobsInParallel(tableBasePath, tableType, totalRecords, ingestionJob3,\n        cfgIngestionJob, backfillJob2, cfgBackfillJob, false);\n  }\n","realPath":"hudi-utilities/src/test/java/org/apache/hudi/utilities/functional/TestHoodieDeltaStreamer.java","repoName":"hudi","snippetEndLine":0,"snippetStartLine":0,"startLine":757,"status":"M"},{"authorDate":"2021-07-01 05:26:30","commitOrder":3,"curCode":"  private void testLatestCheckpointCarryOverWithMultipleWriters(HoodieTableType tableType, String tempDir) throws Exception {\n    \r\n    String tableBasePath = dfsBasePath + \"/\" + tempDir;\n    \r\n    TypedProperties props = prepareMultiWriterProps(PROPS_FILENAME_TEST_MULTI_WRITER);\n    props.setProperty(\"hoodie.write.lock.provider\", \"org.apache.hudi.client.transaction.FileSystemBasedLockProviderTestClass\");\n    props.setProperty(\"hoodie.write.lock.filesystem.path\", tableBasePath);\n    UtilitiesTestBase.Helpers.savePropsToDFS(props, dfs, dfsBasePath + \"/\" + PROPS_FILENAME_TEST_MULTI_WRITER);\n    \r\n    int totalRecords = 3000;\n\n    HoodieDeltaStreamer.Config cfgIngestionJob = TestHelpers.makeConfig(tableBasePath, WriteOperationType.UPSERT,\n        Arrays.asList(TripsWithDistanceTransformer.class.getName()), PROPS_FILENAME_TEST_MULTI_WRITER, false);\n    cfgIngestionJob.continuousMode = true;\n    cfgIngestionJob.tableType = tableType.name();\n    cfgIngestionJob.configs.add(String.format(\"%s=%d\", SourceConfigs.MAX_UNIQUE_RECORDS_PROP, totalRecords));\n    cfgIngestionJob.configs.add(String.format(\"%s=false\", HoodieCompactionConfig.AUTO_CLEAN_PROP.key()));\n    HoodieDeltaStreamer ingestionJob = new HoodieDeltaStreamer(cfgIngestionJob, jsc);\n\n    \r\n    deltaStreamerTestRunner(ingestionJob, cfgIngestionJob, (r) -> {\n      if (tableType.equals(HoodieTableType.MERGE_ON_READ)) {\n        TestHelpers.assertAtleastNDeltaCommits(3, tableBasePath, dfs);\n        TestHelpers.assertAtleastNCompactionCommits(1, tableBasePath, dfs);\n      } else {\n        TestHelpers.assertAtleastNCompactionCommits(3, tableBasePath, dfs);\n      }\n      TestHelpers.assertRecordCount(totalRecords, tableBasePath + \"/*/*.parquet\", sqlContext);\n      TestHelpers.assertDistanceCount(totalRecords, tableBasePath + \"/*/*.parquet\", sqlContext);\n      return true;\n    });\n\n    \r\n    HoodieDeltaStreamer.Config cfgBackfillJob = TestHelpers.makeConfig(tableBasePath, WriteOperationType.UPSERT,\n        Arrays.asList(TripsWithDistanceTransformer.class.getName()), PROPS_FILENAME_TEST_MULTI_WRITER, false);\n    cfgBackfillJob.continuousMode = false;\n    cfgBackfillJob.tableType = tableType.name();\n    HoodieTableMetaClient meta = HoodieTableMetaClient.builder().setConf(dfs.getConf()).setBasePath(tableBasePath).build();\n    HoodieTimeline timeline = meta.getActiveTimeline().getCommitsTimeline().filterCompletedInstants();\n    HoodieCommitMetadata commitMetadataForFirstInstant = HoodieCommitMetadata\n        .fromBytes(timeline.getInstantDetails(timeline.firstInstant().get()).get(), HoodieCommitMetadata.class);\n\n    \r\n    HoodieCommitMetadata commitMetadataForLastInstant = HoodieCommitMetadata\n        .fromBytes(timeline.getInstantDetails(timeline.lastInstant().get()).get(), HoodieCommitMetadata.class);\n    String lastCheckpointBeforeParallelBackfill = commitMetadataForLastInstant.getMetadata(CHECKPOINT_KEY);\n\n    \r\n    props = prepareMultiWriterProps(PROPS_FILENAME_TEST_MULTI_WRITER);\n    props.setProperty(\"hoodie.write.lock.provider\", \"org.apache.hudi.client.transaction.FileSystemBasedLockProviderTestClass\");\n    props.setProperty(\"hoodie.write.lock.filesystem.path\", tableBasePath);\n    props.setProperty(\"hoodie.write.meta.key.prefixes\", CHECKPOINT_KEY);\n    UtilitiesTestBase.Helpers.savePropsToDFS(props, dfs, dfsBasePath + \"/\" + PROPS_FILENAME_TEST_MULTI_WRITER);\n\n    \r\n    \r\n    cfgBackfillJob.checkpoint = commitMetadataForFirstInstant.getMetadata(CHECKPOINT_KEY);\n    cfgBackfillJob.configs.add(String.format(\"%s=%d\", SourceConfigs.MAX_UNIQUE_RECORDS_PROP, totalRecords));\n    cfgBackfillJob.configs.add(String.format(\"%s=false\", HoodieCompactionConfig.AUTO_CLEAN_PROP.key()));\n    HoodieDeltaStreamer backfillJob = new HoodieDeltaStreamer(cfgBackfillJob, jsc);\n    backfillJob.sync();\n\n    \r\n    timeline = meta.getActiveTimeline().reload().getCommitsTimeline().filterCompletedInstants();\n    commitMetadataForLastInstant = HoodieCommitMetadata\n        .fromBytes(timeline.getInstantDetails(timeline.lastInstant().get()).get(), HoodieCommitMetadata.class);\n    String lastCheckpointAfterParallelBackfill = commitMetadataForLastInstant.getMetadata(CHECKPOINT_KEY);\n    Assertions.assertEquals(lastCheckpointBeforeParallelBackfill, lastCheckpointAfterParallelBackfill);\n  }\n","date":"2021-07-01 05:26:30","endLine":912,"groupId":"1268","id":6,"instanceNumber":2,"isCurCommit":0,"methodName":"testLatestCheckpointCarryOverWithMultipleWriters","params":"(HoodieTableTypetableType@StringtempDir)","path":"/mnt/clonedata/CloneManagementServer/ManagementServer/consistResult/result-hudi-10-0.7/blobInfo/CC_OUT/blobs/8a/2648dc4073b890af70b626df8d8a006f1e1490.src","preCode":"  private void testLatestCheckpointCarryOverWithMultipleWriters(HoodieTableType tableType, String tempDir) throws Exception {\n    \r\n    String tableBasePath = dfsBasePath + \"/\" + tempDir;\n    \r\n    TypedProperties props = prepareMultiWriterProps(PROPS_FILENAME_TEST_MULTI_WRITER);\n    props.setProperty(\"hoodie.write.lock.provider\", \"org.apache.hudi.client.transaction.FileSystemBasedLockProviderTestClass\");\n    props.setProperty(\"hoodie.write.lock.filesystem.path\", tableBasePath);\n    UtilitiesTestBase.Helpers.savePropsToDFS(props, dfs, dfsBasePath + \"/\" + PROPS_FILENAME_TEST_MULTI_WRITER);\n    \r\n    int totalRecords = 3000;\n\n    HoodieDeltaStreamer.Config cfgIngestionJob = TestHelpers.makeConfig(tableBasePath, WriteOperationType.UPSERT,\n        Arrays.asList(TripsWithDistanceTransformer.class.getName()), PROPS_FILENAME_TEST_MULTI_WRITER, false);\n    cfgIngestionJob.continuousMode = true;\n    cfgIngestionJob.tableType = tableType.name();\n    cfgIngestionJob.configs.add(String.format(\"%s=%d\", SourceConfigs.MAX_UNIQUE_RECORDS_PROP, totalRecords));\n    cfgIngestionJob.configs.add(String.format(\"%s=false\", HoodieCompactionConfig.AUTO_CLEAN_PROP));\n    HoodieDeltaStreamer ingestionJob = new HoodieDeltaStreamer(cfgIngestionJob, jsc);\n\n    \r\n    deltaStreamerTestRunner(ingestionJob, cfgIngestionJob, (r) -> {\n      if (tableType.equals(HoodieTableType.MERGE_ON_READ)) {\n        TestHelpers.assertAtleastNDeltaCommits(3, tableBasePath, dfs);\n        TestHelpers.assertAtleastNCompactionCommits(1, tableBasePath, dfs);\n      } else {\n        TestHelpers.assertAtleastNCompactionCommits(3, tableBasePath, dfs);\n      }\n      TestHelpers.assertRecordCount(totalRecords, tableBasePath + \"/*/*.parquet\", sqlContext);\n      TestHelpers.assertDistanceCount(totalRecords, tableBasePath + \"/*/*.parquet\", sqlContext);\n      return true;\n    });\n\n    \r\n    HoodieDeltaStreamer.Config cfgBackfillJob = TestHelpers.makeConfig(tableBasePath, WriteOperationType.UPSERT,\n        Arrays.asList(TripsWithDistanceTransformer.class.getName()), PROPS_FILENAME_TEST_MULTI_WRITER, false);\n    cfgBackfillJob.continuousMode = false;\n    cfgBackfillJob.tableType = tableType.name();\n    HoodieTableMetaClient meta = HoodieTableMetaClient.builder().setConf(dfs.getConf()).setBasePath(tableBasePath).build();\n    HoodieTimeline timeline = meta.getActiveTimeline().getCommitsTimeline().filterCompletedInstants();\n    HoodieCommitMetadata commitMetadataForFirstInstant = HoodieCommitMetadata\n        .fromBytes(timeline.getInstantDetails(timeline.firstInstant().get()).get(), HoodieCommitMetadata.class);\n\n    \r\n    HoodieCommitMetadata commitMetadataForLastInstant = HoodieCommitMetadata\n        .fromBytes(timeline.getInstantDetails(timeline.lastInstant().get()).get(), HoodieCommitMetadata.class);\n    String lastCheckpointBeforeParallelBackfill = commitMetadataForLastInstant.getMetadata(CHECKPOINT_KEY);\n\n    \r\n    props = prepareMultiWriterProps(PROPS_FILENAME_TEST_MULTI_WRITER);\n    props.setProperty(\"hoodie.write.lock.provider\", \"org.apache.hudi.client.transaction.FileSystemBasedLockProviderTestClass\");\n    props.setProperty(\"hoodie.write.lock.filesystem.path\", tableBasePath);\n    props.setProperty(\"hoodie.write.meta.key.prefixes\", CHECKPOINT_KEY);\n    UtilitiesTestBase.Helpers.savePropsToDFS(props, dfs, dfsBasePath + \"/\" + PROPS_FILENAME_TEST_MULTI_WRITER);\n\n    \r\n    \r\n    cfgBackfillJob.checkpoint = commitMetadataForFirstInstant.getMetadata(CHECKPOINT_KEY);\n    cfgBackfillJob.configs.add(String.format(\"%s=%d\", SourceConfigs.MAX_UNIQUE_RECORDS_PROP, totalRecords));\n    cfgBackfillJob.configs.add(String.format(\"%s=false\", HoodieCompactionConfig.AUTO_CLEAN_PROP));\n    HoodieDeltaStreamer backfillJob = new HoodieDeltaStreamer(cfgBackfillJob, jsc);\n    backfillJob.sync();\n\n    \r\n    timeline = meta.getActiveTimeline().reload().getCommitsTimeline().filterCompletedInstants();\n    commitMetadataForLastInstant = HoodieCommitMetadata\n        .fromBytes(timeline.getInstantDetails(timeline.lastInstant().get()).get(), HoodieCommitMetadata.class);\n    String lastCheckpointAfterParallelBackfill = commitMetadataForLastInstant.getMetadata(CHECKPOINT_KEY);\n    Assertions.assertEquals(lastCheckpointBeforeParallelBackfill, lastCheckpointAfterParallelBackfill);\n  }\n","realPath":"hudi-utilities/src/test/java/org/apache/hudi/utilities/functional/TestHoodieDeltaStreamer.java","repoName":"hudi","snippetEndLine":0,"snippetStartLine":0,"startLine":844,"status":"M"}],"commitId":"d412fb2fe642417460532044cac162bb68f4bec4","commitMessage":"@@@[HUDI-89] Add configOption & refactor all configs based on that (#2833)\n\nCo-authored-by: Wenning Ding <wenningd@amazon.com>","date":"2021-07-01 05:26:30","modifiedFileCount":"138","status":"M","submitter":"wenningd"},{"authorTime":"2021-08-13 11:31:04","codes":[{"authorDate":"2021-08-13 11:31:04","commitOrder":4,"curCode":"  private void testUpsertsContinuousModeWithMultipleWriters(HoodieTableType tableType, String tempDir) throws Exception {\n    \r\n    String tableBasePath = dfsBasePath + \"/\" + tempDir;\n    \r\n    TypedProperties props = prepareMultiWriterProps(PROPS_FILENAME_TEST_MULTI_WRITER);\n    props.setProperty(\"hoodie.write.lock.provider\", \"org.apache.hudi.client.transaction.FileSystemBasedLockProviderTestClass\");\n    props.setProperty(\"hoodie.write.lock.filesystem.path\", tableBasePath);\n    UtilitiesTestBase.Helpers.savePropsToDFS(props, dfs, dfsBasePath + \"/\" + PROPS_FILENAME_TEST_MULTI_WRITER);\n    \r\n    int totalRecords = 3000;\n\n    HoodieDeltaStreamer.Config cfgIngestionJob = TestHelpers.makeConfig(tableBasePath, WriteOperationType.UPSERT,\n        Arrays.asList(TripsWithDistanceTransformer.class.getName()), PROPS_FILENAME_TEST_MULTI_WRITER, false);\n    cfgIngestionJob.continuousMode = true;\n    cfgIngestionJob.tableType = tableType.name();\n    cfgIngestionJob.configs.add(String.format(\"%s=%d\", SourceConfigs.MAX_UNIQUE_RECORDS_PROP, totalRecords));\n    cfgIngestionJob.configs.add(String.format(\"%s=false\", HoodieCompactionConfig.AUTO_CLEAN.key()));\n    HoodieDeltaStreamer ingestionJob = new HoodieDeltaStreamer(cfgIngestionJob, jsc);\n\n    \r\n    deltaStreamerTestRunner(ingestionJob, cfgIngestionJob, (r) -> {\n      if (tableType.equals(HoodieTableType.MERGE_ON_READ)) {\n        TestHelpers.assertAtleastNDeltaCommits(3, tableBasePath, dfs);\n        TestHelpers.assertAtleastNCompactionCommits(1, tableBasePath, dfs);\n      } else {\n        TestHelpers.assertAtleastNCompactionCommits(3, tableBasePath, dfs);\n      }\n      TestHelpers.assertRecordCount(totalRecords, tableBasePath + \"/*/*.parquet\", sqlContext);\n      TestHelpers.assertDistanceCount(totalRecords, tableBasePath + \"/*/*.parquet\", sqlContext);\n      return true;\n    });\n\n    \r\n    HoodieDeltaStreamer.Config cfgBackfillJob = TestHelpers.makeConfig(tableBasePath, WriteOperationType.UPSERT,\n        Arrays.asList(TripsWithDistanceTransformer.class.getName()), PROPS_FILENAME_TEST_MULTI_WRITER, false);\n    cfgBackfillJob.continuousMode = false;\n    cfgBackfillJob.tableType = tableType.name();\n    HoodieTableMetaClient meta = HoodieTableMetaClient.builder().setConf(dfs.getConf()).setBasePath(tableBasePath).build();\n    HoodieTimeline timeline = meta.getActiveTimeline().getCommitsTimeline().filterCompletedInstants();\n    HoodieCommitMetadata commitMetadata = HoodieCommitMetadata\n        .fromBytes(timeline.getInstantDetails(timeline.firstInstant().get()).get(), HoodieCommitMetadata.class);\n    cfgBackfillJob.checkpoint = commitMetadata.getMetadata(CHECKPOINT_KEY);\n    cfgBackfillJob.configs.add(String.format(\"%s=%d\", SourceConfigs.MAX_UNIQUE_RECORDS_PROP, totalRecords));\n    cfgBackfillJob.configs.add(String.format(\"%s=false\", HoodieCompactionConfig.AUTO_CLEAN.key()));\n    HoodieDeltaStreamer backfillJob = new HoodieDeltaStreamer(cfgBackfillJob, jsc);\n\n    \r\n    HoodieDeltaStreamer ingestionJob2 = new HoodieDeltaStreamer(cfgIngestionJob, jsc);\n\n    \r\n    runJobsInParallel(tableBasePath, tableType, totalRecords, ingestionJob2,\n        cfgIngestionJob, backfillJob, cfgBackfillJob, true);\n\n    \r\n    props = prepareMultiWriterProps(PROPS_FILENAME_TEST_MULTI_WRITER);\n    props.setProperty(\"hoodie.write.lock.provider\", \"org.apache.hudi.client.transaction.FileSystemBasedLockProviderTestClass\");\n    props.setProperty(\"hoodie.write.lock.filesystem.path\", tableBasePath);\n    props.setProperty(\"hoodie.test.source.generate.inserts\", \"true\");\n    UtilitiesTestBase.Helpers.savePropsToDFS(props, dfs, dfsBasePath + \"/\" + PROPS_FILENAME_TEST_MULTI_WRITER);\n    cfgBackfillJob = TestHelpers.makeConfig(tableBasePath, WriteOperationType.INSERT,\n        Arrays.asList(TestIdentityTransformer.class.getName()), PROPS_FILENAME_TEST_MULTI_WRITER, false);\n    cfgBackfillJob.continuousMode = false;\n    cfgBackfillJob.tableType = tableType.name();\n    meta = HoodieTableMetaClient.builder().setConf(dfs.getConf()).setBasePath(tableBasePath).build();\n    timeline = meta.getActiveTimeline().getCommitsTimeline().filterCompletedInstants();\n    commitMetadata = HoodieCommitMetadata\n        .fromBytes(timeline.getInstantDetails(timeline.firstInstant().get()).get(), HoodieCommitMetadata.class);\n    cfgBackfillJob.checkpoint = commitMetadata.getMetadata(CHECKPOINT_KEY);\n    cfgBackfillJob.configs.add(String.format(\"%s=%d\", SourceConfigs.MAX_UNIQUE_RECORDS_PROP, totalRecords));\n    cfgBackfillJob.configs.add(String.format(\"%s=false\", HoodieCompactionConfig.AUTO_CLEAN.key()));\n\n    cfgIngestionJob = TestHelpers.makeConfig(tableBasePath, WriteOperationType.UPSERT,\n        Arrays.asList(TestIdentityTransformer.class.getName()), PROPS_FILENAME_TEST_MULTI_WRITER, false);\n    cfgIngestionJob.continuousMode = true;\n    cfgIngestionJob.tableType = tableType.name();\n    cfgIngestionJob.configs.add(String.format(\"%s=%d\", SourceConfigs.MAX_UNIQUE_RECORDS_PROP, totalRecords));\n    cfgIngestionJob.configs.add(String.format(\"%s=false\", HoodieCompactionConfig.AUTO_CLEAN.key()));\n    \r\n    HoodieDeltaStreamer ingestionJob3 = new HoodieDeltaStreamer(cfgIngestionJob, jsc);\n    \r\n    HoodieDeltaStreamer backfillJob2 = new HoodieDeltaStreamer(cfgBackfillJob, jsc);\n\n    \r\n    runJobsInParallel(tableBasePath, tableType, totalRecords, ingestionJob3,\n        cfgIngestionJob, backfillJob2, cfgBackfillJob, false);\n  }\n","date":"2021-08-13 11:31:04","endLine":814,"groupId":"10252","id":7,"instanceNumber":1,"isCurCommit":0,"methodName":"testUpsertsContinuousModeWithMultipleWriters","params":"(HoodieTableTypetableType@StringtempDir)","path":"/mnt/clonedata/CloneManagementServer/ManagementServer/consistResult/result-hudi-10-0.7/blobInfo/CC_OUT/blobs/78/48679fedc95d91e43238401a487957dc139909.src","preCode":"  private void testUpsertsContinuousModeWithMultipleWriters(HoodieTableType tableType, String tempDir) throws Exception {\n    \r\n    String tableBasePath = dfsBasePath + \"/\" + tempDir;\n    \r\n    TypedProperties props = prepareMultiWriterProps(PROPS_FILENAME_TEST_MULTI_WRITER);\n    props.setProperty(\"hoodie.write.lock.provider\", \"org.apache.hudi.client.transaction.FileSystemBasedLockProviderTestClass\");\n    props.setProperty(\"hoodie.write.lock.filesystem.path\", tableBasePath);\n    UtilitiesTestBase.Helpers.savePropsToDFS(props, dfs, dfsBasePath + \"/\" + PROPS_FILENAME_TEST_MULTI_WRITER);\n    \r\n    int totalRecords = 3000;\n\n    HoodieDeltaStreamer.Config cfgIngestionJob = TestHelpers.makeConfig(tableBasePath, WriteOperationType.UPSERT,\n        Arrays.asList(TripsWithDistanceTransformer.class.getName()), PROPS_FILENAME_TEST_MULTI_WRITER, false);\n    cfgIngestionJob.continuousMode = true;\n    cfgIngestionJob.tableType = tableType.name();\n    cfgIngestionJob.configs.add(String.format(\"%s=%d\", SourceConfigs.MAX_UNIQUE_RECORDS_PROP, totalRecords));\n    cfgIngestionJob.configs.add(String.format(\"%s=false\", HoodieCompactionConfig.AUTO_CLEAN_PROP.key()));\n    HoodieDeltaStreamer ingestionJob = new HoodieDeltaStreamer(cfgIngestionJob, jsc);\n\n    \r\n    deltaStreamerTestRunner(ingestionJob, cfgIngestionJob, (r) -> {\n      if (tableType.equals(HoodieTableType.MERGE_ON_READ)) {\n        TestHelpers.assertAtleastNDeltaCommits(3, tableBasePath, dfs);\n        TestHelpers.assertAtleastNCompactionCommits(1, tableBasePath, dfs);\n      } else {\n        TestHelpers.assertAtleastNCompactionCommits(3, tableBasePath, dfs);\n      }\n      TestHelpers.assertRecordCount(totalRecords, tableBasePath + \"/*/*.parquet\", sqlContext);\n      TestHelpers.assertDistanceCount(totalRecords, tableBasePath + \"/*/*.parquet\", sqlContext);\n      return true;\n    });\n\n    \r\n    HoodieDeltaStreamer.Config cfgBackfillJob = TestHelpers.makeConfig(tableBasePath, WriteOperationType.UPSERT,\n        Arrays.asList(TripsWithDistanceTransformer.class.getName()), PROPS_FILENAME_TEST_MULTI_WRITER, false);\n    cfgBackfillJob.continuousMode = false;\n    cfgBackfillJob.tableType = tableType.name();\n    HoodieTableMetaClient meta = HoodieTableMetaClient.builder().setConf(dfs.getConf()).setBasePath(tableBasePath).build();\n    HoodieTimeline timeline = meta.getActiveTimeline().getCommitsTimeline().filterCompletedInstants();\n    HoodieCommitMetadata commitMetadata = HoodieCommitMetadata\n        .fromBytes(timeline.getInstantDetails(timeline.firstInstant().get()).get(), HoodieCommitMetadata.class);\n    cfgBackfillJob.checkpoint = commitMetadata.getMetadata(CHECKPOINT_KEY);\n    cfgBackfillJob.configs.add(String.format(\"%s=%d\", SourceConfigs.MAX_UNIQUE_RECORDS_PROP, totalRecords));\n    cfgBackfillJob.configs.add(String.format(\"%s=false\", HoodieCompactionConfig.AUTO_CLEAN_PROP.key()));\n    HoodieDeltaStreamer backfillJob = new HoodieDeltaStreamer(cfgBackfillJob, jsc);\n\n    \r\n    HoodieDeltaStreamer ingestionJob2 = new HoodieDeltaStreamer(cfgIngestionJob, jsc);\n\n    \r\n    runJobsInParallel(tableBasePath, tableType, totalRecords, ingestionJob2,\n        cfgIngestionJob, backfillJob, cfgBackfillJob, true);\n\n    \r\n    props = prepareMultiWriterProps(PROPS_FILENAME_TEST_MULTI_WRITER);\n    props.setProperty(\"hoodie.write.lock.provider\", \"org.apache.hudi.client.transaction.FileSystemBasedLockProviderTestClass\");\n    props.setProperty(\"hoodie.write.lock.filesystem.path\", tableBasePath);\n    props.setProperty(\"hoodie.test.source.generate.inserts\", \"true\");\n    UtilitiesTestBase.Helpers.savePropsToDFS(props, dfs, dfsBasePath + \"/\" + PROPS_FILENAME_TEST_MULTI_WRITER);\n    cfgBackfillJob = TestHelpers.makeConfig(tableBasePath, WriteOperationType.INSERT,\n        Arrays.asList(TestIdentityTransformer.class.getName()), PROPS_FILENAME_TEST_MULTI_WRITER, false);\n    cfgBackfillJob.continuousMode = false;\n    cfgBackfillJob.tableType = tableType.name();\n    meta = HoodieTableMetaClient.builder().setConf(dfs.getConf()).setBasePath(tableBasePath).build();\n    timeline = meta.getActiveTimeline().getCommitsTimeline().filterCompletedInstants();\n    commitMetadata = HoodieCommitMetadata\n        .fromBytes(timeline.getInstantDetails(timeline.firstInstant().get()).get(), HoodieCommitMetadata.class);\n    cfgBackfillJob.checkpoint = commitMetadata.getMetadata(CHECKPOINT_KEY);\n    cfgBackfillJob.configs.add(String.format(\"%s=%d\", SourceConfigs.MAX_UNIQUE_RECORDS_PROP, totalRecords));\n    cfgBackfillJob.configs.add(String.format(\"%s=false\", HoodieCompactionConfig.AUTO_CLEAN_PROP.key()));\n\n    cfgIngestionJob = TestHelpers.makeConfig(tableBasePath, WriteOperationType.UPSERT,\n        Arrays.asList(TestIdentityTransformer.class.getName()), PROPS_FILENAME_TEST_MULTI_WRITER, false);\n    cfgIngestionJob.continuousMode = true;\n    cfgIngestionJob.tableType = tableType.name();\n    cfgIngestionJob.configs.add(String.format(\"%s=%d\", SourceConfigs.MAX_UNIQUE_RECORDS_PROP, totalRecords));\n    cfgIngestionJob.configs.add(String.format(\"%s=false\", HoodieCompactionConfig.AUTO_CLEAN_PROP.key()));\n    \r\n    HoodieDeltaStreamer ingestionJob3 = new HoodieDeltaStreamer(cfgIngestionJob, jsc);\n    \r\n    HoodieDeltaStreamer backfillJob2 = new HoodieDeltaStreamer(cfgBackfillJob, jsc);\n\n    \r\n    runJobsInParallel(tableBasePath, tableType, totalRecords, ingestionJob3,\n        cfgIngestionJob, backfillJob2, cfgBackfillJob, false);\n  }\n","realPath":"hudi-utilities/src/test/java/org/apache/hudi/utilities/functional/TestHoodieDeltaStreamer.java","repoName":"hudi","snippetEndLine":0,"snippetStartLine":0,"startLine":729,"status":"M"},{"authorDate":"2021-08-13 11:31:04","commitOrder":4,"curCode":"  private void testLatestCheckpointCarryOverWithMultipleWriters(HoodieTableType tableType, String tempDir) throws Exception {\n    \r\n    String tableBasePath = dfsBasePath + \"/\" + tempDir;\n    \r\n    TypedProperties props = prepareMultiWriterProps(PROPS_FILENAME_TEST_MULTI_WRITER);\n    props.setProperty(\"hoodie.write.lock.provider\", \"org.apache.hudi.client.transaction.FileSystemBasedLockProviderTestClass\");\n    props.setProperty(\"hoodie.write.lock.filesystem.path\", tableBasePath);\n    UtilitiesTestBase.Helpers.savePropsToDFS(props, dfs, dfsBasePath + \"/\" + PROPS_FILENAME_TEST_MULTI_WRITER);\n    \r\n    int totalRecords = 3000;\n\n    HoodieDeltaStreamer.Config cfgIngestionJob = TestHelpers.makeConfig(tableBasePath, WriteOperationType.UPSERT,\n        Arrays.asList(TripsWithDistanceTransformer.class.getName()), PROPS_FILENAME_TEST_MULTI_WRITER, false);\n    cfgIngestionJob.continuousMode = true;\n    cfgIngestionJob.tableType = tableType.name();\n    cfgIngestionJob.configs.add(String.format(\"%s=%d\", SourceConfigs.MAX_UNIQUE_RECORDS_PROP, totalRecords));\n    cfgIngestionJob.configs.add(String.format(\"%s=false\", HoodieCompactionConfig.AUTO_CLEAN.key()));\n    HoodieDeltaStreamer ingestionJob = new HoodieDeltaStreamer(cfgIngestionJob, jsc);\n\n    \r\n    deltaStreamerTestRunner(ingestionJob, cfgIngestionJob, (r) -> {\n      if (tableType.equals(HoodieTableType.MERGE_ON_READ)) {\n        TestHelpers.assertAtleastNDeltaCommits(3, tableBasePath, dfs);\n        TestHelpers.assertAtleastNCompactionCommits(1, tableBasePath, dfs);\n      } else {\n        TestHelpers.assertAtleastNCompactionCommits(3, tableBasePath, dfs);\n      }\n      TestHelpers.assertRecordCount(totalRecords, tableBasePath + \"/*/*.parquet\", sqlContext);\n      TestHelpers.assertDistanceCount(totalRecords, tableBasePath + \"/*/*.parquet\", sqlContext);\n      return true;\n    });\n\n    \r\n    HoodieDeltaStreamer.Config cfgBackfillJob = TestHelpers.makeConfig(tableBasePath, WriteOperationType.UPSERT,\n        Arrays.asList(TripsWithDistanceTransformer.class.getName()), PROPS_FILENAME_TEST_MULTI_WRITER, false);\n    cfgBackfillJob.continuousMode = false;\n    cfgBackfillJob.tableType = tableType.name();\n    HoodieTableMetaClient meta = HoodieTableMetaClient.builder().setConf(dfs.getConf()).setBasePath(tableBasePath).build();\n    HoodieTimeline timeline = meta.getActiveTimeline().getCommitsTimeline().filterCompletedInstants();\n    HoodieCommitMetadata commitMetadataForFirstInstant = HoodieCommitMetadata\n        .fromBytes(timeline.getInstantDetails(timeline.firstInstant().get()).get(), HoodieCommitMetadata.class);\n\n    \r\n    HoodieCommitMetadata commitMetadataForLastInstant = HoodieCommitMetadata\n        .fromBytes(timeline.getInstantDetails(timeline.lastInstant().get()).get(), HoodieCommitMetadata.class);\n    String lastCheckpointBeforeParallelBackfill = commitMetadataForLastInstant.getMetadata(CHECKPOINT_KEY);\n\n    \r\n    props = prepareMultiWriterProps(PROPS_FILENAME_TEST_MULTI_WRITER);\n    props.setProperty(\"hoodie.write.lock.provider\", \"org.apache.hudi.client.transaction.FileSystemBasedLockProviderTestClass\");\n    props.setProperty(\"hoodie.write.lock.filesystem.path\", tableBasePath);\n    props.setProperty(\"hoodie.write.meta.key.prefixes\", CHECKPOINT_KEY);\n    UtilitiesTestBase.Helpers.savePropsToDFS(props, dfs, dfsBasePath + \"/\" + PROPS_FILENAME_TEST_MULTI_WRITER);\n\n    \r\n    \r\n    cfgBackfillJob.checkpoint = commitMetadataForFirstInstant.getMetadata(CHECKPOINT_KEY);\n    cfgBackfillJob.configs.add(String.format(\"%s=%d\", SourceConfigs.MAX_UNIQUE_RECORDS_PROP, totalRecords));\n    cfgBackfillJob.configs.add(String.format(\"%s=false\", HoodieCompactionConfig.AUTO_CLEAN.key()));\n    HoodieDeltaStreamer backfillJob = new HoodieDeltaStreamer(cfgBackfillJob, jsc);\n    backfillJob.sync();\n\n    \r\n    timeline = meta.getActiveTimeline().reload().getCommitsTimeline().filterCompletedInstants();\n    commitMetadataForLastInstant = HoodieCommitMetadata\n        .fromBytes(timeline.getInstantDetails(timeline.lastInstant().get()).get(), HoodieCommitMetadata.class);\n    String lastCheckpointAfterParallelBackfill = commitMetadataForLastInstant.getMetadata(CHECKPOINT_KEY);\n    Assertions.assertEquals(lastCheckpointBeforeParallelBackfill, lastCheckpointAfterParallelBackfill);\n  }\n","date":"2021-08-13 11:31:04","endLine":884,"groupId":"10252","id":8,"instanceNumber":2,"isCurCommit":0,"methodName":"testLatestCheckpointCarryOverWithMultipleWriters","params":"(HoodieTableTypetableType@StringtempDir)","path":"/mnt/clonedata/CloneManagementServer/ManagementServer/consistResult/result-hudi-10-0.7/blobInfo/CC_OUT/blobs/78/48679fedc95d91e43238401a487957dc139909.src","preCode":"  private void testLatestCheckpointCarryOverWithMultipleWriters(HoodieTableType tableType, String tempDir) throws Exception {\n    \r\n    String tableBasePath = dfsBasePath + \"/\" + tempDir;\n    \r\n    TypedProperties props = prepareMultiWriterProps(PROPS_FILENAME_TEST_MULTI_WRITER);\n    props.setProperty(\"hoodie.write.lock.provider\", \"org.apache.hudi.client.transaction.FileSystemBasedLockProviderTestClass\");\n    props.setProperty(\"hoodie.write.lock.filesystem.path\", tableBasePath);\n    UtilitiesTestBase.Helpers.savePropsToDFS(props, dfs, dfsBasePath + \"/\" + PROPS_FILENAME_TEST_MULTI_WRITER);\n    \r\n    int totalRecords = 3000;\n\n    HoodieDeltaStreamer.Config cfgIngestionJob = TestHelpers.makeConfig(tableBasePath, WriteOperationType.UPSERT,\n        Arrays.asList(TripsWithDistanceTransformer.class.getName()), PROPS_FILENAME_TEST_MULTI_WRITER, false);\n    cfgIngestionJob.continuousMode = true;\n    cfgIngestionJob.tableType = tableType.name();\n    cfgIngestionJob.configs.add(String.format(\"%s=%d\", SourceConfigs.MAX_UNIQUE_RECORDS_PROP, totalRecords));\n    cfgIngestionJob.configs.add(String.format(\"%s=false\", HoodieCompactionConfig.AUTO_CLEAN_PROP.key()));\n    HoodieDeltaStreamer ingestionJob = new HoodieDeltaStreamer(cfgIngestionJob, jsc);\n\n    \r\n    deltaStreamerTestRunner(ingestionJob, cfgIngestionJob, (r) -> {\n      if (tableType.equals(HoodieTableType.MERGE_ON_READ)) {\n        TestHelpers.assertAtleastNDeltaCommits(3, tableBasePath, dfs);\n        TestHelpers.assertAtleastNCompactionCommits(1, tableBasePath, dfs);\n      } else {\n        TestHelpers.assertAtleastNCompactionCommits(3, tableBasePath, dfs);\n      }\n      TestHelpers.assertRecordCount(totalRecords, tableBasePath + \"/*/*.parquet\", sqlContext);\n      TestHelpers.assertDistanceCount(totalRecords, tableBasePath + \"/*/*.parquet\", sqlContext);\n      return true;\n    });\n\n    \r\n    HoodieDeltaStreamer.Config cfgBackfillJob = TestHelpers.makeConfig(tableBasePath, WriteOperationType.UPSERT,\n        Arrays.asList(TripsWithDistanceTransformer.class.getName()), PROPS_FILENAME_TEST_MULTI_WRITER, false);\n    cfgBackfillJob.continuousMode = false;\n    cfgBackfillJob.tableType = tableType.name();\n    HoodieTableMetaClient meta = HoodieTableMetaClient.builder().setConf(dfs.getConf()).setBasePath(tableBasePath).build();\n    HoodieTimeline timeline = meta.getActiveTimeline().getCommitsTimeline().filterCompletedInstants();\n    HoodieCommitMetadata commitMetadataForFirstInstant = HoodieCommitMetadata\n        .fromBytes(timeline.getInstantDetails(timeline.firstInstant().get()).get(), HoodieCommitMetadata.class);\n\n    \r\n    HoodieCommitMetadata commitMetadataForLastInstant = HoodieCommitMetadata\n        .fromBytes(timeline.getInstantDetails(timeline.lastInstant().get()).get(), HoodieCommitMetadata.class);\n    String lastCheckpointBeforeParallelBackfill = commitMetadataForLastInstant.getMetadata(CHECKPOINT_KEY);\n\n    \r\n    props = prepareMultiWriterProps(PROPS_FILENAME_TEST_MULTI_WRITER);\n    props.setProperty(\"hoodie.write.lock.provider\", \"org.apache.hudi.client.transaction.FileSystemBasedLockProviderTestClass\");\n    props.setProperty(\"hoodie.write.lock.filesystem.path\", tableBasePath);\n    props.setProperty(\"hoodie.write.meta.key.prefixes\", CHECKPOINT_KEY);\n    UtilitiesTestBase.Helpers.savePropsToDFS(props, dfs, dfsBasePath + \"/\" + PROPS_FILENAME_TEST_MULTI_WRITER);\n\n    \r\n    \r\n    cfgBackfillJob.checkpoint = commitMetadataForFirstInstant.getMetadata(CHECKPOINT_KEY);\n    cfgBackfillJob.configs.add(String.format(\"%s=%d\", SourceConfigs.MAX_UNIQUE_RECORDS_PROP, totalRecords));\n    cfgBackfillJob.configs.add(String.format(\"%s=false\", HoodieCompactionConfig.AUTO_CLEAN_PROP.key()));\n    HoodieDeltaStreamer backfillJob = new HoodieDeltaStreamer(cfgBackfillJob, jsc);\n    backfillJob.sync();\n\n    \r\n    timeline = meta.getActiveTimeline().reload().getCommitsTimeline().filterCompletedInstants();\n    commitMetadataForLastInstant = HoodieCommitMetadata\n        .fromBytes(timeline.getInstantDetails(timeline.lastInstant().get()).get(), HoodieCommitMetadata.class);\n    String lastCheckpointAfterParallelBackfill = commitMetadataForLastInstant.getMetadata(CHECKPOINT_KEY);\n    Assertions.assertEquals(lastCheckpointBeforeParallelBackfill, lastCheckpointAfterParallelBackfill);\n  }\n","realPath":"hudi-utilities/src/test/java/org/apache/hudi/utilities/functional/TestHoodieDeltaStreamer.java","repoName":"hudi","snippetEndLine":0,"snippetStartLine":0,"startLine":816,"status":"M"}],"commitId":"0544d70d8f4204f4e5edfe9144c17f1ed221eb7c","commitMessage":"@@@[MINOR] Deprecate older configs (#3464)\n\nRename and deprecate props in HoodieWriteConfig\n\nRename and deprecate older props","date":"2021-08-13 11:31:04","modifiedFileCount":"38","status":"M","submitter":"Sagar Sumit"}]
