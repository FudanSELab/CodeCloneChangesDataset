[{"authorTime":"2021-07-27 10:58:23","codes":[{"authorDate":"2021-04-21 20:07:27","commitOrder":2,"curCode":"  void testWriteNonPartitionedTable(ExecMode execMode) {\n    TableEnvironment tableEnv = execMode == ExecMode.BATCH ? batchTableEnv : streamTableEnv;\n    String hoodieTableDDL = \"create table t1(\\n\"\n        + \"  uuid varchar(20),\\n\"\n        + \"  name varchar(10),\\n\"\n        + \"  age int,\\n\"\n        + \"  ts timestamp(3),\\n\"\n        + \"  `partition` varchar(20),\\n\"\n        + \"  PRIMARY KEY(uuid) NOT ENFORCED\\n\"\n        + \")\\n\"\n        + \"with (\\n\"\n        + \"  'connector' = 'hudi',\\n\"\n        + \"  'path' = '\" + tempFile.getAbsolutePath() + \"'\\n\"\n        + \")\";\n    tableEnv.executeSql(hoodieTableDDL);\n\n    final String insertInto1 = \"insert into t1 values\\n\"\n        + \"('id1','Danny',23,TIMESTAMP '1970-01-01 00:00:01','par1')\";\n\n    execInsertSql(tableEnv, insertInto1);\n\n    final String insertInto2 = \"insert into t1 values\\n\"\n        + \"('id1','Stephen',33,TIMESTAMP '1970-01-01 00:00:02','par2'),\\n\"\n        + \"('id1','Julian',53,TIMESTAMP '1970-01-01 00:00:03','par3'),\\n\"\n        + \"('id1','Fabian',31,TIMESTAMP '1970-01-01 00:00:04','par4'),\\n\"\n        + \"('id1','Sophia',18,TIMESTAMP '1970-01-01 00:00:05','par5')\";\n\n    execInsertSql(tableEnv, insertInto2);\n\n    List<Row> result = CollectionUtil.iterableToList(\n        () -> tableEnv.sqlQuery(\"select * from t1\").execute().collect());\n    assertRowsEquals(result, \"[id1,Sophia,18,1970-01-01T00:00:05,par5]\");\n  }\n","date":"2021-04-21 20:07:27","endLine":389,"groupId":"5601","id":1,"instanceNumber":1,"isCurCommit":0,"methodName":"testWriteNonPartitionedTable","params":"(ExecModeexecMode)","path":"/mnt/clonedata/CloneManagementServer/ManagementServer/consistResult/result-hudi-10-0.7/blobInfo/CC_OUT/blobs/fe/652c5ffe74fd8523ceeec8845065e66162b189.src","preCode":"  void testWriteNonPartitionedTable(ExecMode execMode) {\n    TableEnvironment tableEnv = execMode == ExecMode.BATCH ? batchTableEnv : streamTableEnv;\n    String hoodieTableDDL = \"create table t1(\\n\"\n        + \"  uuid varchar(20),\\n\"\n        + \"  name varchar(10),\\n\"\n        + \"  age int,\\n\"\n        + \"  ts timestamp(3),\\n\"\n        + \"  `partition` varchar(20),\\n\"\n        + \"  PRIMARY KEY(uuid) NOT ENFORCED\\n\"\n        + \")\\n\"\n        + \"with (\\n\"\n        + \"  'connector' = 'hudi',\\n\"\n        + \"  'path' = '\" + tempFile.getAbsolutePath() + \"'\\n\"\n        + \")\";\n    tableEnv.executeSql(hoodieTableDDL);\n\n    final String insertInto1 = \"insert into t1 values\\n\"\n        + \"('id1','Danny',23,TIMESTAMP '1970-01-01 00:00:01','par1')\";\n\n    execInsertSql(tableEnv, insertInto1);\n\n    final String insertInto2 = \"insert into t1 values\\n\"\n        + \"('id1','Stephen',33,TIMESTAMP '1970-01-01 00:00:02','par2'),\\n\"\n        + \"('id1','Julian',53,TIMESTAMP '1970-01-01 00:00:03','par3'),\\n\"\n        + \"('id1','Fabian',31,TIMESTAMP '1970-01-01 00:00:04','par4'),\\n\"\n        + \"('id1','Sophia',18,TIMESTAMP '1970-01-01 00:00:05','par5')\";\n\n    execInsertSql(tableEnv, insertInto2);\n\n    List<Row> result = CollectionUtil.iterableToList(\n        () -> tableEnv.sqlQuery(\"select * from t1\").execute().collect());\n    assertRowsEquals(result, \"[id1,Sophia,18,1970-01-01T00:00:05,par5]\");\n  }\n","realPath":"hudi-flink/src/test/java/org/apache/hudi/table/HoodieDataSourceITCase.java","repoName":"hudi","snippetEndLine":0,"snippetStartLine":0,"startLine":357,"status":"NB"},{"authorDate":"2021-07-27 10:58:23","commitOrder":2,"curCode":"  void testBulkInsertNonPartitionedTable() {\n    TableEnvironment tableEnv = batchTableEnv;\n    String hoodieTableDDL = \"create table t1(\\n\"\n        + \"  uuid varchar(20),\\n\"\n        + \"  name varchar(10),\\n\"\n        + \"  age int,\\n\"\n        + \"  ts timestamp(3),\\n\"\n        + \"  `partition` varchar(20),\\n\"\n        + \"  PRIMARY KEY(uuid) NOT ENFORCED\\n\"\n        + \")\\n\"\n        + \"with (\\n\"\n        + \"  'connector' = 'hudi',\\n\"\n        + \"  'path' = '\" + tempFile.getAbsolutePath() + \"',\\n\"\n        + \"  'write.operation' = 'bulk_insert'\\n\"\n        + \")\";\n    tableEnv.executeSql(hoodieTableDDL);\n\n    final String insertInto1 = \"insert into t1 values\\n\"\n        + \"('id1','Danny',23,TIMESTAMP '1970-01-01 00:00:01','par1')\";\n\n    execInsertSql(tableEnv, insertInto1);\n\n    final String insertInto2 = \"insert into t1 values\\n\"\n        + \"('id1','Stephen',33,TIMESTAMP '1970-01-01 00:00:02','par2'),\\n\"\n        + \"('id1','Julian',53,TIMESTAMP '1970-01-01 00:00:03','par3'),\\n\"\n        + \"('id1','Fabian',31,TIMESTAMP '1970-01-01 00:00:04','par4'),\\n\"\n        + \"('id1','Sophia',18,TIMESTAMP '1970-01-01 00:00:05','par5')\";\n\n    execInsertSql(tableEnv, insertInto2);\n\n    List<Row> result = CollectionUtil.iterableToList(\n        () -> tableEnv.sqlQuery(\"select * from t1\").execute().collect());\n    assertRowsEquals(result, \"[\"\n        + \"id1,Danny,23,1970-01-01T00:00:01,par1, \"\n        + \"id1,Stephen,33,1970-01-01T00:00:02,par2, \"\n        + \"id1,Julian,53,1970-01-01T00:00:03,par3, \"\n        + \"id1,Fabian,31,1970-01-01T00:00:04,par4, \"\n        + \"id1,Sophia,18,1970-01-01T00:00:05,par5]\", 3);\n  }\n","date":"2021-07-27 10:58:23","endLine":633,"groupId":"5603","id":2,"instanceNumber":2,"isCurCommit":0,"methodName":"testBulkInsertNonPartitionedTable","params":"()","path":"/mnt/clonedata/CloneManagementServer/ManagementServer/consistResult/result-hudi-10-0.7/blobInfo/CC_OUT/blobs/1d/dcb740b9363accc9e5d864ebda7b92b19b3160.src","preCode":"  void testBulkInsertNonPartitionedTable() {\n    TableEnvironment tableEnv = batchTableEnv;\n    String hoodieTableDDL = \"create table t1(\\n\"\n        + \"  uuid varchar(20),\\n\"\n        + \"  name varchar(10),\\n\"\n        + \"  age int,\\n\"\n        + \"  ts timestamp(3),\\n\"\n        + \"  `partition` varchar(20),\\n\"\n        + \"  PRIMARY KEY(uuid) NOT ENFORCED\\n\"\n        + \")\\n\"\n        + \"with (\\n\"\n        + \"  'connector' = 'hudi',\\n\"\n        + \"  'path' = '\" + tempFile.getAbsolutePath() + \"',\\n\"\n        + \"  'write.operation' = 'bulk_insert'\\n\"\n        + \")\";\n    tableEnv.executeSql(hoodieTableDDL);\n\n    final String insertInto1 = \"insert into t1 values\\n\"\n        + \"('id1','Danny',23,TIMESTAMP '1970-01-01 00:00:01','par1')\";\n\n    execInsertSql(tableEnv, insertInto1);\n\n    final String insertInto2 = \"insert into t1 values\\n\"\n        + \"('id1','Stephen',33,TIMESTAMP '1970-01-01 00:00:02','par2'),\\n\"\n        + \"('id1','Julian',53,TIMESTAMP '1970-01-01 00:00:03','par3'),\\n\"\n        + \"('id1','Fabian',31,TIMESTAMP '1970-01-01 00:00:04','par4'),\\n\"\n        + \"('id1','Sophia',18,TIMESTAMP '1970-01-01 00:00:05','par5')\";\n\n    execInsertSql(tableEnv, insertInto2);\n\n    List<Row> result = CollectionUtil.iterableToList(\n        () -> tableEnv.sqlQuery(\"select * from t1\").execute().collect());\n    assertRowsEquals(result, \"[\"\n        + \"id1,Danny,23,1970-01-01T00:00:01,par1, \"\n        + \"id1,Stephen,33,1970-01-01T00:00:02,par2, \"\n        + \"id1,Julian,53,1970-01-01T00:00:03,par3, \"\n        + \"id1,Fabian,31,1970-01-01T00:00:04,par4, \"\n        + \"id1,Sophia,18,1970-01-01T00:00:05,par5]\", 3);\n  }\n","realPath":"hudi-flink/src/test/java/org/apache/hudi/table/HoodieDataSourceITCase.java","repoName":"hudi","snippetEndLine":0,"snippetStartLine":0,"startLine":595,"status":"B"}],"commitId":"9d2a65a6a6ff9add81411147f1cddd03f7c08e6c","commitMessage":"@@@[HUDI-2209] Bulk insert for flink writer (#3334)\n\n","date":"2021-07-27 10:58:23","modifiedFileCount":"8","status":"M","submitter":"Danny Chan"},{"authorTime":"2021-08-16 18:14:05","codes":[{"authorDate":"2021-08-16 18:14:05","commitOrder":3,"curCode":"  void testWriteNonPartitionedTable(ExecMode execMode) {\n    TableEnvironment tableEnv = execMode == ExecMode.BATCH ? batchTableEnv : streamTableEnv;\n    String hoodieTableDDL = \"create table t1(\\n\"\n        + \"  uuid varchar(20),\\n\"\n        + \"  name varchar(10),\\n\"\n        + \"  age int,\\n\"\n        + \"  ts timestamp(3),\\n\"\n        + \"  `partition` varchar(20),\\n\"\n        + \"  PRIMARY KEY(uuid) NOT ENFORCED\\n\"\n        + \")\\n\"\n        + \"with (\\n\"\n        + \"  'connector' = 'hudi',\\n\"\n        + \"  'path' = '\" + tempFile.getAbsolutePath() + \"'\\n\"\n        + \")\";\n    tableEnv.executeSql(hoodieTableDDL);\n\n    final String insertInto1 = \"insert into t1 values\\n\"\n        + \"('id1','Danny',23,TIMESTAMP '1970-01-01 00:00:01','par1')\";\n\n    execInsertSql(tableEnv, insertInto1);\n\n    final String insertInto2 = \"insert into t1 values\\n\"\n        + \"('id1','Stephen',33,TIMESTAMP '1970-01-01 00:00:02','par2'),\\n\"\n        + \"('id1','Julian',53,TIMESTAMP '1970-01-01 00:00:03','par3'),\\n\"\n        + \"('id1','Fabian',31,TIMESTAMP '1970-01-01 00:00:04','par4'),\\n\"\n        + \"('id1','Sophia',18,TIMESTAMP '1970-01-01 00:00:05','par5')\";\n\n    execInsertSql(tableEnv, insertInto2);\n\n    List<Row> result = CollectionUtil.iterableToList(\n        () -> tableEnv.sqlQuery(\"select * from t1\").execute().collect());\n    assertRowsEquals(result, \"[+I[id1, Sophia, 18, 1970-01-01T00:00:05, par5]]\");\n  }\n","date":"2021-08-16 18:14:05","endLine":469,"groupId":"5601","id":3,"instanceNumber":1,"isCurCommit":0,"methodName":"testWriteNonPartitionedTable","params":"(ExecModeexecMode)","path":"/mnt/clonedata/CloneManagementServer/ManagementServer/consistResult/result-hudi-10-0.7/blobInfo/CC_OUT/blobs/07/64f5586ea801d7feba77591330d849b66102fc.src","preCode":"  void testWriteNonPartitionedTable(ExecMode execMode) {\n    TableEnvironment tableEnv = execMode == ExecMode.BATCH ? batchTableEnv : streamTableEnv;\n    String hoodieTableDDL = \"create table t1(\\n\"\n        + \"  uuid varchar(20),\\n\"\n        + \"  name varchar(10),\\n\"\n        + \"  age int,\\n\"\n        + \"  ts timestamp(3),\\n\"\n        + \"  `partition` varchar(20),\\n\"\n        + \"  PRIMARY KEY(uuid) NOT ENFORCED\\n\"\n        + \")\\n\"\n        + \"with (\\n\"\n        + \"  'connector' = 'hudi',\\n\"\n        + \"  'path' = '\" + tempFile.getAbsolutePath() + \"'\\n\"\n        + \")\";\n    tableEnv.executeSql(hoodieTableDDL);\n\n    final String insertInto1 = \"insert into t1 values\\n\"\n        + \"('id1','Danny',23,TIMESTAMP '1970-01-01 00:00:01','par1')\";\n\n    execInsertSql(tableEnv, insertInto1);\n\n    final String insertInto2 = \"insert into t1 values\\n\"\n        + \"('id1','Stephen',33,TIMESTAMP '1970-01-01 00:00:02','par2'),\\n\"\n        + \"('id1','Julian',53,TIMESTAMP '1970-01-01 00:00:03','par3'),\\n\"\n        + \"('id1','Fabian',31,TIMESTAMP '1970-01-01 00:00:04','par4'),\\n\"\n        + \"('id1','Sophia',18,TIMESTAMP '1970-01-01 00:00:05','par5')\";\n\n    execInsertSql(tableEnv, insertInto2);\n\n    List<Row> result = CollectionUtil.iterableToList(\n        () -> tableEnv.sqlQuery(\"select * from t1\").execute().collect());\n    assertRowsEquals(result, \"[id1,Sophia,18,1970-01-01T00:00:05,par5]\");\n  }\n","realPath":"hudi-flink/src/test/java/org/apache/hudi/table/HoodieDataSourceITCase.java","repoName":"hudi","snippetEndLine":0,"snippetStartLine":0,"startLine":437,"status":"M"},{"authorDate":"2021-08-16 18:14:05","commitOrder":3,"curCode":"  void testBulkInsertNonPartitionedTable() {\n    TableEnvironment tableEnv = batchTableEnv;\n    String hoodieTableDDL = \"create table t1(\\n\"\n        + \"  uuid varchar(20),\\n\"\n        + \"  name varchar(10),\\n\"\n        + \"  age int,\\n\"\n        + \"  ts timestamp(3),\\n\"\n        + \"  `partition` varchar(20),\\n\"\n        + \"  PRIMARY KEY(uuid) NOT ENFORCED\\n\"\n        + \")\\n\"\n        + \"with (\\n\"\n        + \"  'connector' = 'hudi',\\n\"\n        + \"  'path' = '\" + tempFile.getAbsolutePath() + \"',\\n\"\n        + \"  'write.operation' = 'bulk_insert'\\n\"\n        + \")\";\n    tableEnv.executeSql(hoodieTableDDL);\n\n    final String insertInto1 = \"insert into t1 values\\n\"\n        + \"('id1','Danny',23,TIMESTAMP '1970-01-01 00:00:01','par1')\";\n\n    execInsertSql(tableEnv, insertInto1);\n\n    final String insertInto2 = \"insert into t1 values\\n\"\n        + \"('id1','Stephen',33,TIMESTAMP '1970-01-01 00:00:02','par2'),\\n\"\n        + \"('id1','Julian',53,TIMESTAMP '1970-01-01 00:00:03','par3'),\\n\"\n        + \"('id1','Fabian',31,TIMESTAMP '1970-01-01 00:00:04','par4'),\\n\"\n        + \"('id1','Sophia',18,TIMESTAMP '1970-01-01 00:00:05','par5')\";\n\n    execInsertSql(tableEnv, insertInto2);\n\n    List<Row> result = CollectionUtil.iterableToList(\n        () -> tableEnv.sqlQuery(\"select * from t1\").execute().collect());\n    assertRowsEquals(result, \"[\"\n        + \"+I[id1, Danny, 23, 1970-01-01T00:00:01, par1], \"\n        + \"+I[id1, Stephen, 33, 1970-01-01T00:00:02, par2], \"\n        + \"+I[id1, Julian, 53, 1970-01-01T00:00:03, par3], \"\n        + \"+I[id1, Fabian, 31, 1970-01-01T00:00:04, par4], \"\n        + \"+I[id1, Sophia, 18, 1970-01-01T00:00:05, par5]]\", 3);\n  }\n","date":"2021-08-16 18:14:05","endLine":666,"groupId":"5603","id":4,"instanceNumber":2,"isCurCommit":0,"methodName":"testBulkInsertNonPartitionedTable","params":"()","path":"/mnt/clonedata/CloneManagementServer/ManagementServer/consistResult/result-hudi-10-0.7/blobInfo/CC_OUT/blobs/07/64f5586ea801d7feba77591330d849b66102fc.src","preCode":"  void testBulkInsertNonPartitionedTable() {\n    TableEnvironment tableEnv = batchTableEnv;\n    String hoodieTableDDL = \"create table t1(\\n\"\n        + \"  uuid varchar(20),\\n\"\n        + \"  name varchar(10),\\n\"\n        + \"  age int,\\n\"\n        + \"  ts timestamp(3),\\n\"\n        + \"  `partition` varchar(20),\\n\"\n        + \"  PRIMARY KEY(uuid) NOT ENFORCED\\n\"\n        + \")\\n\"\n        + \"with (\\n\"\n        + \"  'connector' = 'hudi',\\n\"\n        + \"  'path' = '\" + tempFile.getAbsolutePath() + \"',\\n\"\n        + \"  'write.operation' = 'bulk_insert'\\n\"\n        + \")\";\n    tableEnv.executeSql(hoodieTableDDL);\n\n    final String insertInto1 = \"insert into t1 values\\n\"\n        + \"('id1','Danny',23,TIMESTAMP '1970-01-01 00:00:01','par1')\";\n\n    execInsertSql(tableEnv, insertInto1);\n\n    final String insertInto2 = \"insert into t1 values\\n\"\n        + \"('id1','Stephen',33,TIMESTAMP '1970-01-01 00:00:02','par2'),\\n\"\n        + \"('id1','Julian',53,TIMESTAMP '1970-01-01 00:00:03','par3'),\\n\"\n        + \"('id1','Fabian',31,TIMESTAMP '1970-01-01 00:00:04','par4'),\\n\"\n        + \"('id1','Sophia',18,TIMESTAMP '1970-01-01 00:00:05','par5')\";\n\n    execInsertSql(tableEnv, insertInto2);\n\n    List<Row> result = CollectionUtil.iterableToList(\n        () -> tableEnv.sqlQuery(\"select * from t1\").execute().collect());\n    assertRowsEquals(result, \"[\"\n        + \"id1,Danny,23,1970-01-01T00:00:01,par1, \"\n        + \"id1,Stephen,33,1970-01-01T00:00:02,par2, \"\n        + \"id1,Julian,53,1970-01-01T00:00:03,par3, \"\n        + \"id1,Fabian,31,1970-01-01T00:00:04,par4, \"\n        + \"id1,Sophia,18,1970-01-01T00:00:05,par5]\", 3);\n  }\n","realPath":"hudi-flink/src/test/java/org/apache/hudi/table/HoodieDataSourceITCase.java","repoName":"hudi","snippetEndLine":0,"snippetStartLine":0,"startLine":628,"status":"M"}],"commitId":"66f951322a3872073b86896fa5c10b51a0f6e4ab","commitMessage":"@@@[HUDI-2191] Bump flink version to 1.13.1 (#3291)\n\n","date":"2021-08-16 18:14:05","modifiedFileCount":"17","status":"M","submitter":"Danny Chan"},{"authorTime":"2021-08-19 17:15:26","codes":[{"authorDate":"2021-08-19 17:15:26","commitOrder":4,"curCode":"  void testWriteNonPartitionedTable(ExecMode execMode) {\n    TableEnvironment tableEnv = execMode == ExecMode.BATCH ? batchTableEnv : streamTableEnv;\n    Map<String, String> options = new HashMap<>();\n    options.put(FlinkOptions.PATH.key(), tempFile.getAbsolutePath());\n    String hoodieTableDDL = TestConfigurations.getCreateHoodieTableDDL(\"t1\", options, false);\n    tableEnv.executeSql(hoodieTableDDL);\n\n    final String insertInto1 = \"insert into t1 values\\n\"\n        + \"('id1','Danny',23,TIMESTAMP '1970-01-01 00:00:01','par1')\";\n\n    execInsertSql(tableEnv, insertInto1);\n\n    final String insertInto2 = \"insert into t1 values\\n\"\n        + \"('id1','Stephen',33,TIMESTAMP '1970-01-01 00:00:02','par2'),\\n\"\n        + \"('id1','Julian',53,TIMESTAMP '1970-01-01 00:00:03','par3'),\\n\"\n        + \"('id1','Fabian',31,TIMESTAMP '1970-01-01 00:00:04','par4'),\\n\"\n        + \"('id1','Sophia',18,TIMESTAMP '1970-01-01 00:00:05','par5')\";\n\n    execInsertSql(tableEnv, insertInto2);\n\n    List<Row> result = CollectionUtil.iterableToList(\n        () -> tableEnv.sqlQuery(\"select * from t1\").execute().collect());\n    assertRowsEquals(result, \"[+I[id1, Sophia, 18, 1970-01-01T00:00:05, par5]]\");\n  }\n","date":"2021-08-19 17:15:26","endLine":510,"groupId":"0","id":5,"instanceNumber":1,"isCurCommit":0,"methodName":"testWriteNonPartitionedTable","params":"(ExecModeexecMode)","path":"/mnt/clonedata/CloneManagementServer/ManagementServer/consistResult/result-hudi-10-0.7/blobInfo/CC_OUT/blobs/99/effba3e6f09d7321e176a048413d962b7d0809.src","preCode":"  void testWriteNonPartitionedTable(ExecMode execMode) {\n    TableEnvironment tableEnv = execMode == ExecMode.BATCH ? batchTableEnv : streamTableEnv;\n    String hoodieTableDDL = \"create table t1(\\n\"\n        + \"  uuid varchar(20),\\n\"\n        + \"  name varchar(10),\\n\"\n        + \"  age int,\\n\"\n        + \"  ts timestamp(3),\\n\"\n        + \"  `partition` varchar(20),\\n\"\n        + \"  PRIMARY KEY(uuid) NOT ENFORCED\\n\"\n        + \")\\n\"\n        + \"with (\\n\"\n        + \"  'connector' = 'hudi',\\n\"\n        + \"  'path' = '\" + tempFile.getAbsolutePath() + \"'\\n\"\n        + \")\";\n    tableEnv.executeSql(hoodieTableDDL);\n\n    final String insertInto1 = \"insert into t1 values\\n\"\n        + \"('id1','Danny',23,TIMESTAMP '1970-01-01 00:00:01','par1')\";\n\n    execInsertSql(tableEnv, insertInto1);\n\n    final String insertInto2 = \"insert into t1 values\\n\"\n        + \"('id1','Stephen',33,TIMESTAMP '1970-01-01 00:00:02','par2'),\\n\"\n        + \"('id1','Julian',53,TIMESTAMP '1970-01-01 00:00:03','par3'),\\n\"\n        + \"('id1','Fabian',31,TIMESTAMP '1970-01-01 00:00:04','par4'),\\n\"\n        + \"('id1','Sophia',18,TIMESTAMP '1970-01-01 00:00:05','par5')\";\n\n    execInsertSql(tableEnv, insertInto2);\n\n    List<Row> result = CollectionUtil.iterableToList(\n        () -> tableEnv.sqlQuery(\"select * from t1\").execute().collect());\n    assertRowsEquals(result, \"[+I[id1, Sophia, 18, 1970-01-01T00:00:05, par5]]\");\n  }\n","realPath":"hudi-flink/src/test/java/org/apache/hudi/table/HoodieDataSourceITCase.java","repoName":"hudi","snippetEndLine":0,"snippetStartLine":0,"startLine":487,"status":"M"},{"authorDate":"2021-08-19 17:15:26","commitOrder":4,"curCode":"  void testBulkInsertNonPartitionedTable() {\n    TableEnvironment tableEnv = batchTableEnv;\n    Map<String, String> options = new HashMap<>();\n    options.put(FlinkOptions.PATH.key(), tempFile.getAbsolutePath());\n    options.put(FlinkOptions.OPERATION.key(), \"bulk_insert\");\n    String hoodieTableDDL = TestConfigurations.getCreateHoodieTableDDL(\"t1\", options, false);\n    tableEnv.executeSql(hoodieTableDDL);\n\n    final String insertInto1 = \"insert into t1 values\\n\"\n        + \"('id1','Danny',23,TIMESTAMP '1970-01-01 00:00:01','par1')\";\n\n    execInsertSql(tableEnv, insertInto1);\n\n    final String insertInto2 = \"insert into t1 values\\n\"\n        + \"('id1','Stephen',33,TIMESTAMP '1970-01-01 00:00:02','par2'),\\n\"\n        + \"('id1','Julian',53,TIMESTAMP '1970-01-01 00:00:03','par3'),\\n\"\n        + \"('id1','Fabian',31,TIMESTAMP '1970-01-01 00:00:04','par4'),\\n\"\n        + \"('id1','Sophia',18,TIMESTAMP '1970-01-01 00:00:05','par5')\";\n\n    execInsertSql(tableEnv, insertInto2);\n\n    List<Row> result = CollectionUtil.iterableToList(\n        () -> tableEnv.sqlQuery(\"select * from t1\").execute().collect());\n    assertRowsEquals(result, \"[\"\n        + \"+I[id1, Danny, 23, 1970-01-01T00:00:01, par1], \"\n        + \"+I[id1, Stephen, 33, 1970-01-01T00:00:02, par2], \"\n        + \"+I[id1, Julian, 53, 1970-01-01T00:00:03, par3], \"\n        + \"+I[id1, Fabian, 31, 1970-01-01T00:00:04, par4], \"\n        + \"+I[id1, Sophia, 18, 1970-01-01T00:00:05, par5]]\", 3);\n  }\n","date":"2021-08-19 17:15:26","endLine":698,"groupId":"0","id":6,"instanceNumber":2,"isCurCommit":0,"methodName":"testBulkInsertNonPartitionedTable","params":"()","path":"/mnt/clonedata/CloneManagementServer/ManagementServer/consistResult/result-hudi-10-0.7/blobInfo/CC_OUT/blobs/99/effba3e6f09d7321e176a048413d962b7d0809.src","preCode":"  void testBulkInsertNonPartitionedTable() {\n    TableEnvironment tableEnv = batchTableEnv;\n    String hoodieTableDDL = \"create table t1(\\n\"\n        + \"  uuid varchar(20),\\n\"\n        + \"  name varchar(10),\\n\"\n        + \"  age int,\\n\"\n        + \"  ts timestamp(3),\\n\"\n        + \"  `partition` varchar(20),\\n\"\n        + \"  PRIMARY KEY(uuid) NOT ENFORCED\\n\"\n        + \")\\n\"\n        + \"with (\\n\"\n        + \"  'connector' = 'hudi',\\n\"\n        + \"  'path' = '\" + tempFile.getAbsolutePath() + \"',\\n\"\n        + \"  'write.operation' = 'bulk_insert'\\n\"\n        + \")\";\n    tableEnv.executeSql(hoodieTableDDL);\n\n    final String insertInto1 = \"insert into t1 values\\n\"\n        + \"('id1','Danny',23,TIMESTAMP '1970-01-01 00:00:01','par1')\";\n\n    execInsertSql(tableEnv, insertInto1);\n\n    final String insertInto2 = \"insert into t1 values\\n\"\n        + \"('id1','Stephen',33,TIMESTAMP '1970-01-01 00:00:02','par2'),\\n\"\n        + \"('id1','Julian',53,TIMESTAMP '1970-01-01 00:00:03','par3'),\\n\"\n        + \"('id1','Fabian',31,TIMESTAMP '1970-01-01 00:00:04','par4'),\\n\"\n        + \"('id1','Sophia',18,TIMESTAMP '1970-01-01 00:00:05','par5')\";\n\n    execInsertSql(tableEnv, insertInto2);\n\n    List<Row> result = CollectionUtil.iterableToList(\n        () -> tableEnv.sqlQuery(\"select * from t1\").execute().collect());\n    assertRowsEquals(result, \"[\"\n        + \"+I[id1, Danny, 23, 1970-01-01T00:00:01, par1], \"\n        + \"+I[id1, Stephen, 33, 1970-01-01T00:00:02, par2], \"\n        + \"+I[id1, Julian, 53, 1970-01-01T00:00:03, par3], \"\n        + \"+I[id1, Fabian, 31, 1970-01-01T00:00:04, par4], \"\n        + \"+I[id1, Sophia, 18, 1970-01-01T00:00:05, par5]]\", 3);\n  }\n","realPath":"hudi-flink/src/test/java/org/apache/hudi/table/HoodieDataSourceITCase.java","repoName":"hudi","snippetEndLine":0,"snippetStartLine":0,"startLine":669,"status":"M"}],"commitId":"1fed44af84b4726d40c57f1dad012c8e4a510f91","commitMessage":"@@@[HUDI-2316] Support Flink batch upsert (#3494)\n\n","date":"2021-08-19 17:15:26","modifiedFileCount":"6","status":"M","submitter":"swuferhong"},{"authorTime":"2021-08-19 23:21:20","codes":[{"authorDate":"2021-08-19 23:21:20","commitOrder":5,"curCode":"  void testWriteNonPartitionedTable(ExecMode execMode) {\n    TableEnvironment tableEnv = execMode == ExecMode.BATCH ? batchTableEnv : streamTableEnv;\n    String hoodieTableDDL = sql(\"t1\")\n        .option(FlinkOptions.PATH, tempFile.getAbsolutePath())\n        .withPartition(false)\n        .end();\n    tableEnv.executeSql(hoodieTableDDL);\n\n    final String insertInto1 = \"insert into t1 values\\n\"\n        + \"('id1','Danny',23,TIMESTAMP '1970-01-01 00:00:01','par1')\";\n\n    execInsertSql(tableEnv, insertInto1);\n\n    final String insertInto2 = \"insert into t1 values\\n\"\n        + \"('id1','Stephen',33,TIMESTAMP '1970-01-01 00:00:02','par2'),\\n\"\n        + \"('id1','Julian',53,TIMESTAMP '1970-01-01 00:00:03','par3'),\\n\"\n        + \"('id1','Fabian',31,TIMESTAMP '1970-01-01 00:00:04','par4'),\\n\"\n        + \"('id1','Sophia',18,TIMESTAMP '1970-01-01 00:00:05','par5')\";\n\n    execInsertSql(tableEnv, insertInto2);\n\n    List<Row> result = CollectionUtil.iterableToList(\n        () -> tableEnv.sqlQuery(\"select * from t1\").execute().collect());\n    assertRowsEquals(result, \"[+I[id1, Sophia, 18, 1970-01-01T00:00:05, par5]]\");\n  }\n","date":"2021-08-19 23:21:20","endLine":517,"groupId":"2976","id":7,"instanceNumber":1,"isCurCommit":0,"methodName":"testWriteNonPartitionedTable","params":"(ExecModeexecMode)","path":"/mnt/clonedata/CloneManagementServer/ManagementServer/consistResult/result-hudi-10-0.7/blobInfo/CC_OUT/blobs/9e/ffdcc8c67f59ad51cfd9a0e5ed56ecd41b530e.src","preCode":"  void testWriteNonPartitionedTable(ExecMode execMode) {\n    TableEnvironment tableEnv = execMode == ExecMode.BATCH ? batchTableEnv : streamTableEnv;\n    Map<String, String> options = new HashMap<>();\n    options.put(FlinkOptions.PATH.key(), tempFile.getAbsolutePath());\n    String hoodieTableDDL = TestConfigurations.getCreateHoodieTableDDL(\"t1\", options, false);\n    tableEnv.executeSql(hoodieTableDDL);\n\n    final String insertInto1 = \"insert into t1 values\\n\"\n        + \"('id1','Danny',23,TIMESTAMP '1970-01-01 00:00:01','par1')\";\n\n    execInsertSql(tableEnv, insertInto1);\n\n    final String insertInto2 = \"insert into t1 values\\n\"\n        + \"('id1','Stephen',33,TIMESTAMP '1970-01-01 00:00:02','par2'),\\n\"\n        + \"('id1','Julian',53,TIMESTAMP '1970-01-01 00:00:03','par3'),\\n\"\n        + \"('id1','Fabian',31,TIMESTAMP '1970-01-01 00:00:04','par4'),\\n\"\n        + \"('id1','Sophia',18,TIMESTAMP '1970-01-01 00:00:05','par5')\";\n\n    execInsertSql(tableEnv, insertInto2);\n\n    List<Row> result = CollectionUtil.iterableToList(\n        () -> tableEnv.sqlQuery(\"select * from t1\").execute().collect());\n    assertRowsEquals(result, \"[+I[id1, Sophia, 18, 1970-01-01T00:00:05, par5]]\");\n  }\n","realPath":"hudi-flink/src/test/java/org/apache/hudi/table/HoodieDataSourceITCase.java","repoName":"hudi","snippetEndLine":0,"snippetStartLine":0,"startLine":493,"status":"M"},{"authorDate":"2021-08-19 23:21:20","commitOrder":5,"curCode":"  void testBulkInsertNonPartitionedTable() {\n    TableEnvironment tableEnv = batchTableEnv;\n    String hoodieTableDDL = sql(\"t1\")\n        .option(FlinkOptions.PATH, tempFile.getAbsolutePath())\n        .option(FlinkOptions.OPERATION, \"bulk_insert\")\n        .withPartition(false)\n        .end();\n    tableEnv.executeSql(hoodieTableDDL);\n\n    final String insertInto1 = \"insert into t1 values\\n\"\n        + \"('id1','Danny',23,TIMESTAMP '1970-01-01 00:00:01','par1')\";\n\n    execInsertSql(tableEnv, insertInto1);\n\n    final String insertInto2 = \"insert into t1 values\\n\"\n        + \"('id1','Stephen',33,TIMESTAMP '1970-01-01 00:00:02','par2'),\\n\"\n        + \"('id1','Julian',53,TIMESTAMP '1970-01-01 00:00:03','par3'),\\n\"\n        + \"('id1','Fabian',31,TIMESTAMP '1970-01-01 00:00:04','par4'),\\n\"\n        + \"('id1','Sophia',18,TIMESTAMP '1970-01-01 00:00:05','par5')\";\n\n    execInsertSql(tableEnv, insertInto2);\n\n    List<Row> result = CollectionUtil.iterableToList(\n        () -> tableEnv.sqlQuery(\"select * from t1\").execute().collect());\n    assertRowsEquals(result, \"[\"\n        + \"+I[id1, Danny, 23, 1970-01-01T00:00:01, par1], \"\n        + \"+I[id1, Stephen, 33, 1970-01-01T00:00:02, par2], \"\n        + \"+I[id1, Julian, 53, 1970-01-01T00:00:03, par3], \"\n        + \"+I[id1, Fabian, 31, 1970-01-01T00:00:04, par4], \"\n        + \"+I[id1, Sophia, 18, 1970-01-01T00:00:05, par5]]\", 3);\n  }\n","date":"2021-08-19 23:21:20","endLine":703,"groupId":"792","id":8,"instanceNumber":2,"isCurCommit":0,"methodName":"testBulkInsertNonPartitionedTable","params":"()","path":"/mnt/clonedata/CloneManagementServer/ManagementServer/consistResult/result-hudi-10-0.7/blobInfo/CC_OUT/blobs/9e/ffdcc8c67f59ad51cfd9a0e5ed56ecd41b530e.src","preCode":"  void testBulkInsertNonPartitionedTable() {\n    TableEnvironment tableEnv = batchTableEnv;\n    Map<String, String> options = new HashMap<>();\n    options.put(FlinkOptions.PATH.key(), tempFile.getAbsolutePath());\n    options.put(FlinkOptions.OPERATION.key(), \"bulk_insert\");\n    String hoodieTableDDL = TestConfigurations.getCreateHoodieTableDDL(\"t1\", options, false);\n    tableEnv.executeSql(hoodieTableDDL);\n\n    final String insertInto1 = \"insert into t1 values\\n\"\n        + \"('id1','Danny',23,TIMESTAMP '1970-01-01 00:00:01','par1')\";\n\n    execInsertSql(tableEnv, insertInto1);\n\n    final String insertInto2 = \"insert into t1 values\\n\"\n        + \"('id1','Stephen',33,TIMESTAMP '1970-01-01 00:00:02','par2'),\\n\"\n        + \"('id1','Julian',53,TIMESTAMP '1970-01-01 00:00:03','par3'),\\n\"\n        + \"('id1','Fabian',31,TIMESTAMP '1970-01-01 00:00:04','par4'),\\n\"\n        + \"('id1','Sophia',18,TIMESTAMP '1970-01-01 00:00:05','par5')\";\n\n    execInsertSql(tableEnv, insertInto2);\n\n    List<Row> result = CollectionUtil.iterableToList(\n        () -> tableEnv.sqlQuery(\"select * from t1\").execute().collect());\n    assertRowsEquals(result, \"[\"\n        + \"+I[id1, Danny, 23, 1970-01-01T00:00:01, par1], \"\n        + \"+I[id1, Stephen, 33, 1970-01-01T00:00:02, par2], \"\n        + \"+I[id1, Julian, 53, 1970-01-01T00:00:03, par3], \"\n        + \"+I[id1, Fabian, 31, 1970-01-01T00:00:04, par4], \"\n        + \"+I[id1, Sophia, 18, 1970-01-01T00:00:05, par5]]\", 3);\n  }\n","realPath":"hudi-flink/src/test/java/org/apache/hudi/table/HoodieDataSourceITCase.java","repoName":"hudi","snippetEndLine":0,"snippetStartLine":0,"startLine":673,"status":"M"}],"commitId":"9762e4c08c0ff953cc62e72b1295db4fd4c002c5","commitMessage":"@@@[MINOR] Some cosmetic changes for Flink (#3503)\n\n","date":"2021-08-19 23:21:20","modifiedFileCount":"6","status":"M","submitter":"Danny Chan"},{"authorTime":"2021-08-19 23:21:20","codes":[{"authorDate":"2021-09-11 13:17:16","commitOrder":6,"curCode":"  void testWriteNonPartitionedTable(ExecMode execMode, HoodieTableType tableType) {\n    TableEnvironment tableEnv = execMode == ExecMode.BATCH ? batchTableEnv : streamTableEnv;\n    String hoodieTableDDL = sql(\"t1\")\n        .option(FlinkOptions.PATH, tempFile.getAbsolutePath())\n        .option(FlinkOptions.TABLE_TYPE, tableType)\n        .withPartition(false)\n        .end();\n    tableEnv.executeSql(hoodieTableDDL);\n\n    final String insertInto1 = \"insert into t1 values\\n\"\n        + \"('id1','Danny',23,TIMESTAMP '1970-01-01 00:00:01','par1')\";\n\n    execInsertSql(tableEnv, insertInto1);\n\n    final String insertInto2 = \"insert into t1 values\\n\"\n        + \"('id1','Stephen',33,TIMESTAMP '1970-01-01 00:00:02','par2'),\\n\"\n        + \"('id1','Julian',53,TIMESTAMP '1970-01-01 00:00:03','par3'),\\n\"\n        + \"('id1','Fabian',31,TIMESTAMP '1970-01-01 00:00:04','par4'),\\n\"\n        + \"('id1','Sophia',18,TIMESTAMP '1970-01-01 00:00:05','par5')\";\n\n    execInsertSql(tableEnv, insertInto2);\n\n    List<Row> result = CollectionUtil.iterableToList(\n        () -> tableEnv.sqlQuery(\"select * from t1\").execute().collect());\n    assertRowsEquals(result, \"[+I[id1, Sophia, 18, 1970-01-01T00:00:05, par5]]\");\n  }\n","date":"2021-09-11 13:17:16","endLine":586,"groupId":"2976","id":9,"instanceNumber":1,"isCurCommit":0,"methodName":"testWriteNonPartitionedTable","params":"(ExecModeexecMode@HoodieTableTypetableType)","path":"/mnt/clonedata/CloneManagementServer/ManagementServer/consistResult/result-hudi-10-0.7/blobInfo/CC_OUT/blobs/5b/e603f7838e5195aa26e8cf17398d597ca9ee2f.src","preCode":"  void testWriteNonPartitionedTable(ExecMode execMode) {\n    TableEnvironment tableEnv = execMode == ExecMode.BATCH ? batchTableEnv : streamTableEnv;\n    String hoodieTableDDL = sql(\"t1\")\n        .option(FlinkOptions.PATH, tempFile.getAbsolutePath())\n        .withPartition(false)\n        .end();\n    tableEnv.executeSql(hoodieTableDDL);\n\n    final String insertInto1 = \"insert into t1 values\\n\"\n        + \"('id1','Danny',23,TIMESTAMP '1970-01-01 00:00:01','par1')\";\n\n    execInsertSql(tableEnv, insertInto1);\n\n    final String insertInto2 = \"insert into t1 values\\n\"\n        + \"('id1','Stephen',33,TIMESTAMP '1970-01-01 00:00:02','par2'),\\n\"\n        + \"('id1','Julian',53,TIMESTAMP '1970-01-01 00:00:03','par3'),\\n\"\n        + \"('id1','Fabian',31,TIMESTAMP '1970-01-01 00:00:04','par4'),\\n\"\n        + \"('id1','Sophia',18,TIMESTAMP '1970-01-01 00:00:05','par5')\";\n\n    execInsertSql(tableEnv, insertInto2);\n\n    List<Row> result = CollectionUtil.iterableToList(\n        () -> tableEnv.sqlQuery(\"select * from t1\").execute().collect());\n    assertRowsEquals(result, \"[+I[id1, Sophia, 18, 1970-01-01T00:00:05, par5]]\");\n  }\n","realPath":"hudi-flink/src/test/java/org/apache/hudi/table/HoodieDataSourceITCase.java","repoName":"hudi","snippetEndLine":0,"snippetStartLine":0,"startLine":561,"status":"M"},{"authorDate":"2021-08-19 23:21:20","commitOrder":6,"curCode":"  void testBulkInsertNonPartitionedTable() {\n    TableEnvironment tableEnv = batchTableEnv;\n    String hoodieTableDDL = sql(\"t1\")\n        .option(FlinkOptions.PATH, tempFile.getAbsolutePath())\n        .option(FlinkOptions.OPERATION, \"bulk_insert\")\n        .withPartition(false)\n        .end();\n    tableEnv.executeSql(hoodieTableDDL);\n\n    final String insertInto1 = \"insert into t1 values\\n\"\n        + \"('id1','Danny',23,TIMESTAMP '1970-01-01 00:00:01','par1')\";\n\n    execInsertSql(tableEnv, insertInto1);\n\n    final String insertInto2 = \"insert into t1 values\\n\"\n        + \"('id1','Stephen',33,TIMESTAMP '1970-01-01 00:00:02','par2'),\\n\"\n        + \"('id1','Julian',53,TIMESTAMP '1970-01-01 00:00:03','par3'),\\n\"\n        + \"('id1','Fabian',31,TIMESTAMP '1970-01-01 00:00:04','par4'),\\n\"\n        + \"('id1','Sophia',18,TIMESTAMP '1970-01-01 00:00:05','par5')\";\n\n    execInsertSql(tableEnv, insertInto2);\n\n    List<Row> result = CollectionUtil.iterableToList(\n        () -> tableEnv.sqlQuery(\"select * from t1\").execute().collect());\n    assertRowsEquals(result, \"[\"\n        + \"+I[id1, Danny, 23, 1970-01-01T00:00:01, par1], \"\n        + \"+I[id1, Stephen, 33, 1970-01-01T00:00:02, par2], \"\n        + \"+I[id1, Julian, 53, 1970-01-01T00:00:03, par3], \"\n        + \"+I[id1, Fabian, 31, 1970-01-01T00:00:04, par4], \"\n        + \"+I[id1, Sophia, 18, 1970-01-01T00:00:05, par5]]\", 3);\n  }\n","date":"2021-08-19 23:21:20","endLine":703,"groupId":"792","id":10,"instanceNumber":2,"isCurCommit":0,"methodName":"testBulkInsertNonPartitionedTable","params":"()","path":"/mnt/clonedata/CloneManagementServer/ManagementServer/consistResult/result-hudi-10-0.7/blobInfo/CC_OUT/blobs/9e/ffdcc8c67f59ad51cfd9a0e5ed56ecd41b530e.src","preCode":"  void testBulkInsertNonPartitionedTable() {\n    TableEnvironment tableEnv = batchTableEnv;\n    String hoodieTableDDL = sql(\"t1\")\n        .option(FlinkOptions.PATH, tempFile.getAbsolutePath())\n        .option(FlinkOptions.OPERATION, \"bulk_insert\")\n        .withPartition(false)\n        .end();\n    tableEnv.executeSql(hoodieTableDDL);\n\n    final String insertInto1 = \"insert into t1 values\\n\"\n        + \"('id1','Danny',23,TIMESTAMP '1970-01-01 00:00:01','par1')\";\n\n    execInsertSql(tableEnv, insertInto1);\n\n    final String insertInto2 = \"insert into t1 values\\n\"\n        + \"('id1','Stephen',33,TIMESTAMP '1970-01-01 00:00:02','par2'),\\n\"\n        + \"('id1','Julian',53,TIMESTAMP '1970-01-01 00:00:03','par3'),\\n\"\n        + \"('id1','Fabian',31,TIMESTAMP '1970-01-01 00:00:04','par4'),\\n\"\n        + \"('id1','Sophia',18,TIMESTAMP '1970-01-01 00:00:05','par5')\";\n\n    execInsertSql(tableEnv, insertInto2);\n\n    List<Row> result = CollectionUtil.iterableToList(\n        () -> tableEnv.sqlQuery(\"select * from t1\").execute().collect());\n    assertRowsEquals(result, \"[\"\n        + \"+I[id1, Danny, 23, 1970-01-01T00:00:01, par1], \"\n        + \"+I[id1, Stephen, 33, 1970-01-01T00:00:02, par2], \"\n        + \"+I[id1, Julian, 53, 1970-01-01T00:00:03, par3], \"\n        + \"+I[id1, Fabian, 31, 1970-01-01T00:00:04, par4], \"\n        + \"+I[id1, Sophia, 18, 1970-01-01T00:00:05, par5]]\", 3);\n  }\n","realPath":"hudi-flink/src/test/java/org/apache/hudi/table/HoodieDataSourceITCase.java","repoName":"hudi","snippetEndLine":0,"snippetStartLine":0,"startLine":673,"status":"N"}],"commitId":"b30c5bdaef77aee9f564ac24f80f5c364014bb17","commitMessage":"@@@[HUDI-2412] Add timestamp based partitioning for flink writer (#3638)\n\n","date":"2021-09-11 13:17:16","modifiedFileCount":"11","status":"M","submitter":"Danny Chan"},{"authorTime":"2021-09-15 12:04:46","codes":[{"authorDate":"2021-09-15 12:04:46","commitOrder":7,"curCode":"  void testWriteNonPartitionedTable(ExecMode execMode, HoodieTableType tableType) {\n    TableEnvironment tableEnv = execMode == ExecMode.BATCH ? batchTableEnv : streamTableEnv;\n    String hoodieTableDDL = sql(\"t1\")\n        .option(FlinkOptions.PATH, tempFile.getAbsolutePath())\n        .option(FlinkOptions.TABLE_TYPE, tableType)\n        .noPartition()\n        .end();\n    tableEnv.executeSql(hoodieTableDDL);\n\n    final String insertInto1 = \"insert into t1 values\\n\"\n        + \"('id1','Danny',23,TIMESTAMP '1970-01-01 00:00:01','par1')\";\n\n    execInsertSql(tableEnv, insertInto1);\n\n    final String insertInto2 = \"insert into t1 values\\n\"\n        + \"('id1','Stephen',33,TIMESTAMP '1970-01-01 00:00:02','par2'),\\n\"\n        + \"('id1','Julian',53,TIMESTAMP '1970-01-01 00:00:03','par3'),\\n\"\n        + \"('id1','Fabian',31,TIMESTAMP '1970-01-01 00:00:04','par4'),\\n\"\n        + \"('id1','Sophia',18,TIMESTAMP '1970-01-01 00:00:05','par5')\";\n\n    execInsertSql(tableEnv, insertInto2);\n\n    List<Row> result = CollectionUtil.iterableToList(\n        () -> tableEnv.sqlQuery(\"select * from t1\").execute().collect());\n    assertRowsEquals(result, \"[+I[id1, Sophia, 18, 1970-01-01T00:00:05, par5]]\");\n  }\n","date":"2021-09-15 12:04:46","endLine":586,"groupId":"10388","id":11,"instanceNumber":1,"isCurCommit":0,"methodName":"testWriteNonPartitionedTable","params":"(ExecModeexecMode@HoodieTableTypetableType)","path":"/mnt/clonedata/CloneManagementServer/ManagementServer/consistResult/result-hudi-10-0.7/blobInfo/CC_OUT/blobs/9d/0bcabac6aaad159803c08004869e0ff2db0462.src","preCode":"  void testWriteNonPartitionedTable(ExecMode execMode, HoodieTableType tableType) {\n    TableEnvironment tableEnv = execMode == ExecMode.BATCH ? batchTableEnv : streamTableEnv;\n    String hoodieTableDDL = sql(\"t1\")\n        .option(FlinkOptions.PATH, tempFile.getAbsolutePath())\n        .option(FlinkOptions.TABLE_TYPE, tableType)\n        .withPartition(false)\n        .end();\n    tableEnv.executeSql(hoodieTableDDL);\n\n    final String insertInto1 = \"insert into t1 values\\n\"\n        + \"('id1','Danny',23,TIMESTAMP '1970-01-01 00:00:01','par1')\";\n\n    execInsertSql(tableEnv, insertInto1);\n\n    final String insertInto2 = \"insert into t1 values\\n\"\n        + \"('id1','Stephen',33,TIMESTAMP '1970-01-01 00:00:02','par2'),\\n\"\n        + \"('id1','Julian',53,TIMESTAMP '1970-01-01 00:00:03','par3'),\\n\"\n        + \"('id1','Fabian',31,TIMESTAMP '1970-01-01 00:00:04','par4'),\\n\"\n        + \"('id1','Sophia',18,TIMESTAMP '1970-01-01 00:00:05','par5')\";\n\n    execInsertSql(tableEnv, insertInto2);\n\n    List<Row> result = CollectionUtil.iterableToList(\n        () -> tableEnv.sqlQuery(\"select * from t1\").execute().collect());\n    assertRowsEquals(result, \"[+I[id1, Sophia, 18, 1970-01-01T00:00:05, par5]]\");\n  }\n","realPath":"hudi-flink/src/test/java/org/apache/hudi/table/HoodieDataSourceITCase.java","repoName":"hudi","snippetEndLine":0,"snippetStartLine":0,"startLine":561,"status":"M"},{"authorDate":"2021-09-15 12:04:46","commitOrder":7,"curCode":"  void testBulkInsertNonPartitionedTable() {\n    TableEnvironment tableEnv = batchTableEnv;\n    String hoodieTableDDL = sql(\"t1\")\n        .option(FlinkOptions.PATH, tempFile.getAbsolutePath())\n        .option(FlinkOptions.OPERATION, \"bulk_insert\")\n        .noPartition()\n        .end();\n    tableEnv.executeSql(hoodieTableDDL);\n\n    final String insertInto1 = \"insert into t1 values\\n\"\n        + \"('id1','Danny',23,TIMESTAMP '1970-01-01 00:00:01','par1')\";\n\n    execInsertSql(tableEnv, insertInto1);\n\n    final String insertInto2 = \"insert into t1 values\\n\"\n        + \"('id1','Stephen',33,TIMESTAMP '1970-01-01 00:00:02','par2'),\\n\"\n        + \"('id1','Julian',53,TIMESTAMP '1970-01-01 00:00:03','par3'),\\n\"\n        + \"('id1','Fabian',31,TIMESTAMP '1970-01-01 00:00:04','par4'),\\n\"\n        + \"('id1','Sophia',18,TIMESTAMP '1970-01-01 00:00:05','par5')\";\n\n    execInsertSql(tableEnv, insertInto2);\n\n    List<Row> result = CollectionUtil.iterableToList(\n        () -> tableEnv.sqlQuery(\"select * from t1\").execute().collect());\n    assertRowsEquals(result, \"[\"\n        + \"+I[id1, Danny, 23, 1970-01-01T00:00:01, par1], \"\n        + \"+I[id1, Stephen, 33, 1970-01-01T00:00:02, par2], \"\n        + \"+I[id1, Julian, 53, 1970-01-01T00:00:03, par3], \"\n        + \"+I[id1, Fabian, 31, 1970-01-01T00:00:04, par4], \"\n        + \"+I[id1, Sophia, 18, 1970-01-01T00:00:05, par5]]\", 3);\n  }\n","date":"2021-09-15 12:04:46","endLine":798,"groupId":"10388","id":12,"instanceNumber":2,"isCurCommit":0,"methodName":"testBulkInsertNonPartitionedTable","params":"()","path":"/mnt/clonedata/CloneManagementServer/ManagementServer/consistResult/result-hudi-10-0.7/blobInfo/CC_OUT/blobs/9d/0bcabac6aaad159803c08004869e0ff2db0462.src","preCode":"  void testBulkInsertNonPartitionedTable() {\n    TableEnvironment tableEnv = batchTableEnv;\n    String hoodieTableDDL = sql(\"t1\")\n        .option(FlinkOptions.PATH, tempFile.getAbsolutePath())\n        .option(FlinkOptions.OPERATION, \"bulk_insert\")\n        .withPartition(false)\n        .end();\n    tableEnv.executeSql(hoodieTableDDL);\n\n    final String insertInto1 = \"insert into t1 values\\n\"\n        + \"('id1','Danny',23,TIMESTAMP '1970-01-01 00:00:01','par1')\";\n\n    execInsertSql(tableEnv, insertInto1);\n\n    final String insertInto2 = \"insert into t1 values\\n\"\n        + \"('id1','Stephen',33,TIMESTAMP '1970-01-01 00:00:02','par2'),\\n\"\n        + \"('id1','Julian',53,TIMESTAMP '1970-01-01 00:00:03','par3'),\\n\"\n        + \"('id1','Fabian',31,TIMESTAMP '1970-01-01 00:00:04','par4'),\\n\"\n        + \"('id1','Sophia',18,TIMESTAMP '1970-01-01 00:00:05','par5')\";\n\n    execInsertSql(tableEnv, insertInto2);\n\n    List<Row> result = CollectionUtil.iterableToList(\n        () -> tableEnv.sqlQuery(\"select * from t1\").execute().collect());\n    assertRowsEquals(result, \"[\"\n        + \"+I[id1, Danny, 23, 1970-01-01T00:00:01, par1], \"\n        + \"+I[id1, Stephen, 33, 1970-01-01T00:00:02, par2], \"\n        + \"+I[id1, Julian, 53, 1970-01-01T00:00:03, par3], \"\n        + \"+I[id1, Fabian, 31, 1970-01-01T00:00:04, par4], \"\n        + \"+I[id1, Sophia, 18, 1970-01-01T00:00:05, par5]]\", 3);\n  }\n","realPath":"hudi-flink/src/test/java/org/apache/hudi/table/HoodieDataSourceITCase.java","repoName":"hudi","snippetEndLine":0,"snippetStartLine":0,"startLine":768,"status":"M"}],"commitId":"627f20f9c54a87dded3350b73a6327f5b95632f6","commitMessage":"@@@[HUDI-2430] Make decimal compatible with hudi for flink writer (#3658)\n\n","date":"2021-09-15 12:04:46","modifiedFileCount":"5","status":"M","submitter":"Danny Chan"}]
