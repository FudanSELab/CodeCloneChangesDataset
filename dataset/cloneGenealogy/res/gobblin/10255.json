[{"authorTime":"2017-09-12 17:29:05","codes":[{"authorDate":"2020-03-12 07:14:19","commitOrder":2,"curCode":"  public void testAddSpec() throws Exception {\n    _closer = Closer.create();\n    _properties = new Properties();\n\n    \r\n    _properties.setProperty(KafkaWriterConfigurationKeys.KAFKA_TOPIC, TOPIC);\n    _properties.setProperty(\"spec.kafka.dataWriterClass\", \"org.apache.gobblin.kafka.writer.Kafka09DataWriter\");\n    _properties.setProperty(KafkaWriterConfigurationKeys.KAFKA_PRODUCER_CONFIG_PREFIX + \"bootstrap.servers\", _kafkaBrokers);\n    _properties.setProperty(KafkaWriterConfigurationKeys.KAFKA_PRODUCER_CONFIG_PREFIX+\"value.serializer\", \"org.apache.kafka.common.serialization.ByteArraySerializer\");\n\n    \r\n    _properties.setProperty(KafkaJobMonitor.KAFKA_JOB_MONITOR_PREFIX + \".\" + ConfigurationKeys.KAFKA_BROKERS, _kafkaBrokers);\n    _properties.setProperty(KafkaJobMonitor.KAFKA_JOB_MONITOR_PREFIX + \".\" + Kafka09ConsumerClient.GOBBLIN_CONFIG_VALUE_DESERIALIZER_CLASS_KEY, \"org.apache.kafka.common.serialization.ByteArrayDeserializer\");\n    _properties.setProperty(SimpleKafkaSpecExecutor.SPEC_KAFKA_TOPICS_KEY, TOPIC);\n    _properties.setProperty(\"gobblin.cluster.jobconf.fullyQualifiedPath\", _JOBS_DIR_PATH);\n    _properties.setProperty(KafkaJobMonitor.KAFKA_JOB_MONITOR_PREFIX + \".\" + Kafka09ConsumerClient.CONFIG_PREFIX + Kafka09ConsumerClient.CONSUMER_CONFIG + \".auto.offset.reset\", \"earliest\");\n\n    Config config = ConfigUtils.propertiesToConfig(_properties);\n\n    \r\n    _seip = _closer.register(new SimpleKafkaSpecProducer(config));\n\n    String addedSpecUriString = \"/foo/bar/addedSpec\";\n    Spec spec = initJobSpec(addedSpecUriString);\n    WriteResponse writeResponse = (WriteResponse) _seip.addSpec(spec).get();\n    log.info(\"WriteResponse: \" + writeResponse);\n\n    _jobCatalog = new NonObservingFSJobCatalog(config.getConfig(\"gobblin.cluster\"));\n    _jobCatalog.startAsync().awaitRunning();\n\n    \r\n    _seic = _closer.register(new StreamingKafkaSpecConsumer(config, _jobCatalog));\n    _seic.startAsync().awaitRunning();\n\n    List<Pair<SpecExecutor.Verb, Spec>> consumedEvent = _seic.changedSpecs().get();\n    Assert.assertTrue(consumedEvent.size() == 1, \"Consumption did not match production\");\n\n    Map.Entry<SpecExecutor.Verb, Spec> consumedSpecAction = consumedEvent.get(0);\n    Assert.assertTrue(consumedSpecAction.getKey().equals(SpecExecutor.Verb.ADD), \"Verb did not match\");\n    Assert.assertTrue(consumedSpecAction.getValue().getUri().toString().equals(addedSpecUriString), \"Expected URI did not match\");\n    Assert.assertTrue(consumedSpecAction.getValue() instanceof JobSpec, \"Expected JobSpec\");\n  }\n","date":"2020-03-12 07:14:19","endLine":136,"groupId":"3271","id":1,"instanceNumber":1,"isCurCommit":0,"methodName":"testAddSpec","params":"()","path":"/mnt/clonedata/CloneManagementServer/ManagementServer/consistResult/result-gobblin-10-0.7/blobInfo/CC_OUT/blobs/26/d3e3b5051857f024b28410ef36d06441a07a72.src","preCode":"  public void testAddSpec() throws Exception {\n    _closer = Closer.create();\n    _properties = new Properties();\n\n    \r\n    _properties.setProperty(KafkaWriterConfigurationKeys.KAFKA_TOPIC, TOPIC);\n    _properties.setProperty(\"spec.kafka.dataWriterClass\", \"org.apache.gobblin.kafka.writer.Kafka09DataWriter\");\n    _properties.setProperty(KafkaWriterConfigurationKeys.KAFKA_PRODUCER_CONFIG_PREFIX + \"bootstrap.servers\", _kafkaBrokers);\n    _properties.setProperty(KafkaWriterConfigurationKeys.KAFKA_PRODUCER_CONFIG_PREFIX+\"value.serializer\", \"org.apache.kafka.common.serialization.ByteArraySerializer\");\n\n    \r\n    _properties.setProperty(KafkaJobMonitor.KAFKA_JOB_MONITOR_PREFIX + \".\" + ConfigurationKeys.KAFKA_BROKERS, _kafkaBrokers);\n    _properties.setProperty(KafkaJobMonitor.KAFKA_JOB_MONITOR_PREFIX + \".\" + Kafka09ConsumerClient.GOBBLIN_CONFIG_VALUE_DESERIALIZER_CLASS_KEY, \"org.apache.kafka.common.serialization.ByteArrayDeserializer\");\n    _properties.setProperty(SimpleKafkaSpecExecutor.SPEC_KAFKA_TOPICS_KEY, TOPIC);\n    _properties.setProperty(\"gobblin.cluster.jobconf.fullyQualifiedPath\", _JOBS_DIR_PATH);\n    _properties.setProperty(KafkaJobMonitor.KAFKA_JOB_MONITOR_PREFIX + \".\" + Kafka09ConsumerClient.CONFIG_PREFIX + Kafka09ConsumerClient.CONSUMER_CONFIG + \".auto.offset.reset\", \"earliest\");\n\n    Config config = ConfigUtils.propertiesToConfig(_properties);\n\n    \r\n    _seip = _closer.register(new SimpleKafkaSpecProducer(config));\n\n    String addedSpecUriString = \"/foo/bar/addedSpec\";\n    Spec spec = initJobSpec(addedSpecUriString);\n    WriteResponse writeResponse = (WriteResponse) _seip.addSpec(spec).get();\n    log.info(\"WriteResponse: \" + writeResponse);\n\n    _jobCatalog = new NonObservingFSJobCatalog(config.getConfig(\"gobblin.cluster\"));\n    _jobCatalog.startAsync().awaitRunning();\n\n    \r\n    _seic = _closer.register(new StreamingKafkaSpecConsumer(config, _jobCatalog));\n    _seic.startAsync().awaitRunning();\n\n    List<Pair<SpecExecutor.Verb, Spec>> consumedEvent = _seic.changedSpecs().get();\n    Assert.assertTrue(consumedEvent.size() == 1, \"Consumption did not match production\");\n\n    Map.Entry<SpecExecutor.Verb, Spec> consumedSpecAction = consumedEvent.get(0);\n    Assert.assertTrue(consumedSpecAction.getKey().equals(SpecExecutor.Verb.ADD), \"Verb did not match\");\n    Assert.assertTrue(consumedSpecAction.getValue().getUri().toString().equals(addedSpecUriString), \"Expected URI did not match\");\n    Assert.assertTrue(consumedSpecAction.getValue() instanceof JobSpec, \"Expected JobSpec\");\n  }\n","realPath":"gobblin-modules/gobblin-kafka-09/src/test/java/org/apache/gobblin/service/StreamingKafkaSpecExecutorTest.java","repoName":"gobblin","snippetEndLine":0,"snippetStartLine":0,"startLine":95,"status":"B"},{"authorDate":"2017-09-12 17:29:05","commitOrder":2,"curCode":"  public void testResetConsumption() throws Exception {\n    SimpleKafkaSpecConsumer seic = _closer\n        .register(new SimpleKafkaSpecConsumer(ConfigUtils.propertiesToConfig(_properties)));\n\n    List<Pair<SpecExecutor.Verb, Spec>> consumedEvent = seic.changedSpecs().get();\n    Assert.assertTrue(consumedEvent.size() == 3, \"Consumption was reset, we should see all events\");\n  }\n","date":"2017-09-12 17:29:21","endLine":151,"groupId":"7045","id":2,"instanceNumber":2,"isCurCommit":1,"methodName":"testResetConsumption","params":"()","path":"/mnt/clonedata/CloneManagementServer/ManagementServer/consistResult/result-gobblin-10-0.7/blobInfo/CC_OUT/blobs/55/67c18eebaedc1fe99add455f7e2976e7c57604.src","preCode":"  public void testResetConsumption() throws Exception {\n    SimpleKafkaSpecConsumer seic = _closer\n        .register(new SimpleKafkaSpecConsumer(ConfigUtils.propertiesToConfig(_properties)));\n\n    List<Pair<SpecExecutor.Verb, Spec>> consumedEvent = seic.changedSpecs().get();\n    Assert.assertTrue(consumedEvent.size() == 3, \"Consumption was reset, we should see all events\");\n  }\n","realPath":"gobblin-modules/gobblin-kafka-08/src/test/java/org/apache/gobblin/service/SimpleKafkaSpecExecutorTest.java","repoName":"gobblin","snippetEndLine":0,"snippetStartLine":0,"startLine":145,"status":"NB"}],"commitId":"7a328f9a232a60973d27c50859e6b84e63df90f7","commitMessage":"@@@[GOBBLIN-1040] HighLevelConsumer re-design by removing references to ?\n\nCloses #2900 from vikrambohra/GOBBLIN-1040\n","date":"2020-03-12 07:14:19","modifiedFileCount":"16","status":"M","submitter":"vbohra"},{"authorTime":"2017-09-12 17:29:05","codes":[{"authorDate":"2020-05-21 06:37:12","commitOrder":3,"curCode":"  public void testAddSpec() throws Exception {\n    _closer = Closer.create();\n    _properties = new Properties();\n\n    \r\n    _properties.setProperty(KafkaWriterConfigurationKeys.KAFKA_TOPIC, TOPIC);\n    _properties.setProperty(\"spec.kafka.dataWriterClass\", \"org.apache.gobblin.kafka.writer.Kafka09DataWriter\");\n    _properties.setProperty(KafkaWriterConfigurationKeys.KAFKA_PRODUCER_CONFIG_PREFIX + \"bootstrap.servers\", _kafkaBrokers);\n    _properties.setProperty(KafkaWriterConfigurationKeys.KAFKA_PRODUCER_CONFIG_PREFIX+\"value.serializer\", \"org.apache.kafka.common.serialization.ByteArraySerializer\");\n\n    \r\n    _properties.setProperty(KafkaJobMonitor.KAFKA_JOB_MONITOR_PREFIX + \".\" + ConfigurationKeys.KAFKA_BROKERS, _kafkaBrokers);\n    _properties.setProperty(KafkaJobMonitor.KAFKA_JOB_MONITOR_PREFIX + \".\" + Kafka09ConsumerClient.GOBBLIN_CONFIG_VALUE_DESERIALIZER_CLASS_KEY, \"org.apache.kafka.common.serialization.ByteArrayDeserializer\");\n    _properties.setProperty(SimpleKafkaSpecExecutor.SPEC_KAFKA_TOPICS_KEY, TOPIC);\n    _properties.setProperty(\"gobblin.cluster.jobconf.fullyQualifiedPath\", _JOBS_DIR_PATH);\n    _properties.setProperty(KafkaJobMonitor.KAFKA_JOB_MONITOR_PREFIX + \".\" + Kafka09ConsumerClient.CONFIG_PREFIX + Kafka09ConsumerClient.CONSUMER_CONFIG + \".auto.offset.reset\", \"earliest\");\n\n    Config config = ConfigUtils.propertiesToConfig(_properties);\n\n    \r\n    _seip = _closer.register(new SimpleKafkaSpecProducer(config));\n\n    WriteResponse writeResponse = (WriteResponse) _seip.addSpec(spec).get();\n    log.info(\"WriteResponse: \" + writeResponse);\n\n    _jobCatalog = new NonObservingFSJobCatalog(config.getConfig(\"gobblin.cluster\"));\n    _jobCatalog.startAsync().awaitRunning();\n\n    \r\n    _seic = _closer.register(new StreamingKafkaSpecConsumer(config, _jobCatalog));\n    _seic.startAsync().awaitRunning();\n\n    List<Pair<SpecExecutor.Verb, Spec>> consumedEvent = _seic.changedSpecs().get();\n    Assert.assertTrue(consumedEvent.size() == 1, \"Consumption did not match production\");\n\n    Map.Entry<SpecExecutor.Verb, Spec> consumedSpecAction = consumedEvent.get(0);\n    Assert.assertTrue(consumedSpecAction.getKey().equals(SpecExecutor.Verb.ADD), \"Verb did not match\");\n    Assert.assertTrue(consumedSpecAction.getValue().getUri().toString().equals(specUriString), \"Expected URI did not match\");\n    Assert.assertTrue(consumedSpecAction.getValue() instanceof JobSpec, \"Expected JobSpec\");\n  }\n","date":"2020-05-21 06:37:12","endLine":136,"groupId":"10255","id":3,"instanceNumber":1,"isCurCommit":1,"methodName":"testAddSpec","params":"()","path":"/mnt/clonedata/CloneManagementServer/ManagementServer/consistResult/result-gobblin-10-0.7/blobInfo/CC_OUT/blobs/ab/bdc610ca1f04146ae8cb4c65514bb6004d26c5.src","preCode":"  public void testAddSpec() throws Exception {\n    _closer = Closer.create();\n    _properties = new Properties();\n\n    \r\n    _properties.setProperty(KafkaWriterConfigurationKeys.KAFKA_TOPIC, TOPIC);\n    _properties.setProperty(\"spec.kafka.dataWriterClass\", \"org.apache.gobblin.kafka.writer.Kafka09DataWriter\");\n    _properties.setProperty(KafkaWriterConfigurationKeys.KAFKA_PRODUCER_CONFIG_PREFIX + \"bootstrap.servers\", _kafkaBrokers);\n    _properties.setProperty(KafkaWriterConfigurationKeys.KAFKA_PRODUCER_CONFIG_PREFIX+\"value.serializer\", \"org.apache.kafka.common.serialization.ByteArraySerializer\");\n\n    \r\n    _properties.setProperty(KafkaJobMonitor.KAFKA_JOB_MONITOR_PREFIX + \".\" + ConfigurationKeys.KAFKA_BROKERS, _kafkaBrokers);\n    _properties.setProperty(KafkaJobMonitor.KAFKA_JOB_MONITOR_PREFIX + \".\" + Kafka09ConsumerClient.GOBBLIN_CONFIG_VALUE_DESERIALIZER_CLASS_KEY, \"org.apache.kafka.common.serialization.ByteArrayDeserializer\");\n    _properties.setProperty(SimpleKafkaSpecExecutor.SPEC_KAFKA_TOPICS_KEY, TOPIC);\n    _properties.setProperty(\"gobblin.cluster.jobconf.fullyQualifiedPath\", _JOBS_DIR_PATH);\n    _properties.setProperty(KafkaJobMonitor.KAFKA_JOB_MONITOR_PREFIX + \".\" + Kafka09ConsumerClient.CONFIG_PREFIX + Kafka09ConsumerClient.CONSUMER_CONFIG + \".auto.offset.reset\", \"earliest\");\n\n    Config config = ConfigUtils.propertiesToConfig(_properties);\n\n    \r\n    _seip = _closer.register(new SimpleKafkaSpecProducer(config));\n\n    String addedSpecUriString = \"/foo/bar/addedSpec\";\n    Spec spec = initJobSpec(addedSpecUriString);\n    WriteResponse writeResponse = (WriteResponse) _seip.addSpec(spec).get();\n    log.info(\"WriteResponse: \" + writeResponse);\n\n    _jobCatalog = new NonObservingFSJobCatalog(config.getConfig(\"gobblin.cluster\"));\n    _jobCatalog.startAsync().awaitRunning();\n\n    \r\n    _seic = _closer.register(new StreamingKafkaSpecConsumer(config, _jobCatalog));\n    _seic.startAsync().awaitRunning();\n\n    List<Pair<SpecExecutor.Verb, Spec>> consumedEvent = _seic.changedSpecs().get();\n    Assert.assertTrue(consumedEvent.size() == 1, \"Consumption did not match production\");\n\n    Map.Entry<SpecExecutor.Verb, Spec> consumedSpecAction = consumedEvent.get(0);\n    Assert.assertTrue(consumedSpecAction.getKey().equals(SpecExecutor.Verb.ADD), \"Verb did not match\");\n    Assert.assertTrue(consumedSpecAction.getValue().getUri().toString().equals(addedSpecUriString), \"Expected URI did not match\");\n    Assert.assertTrue(consumedSpecAction.getValue() instanceof JobSpec, \"Expected JobSpec\");\n  }\n","realPath":"gobblin-modules/gobblin-kafka-09/src/test/java/org/apache/gobblin/service/StreamingKafkaSpecExecutorTest.java","repoName":"gobblin","snippetEndLine":0,"snippetStartLine":0,"startLine":97,"status":"M"},{"authorDate":"2017-09-12 17:29:05","commitOrder":3,"curCode":"  public void testResetConsumption() throws Exception {\n    SimpleKafkaSpecConsumer seic = _closer\n        .register(new SimpleKafkaSpecConsumer(ConfigUtils.propertiesToConfig(_properties)));\n\n    List<Pair<SpecExecutor.Verb, Spec>> consumedEvent = seic.changedSpecs().get();\n    Assert.assertTrue(consumedEvent.size() == 3, \"Consumption was reset, we should see all events\");\n  }\n","date":"2017-09-12 17:29:21","endLine":151,"groupId":"10255","id":4,"instanceNumber":2,"isCurCommit":1,"methodName":"testResetConsumption","params":"()","path":"/mnt/clonedata/CloneManagementServer/ManagementServer/consistResult/result-gobblin-10-0.7/blobInfo/CC_OUT/blobs/55/67c18eebaedc1fe99add455f7e2976e7c57604.src","preCode":"  public void testResetConsumption() throws Exception {\n    SimpleKafkaSpecConsumer seic = _closer\n        .register(new SimpleKafkaSpecConsumer(ConfigUtils.propertiesToConfig(_properties)));\n\n    List<Pair<SpecExecutor.Verb, Spec>> consumedEvent = seic.changedSpecs().get();\n    Assert.assertTrue(consumedEvent.size() == 3, \"Consumption was reset, we should see all events\");\n  }\n","realPath":"gobblin-modules/gobblin-kafka-08/src/test/java/org/apache/gobblin/service/SimpleKafkaSpecExecutorTest.java","repoName":"gobblin","snippetEndLine":0,"snippetStartLine":0,"startLine":145,"status":"N"}],"commitId":"72373eebff6b669bd8e001966ffab4e6ded8ab7b","commitMessage":"@@@[GOBBLIN-1150] spec catalog table schema change\n\nCloses #2988 from arjun4084346/jsonConfig\n","date":"2020-05-21 06:37:12","modifiedFileCount":"10","status":"M","submitter":"Arjun"}]
