[{"authorTime":"2020-07-21 12:42:42","codes":[{"authorDate":"2020-07-21 12:42:42","commitOrder":1,"curCode":"    public void testAnalyze(@Injectable Analyzer analyzer) throws UserException {\n        String jobName = \"job1\";\n        String dbName = \"db1\";\n        LabelName labelName = new LabelName(dbName, jobName);\n        String tableNameString = \"table1\";\n        String topicName = \"topic1\";\n        String serverAddress = \"127.0.0.1:8080\";\n        String kafkaPartitionString = \"1,2,3\";\n        String timeZone = \"8:00\";\n        List<String> partitionNameString = Lists.newArrayList();\n        partitionNameString.add(\"p1\");\n        PartitionNames partitionNames = new PartitionNames(false, partitionNameString);\n        ColumnSeparator columnSeparator = new ColumnSeparator(\",\");\n\n        \r\n        TableName tableName = new TableName(dbName, tableNameString);\n        List<ParseNode> loadPropertyList = new ArrayList<>();\n        loadPropertyList.add(columnSeparator);\n        loadPropertyList.add(partitionNames);\n        Map<String, String> properties = Maps.newHashMap();\n        properties.put(CreateRoutineLoadStmt.DESIRED_CONCURRENT_NUMBER_PROPERTY, \"2\");\n        properties.put(LoadStmt.TIMEZONE, timeZone);\n        String typeName = LoadDataSourceType.KAFKA.name();\n        Map<String, String> customProperties = Maps.newHashMap();\n\n        customProperties.put(CreateRoutineLoadStmt.KAFKA_TOPIC_PROPERTY, topicName);\n        customProperties.put(CreateRoutineLoadStmt.KAFKA_BROKER_LIST_PROPERTY, serverAddress);\n        customProperties.put(CreateRoutineLoadStmt.KAFKA_PARTITIONS_PROPERTY, kafkaPartitionString);\n\n        CreateRoutineLoadStmt createRoutineLoadStmt = new CreateRoutineLoadStmt(labelName, tableNameString,\n                                                                                loadPropertyList, properties,\n                                                                                typeName, customProperties);\n        new MockUp<StatementBase>() {\n            @Mock\n            public void analyze(Analyzer analyzer1) {\n                return;\n            }\n        };\n\n        createRoutineLoadStmt.analyze(analyzer);\n\n        Assert.assertNotNull(createRoutineLoadStmt.getRoutineLoadDesc());\n        Assert.assertEquals(columnSeparator, createRoutineLoadStmt.getRoutineLoadDesc().getColumnSeparator());\n        Assert.assertEquals(partitionNames.getPartitionNames(), createRoutineLoadStmt.getRoutineLoadDesc().getPartitionNames().getPartitionNames());\n        Assert.assertEquals(2, createRoutineLoadStmt.getDesiredConcurrentNum());\n        Assert.assertEquals(0, createRoutineLoadStmt.getMaxErrorNum());\n        Assert.assertEquals(serverAddress, createRoutineLoadStmt.getKafkaBrokerList());\n        Assert.assertEquals(topicName, createRoutineLoadStmt.getKafkaTopic());\n        Assert.assertEquals(\"+08:00\", createRoutineLoadStmt.getTimezone());\n    }\n","date":"2020-07-21 12:42:42","endLine":140,"groupId":"6137","id":1,"instanceNumber":1,"isCurCommit":0,"methodName":"testAnalyze","params":"(@InjectableAnalyzeranalyzer)","path":"/mnt/clonedata/CloneManagementServer/ManagementServer/consistResult/result-incubatordoris-10-0.7/blobInfo/CC_OUT/blobs/85/774dcd1ac158fdd3d1296a215bb26d19488745.src","preCode":"    public void testAnalyze(@Injectable Analyzer analyzer) throws UserException {\n        String jobName = \"job1\";\n        String dbName = \"db1\";\n        LabelName labelName = new LabelName(dbName, jobName);\n        String tableNameString = \"table1\";\n        String topicName = \"topic1\";\n        String serverAddress = \"127.0.0.1:8080\";\n        String kafkaPartitionString = \"1,2,3\";\n        String timeZone = \"8:00\";\n        List<String> partitionNameString = Lists.newArrayList();\n        partitionNameString.add(\"p1\");\n        PartitionNames partitionNames = new PartitionNames(false, partitionNameString);\n        ColumnSeparator columnSeparator = new ColumnSeparator(\",\");\n\n        \r\n        TableName tableName = new TableName(dbName, tableNameString);\n        List<ParseNode> loadPropertyList = new ArrayList<>();\n        loadPropertyList.add(columnSeparator);\n        loadPropertyList.add(partitionNames);\n        Map<String, String> properties = Maps.newHashMap();\n        properties.put(CreateRoutineLoadStmt.DESIRED_CONCURRENT_NUMBER_PROPERTY, \"2\");\n        properties.put(LoadStmt.TIMEZONE, timeZone);\n        String typeName = LoadDataSourceType.KAFKA.name();\n        Map<String, String> customProperties = Maps.newHashMap();\n\n        customProperties.put(CreateRoutineLoadStmt.KAFKA_TOPIC_PROPERTY, topicName);\n        customProperties.put(CreateRoutineLoadStmt.KAFKA_BROKER_LIST_PROPERTY, serverAddress);\n        customProperties.put(CreateRoutineLoadStmt.KAFKA_PARTITIONS_PROPERTY, kafkaPartitionString);\n\n        CreateRoutineLoadStmt createRoutineLoadStmt = new CreateRoutineLoadStmt(labelName, tableNameString,\n                                                                                loadPropertyList, properties,\n                                                                                typeName, customProperties);\n        new MockUp<StatementBase>() {\n            @Mock\n            public void analyze(Analyzer analyzer1) {\n                return;\n            }\n        };\n\n        createRoutineLoadStmt.analyze(analyzer);\n\n        Assert.assertNotNull(createRoutineLoadStmt.getRoutineLoadDesc());\n        Assert.assertEquals(columnSeparator, createRoutineLoadStmt.getRoutineLoadDesc().getColumnSeparator());\n        Assert.assertEquals(partitionNames.getPartitionNames(), createRoutineLoadStmt.getRoutineLoadDesc().getPartitionNames().getPartitionNames());\n        Assert.assertEquals(2, createRoutineLoadStmt.getDesiredConcurrentNum());\n        Assert.assertEquals(0, createRoutineLoadStmt.getMaxErrorNum());\n        Assert.assertEquals(serverAddress, createRoutineLoadStmt.getKafkaBrokerList());\n        Assert.assertEquals(topicName, createRoutineLoadStmt.getKafkaTopic());\n        Assert.assertEquals(\"+08:00\", createRoutineLoadStmt.getTimezone());\n    }\n","realPath":"fe/fe-core/src/test/java/org/apache/doris/analysis/CreateRoutineLoadStmtTest.java","repoName":"incubatordoris","snippetEndLine":0,"snippetStartLine":0,"startLine":91,"status":"B"},{"authorDate":"2020-07-21 12:42:42","commitOrder":1,"curCode":"    private CreateRoutineLoadStmt initCreateRoutineLoadStmt() {\n        List<ParseNode> loadPropertyList = new ArrayList<>();\n        loadPropertyList.add(columnSeparator);\n        loadPropertyList.add(partitionNames);\n        Map<String, String> properties = Maps.newHashMap();\n        properties.put(CreateRoutineLoadStmt.DESIRED_CONCURRENT_NUMBER_PROPERTY, \"2\");\n        String typeName = LoadDataSourceType.KAFKA.name();\n        Map<String, String> customProperties = Maps.newHashMap();\n\n        customProperties.put(CreateRoutineLoadStmt.KAFKA_TOPIC_PROPERTY, topicName);\n        customProperties.put(CreateRoutineLoadStmt.KAFKA_BROKER_LIST_PROPERTY, serverAddress);\n        customProperties.put(CreateRoutineLoadStmt.KAFKA_PARTITIONS_PROPERTY, kafkaPartitionString);\n\n        CreateRoutineLoadStmt createRoutineLoadStmt = new CreateRoutineLoadStmt(labelName, tableNameString,\n                                                                                loadPropertyList, properties,\n                                                                                typeName, customProperties);\n        Deencapsulation.setField(createRoutineLoadStmt, \"name\", jobName);\n        return createRoutineLoadStmt;\n    }\n","date":"2020-07-21 12:42:42","endLine":335,"groupId":"6137","id":2,"instanceNumber":2,"isCurCommit":0,"methodName":"initCreateRoutineLoadStmt","params":"()","path":"/mnt/clonedata/CloneManagementServer/ManagementServer/consistResult/result-incubatordoris-10-0.7/blobInfo/CC_OUT/blobs/ec/e07c24fa7faefc27554dc045efcb0f6092ce51.src","preCode":"    private CreateRoutineLoadStmt initCreateRoutineLoadStmt() {\n        List<ParseNode> loadPropertyList = new ArrayList<>();\n        loadPropertyList.add(columnSeparator);\n        loadPropertyList.add(partitionNames);\n        Map<String, String> properties = Maps.newHashMap();\n        properties.put(CreateRoutineLoadStmt.DESIRED_CONCURRENT_NUMBER_PROPERTY, \"2\");\n        String typeName = LoadDataSourceType.KAFKA.name();\n        Map<String, String> customProperties = Maps.newHashMap();\n\n        customProperties.put(CreateRoutineLoadStmt.KAFKA_TOPIC_PROPERTY, topicName);\n        customProperties.put(CreateRoutineLoadStmt.KAFKA_BROKER_LIST_PROPERTY, serverAddress);\n        customProperties.put(CreateRoutineLoadStmt.KAFKA_PARTITIONS_PROPERTY, kafkaPartitionString);\n\n        CreateRoutineLoadStmt createRoutineLoadStmt = new CreateRoutineLoadStmt(labelName, tableNameString,\n                                                                                loadPropertyList, properties,\n                                                                                typeName, customProperties);\n        Deencapsulation.setField(createRoutineLoadStmt, \"name\", jobName);\n        return createRoutineLoadStmt;\n    }\n","realPath":"fe/fe-core/src/test/java/org/apache/doris/load/routineload/KafkaRoutineLoadJobTest.java","repoName":"incubatordoris","snippetEndLine":0,"snippetStartLine":0,"startLine":317,"status":"B"}],"commitId":"ad17afef9139a9aeedeb2e92638e95886d515f14","commitMessage":"@@@[CodeRefactor] #4098 Make FE multi module (#4099)\n\nThis PR change the FE code structure to maven multi module structure. \nSee ISSUE: #4098 for more info.  such as How to resolve conflicts.","date":"2020-07-21 12:42:42","modifiedFileCount":"0","status":"B","submitter":"Mingyu Chen"},{"authorTime":"2020-08-21 22:57:16","codes":[{"authorDate":"2020-08-21 22:57:16","commitOrder":2,"curCode":"    public void testAnalyze(@Injectable Analyzer analyzer) throws UserException {\n        String jobName = \"job1\";\n        String dbName = \"db1\";\n        LabelName labelName = new LabelName(dbName, jobName);\n        String tableNameString = \"table1\";\n        String topicName = \"topic1\";\n        String serverAddress = \"127.0.0.1:8080\";\n        String kafkaPartitionString = \"1,2,3\";\n        String timeZone = \"8:00\";\n        List<String> partitionNameString = Lists.newArrayList();\n        partitionNameString.add(\"p1\");\n        PartitionNames partitionNames = new PartitionNames(false, partitionNameString);\n        ColumnSeparator columnSeparator = new ColumnSeparator(\",\");\n\n        \r\n        TableName tableName = new TableName(dbName, tableNameString);\n        List<ParseNode> loadPropertyList = new ArrayList<>();\n        loadPropertyList.add(columnSeparator);\n        loadPropertyList.add(partitionNames);\n        Map<String, String> properties = Maps.newHashMap();\n        properties.put(CreateRoutineLoadStmt.DESIRED_CONCURRENT_NUMBER_PROPERTY, \"2\");\n        properties.put(LoadStmt.TIMEZONE, timeZone);\n        String typeName = LoadDataSourceType.KAFKA.name();\n        Map<String, String> customProperties = Maps.newHashMap();\n\n        customProperties.put(CreateRoutineLoadStmt.KAFKA_TOPIC_PROPERTY, topicName);\n        customProperties.put(CreateRoutineLoadStmt.KAFKA_BROKER_LIST_PROPERTY, serverAddress);\n        customProperties.put(CreateRoutineLoadStmt.KAFKA_PARTITIONS_PROPERTY, kafkaPartitionString);\n\n        CreateRoutineLoadStmt createRoutineLoadStmt = new CreateRoutineLoadStmt(labelName, tableNameString,\n                                                                                loadPropertyList, properties,\n                                                                                typeName, customProperties,\n                                                                                LoadTask.MergeType.APPEND);\n        new MockUp<StatementBase>() {\n            @Mock\n            public void analyze(Analyzer analyzer1) {\n                return;\n            }\n        };\n\n        createRoutineLoadStmt.analyze(analyzer);\n\n        Assert.assertNotNull(createRoutineLoadStmt.getRoutineLoadDesc());\n        Assert.assertEquals(columnSeparator, createRoutineLoadStmt.getRoutineLoadDesc().getColumnSeparator());\n        Assert.assertEquals(partitionNames.getPartitionNames(), createRoutineLoadStmt.getRoutineLoadDesc().getPartitionNames().getPartitionNames());\n        Assert.assertEquals(2, createRoutineLoadStmt.getDesiredConcurrentNum());\n        Assert.assertEquals(0, createRoutineLoadStmt.getMaxErrorNum());\n        Assert.assertEquals(serverAddress, createRoutineLoadStmt.getKafkaBrokerList());\n        Assert.assertEquals(topicName, createRoutineLoadStmt.getKafkaTopic());\n        Assert.assertEquals(\"+08:00\", createRoutineLoadStmt.getTimezone());\n    }\n","date":"2020-08-21 22:57:16","endLine":143,"groupId":"5031","id":3,"instanceNumber":1,"isCurCommit":0,"methodName":"testAnalyze","params":"(@InjectableAnalyzeranalyzer)","path":"/mnt/clonedata/CloneManagementServer/ManagementServer/consistResult/result-incubatordoris-10-0.7/blobInfo/CC_OUT/blobs/69/22bf1f6f525a863554a450934710db331e0990.src","preCode":"    public void testAnalyze(@Injectable Analyzer analyzer) throws UserException {\n        String jobName = \"job1\";\n        String dbName = \"db1\";\n        LabelName labelName = new LabelName(dbName, jobName);\n        String tableNameString = \"table1\";\n        String topicName = \"topic1\";\n        String serverAddress = \"127.0.0.1:8080\";\n        String kafkaPartitionString = \"1,2,3\";\n        String timeZone = \"8:00\";\n        List<String> partitionNameString = Lists.newArrayList();\n        partitionNameString.add(\"p1\");\n        PartitionNames partitionNames = new PartitionNames(false, partitionNameString);\n        ColumnSeparator columnSeparator = new ColumnSeparator(\",\");\n\n        \r\n        TableName tableName = new TableName(dbName, tableNameString);\n        List<ParseNode> loadPropertyList = new ArrayList<>();\n        loadPropertyList.add(columnSeparator);\n        loadPropertyList.add(partitionNames);\n        Map<String, String> properties = Maps.newHashMap();\n        properties.put(CreateRoutineLoadStmt.DESIRED_CONCURRENT_NUMBER_PROPERTY, \"2\");\n        properties.put(LoadStmt.TIMEZONE, timeZone);\n        String typeName = LoadDataSourceType.KAFKA.name();\n        Map<String, String> customProperties = Maps.newHashMap();\n\n        customProperties.put(CreateRoutineLoadStmt.KAFKA_TOPIC_PROPERTY, topicName);\n        customProperties.put(CreateRoutineLoadStmt.KAFKA_BROKER_LIST_PROPERTY, serverAddress);\n        customProperties.put(CreateRoutineLoadStmt.KAFKA_PARTITIONS_PROPERTY, kafkaPartitionString);\n\n        CreateRoutineLoadStmt createRoutineLoadStmt = new CreateRoutineLoadStmt(labelName, tableNameString,\n                                                                                loadPropertyList, properties,\n                                                                                typeName, customProperties);\n        new MockUp<StatementBase>() {\n            @Mock\n            public void analyze(Analyzer analyzer1) {\n                return;\n            }\n        };\n\n        createRoutineLoadStmt.analyze(analyzer);\n\n        Assert.assertNotNull(createRoutineLoadStmt.getRoutineLoadDesc());\n        Assert.assertEquals(columnSeparator, createRoutineLoadStmt.getRoutineLoadDesc().getColumnSeparator());\n        Assert.assertEquals(partitionNames.getPartitionNames(), createRoutineLoadStmt.getRoutineLoadDesc().getPartitionNames().getPartitionNames());\n        Assert.assertEquals(2, createRoutineLoadStmt.getDesiredConcurrentNum());\n        Assert.assertEquals(0, createRoutineLoadStmt.getMaxErrorNum());\n        Assert.assertEquals(serverAddress, createRoutineLoadStmt.getKafkaBrokerList());\n        Assert.assertEquals(topicName, createRoutineLoadStmt.getKafkaTopic());\n        Assert.assertEquals(\"+08:00\", createRoutineLoadStmt.getTimezone());\n    }\n","realPath":"fe/fe-core/src/test/java/org/apache/doris/analysis/CreateRoutineLoadStmtTest.java","repoName":"incubatordoris","snippetEndLine":0,"snippetStartLine":0,"startLine":93,"status":"M"},{"authorDate":"2020-08-21 22:57:16","commitOrder":2,"curCode":"    private CreateRoutineLoadStmt initCreateRoutineLoadStmt() {\n        List<ParseNode> loadPropertyList = new ArrayList<>();\n        loadPropertyList.add(columnSeparator);\n        loadPropertyList.add(partitionNames);\n        Map<String, String> properties = Maps.newHashMap();\n        properties.put(CreateRoutineLoadStmt.DESIRED_CONCURRENT_NUMBER_PROPERTY, \"2\");\n        String typeName = LoadDataSourceType.KAFKA.name();\n        Map<String, String> customProperties = Maps.newHashMap();\n\n        customProperties.put(CreateRoutineLoadStmt.KAFKA_TOPIC_PROPERTY, topicName);\n        customProperties.put(CreateRoutineLoadStmt.KAFKA_BROKER_LIST_PROPERTY, serverAddress);\n        customProperties.put(CreateRoutineLoadStmt.KAFKA_PARTITIONS_PROPERTY, kafkaPartitionString);\n\n        CreateRoutineLoadStmt createRoutineLoadStmt = new CreateRoutineLoadStmt(labelName, tableNameString,\n                                                                                loadPropertyList, properties,\n                                                                                typeName, customProperties,\n                                                                                LoadTask.MergeType.APPEND);\n        Deencapsulation.setField(createRoutineLoadStmt, \"name\", jobName);\n        return createRoutineLoadStmt;\n    }\n","date":"2020-08-21 22:57:16","endLine":339,"groupId":"6137","id":4,"instanceNumber":2,"isCurCommit":0,"methodName":"initCreateRoutineLoadStmt","params":"()","path":"/mnt/clonedata/CloneManagementServer/ManagementServer/consistResult/result-incubatordoris-10-0.7/blobInfo/CC_OUT/blobs/5f/6750c93658333be1ad8061fb532f5417ed6c60.src","preCode":"    private CreateRoutineLoadStmt initCreateRoutineLoadStmt() {\n        List<ParseNode> loadPropertyList = new ArrayList<>();\n        loadPropertyList.add(columnSeparator);\n        loadPropertyList.add(partitionNames);\n        Map<String, String> properties = Maps.newHashMap();\n        properties.put(CreateRoutineLoadStmt.DESIRED_CONCURRENT_NUMBER_PROPERTY, \"2\");\n        String typeName = LoadDataSourceType.KAFKA.name();\n        Map<String, String> customProperties = Maps.newHashMap();\n\n        customProperties.put(CreateRoutineLoadStmt.KAFKA_TOPIC_PROPERTY, topicName);\n        customProperties.put(CreateRoutineLoadStmt.KAFKA_BROKER_LIST_PROPERTY, serverAddress);\n        customProperties.put(CreateRoutineLoadStmt.KAFKA_PARTITIONS_PROPERTY, kafkaPartitionString);\n\n        CreateRoutineLoadStmt createRoutineLoadStmt = new CreateRoutineLoadStmt(labelName, tableNameString,\n                                                                                loadPropertyList, properties,\n                                                                                typeName, customProperties);\n        Deencapsulation.setField(createRoutineLoadStmt, \"name\", jobName);\n        return createRoutineLoadStmt;\n    }\n","realPath":"fe/fe-core/src/test/java/org/apache/doris/load/routineload/KafkaRoutineLoadJobTest.java","repoName":"incubatordoris","snippetEndLine":0,"snippetStartLine":0,"startLine":320,"status":"M"}],"commitId":"d61c10b76108fdbd04f7d86f89c646708a3934ea","commitMessage":"@@@[Delete] Support batch delete [part 1] (#4310)\n\n* Implements the grammar of the batch delete #4051 \n* Process create.  alter table when table has delete sign column\n* Support the syntax for enabling the delete column\n* Automatically filtered deleted data in the select statement.\n* Automatically add delete sign when create  rollup table\nTODO:\n * Optimize the reading and compaction logic on the be side.  so that the data marked as deleted will be completely deleted during base compaction","date":"2020-08-21 22:57:16","modifiedFileCount":"44","status":"M","submitter":"Zhengguo Yang"},{"authorTime":"2020-08-21 22:57:16","codes":[{"authorDate":"2021-03-09 09:35:39","commitOrder":3,"curCode":"    public void testAnalyze(@Injectable Analyzer analyzer) throws UserException {\n        String jobName = \"job1\";\n        String dbName = \"db1\";\n        LabelName labelName = new LabelName(dbName, jobName);\n        String tableNameString = \"table1\";\n        String topicName = \"topic1\";\n        String serverAddress = \"127.0.0.1:8080\";\n        String kafkaPartitionString = \"1,2,3\";\n        String timeZone = \"8:00\";\n        List<String> partitionNameString = Lists.newArrayList();\n        partitionNameString.add(\"p1\");\n        PartitionNames partitionNames = new PartitionNames(false, partitionNameString);\n        Separator columnSeparator = new Separator(\",\");\n\n        \r\n        TableName tableName = new TableName(dbName, tableNameString);\n        List<ParseNode> loadPropertyList = new ArrayList<>();\n        loadPropertyList.add(columnSeparator);\n        loadPropertyList.add(partitionNames);\n        Map<String, String> properties = Maps.newHashMap();\n        properties.put(CreateRoutineLoadStmt.DESIRED_CONCURRENT_NUMBER_PROPERTY, \"2\");\n        properties.put(LoadStmt.TIMEZONE, timeZone);\n        String typeName = LoadDataSourceType.KAFKA.name();\n        Map<String, String> customProperties = Maps.newHashMap();\n\n        customProperties.put(CreateRoutineLoadStmt.KAFKA_TOPIC_PROPERTY, topicName);\n        customProperties.put(CreateRoutineLoadStmt.KAFKA_BROKER_LIST_PROPERTY, serverAddress);\n        customProperties.put(CreateRoutineLoadStmt.KAFKA_PARTITIONS_PROPERTY, kafkaPartitionString);\n\n        CreateRoutineLoadStmt createRoutineLoadStmt = new CreateRoutineLoadStmt(labelName, tableNameString,\n                                                                                loadPropertyList, properties,\n                                                                                typeName, customProperties,\n                                                                                LoadTask.MergeType.APPEND);\n        new MockUp<StatementBase>() {\n            @Mock\n            public void analyze(Analyzer analyzer1) {\n                return;\n            }\n        };\n\n        createRoutineLoadStmt.analyze(analyzer);\n\n        Assert.assertNotNull(createRoutineLoadStmt.getRoutineLoadDesc());\n        Assert.assertEquals(columnSeparator, createRoutineLoadStmt.getRoutineLoadDesc().getColumnSeparator());\n        Assert.assertEquals(partitionNames.getPartitionNames(), createRoutineLoadStmt.getRoutineLoadDesc().getPartitionNames().getPartitionNames());\n        Assert.assertEquals(2, createRoutineLoadStmt.getDesiredConcurrentNum());\n        Assert.assertEquals(0, createRoutineLoadStmt.getMaxErrorNum());\n        Assert.assertEquals(serverAddress, createRoutineLoadStmt.getKafkaBrokerList());\n        Assert.assertEquals(topicName, createRoutineLoadStmt.getKafkaTopic());\n        Assert.assertEquals(\"+08:00\", createRoutineLoadStmt.getTimezone());\n    }\n","date":"2021-03-09 09:35:39","endLine":183,"groupId":"5031","id":5,"instanceNumber":1,"isCurCommit":0,"methodName":"testAnalyze","params":"(@InjectableAnalyzeranalyzer)","path":"/mnt/clonedata/CloneManagementServer/ManagementServer/consistResult/result-incubatordoris-10-0.7/blobInfo/CC_OUT/blobs/8c/3a1fac86f84b41dff726f015a163bb57496e76.src","preCode":"    public void testAnalyze(@Injectable Analyzer analyzer) throws UserException {\n        String jobName = \"job1\";\n        String dbName = \"db1\";\n        LabelName labelName = new LabelName(dbName, jobName);\n        String tableNameString = \"table1\";\n        String topicName = \"topic1\";\n        String serverAddress = \"127.0.0.1:8080\";\n        String kafkaPartitionString = \"1,2,3\";\n        String timeZone = \"8:00\";\n        List<String> partitionNameString = Lists.newArrayList();\n        partitionNameString.add(\"p1\");\n        PartitionNames partitionNames = new PartitionNames(false, partitionNameString);\n        ColumnSeparator columnSeparator = new ColumnSeparator(\",\");\n\n        \r\n        TableName tableName = new TableName(dbName, tableNameString);\n        List<ParseNode> loadPropertyList = new ArrayList<>();\n        loadPropertyList.add(columnSeparator);\n        loadPropertyList.add(partitionNames);\n        Map<String, String> properties = Maps.newHashMap();\n        properties.put(CreateRoutineLoadStmt.DESIRED_CONCURRENT_NUMBER_PROPERTY, \"2\");\n        properties.put(LoadStmt.TIMEZONE, timeZone);\n        String typeName = LoadDataSourceType.KAFKA.name();\n        Map<String, String> customProperties = Maps.newHashMap();\n\n        customProperties.put(CreateRoutineLoadStmt.KAFKA_TOPIC_PROPERTY, topicName);\n        customProperties.put(CreateRoutineLoadStmt.KAFKA_BROKER_LIST_PROPERTY, serverAddress);\n        customProperties.put(CreateRoutineLoadStmt.KAFKA_PARTITIONS_PROPERTY, kafkaPartitionString);\n\n        CreateRoutineLoadStmt createRoutineLoadStmt = new CreateRoutineLoadStmt(labelName, tableNameString,\n                                                                                loadPropertyList, properties,\n                                                                                typeName, customProperties,\n                                                                                LoadTask.MergeType.APPEND);\n        new MockUp<StatementBase>() {\n            @Mock\n            public void analyze(Analyzer analyzer1) {\n                return;\n            }\n        };\n\n        createRoutineLoadStmt.analyze(analyzer);\n\n        Assert.assertNotNull(createRoutineLoadStmt.getRoutineLoadDesc());\n        Assert.assertEquals(columnSeparator, createRoutineLoadStmt.getRoutineLoadDesc().getColumnSeparator());\n        Assert.assertEquals(partitionNames.getPartitionNames(), createRoutineLoadStmt.getRoutineLoadDesc().getPartitionNames().getPartitionNames());\n        Assert.assertEquals(2, createRoutineLoadStmt.getDesiredConcurrentNum());\n        Assert.assertEquals(0, createRoutineLoadStmt.getMaxErrorNum());\n        Assert.assertEquals(serverAddress, createRoutineLoadStmt.getKafkaBrokerList());\n        Assert.assertEquals(topicName, createRoutineLoadStmt.getKafkaTopic());\n        Assert.assertEquals(\"+08:00\", createRoutineLoadStmt.getTimezone());\n    }\n","realPath":"fe/fe-core/src/test/java/org/apache/doris/analysis/CreateRoutineLoadStmtTest.java","repoName":"incubatordoris","snippetEndLine":0,"snippetStartLine":0,"startLine":133,"status":"M"},{"authorDate":"2020-08-21 22:57:16","commitOrder":3,"curCode":"    private CreateRoutineLoadStmt initCreateRoutineLoadStmt() {\n        List<ParseNode> loadPropertyList = new ArrayList<>();\n        loadPropertyList.add(columnSeparator);\n        loadPropertyList.add(partitionNames);\n        Map<String, String> properties = Maps.newHashMap();\n        properties.put(CreateRoutineLoadStmt.DESIRED_CONCURRENT_NUMBER_PROPERTY, \"2\");\n        String typeName = LoadDataSourceType.KAFKA.name();\n        Map<String, String> customProperties = Maps.newHashMap();\n\n        customProperties.put(CreateRoutineLoadStmt.KAFKA_TOPIC_PROPERTY, topicName);\n        customProperties.put(CreateRoutineLoadStmt.KAFKA_BROKER_LIST_PROPERTY, serverAddress);\n        customProperties.put(CreateRoutineLoadStmt.KAFKA_PARTITIONS_PROPERTY, kafkaPartitionString);\n\n        CreateRoutineLoadStmt createRoutineLoadStmt = new CreateRoutineLoadStmt(labelName, tableNameString,\n                                                                                loadPropertyList, properties,\n                                                                                typeName, customProperties,\n                                                                                LoadTask.MergeType.APPEND);\n        Deencapsulation.setField(createRoutineLoadStmt, \"name\", jobName);\n        return createRoutineLoadStmt;\n    }\n","date":"2020-08-21 22:57:16","endLine":339,"groupId":"6137","id":6,"instanceNumber":2,"isCurCommit":0,"methodName":"initCreateRoutineLoadStmt","params":"()","path":"/mnt/clonedata/CloneManagementServer/ManagementServer/consistResult/result-incubatordoris-10-0.7/blobInfo/CC_OUT/blobs/5f/6750c93658333be1ad8061fb532f5417ed6c60.src","preCode":"    private CreateRoutineLoadStmt initCreateRoutineLoadStmt() {\n        List<ParseNode> loadPropertyList = new ArrayList<>();\n        loadPropertyList.add(columnSeparator);\n        loadPropertyList.add(partitionNames);\n        Map<String, String> properties = Maps.newHashMap();\n        properties.put(CreateRoutineLoadStmt.DESIRED_CONCURRENT_NUMBER_PROPERTY, \"2\");\n        String typeName = LoadDataSourceType.KAFKA.name();\n        Map<String, String> customProperties = Maps.newHashMap();\n\n        customProperties.put(CreateRoutineLoadStmt.KAFKA_TOPIC_PROPERTY, topicName);\n        customProperties.put(CreateRoutineLoadStmt.KAFKA_BROKER_LIST_PROPERTY, serverAddress);\n        customProperties.put(CreateRoutineLoadStmt.KAFKA_PARTITIONS_PROPERTY, kafkaPartitionString);\n\n        CreateRoutineLoadStmt createRoutineLoadStmt = new CreateRoutineLoadStmt(labelName, tableNameString,\n                                                                                loadPropertyList, properties,\n                                                                                typeName, customProperties,\n                                                                                LoadTask.MergeType.APPEND);\n        Deencapsulation.setField(createRoutineLoadStmt, \"name\", jobName);\n        return createRoutineLoadStmt;\n    }\n","realPath":"fe/fe-core/src/test/java/org/apache/doris/load/routineload/KafkaRoutineLoadJobTest.java","repoName":"incubatordoris","snippetEndLine":0,"snippetStartLine":0,"startLine":320,"status":"N"}],"commitId":"e023ef5404493fa0b1aece6b72d1c2202f190dc2","commitMessage":"@@@[Load] Support multi bytes LineDelimiter and ColumnSeparator (#5462)\n\n* [Internal][Support Multibytes Separator] doris-1079\nsupport multi bytes LineDelimiter and ColumnSeparator","date":"2021-03-09 09:35:39","modifiedFileCount":"15","status":"M","submitter":"Zhengguo Yang"},{"authorTime":"2020-08-21 22:57:16","codes":[{"authorDate":"2021-09-07 11:53:32","commitOrder":4,"curCode":"    public void testAnalyze(@Injectable Analyzer analyzer,\n                            @Injectable SessionVariable sessionVariable) throws UserException {\n        String jobName = \"job1\";\n        String dbName = \"db1\";\n        LabelName labelName = new LabelName(dbName, jobName);\n        String tableNameString = \"table1\";\n        String topicName = \"topic1\";\n        String serverAddress = \"127.0.0.1:8080\";\n        String kafkaPartitionString = \"1,2,3\";\n        String timeZone = \"8:00\";\n        List<String> partitionNameString = Lists.newArrayList();\n        partitionNameString.add(\"p1\");\n        PartitionNames partitionNames = new PartitionNames(false, partitionNameString);\n        Separator columnSeparator = new Separator(\",\");\n\n        \r\n        TableName tableName = new TableName(dbName, tableNameString);\n        List<ParseNode> loadPropertyList = new ArrayList<>();\n        loadPropertyList.add(columnSeparator);\n        loadPropertyList.add(partitionNames);\n        Map<String, String> properties = Maps.newHashMap();\n        properties.put(CreateRoutineLoadStmt.DESIRED_CONCURRENT_NUMBER_PROPERTY, \"2\");\n        properties.put(LoadStmt.TIMEZONE, timeZone);\n        String typeName = LoadDataSourceType.KAFKA.name();\n        Map<String, String> customProperties = Maps.newHashMap();\n\n        customProperties.put(CreateRoutineLoadStmt.KAFKA_TOPIC_PROPERTY, topicName);\n        customProperties.put(CreateRoutineLoadStmt.KAFKA_BROKER_LIST_PROPERTY, serverAddress);\n        customProperties.put(CreateRoutineLoadStmt.KAFKA_PARTITIONS_PROPERTY, kafkaPartitionString);\n\n        CreateRoutineLoadStmt createRoutineLoadStmt = new CreateRoutineLoadStmt(labelName, tableNameString,\n                                                                                loadPropertyList, properties,\n                                                                                typeName, customProperties,\n                                                                                LoadTask.MergeType.APPEND);\n        new MockUp<StatementBase>() {\n            @Mock\n            public void analyze(Analyzer analyzer1) {\n                return;\n            }\n        };\n\n        new Expectations(){\n            {\n                ctx.getSessionVariable();\n                result = sessionVariable;\n                sessionVariable.getSendBatchParallelism();\n                result = 1;\n            }\n        };\n\n        createRoutineLoadStmt.analyze(analyzer);\n\n        Assert.assertNotNull(createRoutineLoadStmt.getRoutineLoadDesc());\n        Assert.assertEquals(columnSeparator, createRoutineLoadStmt.getRoutineLoadDesc().getColumnSeparator());\n        Assert.assertEquals(partitionNames.getPartitionNames(), createRoutineLoadStmt.getRoutineLoadDesc().getPartitionNames().getPartitionNames());\n        Assert.assertEquals(2, createRoutineLoadStmt.getDesiredConcurrentNum());\n        Assert.assertEquals(0, createRoutineLoadStmt.getMaxErrorNum());\n        Assert.assertEquals(serverAddress, createRoutineLoadStmt.getKafkaBrokerList());\n        Assert.assertEquals(topicName, createRoutineLoadStmt.getKafkaTopic());\n        Assert.assertEquals(\"+08:00\", createRoutineLoadStmt.getTimezone());\n    }\n","date":"2021-09-07 11:53:32","endLine":202,"groupId":"10148","id":7,"instanceNumber":1,"isCurCommit":1,"methodName":"testAnalyze","params":"(@InjectableAnalyzeranalyzer@@InjectableSessionVariablesessionVariable)","path":"/mnt/clonedata/CloneManagementServer/ManagementServer/consistResult/result-incubatordoris-10-0.7/blobInfo/CC_OUT/blobs/8b/55b5f0a462171bbbb6ea0692274f1c8307404f.src","preCode":"    public void testAnalyze(@Injectable Analyzer analyzer) throws UserException {\n        String jobName = \"job1\";\n        String dbName = \"db1\";\n        LabelName labelName = new LabelName(dbName, jobName);\n        String tableNameString = \"table1\";\n        String topicName = \"topic1\";\n        String serverAddress = \"127.0.0.1:8080\";\n        String kafkaPartitionString = \"1,2,3\";\n        String timeZone = \"8:00\";\n        List<String> partitionNameString = Lists.newArrayList();\n        partitionNameString.add(\"p1\");\n        PartitionNames partitionNames = new PartitionNames(false, partitionNameString);\n        Separator columnSeparator = new Separator(\",\");\n\n        \r\n        TableName tableName = new TableName(dbName, tableNameString);\n        List<ParseNode> loadPropertyList = new ArrayList<>();\n        loadPropertyList.add(columnSeparator);\n        loadPropertyList.add(partitionNames);\n        Map<String, String> properties = Maps.newHashMap();\n        properties.put(CreateRoutineLoadStmt.DESIRED_CONCURRENT_NUMBER_PROPERTY, \"2\");\n        properties.put(LoadStmt.TIMEZONE, timeZone);\n        String typeName = LoadDataSourceType.KAFKA.name();\n        Map<String, String> customProperties = Maps.newHashMap();\n\n        customProperties.put(CreateRoutineLoadStmt.KAFKA_TOPIC_PROPERTY, topicName);\n        customProperties.put(CreateRoutineLoadStmt.KAFKA_BROKER_LIST_PROPERTY, serverAddress);\n        customProperties.put(CreateRoutineLoadStmt.KAFKA_PARTITIONS_PROPERTY, kafkaPartitionString);\n\n        CreateRoutineLoadStmt createRoutineLoadStmt = new CreateRoutineLoadStmt(labelName, tableNameString,\n                                                                                loadPropertyList, properties,\n                                                                                typeName, customProperties,\n                                                                                LoadTask.MergeType.APPEND);\n        new MockUp<StatementBase>() {\n            @Mock\n            public void analyze(Analyzer analyzer1) {\n                return;\n            }\n        };\n\n        createRoutineLoadStmt.analyze(analyzer);\n\n        Assert.assertNotNull(createRoutineLoadStmt.getRoutineLoadDesc());\n        Assert.assertEquals(columnSeparator, createRoutineLoadStmt.getRoutineLoadDesc().getColumnSeparator());\n        Assert.assertEquals(partitionNames.getPartitionNames(), createRoutineLoadStmt.getRoutineLoadDesc().getPartitionNames().getPartitionNames());\n        Assert.assertEquals(2, createRoutineLoadStmt.getDesiredConcurrentNum());\n        Assert.assertEquals(0, createRoutineLoadStmt.getMaxErrorNum());\n        Assert.assertEquals(serverAddress, createRoutineLoadStmt.getKafkaBrokerList());\n        Assert.assertEquals(topicName, createRoutineLoadStmt.getKafkaTopic());\n        Assert.assertEquals(\"+08:00\", createRoutineLoadStmt.getTimezone());\n    }\n","realPath":"fe/fe-core/src/test/java/org/apache/doris/analysis/CreateRoutineLoadStmtTest.java","repoName":"incubatordoris","snippetEndLine":0,"snippetStartLine":0,"startLine":142,"status":"M"},{"authorDate":"2020-08-21 22:57:16","commitOrder":4,"curCode":"    private CreateRoutineLoadStmt initCreateRoutineLoadStmt() {\n        List<ParseNode> loadPropertyList = new ArrayList<>();\n        loadPropertyList.add(columnSeparator);\n        loadPropertyList.add(partitionNames);\n        Map<String, String> properties = Maps.newHashMap();\n        properties.put(CreateRoutineLoadStmt.DESIRED_CONCURRENT_NUMBER_PROPERTY, \"2\");\n        String typeName = LoadDataSourceType.KAFKA.name();\n        Map<String, String> customProperties = Maps.newHashMap();\n\n        customProperties.put(CreateRoutineLoadStmt.KAFKA_TOPIC_PROPERTY, topicName);\n        customProperties.put(CreateRoutineLoadStmt.KAFKA_BROKER_LIST_PROPERTY, serverAddress);\n        customProperties.put(CreateRoutineLoadStmt.KAFKA_PARTITIONS_PROPERTY, kafkaPartitionString);\n\n        CreateRoutineLoadStmt createRoutineLoadStmt = new CreateRoutineLoadStmt(labelName, tableNameString,\n                                                                                loadPropertyList, properties,\n                                                                                typeName, customProperties,\n                                                                                LoadTask.MergeType.APPEND);\n        Deencapsulation.setField(createRoutineLoadStmt, \"name\", jobName);\n        return createRoutineLoadStmt;\n    }\n","date":"2020-08-21 22:57:16","endLine":339,"groupId":"10148","id":8,"instanceNumber":2,"isCurCommit":0,"methodName":"initCreateRoutineLoadStmt","params":"()","path":"/mnt/clonedata/CloneManagementServer/ManagementServer/consistResult/result-incubatordoris-10-0.7/blobInfo/CC_OUT/blobs/5f/6750c93658333be1ad8061fb532f5417ed6c60.src","preCode":"    private CreateRoutineLoadStmt initCreateRoutineLoadStmt() {\n        List<ParseNode> loadPropertyList = new ArrayList<>();\n        loadPropertyList.add(columnSeparator);\n        loadPropertyList.add(partitionNames);\n        Map<String, String> properties = Maps.newHashMap();\n        properties.put(CreateRoutineLoadStmt.DESIRED_CONCURRENT_NUMBER_PROPERTY, \"2\");\n        String typeName = LoadDataSourceType.KAFKA.name();\n        Map<String, String> customProperties = Maps.newHashMap();\n\n        customProperties.put(CreateRoutineLoadStmt.KAFKA_TOPIC_PROPERTY, topicName);\n        customProperties.put(CreateRoutineLoadStmt.KAFKA_BROKER_LIST_PROPERTY, serverAddress);\n        customProperties.put(CreateRoutineLoadStmt.KAFKA_PARTITIONS_PROPERTY, kafkaPartitionString);\n\n        CreateRoutineLoadStmt createRoutineLoadStmt = new CreateRoutineLoadStmt(labelName, tableNameString,\n                                                                                loadPropertyList, properties,\n                                                                                typeName, customProperties,\n                                                                                LoadTask.MergeType.APPEND);\n        Deencapsulation.setField(createRoutineLoadStmt, \"name\", jobName);\n        return createRoutineLoadStmt;\n    }\n","realPath":"fe/fe-core/src/test/java/org/apache/doris/load/routineload/KafkaRoutineLoadJobTest.java","repoName":"incubatordoris","snippetEndLine":0,"snippetStartLine":0,"startLine":320,"status":"N"}],"commitId":"9469b2ce1ab4b6db02dacdd8159d272ed0ed6981","commitMessage":"@@@[Outfile] Support concurrent export of query results (#6539)\n\nThis pr mainly supports\n1. Export query result sets concurrently\n2. Query result set export supports s3 protocol\n\nAmong them.  there are several preconditions for concurrently exporting query result sets\n1. Enable concurrent export variables\n2. The query itself can be exported concurrently\n    (some queries containing sort nodes at the top level cannot be exported concurrently)\n3. Export the s3 protocol used instead of the broker\n\nAfter exporting the result set concurrently. \nthe file prefix is changed to outfile_{query_instance_id}_filenumber.{file_format}","date":"2021-09-07 11:53:32","modifiedFileCount":"16","status":"M","submitter":"EmmyMiao87"}]
