[{"authorTime":"2018-07-16 16:30:07","codes":[{"authorDate":"2018-07-16 16:30:07","commitOrder":1,"curCode":"    public void testIntervalHour() throws IOException {\n        testSearchCase(new MatchAllDocsQuery(),\n                Arrays.asList(\n                        \"2017-02-01T09:02:00.000Z\",\n                        \"2017-02-01T09:35:00.000Z\",\n                        \"2017-02-01T10:15:00.000Z\",\n                        \"2017-02-01T13:06:00.000Z\",\n                        \"2017-02-01T14:04:00.000Z\",\n                        \"2017-02-01T14:05:00.000Z\",\n                        \"2017-02-01T15:59:00.000Z\",\n                        \"2017-02-01T16:06:00.000Z\",\n                        \"2017-02-01T16:48:00.000Z\",\n                        \"2017-02-01T16:59:00.000Z\"\n                ),\n                aggregation -> aggregation.setNumBuckets(8).field(DATE_FIELD),\n                histogram -> {\n                    List<? extends Histogram.Bucket> buckets = histogram.getBuckets();\n                    assertEquals(10, buckets.size());\n\n                    Histogram.Bucket bucket = buckets.get(0);\n                    assertEquals(\"2017-02-01T09:02:00.000Z\", bucket.getKeyAsString());\n                    assertEquals(1, bucket.getDocCount());\n\n                    bucket = buckets.get(1);\n                    assertEquals(\"2017-02-01T09:35:00.000Z\", bucket.getKeyAsString());\n                    assertEquals(1, bucket.getDocCount());\n\n                    bucket = buckets.get(2);\n                    assertEquals(\"2017-02-01T10:15:00.000Z\", bucket.getKeyAsString());\n                    assertEquals(1, bucket.getDocCount());\n\n                    bucket = buckets.get(3);\n                    assertEquals(\"2017-02-01T13:06:00.000Z\", bucket.getKeyAsString());\n                    assertEquals(1, bucket.getDocCount());\n\n                    bucket = buckets.get(4);\n                    assertEquals(\"2017-02-01T14:04:00.000Z\", bucket.getKeyAsString());\n                    assertEquals(1, bucket.getDocCount());\n\n                    bucket = buckets.get(5);\n                    assertEquals(\"2017-02-01T14:05:00.000Z\", bucket.getKeyAsString());\n                    assertEquals(1, bucket.getDocCount());\n\n                    bucket = buckets.get(6);\n                    assertEquals(\"2017-02-01T15:59:00.000Z\", bucket.getKeyAsString());\n                    assertEquals(1, bucket.getDocCount());\n\n                    bucket = buckets.get(7);\n                    assertEquals(\"2017-02-01T16:06:00.000Z\", bucket.getKeyAsString());\n                    assertEquals(1, bucket.getDocCount());\n\n                    bucket = buckets.get(8);\n                    assertEquals(\"2017-02-01T16:48:00.000Z\", bucket.getKeyAsString());\n                    assertEquals(1, bucket.getDocCount());\n\n                    bucket = buckets.get(9);\n                    assertEquals(\"2017-02-01T16:59:00.000Z\", bucket.getKeyAsString());\n                    assertEquals(1, bucket.getDocCount());\n                }\n        );\n        testSearchAndReduceCase(new MatchAllDocsQuery(),\n                Arrays.asList(\n                        \"2017-02-01T09:02:00.000Z\",\n                        \"2017-02-01T09:35:00.000Z\",\n                        \"2017-02-01T10:15:00.000Z\",\n                        \"2017-02-01T13:06:00.000Z\",\n                        \"2017-02-01T14:04:00.000Z\",\n                        \"2017-02-01T14:05:00.000Z\",\n                        \"2017-02-01T15:59:00.000Z\",\n                        \"2017-02-01T16:06:00.000Z\",\n                        \"2017-02-01T16:48:00.000Z\",\n                        \"2017-02-01T16:59:00.000Z\"\n                ),\n                aggregation -> aggregation.setNumBuckets(10).field(DATE_FIELD),\n                histogram -> {\n                    List<? extends Histogram.Bucket> buckets = histogram.getBuckets();\n                    assertEquals(8, buckets.size());\n\n                    Histogram.Bucket bucket = buckets.get(0);\n                    assertEquals(\"2017-02-01T09:00:00.000Z\", bucket.getKeyAsString());\n                    assertEquals(2, bucket.getDocCount());\n\n                    bucket = buckets.get(1);\n                    assertEquals(\"2017-02-01T10:00:00.000Z\", bucket.getKeyAsString());\n                    assertEquals(1, bucket.getDocCount());\n\n                    bucket = buckets.get(2);\n                    assertEquals(\"2017-02-01T11:00:00.000Z\", bucket.getKeyAsString());\n                    assertEquals(0, bucket.getDocCount());\n\n                    bucket = buckets.get(3);\n                    assertEquals(\"2017-02-01T12:00:00.000Z\", bucket.getKeyAsString());\n                    assertEquals(0, bucket.getDocCount());\n\n                    bucket = buckets.get(4);\n                    assertEquals(\"2017-02-01T13:00:00.000Z\", bucket.getKeyAsString());\n                    assertEquals(1, bucket.getDocCount());\n\n                    bucket = buckets.get(5);\n                    assertEquals(\"2017-02-01T14:00:00.000Z\", bucket.getKeyAsString());\n                    assertEquals(2, bucket.getDocCount());\n\n                    bucket = buckets.get(6);\n                    assertEquals(\"2017-02-01T15:00:00.000Z\", bucket.getKeyAsString());\n                    assertEquals(1, bucket.getDocCount());\n\n                    bucket = buckets.get(7);\n                    assertEquals(\"2017-02-01T16:00:00.000Z\", bucket.getKeyAsString());\n                    assertEquals(3, bucket.getDocCount());\n                }\n        );\n    }\n","date":"2018-07-16 16:30:07","endLine":526,"groupId":"65636","id":1,"instanceNumber":1,"isCurCommit":0,"methodName":"testIntervalHour","params":"()","path":"/mnt/clonedata/CloneManagementServer/ManagementServer/consistResult/result-elasticsearch-10-0.7/blobInfo/CC_OUT/blobs/7c/f29e3aa9cc540ec5b371f56f24a3654b10f0c1.src","preCode":"    public void testIntervalHour() throws IOException {\n        testSearchCase(new MatchAllDocsQuery(),\n                Arrays.asList(\n                        \"2017-02-01T09:02:00.000Z\",\n                        \"2017-02-01T09:35:00.000Z\",\n                        \"2017-02-01T10:15:00.000Z\",\n                        \"2017-02-01T13:06:00.000Z\",\n                        \"2017-02-01T14:04:00.000Z\",\n                        \"2017-02-01T14:05:00.000Z\",\n                        \"2017-02-01T15:59:00.000Z\",\n                        \"2017-02-01T16:06:00.000Z\",\n                        \"2017-02-01T16:48:00.000Z\",\n                        \"2017-02-01T16:59:00.000Z\"\n                ),\n                aggregation -> aggregation.setNumBuckets(8).field(DATE_FIELD),\n                histogram -> {\n                    List<? extends Histogram.Bucket> buckets = histogram.getBuckets();\n                    assertEquals(10, buckets.size());\n\n                    Histogram.Bucket bucket = buckets.get(0);\n                    assertEquals(\"2017-02-01T09:02:00.000Z\", bucket.getKeyAsString());\n                    assertEquals(1, bucket.getDocCount());\n\n                    bucket = buckets.get(1);\n                    assertEquals(\"2017-02-01T09:35:00.000Z\", bucket.getKeyAsString());\n                    assertEquals(1, bucket.getDocCount());\n\n                    bucket = buckets.get(2);\n                    assertEquals(\"2017-02-01T10:15:00.000Z\", bucket.getKeyAsString());\n                    assertEquals(1, bucket.getDocCount());\n\n                    bucket = buckets.get(3);\n                    assertEquals(\"2017-02-01T13:06:00.000Z\", bucket.getKeyAsString());\n                    assertEquals(1, bucket.getDocCount());\n\n                    bucket = buckets.get(4);\n                    assertEquals(\"2017-02-01T14:04:00.000Z\", bucket.getKeyAsString());\n                    assertEquals(1, bucket.getDocCount());\n\n                    bucket = buckets.get(5);\n                    assertEquals(\"2017-02-01T14:05:00.000Z\", bucket.getKeyAsString());\n                    assertEquals(1, bucket.getDocCount());\n\n                    bucket = buckets.get(6);\n                    assertEquals(\"2017-02-01T15:59:00.000Z\", bucket.getKeyAsString());\n                    assertEquals(1, bucket.getDocCount());\n\n                    bucket = buckets.get(7);\n                    assertEquals(\"2017-02-01T16:06:00.000Z\", bucket.getKeyAsString());\n                    assertEquals(1, bucket.getDocCount());\n\n                    bucket = buckets.get(8);\n                    assertEquals(\"2017-02-01T16:48:00.000Z\", bucket.getKeyAsString());\n                    assertEquals(1, bucket.getDocCount());\n\n                    bucket = buckets.get(9);\n                    assertEquals(\"2017-02-01T16:59:00.000Z\", bucket.getKeyAsString());\n                    assertEquals(1, bucket.getDocCount());\n                }\n        );\n        testSearchAndReduceCase(new MatchAllDocsQuery(),\n                Arrays.asList(\n                        \"2017-02-01T09:02:00.000Z\",\n                        \"2017-02-01T09:35:00.000Z\",\n                        \"2017-02-01T10:15:00.000Z\",\n                        \"2017-02-01T13:06:00.000Z\",\n                        \"2017-02-01T14:04:00.000Z\",\n                        \"2017-02-01T14:05:00.000Z\",\n                        \"2017-02-01T15:59:00.000Z\",\n                        \"2017-02-01T16:06:00.000Z\",\n                        \"2017-02-01T16:48:00.000Z\",\n                        \"2017-02-01T16:59:00.000Z\"\n                ),\n                aggregation -> aggregation.setNumBuckets(10).field(DATE_FIELD),\n                histogram -> {\n                    List<? extends Histogram.Bucket> buckets = histogram.getBuckets();\n                    assertEquals(8, buckets.size());\n\n                    Histogram.Bucket bucket = buckets.get(0);\n                    assertEquals(\"2017-02-01T09:00:00.000Z\", bucket.getKeyAsString());\n                    assertEquals(2, bucket.getDocCount());\n\n                    bucket = buckets.get(1);\n                    assertEquals(\"2017-02-01T10:00:00.000Z\", bucket.getKeyAsString());\n                    assertEquals(1, bucket.getDocCount());\n\n                    bucket = buckets.get(2);\n                    assertEquals(\"2017-02-01T11:00:00.000Z\", bucket.getKeyAsString());\n                    assertEquals(0, bucket.getDocCount());\n\n                    bucket = buckets.get(3);\n                    assertEquals(\"2017-02-01T12:00:00.000Z\", bucket.getKeyAsString());\n                    assertEquals(0, bucket.getDocCount());\n\n                    bucket = buckets.get(4);\n                    assertEquals(\"2017-02-01T13:00:00.000Z\", bucket.getKeyAsString());\n                    assertEquals(1, bucket.getDocCount());\n\n                    bucket = buckets.get(5);\n                    assertEquals(\"2017-02-01T14:00:00.000Z\", bucket.getKeyAsString());\n                    assertEquals(2, bucket.getDocCount());\n\n                    bucket = buckets.get(6);\n                    assertEquals(\"2017-02-01T15:00:00.000Z\", bucket.getKeyAsString());\n                    assertEquals(1, bucket.getDocCount());\n\n                    bucket = buckets.get(7);\n                    assertEquals(\"2017-02-01T16:00:00.000Z\", bucket.getKeyAsString());\n                    assertEquals(3, bucket.getDocCount());\n                }\n        );\n    }\n","realPath":"server/src/test/java/org/elasticsearch/search/aggregations/bucket/histogram/AutoDateHistogramAggregatorTests.java","repoName":"elasticsearch","snippetEndLine":0,"snippetStartLine":0,"startLine":415,"status":"B"},{"authorDate":"2018-07-16 16:30:07","commitOrder":1,"curCode":"    public void testIntervalHourWithTZ() throws IOException {\n        testSearchCase(new MatchAllDocsQuery(),\n                Arrays.asList(\n                        \"2017-02-01T09:02:00.000Z\",\n                        \"2017-02-01T09:35:00.000Z\",\n                        \"2017-02-01T10:15:00.000Z\",\n                        \"2017-02-01T13:06:00.000Z\",\n                        \"2017-02-01T14:04:00.000Z\",\n                        \"2017-02-01T14:05:00.000Z\",\n                        \"2017-02-01T15:59:00.000Z\",\n                        \"2017-02-01T16:06:00.000Z\",\n                        \"2017-02-01T16:48:00.000Z\",\n                        \"2017-02-01T16:59:00.000Z\"\n                ),\n                aggregation -> aggregation.setNumBuckets(8).field(DATE_FIELD).timeZone(DateTimeZone.forOffsetHours(-1)),\n                histogram -> {\n                    List<? extends Histogram.Bucket> buckets = histogram.getBuckets();\n                    assertEquals(10, buckets.size());\n\n                    Histogram.Bucket bucket = buckets.get(0);\n                    assertEquals(\"2017-02-01T08:02:00.000-01:00\", bucket.getKeyAsString());\n                    assertEquals(1, bucket.getDocCount());\n\n                    bucket = buckets.get(1);\n                    assertEquals(\"2017-02-01T08:35:00.000-01:00\", bucket.getKeyAsString());\n                    assertEquals(1, bucket.getDocCount());\n\n                    bucket = buckets.get(2);\n                    assertEquals(\"2017-02-01T09:15:00.000-01:00\", bucket.getKeyAsString());\n                    assertEquals(1, bucket.getDocCount());\n\n                    bucket = buckets.get(3);\n                    assertEquals(\"2017-02-01T12:06:00.000-01:00\", bucket.getKeyAsString());\n                    assertEquals(1, bucket.getDocCount());\n\n                    bucket = buckets.get(4);\n                    assertEquals(\"2017-02-01T13:04:00.000-01:00\", bucket.getKeyAsString());\n                    assertEquals(1, bucket.getDocCount());\n\n                    bucket = buckets.get(5);\n                    assertEquals(\"2017-02-01T13:05:00.000-01:00\", bucket.getKeyAsString());\n                    assertEquals(1, bucket.getDocCount());\n\n                    bucket = buckets.get(6);\n                    assertEquals(\"2017-02-01T14:59:00.000-01:00\", bucket.getKeyAsString());\n                    assertEquals(1, bucket.getDocCount());\n\n                    bucket = buckets.get(7);\n                    assertEquals(\"2017-02-01T15:06:00.000-01:00\", bucket.getKeyAsString());\n                    assertEquals(1, bucket.getDocCount());\n\n                    bucket = buckets.get(8);\n                    assertEquals(\"2017-02-01T15:48:00.000-01:00\", bucket.getKeyAsString());\n                    assertEquals(1, bucket.getDocCount());\n\n                    bucket = buckets.get(9);\n                    assertEquals(\"2017-02-01T15:59:00.000-01:00\", bucket.getKeyAsString());\n                    assertEquals(1, bucket.getDocCount());\n                }\n        );\n        testSearchAndReduceCase(new MatchAllDocsQuery(),\n                Arrays.asList(\n                        \"2017-02-01T09:02:00.000Z\",\n                        \"2017-02-01T09:35:00.000Z\",\n                        \"2017-02-01T10:15:00.000Z\",\n                        \"2017-02-01T13:06:00.000Z\",\n                        \"2017-02-01T14:04:00.000Z\",\n                        \"2017-02-01T14:05:00.000Z\",\n                        \"2017-02-01T15:59:00.000Z\",\n                        \"2017-02-01T16:06:00.000Z\",\n                        \"2017-02-01T16:48:00.000Z\",\n                        \"2017-02-01T16:59:00.000Z\"\n                ),\n                aggregation -> aggregation.setNumBuckets(10).field(DATE_FIELD).timeZone(DateTimeZone.forOffsetHours(-1)),\n                histogram -> {\n                    List<? extends Histogram.Bucket> buckets = histogram.getBuckets();\n                    assertEquals(8, buckets.size());\n\n                    Histogram.Bucket bucket = buckets.get(0);\n                    assertEquals(\"2017-02-01T08:00:00.000-01:00\", bucket.getKeyAsString());\n                    assertEquals(2, bucket.getDocCount());\n\n                    bucket = buckets.get(1);\n                    assertEquals(\"2017-02-01T09:00:00.000-01:00\", bucket.getKeyAsString());\n                    assertEquals(1, bucket.getDocCount());\n\n                    bucket = buckets.get(2);\n                    assertEquals(\"2017-02-01T10:00:00.000-01:00\", bucket.getKeyAsString());\n                    assertEquals(0, bucket.getDocCount());\n\n                    bucket = buckets.get(3);\n                    assertEquals(\"2017-02-01T11:00:00.000-01:00\", bucket.getKeyAsString());\n                    assertEquals(0, bucket.getDocCount());\n\n                    bucket = buckets.get(4);\n                    assertEquals(\"2017-02-01T12:00:00.000-01:00\", bucket.getKeyAsString());\n                    assertEquals(1, bucket.getDocCount());\n\n                    bucket = buckets.get(5);\n                    assertEquals(\"2017-02-01T13:00:00.000-01:00\", bucket.getKeyAsString());\n                    assertEquals(2, bucket.getDocCount());\n\n                    bucket = buckets.get(6);\n                    assertEquals(\"2017-02-01T14:00:00.000-01:00\", bucket.getKeyAsString());\n                    assertEquals(1, bucket.getDocCount());\n\n                    bucket = buckets.get(7);\n                    assertEquals(\"2017-02-01T15:00:00.000-01:00\", bucket.getKeyAsString());\n                    assertEquals(3, bucket.getDocCount());\n                }\n        );\n    }\n","date":"2018-07-16 16:30:07","endLine":639,"groupId":"48920","id":2,"instanceNumber":2,"isCurCommit":0,"methodName":"testIntervalHourWithTZ","params":"()","path":"/mnt/clonedata/CloneManagementServer/ManagementServer/consistResult/result-elasticsearch-10-0.7/blobInfo/CC_OUT/blobs/7c/f29e3aa9cc540ec5b371f56f24a3654b10f0c1.src","preCode":"    public void testIntervalHourWithTZ() throws IOException {\n        testSearchCase(new MatchAllDocsQuery(),\n                Arrays.asList(\n                        \"2017-02-01T09:02:00.000Z\",\n                        \"2017-02-01T09:35:00.000Z\",\n                        \"2017-02-01T10:15:00.000Z\",\n                        \"2017-02-01T13:06:00.000Z\",\n                        \"2017-02-01T14:04:00.000Z\",\n                        \"2017-02-01T14:05:00.000Z\",\n                        \"2017-02-01T15:59:00.000Z\",\n                        \"2017-02-01T16:06:00.000Z\",\n                        \"2017-02-01T16:48:00.000Z\",\n                        \"2017-02-01T16:59:00.000Z\"\n                ),\n                aggregation -> aggregation.setNumBuckets(8).field(DATE_FIELD).timeZone(DateTimeZone.forOffsetHours(-1)),\n                histogram -> {\n                    List<? extends Histogram.Bucket> buckets = histogram.getBuckets();\n                    assertEquals(10, buckets.size());\n\n                    Histogram.Bucket bucket = buckets.get(0);\n                    assertEquals(\"2017-02-01T08:02:00.000-01:00\", bucket.getKeyAsString());\n                    assertEquals(1, bucket.getDocCount());\n\n                    bucket = buckets.get(1);\n                    assertEquals(\"2017-02-01T08:35:00.000-01:00\", bucket.getKeyAsString());\n                    assertEquals(1, bucket.getDocCount());\n\n                    bucket = buckets.get(2);\n                    assertEquals(\"2017-02-01T09:15:00.000-01:00\", bucket.getKeyAsString());\n                    assertEquals(1, bucket.getDocCount());\n\n                    bucket = buckets.get(3);\n                    assertEquals(\"2017-02-01T12:06:00.000-01:00\", bucket.getKeyAsString());\n                    assertEquals(1, bucket.getDocCount());\n\n                    bucket = buckets.get(4);\n                    assertEquals(\"2017-02-01T13:04:00.000-01:00\", bucket.getKeyAsString());\n                    assertEquals(1, bucket.getDocCount());\n\n                    bucket = buckets.get(5);\n                    assertEquals(\"2017-02-01T13:05:00.000-01:00\", bucket.getKeyAsString());\n                    assertEquals(1, bucket.getDocCount());\n\n                    bucket = buckets.get(6);\n                    assertEquals(\"2017-02-01T14:59:00.000-01:00\", bucket.getKeyAsString());\n                    assertEquals(1, bucket.getDocCount());\n\n                    bucket = buckets.get(7);\n                    assertEquals(\"2017-02-01T15:06:00.000-01:00\", bucket.getKeyAsString());\n                    assertEquals(1, bucket.getDocCount());\n\n                    bucket = buckets.get(8);\n                    assertEquals(\"2017-02-01T15:48:00.000-01:00\", bucket.getKeyAsString());\n                    assertEquals(1, bucket.getDocCount());\n\n                    bucket = buckets.get(9);\n                    assertEquals(\"2017-02-01T15:59:00.000-01:00\", bucket.getKeyAsString());\n                    assertEquals(1, bucket.getDocCount());\n                }\n        );\n        testSearchAndReduceCase(new MatchAllDocsQuery(),\n                Arrays.asList(\n                        \"2017-02-01T09:02:00.000Z\",\n                        \"2017-02-01T09:35:00.000Z\",\n                        \"2017-02-01T10:15:00.000Z\",\n                        \"2017-02-01T13:06:00.000Z\",\n                        \"2017-02-01T14:04:00.000Z\",\n                        \"2017-02-01T14:05:00.000Z\",\n                        \"2017-02-01T15:59:00.000Z\",\n                        \"2017-02-01T16:06:00.000Z\",\n                        \"2017-02-01T16:48:00.000Z\",\n                        \"2017-02-01T16:59:00.000Z\"\n                ),\n                aggregation -> aggregation.setNumBuckets(10).field(DATE_FIELD).timeZone(DateTimeZone.forOffsetHours(-1)),\n                histogram -> {\n                    List<? extends Histogram.Bucket> buckets = histogram.getBuckets();\n                    assertEquals(8, buckets.size());\n\n                    Histogram.Bucket bucket = buckets.get(0);\n                    assertEquals(\"2017-02-01T08:00:00.000-01:00\", bucket.getKeyAsString());\n                    assertEquals(2, bucket.getDocCount());\n\n                    bucket = buckets.get(1);\n                    assertEquals(\"2017-02-01T09:00:00.000-01:00\", bucket.getKeyAsString());\n                    assertEquals(1, bucket.getDocCount());\n\n                    bucket = buckets.get(2);\n                    assertEquals(\"2017-02-01T10:00:00.000-01:00\", bucket.getKeyAsString());\n                    assertEquals(0, bucket.getDocCount());\n\n                    bucket = buckets.get(3);\n                    assertEquals(\"2017-02-01T11:00:00.000-01:00\", bucket.getKeyAsString());\n                    assertEquals(0, bucket.getDocCount());\n\n                    bucket = buckets.get(4);\n                    assertEquals(\"2017-02-01T12:00:00.000-01:00\", bucket.getKeyAsString());\n                    assertEquals(1, bucket.getDocCount());\n\n                    bucket = buckets.get(5);\n                    assertEquals(\"2017-02-01T13:00:00.000-01:00\", bucket.getKeyAsString());\n                    assertEquals(2, bucket.getDocCount());\n\n                    bucket = buckets.get(6);\n                    assertEquals(\"2017-02-01T14:00:00.000-01:00\", bucket.getKeyAsString());\n                    assertEquals(1, bucket.getDocCount());\n\n                    bucket = buckets.get(7);\n                    assertEquals(\"2017-02-01T15:00:00.000-01:00\", bucket.getKeyAsString());\n                    assertEquals(3, bucket.getDocCount());\n                }\n        );\n    }\n","realPath":"server/src/test/java/org/elasticsearch/search/aggregations/bucket/histogram/AutoDateHistogramAggregatorTests.java","repoName":"elasticsearch","snippetEndLine":0,"snippetStartLine":0,"startLine":528,"status":"B"}],"commitId":"5d3a53843a21098de3d1a342482db555eb660976","commitMessage":"@@@Merge branch 'master' into index-lifecycle\n","date":"2018-07-16 16:30:07","modifiedFileCount":"183","status":"B","submitter":"Colin Goodheart-Smithe"},{"authorTime":"2018-11-19 22:21:01","codes":[{"authorDate":"2018-11-19 22:21:01","commitOrder":2,"curCode":"    public void testIntervalHour() throws IOException {\n        final List<DateTime> datesForHourInterval = Arrays.asList(\n            new DateTime(2017, 2, 1, 9, 2, 0, DateTimeZone.UTC),\n            new DateTime(2017, 2, 1, 9, 35, 0, DateTimeZone.UTC),\n            new DateTime(2017, 2, 1, 10, 15, 0, DateTimeZone.UTC),\n            new DateTime(2017, 2, 1, 13, 6, 0, DateTimeZone.UTC),\n            new DateTime(2017, 2, 1, 14, 4, 0, DateTimeZone.UTC),\n            new DateTime(2017, 2, 1, 14, 5, 0, DateTimeZone.UTC),\n            new DateTime(2017, 2, 1, 15, 59, 0, DateTimeZone.UTC),\n            new DateTime(2017, 2, 1, 16, 6, 0, DateTimeZone.UTC),\n            new DateTime(2017, 2, 1, 16, 48, 0, DateTimeZone.UTC),\n            new DateTime(2017, 2, 1, 16, 59, 0, DateTimeZone.UTC));\n        testSearchCase(DEFAULT_QUERY, datesForHourInterval,\n            aggregation -> aggregation.setNumBuckets(8).field(DATE_FIELD),\n            histogram -> {\n                final List<? extends Histogram.Bucket> buckets = histogram.getBuckets();\n                assertEquals(datesForHourInterval.size(), buckets.size());\n                for (int i = 0; i < buckets.size(); i++) {\n                    final Histogram.Bucket bucket = buckets.get(i);\n                    assertEquals(datesForHourInterval.get(i), bucket.getKey());\n                    assertEquals(1, bucket.getDocCount());\n                }\n            }\n        );\n        testSearchAndReduceCase(DEFAULT_QUERY, datesForHourInterval,\n            aggregation -> aggregation.setNumBuckets(10).field(DATE_FIELD),\n            histogram -> {\n                final Map<DateTime, Integer> expectedDocCount = new HashMap<>();\n                expectedDocCount.put(datesForHourInterval.get(0).withMinuteOfHour(0), 2);\n                expectedDocCount.put(datesForHourInterval.get(2).withMinuteOfHour(0), 1);\n                expectedDocCount.put(datesForHourInterval.get(3).withMinuteOfHour(0), 1);\n                expectedDocCount.put(datesForHourInterval.get(4).withMinuteOfHour(0), 2);\n                expectedDocCount.put(datesForHourInterval.get(6).withMinuteOfHour(0), 1);\n                expectedDocCount.put(datesForHourInterval.get(7).withMinuteOfHour(0), 3);\n                final List<? extends Histogram.Bucket> buckets = histogram.getBuckets();\n                assertEquals(8, buckets.size());\n                buckets.forEach(bucket ->\n                    assertEquals(expectedDocCount.getOrDefault(bucket.getKey(), 0).longValue(), bucket.getDocCount()));\n            }\n        );\n        testSearchAndReduceCase(DEFAULT_QUERY, datesForHourInterval,\n            aggregation -> aggregation.setNumBuckets(6).field(DATE_FIELD),\n            histogram -> {\n                final Map<DateTime, Integer> expectedDocCount = new HashMap<>();\n                expectedDocCount.put(datesForHourInterval.get(0).withMinuteOfHour(0), 3);\n                expectedDocCount.put(datesForHourInterval.get(0).plusHours(3).withMinuteOfHour(0), 3);\n                expectedDocCount.put(datesForHourInterval.get(0).plusHours(6).withMinuteOfHour(0), 4);\n                final List<? extends Histogram.Bucket> buckets = histogram.getBuckets();\n                assertEquals(expectedDocCount.size(), buckets.size());\n                buckets.forEach(bucket ->\n                    assertEquals(expectedDocCount.getOrDefault(bucket.getKey(), 0).longValue(), bucket.getDocCount()));\n            }\n        );\n    }\n","date":"2018-11-19 22:21:01","endLine":382,"groupId":"39068","id":3,"instanceNumber":1,"isCurCommit":0,"methodName":"testIntervalHour","params":"()","path":"/mnt/clonedata/CloneManagementServer/ManagementServer/consistResult/result-elasticsearch-10-0.7/blobInfo/CC_OUT/blobs/2d/5109405dc1c23b27b6f3f7dd3e8280cc59df6f.src","preCode":"    public void testIntervalHour() throws IOException {\n        testSearchCase(new MatchAllDocsQuery(),\n                Arrays.asList(\n                        \"2017-02-01T09:02:00.000Z\",\n                        \"2017-02-01T09:35:00.000Z\",\n                        \"2017-02-01T10:15:00.000Z\",\n                        \"2017-02-01T13:06:00.000Z\",\n                        \"2017-02-01T14:04:00.000Z\",\n                        \"2017-02-01T14:05:00.000Z\",\n                        \"2017-02-01T15:59:00.000Z\",\n                        \"2017-02-01T16:06:00.000Z\",\n                        \"2017-02-01T16:48:00.000Z\",\n                        \"2017-02-01T16:59:00.000Z\"\n                ),\n                aggregation -> aggregation.setNumBuckets(8).field(DATE_FIELD),\n                histogram -> {\n                    List<? extends Histogram.Bucket> buckets = histogram.getBuckets();\n                    assertEquals(10, buckets.size());\n\n                    Histogram.Bucket bucket = buckets.get(0);\n                    assertEquals(\"2017-02-01T09:02:00.000Z\", bucket.getKeyAsString());\n                    assertEquals(1, bucket.getDocCount());\n\n                    bucket = buckets.get(1);\n                    assertEquals(\"2017-02-01T09:35:00.000Z\", bucket.getKeyAsString());\n                    assertEquals(1, bucket.getDocCount());\n\n                    bucket = buckets.get(2);\n                    assertEquals(\"2017-02-01T10:15:00.000Z\", bucket.getKeyAsString());\n                    assertEquals(1, bucket.getDocCount());\n\n                    bucket = buckets.get(3);\n                    assertEquals(\"2017-02-01T13:06:00.000Z\", bucket.getKeyAsString());\n                    assertEquals(1, bucket.getDocCount());\n\n                    bucket = buckets.get(4);\n                    assertEquals(\"2017-02-01T14:04:00.000Z\", bucket.getKeyAsString());\n                    assertEquals(1, bucket.getDocCount());\n\n                    bucket = buckets.get(5);\n                    assertEquals(\"2017-02-01T14:05:00.000Z\", bucket.getKeyAsString());\n                    assertEquals(1, bucket.getDocCount());\n\n                    bucket = buckets.get(6);\n                    assertEquals(\"2017-02-01T15:59:00.000Z\", bucket.getKeyAsString());\n                    assertEquals(1, bucket.getDocCount());\n\n                    bucket = buckets.get(7);\n                    assertEquals(\"2017-02-01T16:06:00.000Z\", bucket.getKeyAsString());\n                    assertEquals(1, bucket.getDocCount());\n\n                    bucket = buckets.get(8);\n                    assertEquals(\"2017-02-01T16:48:00.000Z\", bucket.getKeyAsString());\n                    assertEquals(1, bucket.getDocCount());\n\n                    bucket = buckets.get(9);\n                    assertEquals(\"2017-02-01T16:59:00.000Z\", bucket.getKeyAsString());\n                    assertEquals(1, bucket.getDocCount());\n                }\n        );\n        testSearchAndReduceCase(new MatchAllDocsQuery(),\n                Arrays.asList(\n                        \"2017-02-01T09:02:00.000Z\",\n                        \"2017-02-01T09:35:00.000Z\",\n                        \"2017-02-01T10:15:00.000Z\",\n                        \"2017-02-01T13:06:00.000Z\",\n                        \"2017-02-01T14:04:00.000Z\",\n                        \"2017-02-01T14:05:00.000Z\",\n                        \"2017-02-01T15:59:00.000Z\",\n                        \"2017-02-01T16:06:00.000Z\",\n                        \"2017-02-01T16:48:00.000Z\",\n                        \"2017-02-01T16:59:00.000Z\"\n                ),\n                aggregation -> aggregation.setNumBuckets(10).field(DATE_FIELD),\n                histogram -> {\n                    List<? extends Histogram.Bucket> buckets = histogram.getBuckets();\n                    assertEquals(8, buckets.size());\n\n                    Histogram.Bucket bucket = buckets.get(0);\n                    assertEquals(\"2017-02-01T09:00:00.000Z\", bucket.getKeyAsString());\n                    assertEquals(2, bucket.getDocCount());\n\n                    bucket = buckets.get(1);\n                    assertEquals(\"2017-02-01T10:00:00.000Z\", bucket.getKeyAsString());\n                    assertEquals(1, bucket.getDocCount());\n\n                    bucket = buckets.get(2);\n                    assertEquals(\"2017-02-01T11:00:00.000Z\", bucket.getKeyAsString());\n                    assertEquals(0, bucket.getDocCount());\n\n                    bucket = buckets.get(3);\n                    assertEquals(\"2017-02-01T12:00:00.000Z\", bucket.getKeyAsString());\n                    assertEquals(0, bucket.getDocCount());\n\n                    bucket = buckets.get(4);\n                    assertEquals(\"2017-02-01T13:00:00.000Z\", bucket.getKeyAsString());\n                    assertEquals(1, bucket.getDocCount());\n\n                    bucket = buckets.get(5);\n                    assertEquals(\"2017-02-01T14:00:00.000Z\", bucket.getKeyAsString());\n                    assertEquals(2, bucket.getDocCount());\n\n                    bucket = buckets.get(6);\n                    assertEquals(\"2017-02-01T15:00:00.000Z\", bucket.getKeyAsString());\n                    assertEquals(1, bucket.getDocCount());\n\n                    bucket = buckets.get(7);\n                    assertEquals(\"2017-02-01T16:00:00.000Z\", bucket.getKeyAsString());\n                    assertEquals(3, bucket.getDocCount());\n                }\n        );\n    }\n","realPath":"server/src/test/java/org/elasticsearch/search/aggregations/bucket/histogram/AutoDateHistogramAggregatorTests.java","repoName":"elasticsearch","snippetEndLine":0,"snippetStartLine":0,"startLine":329,"status":"M"},{"authorDate":"2018-11-19 22:21:01","commitOrder":2,"curCode":"    public void testIntervalHourWithTZ() throws IOException {\n        final List<DateTime> datesForHourInterval = Arrays.asList(\n            new DateTime(2017, 2, 1, 9, 2, 0, DateTimeZone.UTC),\n            new DateTime(2017, 2, 1, 9, 35, 0, DateTimeZone.UTC),\n            new DateTime(2017, 2, 1, 10, 15, 0, DateTimeZone.UTC),\n            new DateTime(2017, 2, 1, 13, 6, 0, DateTimeZone.UTC),\n            new DateTime(2017, 2, 1, 14, 4, 0, DateTimeZone.UTC),\n            new DateTime(2017, 2, 1, 14, 5, 0, DateTimeZone.UTC),\n            new DateTime(2017, 2, 1, 15, 59, 0, DateTimeZone.UTC),\n            new DateTime(2017, 2, 1, 16, 6, 0, DateTimeZone.UTC),\n            new DateTime(2017, 2, 1, 16, 48, 0, DateTimeZone.UTC),\n            new DateTime(2017, 2, 1, 16, 59, 0, DateTimeZone.UTC));\n        testSearchCase(DEFAULT_QUERY, datesForHourInterval,\n            aggregation -> aggregation.setNumBuckets(8).field(DATE_FIELD).timeZone(DateTimeZone.forOffsetHours(-1)),\n            histogram -> {\n                final List<String> dateStrings = datesForHourInterval.stream()\n                    .map(dateTime -> dateTime.withZone(DateTimeZone.forOffsetHours(-1)).toString()).collect(Collectors.toList());\n                final List<? extends Histogram.Bucket> buckets = histogram.getBuckets();\n                assertEquals(datesForHourInterval.size(), buckets.size());\n                for (int i = 0; i < buckets.size(); i++) {\n                    final Histogram.Bucket bucket = buckets.get(i);\n                    assertEquals(dateStrings.get(i), bucket.getKeyAsString());\n                    assertEquals(1, bucket.getDocCount());\n                }\n            }\n        );\n        testSearchAndReduceCase(DEFAULT_QUERY, datesForHourInterval,\n            aggregation -> aggregation.setNumBuckets(10).field(DATE_FIELD).timeZone(DateTimeZone.forOffsetHours(-1)),\n            histogram -> {\n                final Map<String, Integer> expectedDocCount = new HashMap<>();\n                expectedDocCount.put(\"2017-02-01T08:00:00.000-01:00\", 2);\n                expectedDocCount.put(\"2017-02-01T09:00:00.000-01:00\", 1);\n                expectedDocCount.put(\"2017-02-01T12:00:00.000-01:00\", 1);\n                expectedDocCount.put(\"2017-02-01T13:00:00.000-01:00\", 2);\n                expectedDocCount.put(\"2017-02-01T14:00:00.000-01:00\", 1);\n                expectedDocCount.put(\"2017-02-01T15:00:00.000-01:00\", 3);\n                final List<? extends Histogram.Bucket> buckets = histogram.getBuckets();\n                assertEquals(8, buckets.size());\n                buckets.forEach(bucket ->\n                    assertEquals(expectedDocCount.getOrDefault(bucket.getKeyAsString(), 0).longValue(), bucket.getDocCount()));\n            }\n        );\n    }\n","date":"2018-11-19 22:21:01","endLine":426,"groupId":"39066","id":4,"instanceNumber":2,"isCurCommit":0,"methodName":"testIntervalHourWithTZ","params":"()","path":"/mnt/clonedata/CloneManagementServer/ManagementServer/consistResult/result-elasticsearch-10-0.7/blobInfo/CC_OUT/blobs/2d/5109405dc1c23b27b6f3f7dd3e8280cc59df6f.src","preCode":"    public void testIntervalHourWithTZ() throws IOException {\n        testSearchCase(new MatchAllDocsQuery(),\n                Arrays.asList(\n                        \"2017-02-01T09:02:00.000Z\",\n                        \"2017-02-01T09:35:00.000Z\",\n                        \"2017-02-01T10:15:00.000Z\",\n                        \"2017-02-01T13:06:00.000Z\",\n                        \"2017-02-01T14:04:00.000Z\",\n                        \"2017-02-01T14:05:00.000Z\",\n                        \"2017-02-01T15:59:00.000Z\",\n                        \"2017-02-01T16:06:00.000Z\",\n                        \"2017-02-01T16:48:00.000Z\",\n                        \"2017-02-01T16:59:00.000Z\"\n                ),\n                aggregation -> aggregation.setNumBuckets(8).field(DATE_FIELD).timeZone(DateTimeZone.forOffsetHours(-1)),\n                histogram -> {\n                    List<? extends Histogram.Bucket> buckets = histogram.getBuckets();\n                    assertEquals(10, buckets.size());\n\n                    Histogram.Bucket bucket = buckets.get(0);\n                    assertEquals(\"2017-02-01T08:02:00.000-01:00\", bucket.getKeyAsString());\n                    assertEquals(1, bucket.getDocCount());\n\n                    bucket = buckets.get(1);\n                    assertEquals(\"2017-02-01T08:35:00.000-01:00\", bucket.getKeyAsString());\n                    assertEquals(1, bucket.getDocCount());\n\n                    bucket = buckets.get(2);\n                    assertEquals(\"2017-02-01T09:15:00.000-01:00\", bucket.getKeyAsString());\n                    assertEquals(1, bucket.getDocCount());\n\n                    bucket = buckets.get(3);\n                    assertEquals(\"2017-02-01T12:06:00.000-01:00\", bucket.getKeyAsString());\n                    assertEquals(1, bucket.getDocCount());\n\n                    bucket = buckets.get(4);\n                    assertEquals(\"2017-02-01T13:04:00.000-01:00\", bucket.getKeyAsString());\n                    assertEquals(1, bucket.getDocCount());\n\n                    bucket = buckets.get(5);\n                    assertEquals(\"2017-02-01T13:05:00.000-01:00\", bucket.getKeyAsString());\n                    assertEquals(1, bucket.getDocCount());\n\n                    bucket = buckets.get(6);\n                    assertEquals(\"2017-02-01T14:59:00.000-01:00\", bucket.getKeyAsString());\n                    assertEquals(1, bucket.getDocCount());\n\n                    bucket = buckets.get(7);\n                    assertEquals(\"2017-02-01T15:06:00.000-01:00\", bucket.getKeyAsString());\n                    assertEquals(1, bucket.getDocCount());\n\n                    bucket = buckets.get(8);\n                    assertEquals(\"2017-02-01T15:48:00.000-01:00\", bucket.getKeyAsString());\n                    assertEquals(1, bucket.getDocCount());\n\n                    bucket = buckets.get(9);\n                    assertEquals(\"2017-02-01T15:59:00.000-01:00\", bucket.getKeyAsString());\n                    assertEquals(1, bucket.getDocCount());\n                }\n        );\n        testSearchAndReduceCase(new MatchAllDocsQuery(),\n                Arrays.asList(\n                        \"2017-02-01T09:02:00.000Z\",\n                        \"2017-02-01T09:35:00.000Z\",\n                        \"2017-02-01T10:15:00.000Z\",\n                        \"2017-02-01T13:06:00.000Z\",\n                        \"2017-02-01T14:04:00.000Z\",\n                        \"2017-02-01T14:05:00.000Z\",\n                        \"2017-02-01T15:59:00.000Z\",\n                        \"2017-02-01T16:06:00.000Z\",\n                        \"2017-02-01T16:48:00.000Z\",\n                        \"2017-02-01T16:59:00.000Z\"\n                ),\n                aggregation -> aggregation.setNumBuckets(10).field(DATE_FIELD).timeZone(DateTimeZone.forOffsetHours(-1)),\n                histogram -> {\n                    List<? extends Histogram.Bucket> buckets = histogram.getBuckets();\n                    assertEquals(8, buckets.size());\n\n                    Histogram.Bucket bucket = buckets.get(0);\n                    assertEquals(\"2017-02-01T08:00:00.000-01:00\", bucket.getKeyAsString());\n                    assertEquals(2, bucket.getDocCount());\n\n                    bucket = buckets.get(1);\n                    assertEquals(\"2017-02-01T09:00:00.000-01:00\", bucket.getKeyAsString());\n                    assertEquals(1, bucket.getDocCount());\n\n                    bucket = buckets.get(2);\n                    assertEquals(\"2017-02-01T10:00:00.000-01:00\", bucket.getKeyAsString());\n                    assertEquals(0, bucket.getDocCount());\n\n                    bucket = buckets.get(3);\n                    assertEquals(\"2017-02-01T11:00:00.000-01:00\", bucket.getKeyAsString());\n                    assertEquals(0, bucket.getDocCount());\n\n                    bucket = buckets.get(4);\n                    assertEquals(\"2017-02-01T12:00:00.000-01:00\", bucket.getKeyAsString());\n                    assertEquals(1, bucket.getDocCount());\n\n                    bucket = buckets.get(5);\n                    assertEquals(\"2017-02-01T13:00:00.000-01:00\", bucket.getKeyAsString());\n                    assertEquals(2, bucket.getDocCount());\n\n                    bucket = buckets.get(6);\n                    assertEquals(\"2017-02-01T14:00:00.000-01:00\", bucket.getKeyAsString());\n                    assertEquals(1, bucket.getDocCount());\n\n                    bucket = buckets.get(7);\n                    assertEquals(\"2017-02-01T15:00:00.000-01:00\", bucket.getKeyAsString());\n                    assertEquals(3, bucket.getDocCount());\n                }\n        );\n    }\n","realPath":"server/src/test/java/org/elasticsearch/search/aggregations/bucket/histogram/AutoDateHistogramAggregatorTests.java","repoName":"elasticsearch","snippetEndLine":0,"snippetStartLine":0,"startLine":384,"status":"M"}],"commitId":"450db7fcf7b99066b42507be15727856765729ff","commitMessage":"@@@[Tests] Fix slowness of AutoDateHistogramAggregatorTests (#35072)\n\nRandomize test assertion and test set size instead of asserting on an\nexhaustive list of dates with fixed test set size. Also refactor common \nobjects used to avoid recreating them.  avoid date to string conversion\nand reduce duplicate test code\n\nCloses #33181","date":"2018-11-19 22:21:01","modifiedFileCount":"1","status":"M","submitter":"Ekal Golas"},{"authorTime":"2019-01-23 17:40:05","codes":[{"authorDate":"2019-01-23 17:40:05","commitOrder":3,"curCode":"    public void testIntervalHour() throws IOException {\n        final List<ZonedDateTime> datesForHourInterval = Arrays.asList(\n            ZonedDateTime.of(2017, 2, 1, 9, 2, 0, 0, ZoneOffset.UTC),\n            ZonedDateTime.of(2017, 2, 1, 9, 35, 0, 0, ZoneOffset.UTC),\n            ZonedDateTime.of(2017, 2, 1, 10, 15, 0, 0, ZoneOffset.UTC),\n            ZonedDateTime.of(2017, 2, 1, 13, 6, 0, 0, ZoneOffset.UTC),\n            ZonedDateTime.of(2017, 2, 1, 14, 4, 0, 0, ZoneOffset.UTC),\n            ZonedDateTime.of(2017, 2, 1, 14, 5, 0, 0, ZoneOffset.UTC),\n            ZonedDateTime.of(2017, 2, 1, 15, 59, 0, 0, ZoneOffset.UTC),\n            ZonedDateTime.of(2017, 2, 1, 16, 6, 0, 0, ZoneOffset.UTC),\n            ZonedDateTime.of(2017, 2, 1, 16, 48, 0, 0, ZoneOffset.UTC),\n            ZonedDateTime.of(2017, 2, 1, 16, 59, 0, 0, ZoneOffset.UTC));\n        testSearchCase(DEFAULT_QUERY, datesForHourInterval,\n            aggregation -> aggregation.setNumBuckets(8).field(DATE_FIELD),\n            histogram -> {\n                final List<? extends Histogram.Bucket> buckets = histogram.getBuckets();\n                assertEquals(datesForHourInterval.size(), buckets.size());\n                for (int i = 0; i < buckets.size(); i++) {\n                    final Histogram.Bucket bucket = buckets.get(i);\n                    assertEquals(datesForHourInterval.get(i), bucket.getKey());\n                    assertEquals(1, bucket.getDocCount());\n                }\n            }\n        );\n        testSearchAndReduceCase(DEFAULT_QUERY, datesForHourInterval,\n            aggregation -> aggregation.setNumBuckets(10).field(DATE_FIELD),\n            histogram -> {\n                final Map<ZonedDateTime, Integer> expectedDocCount = new HashMap<>();\n                expectedDocCount.put(datesForHourInterval.get(0).withMinute(0), 2);\n                expectedDocCount.put(datesForHourInterval.get(2).withMinute(0), 1);\n                expectedDocCount.put(datesForHourInterval.get(3).withMinute(0), 1);\n                expectedDocCount.put(datesForHourInterval.get(4).withMinute(0), 2);\n                expectedDocCount.put(datesForHourInterval.get(6).withMinute(0), 1);\n                expectedDocCount.put(datesForHourInterval.get(7).withMinute(0), 3);\n                final List<? extends Histogram.Bucket> buckets = histogram.getBuckets();\n                assertEquals(8, buckets.size());\n                buckets.forEach(bucket ->\n                    assertEquals(expectedDocCount.getOrDefault(bucket.getKey(), 0).longValue(), bucket.getDocCount()));\n            }\n        );\n        testSearchAndReduceCase(DEFAULT_QUERY, datesForHourInterval,\n            aggregation -> aggregation.setNumBuckets(6).field(DATE_FIELD),\n            histogram -> {\n                final Map<ZonedDateTime, Integer> expectedDocCount = new HashMap<>();\n                expectedDocCount.put(datesForHourInterval.get(0).withMinute(0), 3);\n                expectedDocCount.put(datesForHourInterval.get(0).plusHours(3).withMinute(0), 3);\n                expectedDocCount.put(datesForHourInterval.get(0).plusHours(6).withMinute(0), 4);\n                final List<? extends Histogram.Bucket> buckets = histogram.getBuckets();\n                assertEquals(expectedDocCount.size(), buckets.size());\n                buckets.forEach(bucket ->\n                    assertEquals(expectedDocCount.getOrDefault(bucket.getKey(), 0).longValue(), bucket.getDocCount()));\n            }\n        );\n    }\n","date":"2019-01-23 17:40:05","endLine":418,"groupId":"39071","id":5,"instanceNumber":1,"isCurCommit":0,"methodName":"testIntervalHour","params":"()","path":"/mnt/clonedata/CloneManagementServer/ManagementServer/consistResult/result-elasticsearch-10-0.7/blobInfo/CC_OUT/blobs/92/93b33e22f4361b1e80b1addfd49310f176fabb.src","preCode":"    public void testIntervalHour() throws IOException {\n        final List<DateTime> datesForHourInterval = Arrays.asList(\n            new DateTime(2017, 2, 1, 9, 2, 0, DateTimeZone.UTC),\n            new DateTime(2017, 2, 1, 9, 35, 0, DateTimeZone.UTC),\n            new DateTime(2017, 2, 1, 10, 15, 0, DateTimeZone.UTC),\n            new DateTime(2017, 2, 1, 13, 6, 0, DateTimeZone.UTC),\n            new DateTime(2017, 2, 1, 14, 4, 0, DateTimeZone.UTC),\n            new DateTime(2017, 2, 1, 14, 5, 0, DateTimeZone.UTC),\n            new DateTime(2017, 2, 1, 15, 59, 0, DateTimeZone.UTC),\n            new DateTime(2017, 2, 1, 16, 6, 0, DateTimeZone.UTC),\n            new DateTime(2017, 2, 1, 16, 48, 0, DateTimeZone.UTC),\n            new DateTime(2017, 2, 1, 16, 59, 0, DateTimeZone.UTC));\n        testSearchCase(DEFAULT_QUERY, datesForHourInterval,\n            aggregation -> aggregation.setNumBuckets(8).field(DATE_FIELD),\n            histogram -> {\n                final List<? extends Histogram.Bucket> buckets = histogram.getBuckets();\n                assertEquals(datesForHourInterval.size(), buckets.size());\n                for (int i = 0; i < buckets.size(); i++) {\n                    final Histogram.Bucket bucket = buckets.get(i);\n                    assertEquals(datesForHourInterval.get(i), bucket.getKey());\n                    assertEquals(1, bucket.getDocCount());\n                }\n            }\n        );\n        testSearchAndReduceCase(DEFAULT_QUERY, datesForHourInterval,\n            aggregation -> aggregation.setNumBuckets(10).field(DATE_FIELD),\n            histogram -> {\n                final Map<DateTime, Integer> expectedDocCount = new HashMap<>();\n                expectedDocCount.put(datesForHourInterval.get(0).withMinuteOfHour(0), 2);\n                expectedDocCount.put(datesForHourInterval.get(2).withMinuteOfHour(0), 1);\n                expectedDocCount.put(datesForHourInterval.get(3).withMinuteOfHour(0), 1);\n                expectedDocCount.put(datesForHourInterval.get(4).withMinuteOfHour(0), 2);\n                expectedDocCount.put(datesForHourInterval.get(6).withMinuteOfHour(0), 1);\n                expectedDocCount.put(datesForHourInterval.get(7).withMinuteOfHour(0), 3);\n                final List<? extends Histogram.Bucket> buckets = histogram.getBuckets();\n                assertEquals(8, buckets.size());\n                buckets.forEach(bucket ->\n                    assertEquals(expectedDocCount.getOrDefault(bucket.getKey(), 0).longValue(), bucket.getDocCount()));\n            }\n        );\n        testSearchAndReduceCase(DEFAULT_QUERY, datesForHourInterval,\n            aggregation -> aggregation.setNumBuckets(6).field(DATE_FIELD),\n            histogram -> {\n                final Map<DateTime, Integer> expectedDocCount = new HashMap<>();\n                expectedDocCount.put(datesForHourInterval.get(0).withMinuteOfHour(0), 3);\n                expectedDocCount.put(datesForHourInterval.get(0).plusHours(3).withMinuteOfHour(0), 3);\n                expectedDocCount.put(datesForHourInterval.get(0).plusHours(6).withMinuteOfHour(0), 4);\n                final List<? extends Histogram.Bucket> buckets = histogram.getBuckets();\n                assertEquals(expectedDocCount.size(), buckets.size());\n                buckets.forEach(bucket ->\n                    assertEquals(expectedDocCount.getOrDefault(bucket.getKey(), 0).longValue(), bucket.getDocCount()));\n            }\n        );\n    }\n","realPath":"server/src/test/java/org/elasticsearch/search/aggregations/bucket/histogram/AutoDateHistogramAggregatorTests.java","repoName":"elasticsearch","snippetEndLine":0,"snippetStartLine":0,"startLine":365,"status":"M"},{"authorDate":"2019-01-23 17:40:05","commitOrder":3,"curCode":"    public void testIntervalHourWithTZ() throws IOException {\n        final List<ZonedDateTime> datesForHourInterval = Arrays.asList(\n            ZonedDateTime.of(2017, 2, 1, 9, 2, 0, 0, ZoneOffset.UTC),\n            ZonedDateTime.of(2017, 2, 1, 9, 35, 0, 0, ZoneOffset.UTC),\n            ZonedDateTime.of(2017, 2, 1, 10, 15, 0, 0, ZoneOffset.UTC),\n            ZonedDateTime.of(2017, 2, 1, 13, 6, 0, 0, ZoneOffset.UTC),\n            ZonedDateTime.of(2017, 2, 1, 14, 4, 0, 0, ZoneOffset.UTC),\n            ZonedDateTime.of(2017, 2, 1, 14, 5, 0, 0, ZoneOffset.UTC),\n            ZonedDateTime.of(2017, 2, 1, 15, 59, 0, 0, ZoneOffset.UTC),\n            ZonedDateTime.of(2017, 2, 1, 16, 6, 0, 0, ZoneOffset.UTC),\n            ZonedDateTime.of(2017, 2, 1, 16, 48, 0, 0, ZoneOffset.UTC),\n            ZonedDateTime.of(2017, 2, 1, 16, 59, 0, 0, ZoneOffset.UTC));\n        testSearchCase(DEFAULT_QUERY, datesForHourInterval,\n            aggregation -> aggregation.setNumBuckets(8).field(DATE_FIELD).timeZone(ZoneOffset.ofHours(-1)),\n            histogram -> {\n                final List<String> dateStrings = datesForHourInterval.stream()\n                    .map(dateTime -> DateFormatter.forPattern(\"strict_date_time\")\n                        .format(dateTime.withZoneSameInstant(ZoneOffset.ofHours(-1)))).collect(Collectors.toList());\n                final List<? extends Histogram.Bucket> buckets = histogram.getBuckets();\n                assertEquals(datesForHourInterval.size(), buckets.size());\n                for (int i = 0; i < buckets.size(); i++) {\n                    final Histogram.Bucket bucket = buckets.get(i);\n                    assertEquals(dateStrings.get(i), bucket.getKeyAsString());\n                    assertEquals(1, bucket.getDocCount());\n                }\n            }\n        );\n        testSearchAndReduceCase(DEFAULT_QUERY, datesForHourInterval,\n            aggregation -> aggregation.setNumBuckets(10).field(DATE_FIELD).timeZone(ZoneOffset.ofHours(-1)),\n            histogram -> {\n                final Map<String, Integer> expectedDocCount = new HashMap<>();\n                expectedDocCount.put(\"2017-02-01T08:00:00.000-01:00\", 2);\n                expectedDocCount.put(\"2017-02-01T09:00:00.000-01:00\", 1);\n                expectedDocCount.put(\"2017-02-01T12:00:00.000-01:00\", 1);\n                expectedDocCount.put(\"2017-02-01T13:00:00.000-01:00\", 2);\n                expectedDocCount.put(\"2017-02-01T14:00:00.000-01:00\", 1);\n                expectedDocCount.put(\"2017-02-01T15:00:00.000-01:00\", 3);\n                final List<? extends Histogram.Bucket> buckets = histogram.getBuckets();\n                assertEquals(8, buckets.size());\n                buckets.forEach(bucket ->\n                    assertEquals(expectedDocCount.getOrDefault(bucket.getKeyAsString(), 0).longValue(), bucket.getDocCount()));\n            }\n        );\n    }\n","date":"2019-01-23 17:40:05","endLine":463,"groupId":"8738","id":6,"instanceNumber":2,"isCurCommit":0,"methodName":"testIntervalHourWithTZ","params":"()","path":"/mnt/clonedata/CloneManagementServer/ManagementServer/consistResult/result-elasticsearch-10-0.7/blobInfo/CC_OUT/blobs/92/93b33e22f4361b1e80b1addfd49310f176fabb.src","preCode":"    public void testIntervalHourWithTZ() throws IOException {\n        final List<DateTime> datesForHourInterval = Arrays.asList(\n            new DateTime(2017, 2, 1, 9, 2, 0, DateTimeZone.UTC),\n            new DateTime(2017, 2, 1, 9, 35, 0, DateTimeZone.UTC),\n            new DateTime(2017, 2, 1, 10, 15, 0, DateTimeZone.UTC),\n            new DateTime(2017, 2, 1, 13, 6, 0, DateTimeZone.UTC),\n            new DateTime(2017, 2, 1, 14, 4, 0, DateTimeZone.UTC),\n            new DateTime(2017, 2, 1, 14, 5, 0, DateTimeZone.UTC),\n            new DateTime(2017, 2, 1, 15, 59, 0, DateTimeZone.UTC),\n            new DateTime(2017, 2, 1, 16, 6, 0, DateTimeZone.UTC),\n            new DateTime(2017, 2, 1, 16, 48, 0, DateTimeZone.UTC),\n            new DateTime(2017, 2, 1, 16, 59, 0, DateTimeZone.UTC));\n        testSearchCase(DEFAULT_QUERY, datesForHourInterval,\n            aggregation -> aggregation.setNumBuckets(8).field(DATE_FIELD).timeZone(DateTimeZone.forOffsetHours(-1)),\n            histogram -> {\n                final List<String> dateStrings = datesForHourInterval.stream()\n                    .map(dateTime -> dateTime.withZone(DateTimeZone.forOffsetHours(-1)).toString()).collect(Collectors.toList());\n                final List<? extends Histogram.Bucket> buckets = histogram.getBuckets();\n                assertEquals(datesForHourInterval.size(), buckets.size());\n                for (int i = 0; i < buckets.size(); i++) {\n                    final Histogram.Bucket bucket = buckets.get(i);\n                    assertEquals(dateStrings.get(i), bucket.getKeyAsString());\n                    assertEquals(1, bucket.getDocCount());\n                }\n            }\n        );\n        testSearchAndReduceCase(DEFAULT_QUERY, datesForHourInterval,\n            aggregation -> aggregation.setNumBuckets(10).field(DATE_FIELD).timeZone(DateTimeZone.forOffsetHours(-1)),\n            histogram -> {\n                final Map<String, Integer> expectedDocCount = new HashMap<>();\n                expectedDocCount.put(\"2017-02-01T08:00:00.000-01:00\", 2);\n                expectedDocCount.put(\"2017-02-01T09:00:00.000-01:00\", 1);\n                expectedDocCount.put(\"2017-02-01T12:00:00.000-01:00\", 1);\n                expectedDocCount.put(\"2017-02-01T13:00:00.000-01:00\", 2);\n                expectedDocCount.put(\"2017-02-01T14:00:00.000-01:00\", 1);\n                expectedDocCount.put(\"2017-02-01T15:00:00.000-01:00\", 3);\n                final List<? extends Histogram.Bucket> buckets = histogram.getBuckets();\n                assertEquals(8, buckets.size());\n                buckets.forEach(bucket ->\n                    assertEquals(expectedDocCount.getOrDefault(bucket.getKeyAsString(), 0).longValue(), bucket.getDocCount()));\n            }\n        );\n    }\n","realPath":"server/src/test/java/org/elasticsearch/search/aggregations/bucket/histogram/AutoDateHistogramAggregatorTests.java","repoName":"elasticsearch","snippetEndLine":0,"snippetStartLine":0,"startLine":420,"status":"M"}],"commitId":"daa2ec8a605d385a65b9ab3e89d016b3fd0dffe2","commitMessage":"@@@Switch mapping/aggregations over to java time (#36363)\n\nThis commit moves the aggregation and mapping code from joda time to\njava time. This includes field mappers.  root object mappers.  aggregations with date\nhistograms.  query builders and a lot of changes within tests.\n\nThe cut-over to java time is a requirement so that we can support nanoseconds\nproperly in a future field mapper.\n\nRelates #27330","date":"2019-01-23 17:40:05","modifiedFileCount":"154","status":"M","submitter":"Alexander Reelsen"},{"authorTime":"2020-06-16 02:33:31","codes":[{"authorDate":"2020-06-16 02:33:31","commitOrder":4,"curCode":"    public void testIntervalHour() throws IOException {\n        final List<ZonedDateTime> datesForHourInterval = Arrays.asList(\n            ZonedDateTime.of(2017, 2, 1, 9, 2, 0, 0, ZoneOffset.UTC),\n            ZonedDateTime.of(2017, 2, 1, 9, 35, 0, 0, ZoneOffset.UTC),\n            ZonedDateTime.of(2017, 2, 1, 10, 15, 0, 0, ZoneOffset.UTC),\n            ZonedDateTime.of(2017, 2, 1, 13, 6, 0, 0, ZoneOffset.UTC),\n            ZonedDateTime.of(2017, 2, 1, 14, 4, 0, 0, ZoneOffset.UTC),\n            ZonedDateTime.of(2017, 2, 1, 14, 5, 0, 0, ZoneOffset.UTC),\n            ZonedDateTime.of(2017, 2, 1, 15, 59, 0, 0, ZoneOffset.UTC),\n            ZonedDateTime.of(2017, 2, 1, 16, 6, 0, 0, ZoneOffset.UTC),\n            ZonedDateTime.of(2017, 2, 1, 16, 48, 0, 0, ZoneOffset.UTC),\n            ZonedDateTime.of(2017, 2, 1, 16, 59, 0, 0, ZoneOffset.UTC));\n        Map<String, Integer> expectedDocCount = new TreeMap<>();\n        expectedDocCount.put(\"2017-02-01T09:00:00.000Z\", 2);\n        expectedDocCount.put(\"2017-02-01T10:00:00.000Z\", 1);\n        expectedDocCount.put(\"2017-02-01T13:00:00.000Z\", 1);\n        expectedDocCount.put(\"2017-02-01T14:00:00.000Z\", 2);\n        expectedDocCount.put(\"2017-02-01T15:00:00.000Z\", 1);\n        expectedDocCount.put(\"2017-02-01T15:00:00.000Z\", 1);\n        expectedDocCount.put(\"2017-02-01T16:00:00.000Z\", 3);\n        testSearchCase(DEFAULT_QUERY, datesForHourInterval,\n            aggregation -> aggregation.setNumBuckets(8).field(DATE_FIELD),\n            result -> assertThat(bucketCountsAsMap(result), equalTo(expectedDocCount))\n        );\n        expectedDocCount.put(\"2017-02-01T11:00:00.000Z\", 0);\n        expectedDocCount.put(\"2017-02-01T12:00:00.000Z\", 0);\n        testSearchAndReduceCase(DEFAULT_QUERY, datesForHourInterval,\n            aggregation -> aggregation.setNumBuckets(10).field(DATE_FIELD),\n            result -> assertThat(bucketCountsAsMap(result), equalTo(expectedDocCount))\n        );\n        expectedDocCount.clear();\n        expectedDocCount.put(\"2017-02-01T09:00:00.000Z\", 3);\n        expectedDocCount.put(\"2017-02-01T12:00:00.000Z\", 3);\n        expectedDocCount.put(\"2017-02-01T15:00:00.000Z\", 4);\n        testSearchAndReduceCase(DEFAULT_QUERY, datesForHourInterval,\n            aggregation -> aggregation.setNumBuckets(6).field(DATE_FIELD),\n            result -> assertThat(bucketCountsAsMap(result), equalTo(expectedDocCount))\n        );\n    }\n","date":"2020-06-16 02:33:31","endLine":567,"groupId":"10690","id":7,"instanceNumber":1,"isCurCommit":0,"methodName":"testIntervalHour","params":"()","path":"/mnt/clonedata/CloneManagementServer/ManagementServer/consistResult/result-elasticsearch-10-0.7/blobInfo/CC_OUT/blobs/47/7df9e59163c957ddad7d1665b8af353b9129c9.src","preCode":"    public void testIntervalHour() throws IOException {\n        final List<ZonedDateTime> datesForHourInterval = Arrays.asList(\n            ZonedDateTime.of(2017, 2, 1, 9, 2, 0, 0, ZoneOffset.UTC),\n            ZonedDateTime.of(2017, 2, 1, 9, 35, 0, 0, ZoneOffset.UTC),\n            ZonedDateTime.of(2017, 2, 1, 10, 15, 0, 0, ZoneOffset.UTC),\n            ZonedDateTime.of(2017, 2, 1, 13, 6, 0, 0, ZoneOffset.UTC),\n            ZonedDateTime.of(2017, 2, 1, 14, 4, 0, 0, ZoneOffset.UTC),\n            ZonedDateTime.of(2017, 2, 1, 14, 5, 0, 0, ZoneOffset.UTC),\n            ZonedDateTime.of(2017, 2, 1, 15, 59, 0, 0, ZoneOffset.UTC),\n            ZonedDateTime.of(2017, 2, 1, 16, 6, 0, 0, ZoneOffset.UTC),\n            ZonedDateTime.of(2017, 2, 1, 16, 48, 0, 0, ZoneOffset.UTC),\n            ZonedDateTime.of(2017, 2, 1, 16, 59, 0, 0, ZoneOffset.UTC));\n        testSearchCase(DEFAULT_QUERY, datesForHourInterval,\n            aggregation -> aggregation.setNumBuckets(8).field(DATE_FIELD),\n            histogram -> {\n                final List<? extends Histogram.Bucket> buckets = histogram.getBuckets();\n                assertEquals(datesForHourInterval.size(), buckets.size());\n                for (int i = 0; i < buckets.size(); i++) {\n                    final Histogram.Bucket bucket = buckets.get(i);\n                    assertEquals(datesForHourInterval.get(i), bucket.getKey());\n                    assertEquals(1, bucket.getDocCount());\n                }\n            }\n        );\n        testSearchAndReduceCase(DEFAULT_QUERY, datesForHourInterval,\n            aggregation -> aggregation.setNumBuckets(10).field(DATE_FIELD),\n            histogram -> {\n                final Map<ZonedDateTime, Integer> expectedDocCount = new HashMap<>();\n                expectedDocCount.put(datesForHourInterval.get(0).withMinute(0), 2);\n                expectedDocCount.put(datesForHourInterval.get(2).withMinute(0), 1);\n                expectedDocCount.put(datesForHourInterval.get(3).withMinute(0), 1);\n                expectedDocCount.put(datesForHourInterval.get(4).withMinute(0), 2);\n                expectedDocCount.put(datesForHourInterval.get(6).withMinute(0), 1);\n                expectedDocCount.put(datesForHourInterval.get(7).withMinute(0), 3);\n                final List<? extends Histogram.Bucket> buckets = histogram.getBuckets();\n                assertEquals(8, buckets.size());\n                buckets.forEach(bucket ->\n                    assertEquals(expectedDocCount.getOrDefault(bucket.getKey(), 0).longValue(), bucket.getDocCount()));\n            }\n        );\n        testSearchAndReduceCase(DEFAULT_QUERY, datesForHourInterval,\n            aggregation -> aggregation.setNumBuckets(6).field(DATE_FIELD),\n            histogram -> {\n                final Map<ZonedDateTime, Integer> expectedDocCount = new HashMap<>();\n                expectedDocCount.put(datesForHourInterval.get(0).withMinute(0), 3);\n                expectedDocCount.put(datesForHourInterval.get(0).plusHours(3).withMinute(0), 3);\n                expectedDocCount.put(datesForHourInterval.get(0).plusHours(6).withMinute(0), 4);\n                final List<? extends Histogram.Bucket> buckets = histogram.getBuckets();\n                assertEquals(expectedDocCount.size(), buckets.size());\n                buckets.forEach(bucket ->\n                    assertEquals(expectedDocCount.getOrDefault(bucket.getKey(), 0).longValue(), bucket.getDocCount()));\n            }\n        );\n    }\n","realPath":"server/src/test/java/org/elasticsearch/search/aggregations/bucket/histogram/AutoDateHistogramAggregatorTests.java","repoName":"elasticsearch","snippetEndLine":0,"snippetStartLine":0,"startLine":529,"status":"M"},{"authorDate":"2020-06-16 02:33:31","commitOrder":4,"curCode":"    public void testIntervalHourWithTZ() throws IOException {\n        List<ZonedDateTime> datesForHourInterval = Arrays.asList(\n            ZonedDateTime.of(2017, 2, 1, 9, 2, 0, 0, ZoneOffset.UTC),\n            ZonedDateTime.of(2017, 2, 1, 9, 35, 0, 0, ZoneOffset.UTC),\n            ZonedDateTime.of(2017, 2, 1, 10, 15, 0, 0, ZoneOffset.UTC),\n            ZonedDateTime.of(2017, 2, 1, 13, 6, 0, 0, ZoneOffset.UTC),\n            ZonedDateTime.of(2017, 2, 1, 14, 4, 0, 0, ZoneOffset.UTC),\n            ZonedDateTime.of(2017, 2, 1, 14, 5, 0, 0, ZoneOffset.UTC),\n            ZonedDateTime.of(2017, 2, 1, 15, 59, 0, 0, ZoneOffset.UTC),\n            ZonedDateTime.of(2017, 2, 1, 16, 6, 0, 0, ZoneOffset.UTC),\n            ZonedDateTime.of(2017, 2, 1, 16, 48, 0, 0, ZoneOffset.UTC),\n            ZonedDateTime.of(2017, 2, 1, 16, 59, 0, 0, ZoneOffset.UTC));\n        Map<String, Integer> expectedDocCount = new TreeMap<>();\n        expectedDocCount.put(\"2017-02-01T08:00:00.000-01:00\", 2);\n        expectedDocCount.put(\"2017-02-01T09:00:00.000-01:00\", 1);\n        expectedDocCount.put(\"2017-02-01T12:00:00.000-01:00\", 1);\n        expectedDocCount.put(\"2017-02-01T13:00:00.000-01:00\", 2);\n        expectedDocCount.put(\"2017-02-01T14:00:00.000-01:00\", 1);\n        expectedDocCount.put(\"2017-02-01T15:00:00.000-01:00\", 3);\n        testSearchCase(DEFAULT_QUERY, datesForHourInterval,\n            aggregation -> aggregation.setNumBuckets(8).field(DATE_FIELD).timeZone(ZoneOffset.ofHours(-1)),\n            result -> assertThat(bucketCountsAsMap(result), equalTo(expectedDocCount))\n        );\n        expectedDocCount.put(\"2017-02-01T10:00:00.000-01:00\", 0);\n        expectedDocCount.put(\"2017-02-01T11:00:00.000-01:00\", 0);\n        testSearchAndReduceCase(DEFAULT_QUERY, datesForHourInterval,\n            aggregation -> aggregation.setNumBuckets(10).field(DATE_FIELD).timeZone(ZoneOffset.ofHours(-1)),\n            result -> assertThat(bucketCountsAsMap(result), equalTo(expectedDocCount))\n        );\n    }\n","date":"2020-06-16 02:33:31","endLine":598,"groupId":"10681","id":8,"instanceNumber":2,"isCurCommit":0,"methodName":"testIntervalHourWithTZ","params":"()","path":"/mnt/clonedata/CloneManagementServer/ManagementServer/consistResult/result-elasticsearch-10-0.7/blobInfo/CC_OUT/blobs/47/7df9e59163c957ddad7d1665b8af353b9129c9.src","preCode":"    public void testIntervalHourWithTZ() throws IOException {\n        final List<ZonedDateTime> datesForHourInterval = Arrays.asList(\n            ZonedDateTime.of(2017, 2, 1, 9, 2, 0, 0, ZoneOffset.UTC),\n            ZonedDateTime.of(2017, 2, 1, 9, 35, 0, 0, ZoneOffset.UTC),\n            ZonedDateTime.of(2017, 2, 1, 10, 15, 0, 0, ZoneOffset.UTC),\n            ZonedDateTime.of(2017, 2, 1, 13, 6, 0, 0, ZoneOffset.UTC),\n            ZonedDateTime.of(2017, 2, 1, 14, 4, 0, 0, ZoneOffset.UTC),\n            ZonedDateTime.of(2017, 2, 1, 14, 5, 0, 0, ZoneOffset.UTC),\n            ZonedDateTime.of(2017, 2, 1, 15, 59, 0, 0, ZoneOffset.UTC),\n            ZonedDateTime.of(2017, 2, 1, 16, 6, 0, 0, ZoneOffset.UTC),\n            ZonedDateTime.of(2017, 2, 1, 16, 48, 0, 0, ZoneOffset.UTC),\n            ZonedDateTime.of(2017, 2, 1, 16, 59, 0, 0, ZoneOffset.UTC));\n        testSearchCase(DEFAULT_QUERY, datesForHourInterval,\n            aggregation -> aggregation.setNumBuckets(8).field(DATE_FIELD).timeZone(ZoneOffset.ofHours(-1)),\n            histogram -> {\n                final List<String> dateStrings = datesForHourInterval.stream()\n                    .map(dateTime -> DateFormatter.forPattern(\"strict_date_time\")\n                        .format(dateTime.withZoneSameInstant(ZoneOffset.ofHours(-1)))).collect(Collectors.toList());\n                final List<? extends Histogram.Bucket> buckets = histogram.getBuckets();\n                assertEquals(datesForHourInterval.size(), buckets.size());\n                for (int i = 0; i < buckets.size(); i++) {\n                    final Histogram.Bucket bucket = buckets.get(i);\n                    assertEquals(dateStrings.get(i), bucket.getKeyAsString());\n                    assertEquals(1, bucket.getDocCount());\n                }\n            }\n        );\n        testSearchAndReduceCase(DEFAULT_QUERY, datesForHourInterval,\n            aggregation -> aggregation.setNumBuckets(10).field(DATE_FIELD).timeZone(ZoneOffset.ofHours(-1)),\n            histogram -> {\n                final Map<String, Integer> expectedDocCount = new HashMap<>();\n                expectedDocCount.put(\"2017-02-01T08:00:00.000-01:00\", 2);\n                expectedDocCount.put(\"2017-02-01T09:00:00.000-01:00\", 1);\n                expectedDocCount.put(\"2017-02-01T12:00:00.000-01:00\", 1);\n                expectedDocCount.put(\"2017-02-01T13:00:00.000-01:00\", 2);\n                expectedDocCount.put(\"2017-02-01T14:00:00.000-01:00\", 1);\n                expectedDocCount.put(\"2017-02-01T15:00:00.000-01:00\", 3);\n                final List<? extends Histogram.Bucket> buckets = histogram.getBuckets();\n                assertEquals(8, buckets.size());\n                buckets.forEach(bucket ->\n                    assertEquals(expectedDocCount.getOrDefault(bucket.getKeyAsString(), 0).longValue(), bucket.getDocCount()));\n            }\n        );\n    }\n","realPath":"server/src/test/java/org/elasticsearch/search/aggregations/bucket/histogram/AutoDateHistogramAggregatorTests.java","repoName":"elasticsearch","snippetEndLine":0,"snippetStartLine":0,"startLine":569,"status":"M"}],"commitId":"7c7fe0152d43a19379942a144dd4904b3d1db570","commitMessage":"@@@Save memory when auto_date_histogram is not on top (#57304)\n\nThis builds an `auto_date_histogram` aggregator that natively aggregates\nfrom many buckets and uses it when the `auto_date_histogram` used to use\n`asMultiBucketAggregator` which should save a significant amount of\nmemory in those cases. In particular.  this happens when\n`auto_date_histogram` is a sub-aggregator of a multi-bucketing aggregator\nlike `terms` or `histogram` or `filters`. For the most part we preserve\nthe original implementation when `auto_date_histogram` only collects from\na single bucket.\n\nIt isn't possible to \"just port the aggregator\" without taking a pretty\nsignificant performance hit because we used to rewrite all of the\nbuckets every time we switched to a coarser and coarser rounding\nconfiguration. Without some major surgery to how to delay sub-aggs\nwe'd end up rewriting the delay list zillions of time if there are many\nbuckets.\n\nThe multi-bucket version of the aggregator has a \"budget\" of \"wasted\"\nbuckets and only rewrites all of the buckets when we exceed that budget.\nNow that we don't rebucket every time we increase the rounding we can no\nlonger get an accurate count of the number of buckets! So instead the\naggregator uses an estimate of the number of buckets to trigger switching\nto a coarser rounding. This estimate is likely to be *terrible* when\nbuckets are far apart compared to the rounding. So it also uses the\ndifference between the first and last bucket to trigger switching to a\ncoarser rounding. Which covers for the shortcomings of the bucket\nestimation technique pretty well. It also causes the aggregator to emit\nfewer buckets in cases where they'd be reduced together on the\ncoordinating node. This is wonderful! But probably fairly rare.\n\nAll of that does buy us some speed improvements when the aggregator is\na child of multi-bucket aggregator:\nWithout metrics or time zone: 25% faster\nWith metrics: 15% faster\nWith time zone: 22% faster\n\nRelates to #56487\n","date":"2020-06-16 02:33:31","modifiedFileCount":"14","status":"M","submitter":"Nik Everett"},{"authorTime":"2020-08-07 05:14:20","codes":[{"authorDate":"2020-08-07 05:14:20","commitOrder":5,"curCode":"    public void testIntervalHour() throws IOException {\n        final List<ZonedDateTime> datesForHourInterval = Arrays.asList(\n            ZonedDateTime.of(2017, 2, 1, 9, 2, 0, 0, ZoneOffset.UTC),\n            ZonedDateTime.of(2017, 2, 1, 9, 35, 0, 0, ZoneOffset.UTC),\n            ZonedDateTime.of(2017, 2, 1, 10, 15, 0, 0, ZoneOffset.UTC),\n            ZonedDateTime.of(2017, 2, 1, 13, 6, 0, 0, ZoneOffset.UTC),\n            ZonedDateTime.of(2017, 2, 1, 14, 4, 0, 0, ZoneOffset.UTC),\n            ZonedDateTime.of(2017, 2, 1, 14, 5, 0, 0, ZoneOffset.UTC),\n            ZonedDateTime.of(2017, 2, 1, 15, 59, 0, 0, ZoneOffset.UTC),\n            ZonedDateTime.of(2017, 2, 1, 16, 6, 0, 0, ZoneOffset.UTC),\n            ZonedDateTime.of(2017, 2, 1, 16, 48, 0, 0, ZoneOffset.UTC),\n            ZonedDateTime.of(2017, 2, 1, 16, 59, 0, 0, ZoneOffset.UTC));\n        Map<String, Integer> expectedDocCount = new TreeMap<>();\n        expectedDocCount.put(\"2017-02-01T09:00:00.000Z\", 2);\n        expectedDocCount.put(\"2017-02-01T10:00:00.000Z\", 1);\n        expectedDocCount.put(\"2017-02-01T13:00:00.000Z\", 1);\n        expectedDocCount.put(\"2017-02-01T14:00:00.000Z\", 2);\n        expectedDocCount.put(\"2017-02-01T15:00:00.000Z\", 1);\n        expectedDocCount.put(\"2017-02-01T15:00:00.000Z\", 1);\n        expectedDocCount.put(\"2017-02-01T16:00:00.000Z\", 3);\n        expectedDocCount.put(\"2017-02-01T11:00:00.000Z\", 0);\n        expectedDocCount.put(\"2017-02-01T12:00:00.000Z\", 0);\n        testSearchCase(DEFAULT_QUERY, datesForHourInterval,\n            aggregation -> aggregation.setNumBuckets(10).field(DATE_FIELD),\n            result -> assertThat(bucketCountsAsMap(result), equalTo(expectedDocCount))\n        );\n        expectedDocCount.clear();\n        expectedDocCount.put(\"2017-02-01T09:00:00.000Z\", 3);\n        expectedDocCount.put(\"2017-02-01T12:00:00.000Z\", 3);\n        expectedDocCount.put(\"2017-02-01T15:00:00.000Z\", 4);\n        testSearchCase(DEFAULT_QUERY, datesForHourInterval,\n            aggregation -> aggregation.setNumBuckets(6).field(DATE_FIELD),\n            result -> assertThat(bucketCountsAsMap(result), equalTo(expectedDocCount))\n        );\n    }\n","date":"2020-08-07 05:14:20","endLine":551,"groupId":"105011","id":9,"instanceNumber":1,"isCurCommit":0,"methodName":"testIntervalHour","params":"()","path":"/mnt/clonedata/CloneManagementServer/ManagementServer/consistResult/result-elasticsearch-10-0.7/blobInfo/CC_OUT/blobs/4c/b34acab2449fdaa8d6149b4effef11af92b7f1.src","preCode":"    public void testIntervalHour() throws IOException {\n        final List<ZonedDateTime> datesForHourInterval = Arrays.asList(\n            ZonedDateTime.of(2017, 2, 1, 9, 2, 0, 0, ZoneOffset.UTC),\n            ZonedDateTime.of(2017, 2, 1, 9, 35, 0, 0, ZoneOffset.UTC),\n            ZonedDateTime.of(2017, 2, 1, 10, 15, 0, 0, ZoneOffset.UTC),\n            ZonedDateTime.of(2017, 2, 1, 13, 6, 0, 0, ZoneOffset.UTC),\n            ZonedDateTime.of(2017, 2, 1, 14, 4, 0, 0, ZoneOffset.UTC),\n            ZonedDateTime.of(2017, 2, 1, 14, 5, 0, 0, ZoneOffset.UTC),\n            ZonedDateTime.of(2017, 2, 1, 15, 59, 0, 0, ZoneOffset.UTC),\n            ZonedDateTime.of(2017, 2, 1, 16, 6, 0, 0, ZoneOffset.UTC),\n            ZonedDateTime.of(2017, 2, 1, 16, 48, 0, 0, ZoneOffset.UTC),\n            ZonedDateTime.of(2017, 2, 1, 16, 59, 0, 0, ZoneOffset.UTC));\n        Map<String, Integer> expectedDocCount = new TreeMap<>();\n        expectedDocCount.put(\"2017-02-01T09:00:00.000Z\", 2);\n        expectedDocCount.put(\"2017-02-01T10:00:00.000Z\", 1);\n        expectedDocCount.put(\"2017-02-01T13:00:00.000Z\", 1);\n        expectedDocCount.put(\"2017-02-01T14:00:00.000Z\", 2);\n        expectedDocCount.put(\"2017-02-01T15:00:00.000Z\", 1);\n        expectedDocCount.put(\"2017-02-01T15:00:00.000Z\", 1);\n        expectedDocCount.put(\"2017-02-01T16:00:00.000Z\", 3);\n        testSearchCase(DEFAULT_QUERY, datesForHourInterval,\n            aggregation -> aggregation.setNumBuckets(8).field(DATE_FIELD),\n            result -> assertThat(bucketCountsAsMap(result), equalTo(expectedDocCount))\n        );\n        expectedDocCount.put(\"2017-02-01T11:00:00.000Z\", 0);\n        expectedDocCount.put(\"2017-02-01T12:00:00.000Z\", 0);\n        testSearchAndReduceCase(DEFAULT_QUERY, datesForHourInterval,\n            aggregation -> aggregation.setNumBuckets(10).field(DATE_FIELD),\n            result -> assertThat(bucketCountsAsMap(result), equalTo(expectedDocCount))\n        );\n        expectedDocCount.clear();\n        expectedDocCount.put(\"2017-02-01T09:00:00.000Z\", 3);\n        expectedDocCount.put(\"2017-02-01T12:00:00.000Z\", 3);\n        expectedDocCount.put(\"2017-02-01T15:00:00.000Z\", 4);\n        testSearchAndReduceCase(DEFAULT_QUERY, datesForHourInterval,\n            aggregation -> aggregation.setNumBuckets(6).field(DATE_FIELD),\n            result -> assertThat(bucketCountsAsMap(result), equalTo(expectedDocCount))\n        );\n    }\n","realPath":"server/src/test/java/org/elasticsearch/search/aggregations/bucket/histogram/AutoDateHistogramAggregatorTests.java","repoName":"elasticsearch","snippetEndLine":0,"snippetStartLine":0,"startLine":517,"status":"M"},{"authorDate":"2020-08-07 05:14:20","commitOrder":5,"curCode":"    public void testIntervalHourWithTZ() throws IOException {\n        List<ZonedDateTime> datesForHourInterval = Arrays.asList(\n            ZonedDateTime.of(2017, 2, 1, 9, 2, 0, 0, ZoneOffset.UTC),\n            ZonedDateTime.of(2017, 2, 1, 9, 35, 0, 0, ZoneOffset.UTC),\n            ZonedDateTime.of(2017, 2, 1, 10, 15, 0, 0, ZoneOffset.UTC),\n            ZonedDateTime.of(2017, 2, 1, 13, 6, 0, 0, ZoneOffset.UTC),\n            ZonedDateTime.of(2017, 2, 1, 14, 4, 0, 0, ZoneOffset.UTC),\n            ZonedDateTime.of(2017, 2, 1, 14, 5, 0, 0, ZoneOffset.UTC),\n            ZonedDateTime.of(2017, 2, 1, 15, 59, 0, 0, ZoneOffset.UTC),\n            ZonedDateTime.of(2017, 2, 1, 16, 6, 0, 0, ZoneOffset.UTC),\n            ZonedDateTime.of(2017, 2, 1, 16, 48, 0, 0, ZoneOffset.UTC),\n            ZonedDateTime.of(2017, 2, 1, 16, 59, 0, 0, ZoneOffset.UTC));\n        Map<String, Integer> expectedDocCount = new TreeMap<>();\n        expectedDocCount.put(\"2017-02-01T08:00:00.000-01:00\", 2);\n        expectedDocCount.put(\"2017-02-01T09:00:00.000-01:00\", 1);\n        expectedDocCount.put(\"2017-02-01T12:00:00.000-01:00\", 1);\n        expectedDocCount.put(\"2017-02-01T13:00:00.000-01:00\", 2);\n        expectedDocCount.put(\"2017-02-01T14:00:00.000-01:00\", 1);\n        expectedDocCount.put(\"2017-02-01T15:00:00.000-01:00\", 3);\n        expectedDocCount.put(\"2017-02-01T10:00:00.000-01:00\", 0);\n        expectedDocCount.put(\"2017-02-01T11:00:00.000-01:00\", 0);\n        testSearchCase(DEFAULT_QUERY, datesForHourInterval,\n            aggregation -> aggregation.setNumBuckets(10).field(DATE_FIELD).timeZone(ZoneOffset.ofHours(-1)),\n            result -> assertThat(bucketCountsAsMap(result), equalTo(expectedDocCount))\n        );\n    }\n","date":"2020-08-07 05:14:20","endLine":578,"groupId":"105011","id":10,"instanceNumber":2,"isCurCommit":0,"methodName":"testIntervalHourWithTZ","params":"()","path":"/mnt/clonedata/CloneManagementServer/ManagementServer/consistResult/result-elasticsearch-10-0.7/blobInfo/CC_OUT/blobs/4c/b34acab2449fdaa8d6149b4effef11af92b7f1.src","preCode":"    public void testIntervalHourWithTZ() throws IOException {\n        List<ZonedDateTime> datesForHourInterval = Arrays.asList(\n            ZonedDateTime.of(2017, 2, 1, 9, 2, 0, 0, ZoneOffset.UTC),\n            ZonedDateTime.of(2017, 2, 1, 9, 35, 0, 0, ZoneOffset.UTC),\n            ZonedDateTime.of(2017, 2, 1, 10, 15, 0, 0, ZoneOffset.UTC),\n            ZonedDateTime.of(2017, 2, 1, 13, 6, 0, 0, ZoneOffset.UTC),\n            ZonedDateTime.of(2017, 2, 1, 14, 4, 0, 0, ZoneOffset.UTC),\n            ZonedDateTime.of(2017, 2, 1, 14, 5, 0, 0, ZoneOffset.UTC),\n            ZonedDateTime.of(2017, 2, 1, 15, 59, 0, 0, ZoneOffset.UTC),\n            ZonedDateTime.of(2017, 2, 1, 16, 6, 0, 0, ZoneOffset.UTC),\n            ZonedDateTime.of(2017, 2, 1, 16, 48, 0, 0, ZoneOffset.UTC),\n            ZonedDateTime.of(2017, 2, 1, 16, 59, 0, 0, ZoneOffset.UTC));\n        Map<String, Integer> expectedDocCount = new TreeMap<>();\n        expectedDocCount.put(\"2017-02-01T08:00:00.000-01:00\", 2);\n        expectedDocCount.put(\"2017-02-01T09:00:00.000-01:00\", 1);\n        expectedDocCount.put(\"2017-02-01T12:00:00.000-01:00\", 1);\n        expectedDocCount.put(\"2017-02-01T13:00:00.000-01:00\", 2);\n        expectedDocCount.put(\"2017-02-01T14:00:00.000-01:00\", 1);\n        expectedDocCount.put(\"2017-02-01T15:00:00.000-01:00\", 3);\n        testSearchCase(DEFAULT_QUERY, datesForHourInterval,\n            aggregation -> aggregation.setNumBuckets(8).field(DATE_FIELD).timeZone(ZoneOffset.ofHours(-1)),\n            result -> assertThat(bucketCountsAsMap(result), equalTo(expectedDocCount))\n        );\n        expectedDocCount.put(\"2017-02-01T10:00:00.000-01:00\", 0);\n        expectedDocCount.put(\"2017-02-01T11:00:00.000-01:00\", 0);\n        testSearchAndReduceCase(DEFAULT_QUERY, datesForHourInterval,\n            aggregation -> aggregation.setNumBuckets(10).field(DATE_FIELD).timeZone(ZoneOffset.ofHours(-1)),\n            result -> assertThat(bucketCountsAsMap(result), equalTo(expectedDocCount))\n        );\n    }\n","realPath":"server/src/test/java/org/elasticsearch/search/aggregations/bucket/histogram/AutoDateHistogramAggregatorTests.java","repoName":"elasticsearch","snippetEndLine":0,"snippetStartLine":0,"startLine":553,"status":"M"}],"commitId":"5e3ea6eb11c68bdcc9adda51715a6e1fea9186d6","commitMessage":"@@@Merge branch 'master' into feature/runtime_fields\n","date":"2020-08-07 05:14:20","modifiedFileCount":"73","status":"M","submitter":"Nik Everett"}]
