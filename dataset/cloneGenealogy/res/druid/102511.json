[{"authorTime":"2018-08-31 00:56:26","codes":[{"authorDate":"2018-09-08 04:17:49","commitOrder":2,"curCode":"  public void testIncrementalHandOffMaxTotalRows() throws Exception\n  {\n    if (!isIncrementalHandoffSupported) {\n      return;\n    }\n    final String baseSequenceName = \"sequence0\";\n    \r\n    maxRowsPerSegment = Integer.MAX_VALUE;\n    maxTotalRows = 3L;\n\n    \r\n    int numToAdd = records.size() - 2;\n\n    try (final KafkaProducer<byte[], byte[]> kafkaProducer = kafkaServer.newProducer()) {\n      for (int i = 0; i < numToAdd; i++) {\n        kafkaProducer.send(records.get(i)).get();\n      }\n\n      Map<String, String> consumerProps = kafkaServer.consumerProperties();\n      consumerProps.put(\"max.poll.records\", \"1\");\n\n      final KafkaPartitions startPartitions = new KafkaPartitions(topic, ImmutableMap.of(0, 0L, 1, 0L));\n      final KafkaPartitions checkpoint1 = new KafkaPartitions(topic, ImmutableMap.of(0, 3L, 1, 0L));\n      final KafkaPartitions checkpoint2 = new KafkaPartitions(topic, ImmutableMap.of(0, 10L, 1, 0L));\n\n      final KafkaPartitions endPartitions = new KafkaPartitions(topic, ImmutableMap.of(0, 10L, 1, 2L));\n      final KafkaIndexTask task = createTask(\n          null,\n          new KafkaIOConfig(\n              0,\n              baseSequenceName,\n              startPartitions,\n              endPartitions,\n              consumerProps,\n              true,\n              null,\n              null,\n              false\n          )\n      );\n      final ListenableFuture<TaskStatus> future = runTask(task);\n      while (task.getRunner().getStatus() != KafkaIndexTask.Status.PAUSED) {\n        Thread.sleep(10);\n      }\n      final Map<Integer, Long> currentOffsets = ImmutableMap.copyOf(task.getRunner().getCurrentOffsets());\n\n      Assert.assertTrue(checkpoint1.getPartitionOffsetMap().equals(currentOffsets));\n      task.getRunner().setEndOffsets(currentOffsets, false);\n\n      while (task.getRunner().getStatus() != KafkaIndexTask.Status.PAUSED) {\n        Thread.sleep(10);\n      }\n\n      \r\n      for (int i = numToAdd; i < records.size(); i++) {\n        kafkaProducer.send(records.get(i)).get();\n      }\n      final Map<Integer, Long> nextOffsets = ImmutableMap.copyOf(task.getRunner().getCurrentOffsets());\n\n      Assert.assertTrue(checkpoint2.getPartitionOffsetMap().equals(nextOffsets));\n      task.getRunner().setEndOffsets(nextOffsets, false);\n\n      Assert.assertEquals(TaskState.SUCCESS, future.get().getStatusCode());\n\n      Assert.assertEquals(2, checkpointRequestsHash.size());\n      Assert.assertTrue(\n          checkpointRequestsHash.contains(\n              Objects.hash(\n                  DATA_SCHEMA.getDataSource(),\n                  0,\n                  new KafkaDataSourceMetadata(startPartitions),\n                  new KafkaDataSourceMetadata(new KafkaPartitions(topic, currentOffsets))\n              )\n          )\n      );\n      Assert.assertTrue(\n          checkpointRequestsHash.contains(\n              Objects.hash(\n                  DATA_SCHEMA.getDataSource(),\n                  0,\n                  new KafkaDataSourceMetadata(new KafkaPartitions(topic, currentOffsets)),\n                  new KafkaDataSourceMetadata(new KafkaPartitions(topic, nextOffsets))\n              )\n          )\n      );\n\n      \r\n      Assert.assertEquals(8, task.getRunner().getRowIngestionMeters().getProcessed());\n      Assert.assertEquals(3, task.getRunner().getRowIngestionMeters().getUnparseable());\n      Assert.assertEquals(1, task.getRunner().getRowIngestionMeters().getThrownAway());\n\n      \r\n      SegmentDescriptor desc1 = SD(task, \"2008/P1D\", 0);\n      SegmentDescriptor desc2 = SD(task, \"2009/P1D\", 0);\n      SegmentDescriptor desc3 = SD(task, \"2010/P1D\", 0);\n      SegmentDescriptor desc4 = SD(task, \"2011/P1D\", 0);\n      SegmentDescriptor desc5 = SD(task, \"2011/P1D\", 1);\n      SegmentDescriptor desc6 = SD(task, \"2012/P1D\", 0);\n      SegmentDescriptor desc7 = SD(task, \"2013/P1D\", 0);\n      Assert.assertEquals(ImmutableSet.of(desc1, desc2, desc3, desc4, desc5, desc6, desc7), publishedDescriptors());\n      Assert.assertEquals(\n          new KafkaDataSourceMetadata(new KafkaPartitions(topic, ImmutableMap.of(0, 10L, 1, 2L))),\n          metadataStorageCoordinator.getDataSourceMetadata(DATA_SCHEMA.getDataSource())\n      );\n\n      \r\n      Assert.assertEquals(ImmutableList.of(\"a\"), readSegmentColumn(\"dim1\", desc1));\n      Assert.assertEquals(ImmutableList.of(\"b\"), readSegmentColumn(\"dim1\", desc2));\n      Assert.assertEquals(ImmutableList.of(\"c\"), readSegmentColumn(\"dim1\", desc3));\n      Assert.assertTrue((ImmutableList.of(\"d\", \"e\").equals(readSegmentColumn(\"dim1\", desc4))\n                         && ImmutableList.of(\"h\").equals(readSegmentColumn(\"dim1\", desc5))) ||\n                        (ImmutableList.of(\"d\", \"h\").equals(readSegmentColumn(\"dim1\", desc4))\n                         && ImmutableList.of(\"e\").equals(readSegmentColumn(\"dim1\", desc5))));\n      Assert.assertEquals(ImmutableList.of(\"g\"), readSegmentColumn(\"dim1\", desc6));\n      Assert.assertEquals(ImmutableList.of(\"f\"), readSegmentColumn(\"dim1\", desc7));\n    }\n  }\n","date":"2018-09-08 04:17:49","endLine":681,"groupId":"3673","id":1,"instanceNumber":1,"isCurCommit":0,"methodName":"testIncrementalHandOffMaxTotalRows","params":"()","path":"/mnt/clonedata/CloneManagementServer/ManagementServer/consistResult/result-druid-10-0.7/blobInfo/CC_OUT/blobs/61/1533bbcfb718ec11a1adb259b68928f05526f3.src","preCode":"  public void testIncrementalHandOffMaxTotalRows() throws Exception\n  {\n    if (!isIncrementalHandoffSupported) {\n      return;\n    }\n    final String baseSequenceName = \"sequence0\";\n    \r\n    maxRowsPerSegment = Integer.MAX_VALUE;\n    maxTotalRows = 3L;\n\n    \r\n    int numToAdd = records.size() - 2;\n\n    try (final KafkaProducer<byte[], byte[]> kafkaProducer = kafkaServer.newProducer()) {\n      for (int i = 0; i < numToAdd; i++) {\n        kafkaProducer.send(records.get(i)).get();\n      }\n\n      Map<String, String> consumerProps = kafkaServer.consumerProperties();\n      consumerProps.put(\"max.poll.records\", \"1\");\n\n      final KafkaPartitions startPartitions = new KafkaPartitions(topic, ImmutableMap.of(0, 0L, 1, 0L));\n      final KafkaPartitions checkpoint1 = new KafkaPartitions(topic, ImmutableMap.of(0, 3L, 1, 0L));\n      final KafkaPartitions checkpoint2 = new KafkaPartitions(topic, ImmutableMap.of(0, 10L, 1, 0L));\n\n      final KafkaPartitions endPartitions = new KafkaPartitions(topic, ImmutableMap.of(0, 10L, 1, 2L));\n      final KafkaIndexTask task = createTask(\n          null,\n          new KafkaIOConfig(\n              0,\n              baseSequenceName,\n              startPartitions,\n              endPartitions,\n              consumerProps,\n              true,\n              null,\n              null,\n              false\n          )\n      );\n      final ListenableFuture<TaskStatus> future = runTask(task);\n      while (task.getRunner().getStatus() != KafkaIndexTask.Status.PAUSED) {\n        Thread.sleep(10);\n      }\n      final Map<Integer, Long> currentOffsets = ImmutableMap.copyOf(task.getRunner().getCurrentOffsets());\n\n      Assert.assertTrue(checkpoint1.getPartitionOffsetMap().equals(currentOffsets));\n      task.getRunner().setEndOffsets(currentOffsets, false);\n\n      while (task.getRunner().getStatus() != KafkaIndexTask.Status.PAUSED) {\n        Thread.sleep(10);\n      }\n\n      \r\n      for (int i = numToAdd; i < records.size(); i++) {\n        kafkaProducer.send(records.get(i)).get();\n      }\n      final Map<Integer, Long> nextOffsets = ImmutableMap.copyOf(task.getRunner().getCurrentOffsets());\n\n      Assert.assertTrue(checkpoint2.getPartitionOffsetMap().equals(nextOffsets));\n      task.getRunner().setEndOffsets(nextOffsets, false);\n\n      Assert.assertEquals(TaskState.SUCCESS, future.get().getStatusCode());\n\n      Assert.assertEquals(2, checkpointRequestsHash.size());\n      Assert.assertTrue(\n          checkpointRequestsHash.contains(\n              Objects.hash(\n                  DATA_SCHEMA.getDataSource(),\n                  0,\n                  new KafkaDataSourceMetadata(startPartitions),\n                  new KafkaDataSourceMetadata(new KafkaPartitions(topic, currentOffsets))\n              )\n          )\n      );\n      Assert.assertTrue(\n          checkpointRequestsHash.contains(\n              Objects.hash(\n                  DATA_SCHEMA.getDataSource(),\n                  0,\n                  new KafkaDataSourceMetadata(new KafkaPartitions(topic, currentOffsets)),\n                  new KafkaDataSourceMetadata(new KafkaPartitions(topic, nextOffsets))\n              )\n          )\n      );\n\n      \r\n      Assert.assertEquals(8, task.getRunner().getRowIngestionMeters().getProcessed());\n      Assert.assertEquals(3, task.getRunner().getRowIngestionMeters().getUnparseable());\n      Assert.assertEquals(1, task.getRunner().getRowIngestionMeters().getThrownAway());\n\n      \r\n      SegmentDescriptor desc1 = SD(task, \"2008/P1D\", 0);\n      SegmentDescriptor desc2 = SD(task, \"2009/P1D\", 0);\n      SegmentDescriptor desc3 = SD(task, \"2010/P1D\", 0);\n      SegmentDescriptor desc4 = SD(task, \"2011/P1D\", 0);\n      SegmentDescriptor desc5 = SD(task, \"2011/P1D\", 1);\n      SegmentDescriptor desc6 = SD(task, \"2012/P1D\", 0);\n      SegmentDescriptor desc7 = SD(task, \"2013/P1D\", 0);\n      Assert.assertEquals(ImmutableSet.of(desc1, desc2, desc3, desc4, desc5, desc6, desc7), publishedDescriptors());\n      Assert.assertEquals(\n          new KafkaDataSourceMetadata(new KafkaPartitions(topic, ImmutableMap.of(0, 10L, 1, 2L))),\n          metadataStorageCoordinator.getDataSourceMetadata(DATA_SCHEMA.getDataSource())\n      );\n\n      \r\n      Assert.assertEquals(ImmutableList.of(\"a\"), readSegmentColumn(\"dim1\", desc1));\n      Assert.assertEquals(ImmutableList.of(\"b\"), readSegmentColumn(\"dim1\", desc2));\n      Assert.assertEquals(ImmutableList.of(\"c\"), readSegmentColumn(\"dim1\", desc3));\n      Assert.assertTrue((ImmutableList.of(\"d\", \"e\").equals(readSegmentColumn(\"dim1\", desc4))\n                         && ImmutableList.of(\"h\").equals(readSegmentColumn(\"dim1\", desc5))) ||\n                        (ImmutableList.of(\"d\", \"h\").equals(readSegmentColumn(\"dim1\", desc4))\n                         && ImmutableList.of(\"e\").equals(readSegmentColumn(\"dim1\", desc5))));\n      Assert.assertEquals(ImmutableList.of(\"g\"), readSegmentColumn(\"dim1\", desc6));\n      Assert.assertEquals(ImmutableList.of(\"f\"), readSegmentColumn(\"dim1\", desc7));\n    }\n  }\n","realPath":"extensions-core/kafka-indexing-service/src/test/java/org/apache/druid/indexing/kafka/KafkaIndexTaskTest.java","repoName":"druid","snippetEndLine":0,"snippetStartLine":0,"startLine":565,"status":"B"},{"authorDate":"2018-08-31 00:56:26","commitOrder":2,"curCode":"  public void testTimeBasedIncrementalHandOff() throws Exception\n  {\n    if (!isIncrementalHandoffSupported) {\n      return;\n    }\n    final String baseSequenceName = \"sequence0\";\n    \r\n    maxRowsPerSegment = Integer.MAX_VALUE;\n    intermediateHandoffPeriod = new Period().withSeconds(0);\n\n    \r\n    try (final KafkaProducer<byte[], byte[]> kafkaProducer = kafkaServer.newProducer()) {\n      for (ProducerRecord<byte[], byte[]> record : records.subList(0, 2)) {\n        kafkaProducer.send(record).get();\n      }\n    }\n    Map<String, String> consumerProps = kafkaServer.consumerProperties();\n    consumerProps.put(\"max.poll.records\", \"1\");\n\n    final KafkaPartitions startPartitions = new KafkaPartitions(topic, ImmutableMap.of(0, 0L, 1, 0L));\n    \r\n    final KafkaPartitions checkpoint = new KafkaPartitions(topic, ImmutableMap.of(0, 1L, 1, 0L));\n    final KafkaPartitions endPartitions = new KafkaPartitions(topic, ImmutableMap.of(0, 2L, 1, 0L));\n    final KafkaIndexTask task = createTask(\n        null,\n        new KafkaIOConfig(\n            0,\n            baseSequenceName,\n            startPartitions,\n            endPartitions,\n            consumerProps,\n            true,\n            null,\n            null,\n            false\n        )\n    );\n    final ListenableFuture<TaskStatus> future = runTask(task);\n\n    \r\n    while (task.getRunner().getStatus() != KafkaIndexTask.Status.PAUSED) {\n      Thread.sleep(10);\n    }\n    final Map<Integer, Long> currentOffsets = ImmutableMap.copyOf(task.getRunner().getCurrentOffsets());\n    Assert.assertTrue(checkpoint.getPartitionOffsetMap().equals(currentOffsets));\n    task.getRunner().setEndOffsets(currentOffsets, false);\n    Assert.assertEquals(TaskState.SUCCESS, future.get().getStatusCode());\n\n    Assert.assertEquals(1, checkpointRequestsHash.size());\n    Assert.assertTrue(\n        checkpointRequestsHash.contains(\n            Objects.hash(\n                DATA_SCHEMA.getDataSource(),\n                0,\n                new KafkaDataSourceMetadata(startPartitions),\n                new KafkaDataSourceMetadata(new KafkaPartitions(topic, checkpoint.getPartitionOffsetMap()))\n            )\n        )\n    );\n\n    \r\n    Assert.assertEquals(2, task.getRunner().getRowIngestionMeters().getProcessed());\n    Assert.assertEquals(0, task.getRunner().getRowIngestionMeters().getUnparseable());\n    Assert.assertEquals(0, task.getRunner().getRowIngestionMeters().getThrownAway());\n\n    \r\n    SegmentDescriptor desc1 = SD(task, \"2008/P1D\", 0);\n    SegmentDescriptor desc2 = SD(task, \"2009/P1D\", 0);\n    Assert.assertEquals(ImmutableSet.of(desc1, desc2), publishedDescriptors());\n    Assert.assertEquals(\n        new KafkaDataSourceMetadata(new KafkaPartitions(topic, ImmutableMap.of(0, 2L, 1, 0L))),\n        metadataStorageCoordinator.getDataSourceMetadata(DATA_SCHEMA.getDataSource())\n    );\n\n    \r\n    Assert.assertEquals(ImmutableList.of(\"a\"), readSegmentColumn(\"dim1\", desc1));\n    Assert.assertEquals(ImmutableList.of(\"b\"), readSegmentColumn(\"dim1\", desc2));\n  }\n","date":"2018-08-31 00:56:26","endLine":641,"groupId":"11125","id":2,"instanceNumber":2,"isCurCommit":0,"methodName":"testTimeBasedIncrementalHandOff","params":"()","path":"/mnt/clonedata/CloneManagementServer/ManagementServer/consistResult/result-druid-10-0.7/blobInfo/CC_OUT/blobs/a5/d450fc656ab61dcc37bbc18c28d1a253ebd3b3.src","preCode":"  public void testTimeBasedIncrementalHandOff() throws Exception\n  {\n    if (!isIncrementalHandoffSupported) {\n      return;\n    }\n    final String baseSequenceName = \"sequence0\";\n    \r\n    maxRowsPerSegment = Integer.MAX_VALUE;\n    intermediateHandoffPeriod = new Period().withSeconds(0);\n\n    \r\n    try (final KafkaProducer<byte[], byte[]> kafkaProducer = kafkaServer.newProducer()) {\n      for (ProducerRecord<byte[], byte[]> record : records.subList(0, 2)) {\n        kafkaProducer.send(record).get();\n      }\n    }\n    Map<String, String> consumerProps = kafkaServer.consumerProperties();\n    consumerProps.put(\"max.poll.records\", \"1\");\n\n    final KafkaPartitions startPartitions = new KafkaPartitions(topic, ImmutableMap.of(0, 0L, 1, 0L));\n    \r\n    final KafkaPartitions checkpoint = new KafkaPartitions(topic, ImmutableMap.of(0, 1L, 1, 0L));\n    final KafkaPartitions endPartitions = new KafkaPartitions(topic, ImmutableMap.of(0, 2L, 1, 0L));\n    final KafkaIndexTask task = createTask(\n        null,\n        new KafkaIOConfig(\n            0,\n            baseSequenceName,\n            startPartitions,\n            endPartitions,\n            consumerProps,\n            true,\n            null,\n            null,\n            false\n        )\n    );\n    final ListenableFuture<TaskStatus> future = runTask(task);\n\n    \r\n    while (task.getRunner().getStatus() != KafkaIndexTask.Status.PAUSED) {\n      Thread.sleep(10);\n    }\n    final Map<Integer, Long> currentOffsets = ImmutableMap.copyOf(task.getRunner().getCurrentOffsets());\n    Assert.assertTrue(checkpoint.getPartitionOffsetMap().equals(currentOffsets));\n    task.getRunner().setEndOffsets(currentOffsets, false);\n    Assert.assertEquals(TaskState.SUCCESS, future.get().getStatusCode());\n\n    Assert.assertEquals(1, checkpointRequestsHash.size());\n    Assert.assertTrue(\n        checkpointRequestsHash.contains(\n            Objects.hash(\n                DATA_SCHEMA.getDataSource(),\n                0,\n                new KafkaDataSourceMetadata(startPartitions),\n                new KafkaDataSourceMetadata(new KafkaPartitions(topic, checkpoint.getPartitionOffsetMap()))\n            )\n        )\n    );\n\n    \r\n    Assert.assertEquals(2, task.getRunner().getRowIngestionMeters().getProcessed());\n    Assert.assertEquals(0, task.getRunner().getRowIngestionMeters().getUnparseable());\n    Assert.assertEquals(0, task.getRunner().getRowIngestionMeters().getThrownAway());\n\n    \r\n    SegmentDescriptor desc1 = SD(task, \"2008/P1D\", 0);\n    SegmentDescriptor desc2 = SD(task, \"2009/P1D\", 0);\n    Assert.assertEquals(ImmutableSet.of(desc1, desc2), publishedDescriptors());\n    Assert.assertEquals(\n        new KafkaDataSourceMetadata(new KafkaPartitions(topic, ImmutableMap.of(0, 2L, 1, 0L))),\n        metadataStorageCoordinator.getDataSourceMetadata(DATA_SCHEMA.getDataSource())\n    );\n\n    \r\n    Assert.assertEquals(ImmutableList.of(\"a\"), readSegmentColumn(\"dim1\", desc1));\n    Assert.assertEquals(ImmutableList.of(\"b\"), readSegmentColumn(\"dim1\", desc2));\n  }\n","realPath":"extensions-core/kafka-indexing-service/src/test/java/org/apache/druid/indexing/kafka/KafkaIndexTaskTest.java","repoName":"druid","snippetEndLine":0,"snippetStartLine":0,"startLine":564,"status":"NB"}],"commitId":"e6e068ce60a073700701dd2a7b2b59218d59a2c3","commitMessage":"@@@Add support for 'maxTotalRows' to incremental publishing kafka indexing task and appenderator based realtime task (#6129)\n\n* resolves #5898 by adding maxTotalRows to incremental publishing kafka index task and appenderator based realtime indexing task.  as available in IndexTask\n\n* address review comments\n\n* changes due to review\n\n* merge fail\n","date":"2018-09-08 04:17:49","modifiedFileCount":"16","status":"M","submitter":"Clint Wylie"},{"authorTime":"2018-10-12 01:03:01","codes":[{"authorDate":"2018-10-12 01:03:01","commitOrder":3,"curCode":"  public void testIncrementalHandOffMaxTotalRows() throws Exception\n  {\n    if (!isIncrementalHandoffSupported) {\n      return;\n    }\n    final String baseSequenceName = \"sequence0\";\n    \r\n    maxRowsPerSegment = Integer.MAX_VALUE;\n    maxTotalRows = 3L;\n\n    \r\n    int numToAdd = records.size() - 2;\n\n    try (final KafkaProducer<byte[], byte[]> kafkaProducer = kafkaServer.newProducer()) {\n      for (int i = 0; i < numToAdd; i++) {\n        kafkaProducer.send(records.get(i)).get();\n      }\n\n      Map<String, Object> consumerProps = kafkaServer.consumerProperties();\n      consumerProps.put(\"max.poll.records\", \"1\");\n\n      final KafkaPartitions startPartitions = new KafkaPartitions(topic, ImmutableMap.of(0, 0L, 1, 0L));\n      final KafkaPartitions checkpoint1 = new KafkaPartitions(topic, ImmutableMap.of(0, 3L, 1, 0L));\n      final KafkaPartitions checkpoint2 = new KafkaPartitions(topic, ImmutableMap.of(0, 10L, 1, 0L));\n\n      final KafkaPartitions endPartitions = new KafkaPartitions(topic, ImmutableMap.of(0, 10L, 1, 2L));\n      final KafkaIndexTask task = createTask(\n          null,\n          new KafkaIOConfig(\n              0,\n              baseSequenceName,\n              startPartitions,\n              endPartitions,\n              consumerProps,\n              true,\n              null,\n              null,\n              false\n          )\n      );\n      final ListenableFuture<TaskStatus> future = runTask(task);\n      while (task.getRunner().getStatus() != KafkaIndexTask.Status.PAUSED) {\n        Thread.sleep(10);\n      }\n      final Map<Integer, Long> currentOffsets = ImmutableMap.copyOf(task.getRunner().getCurrentOffsets());\n\n      Assert.assertTrue(checkpoint1.getPartitionOffsetMap().equals(currentOffsets));\n      task.getRunner().setEndOffsets(currentOffsets, false);\n\n      while (task.getRunner().getStatus() != KafkaIndexTask.Status.PAUSED) {\n        Thread.sleep(10);\n      }\n\n      \r\n      for (int i = numToAdd; i < records.size(); i++) {\n        kafkaProducer.send(records.get(i)).get();\n      }\n      final Map<Integer, Long> nextOffsets = ImmutableMap.copyOf(task.getRunner().getCurrentOffsets());\n\n      Assert.assertTrue(checkpoint2.getPartitionOffsetMap().equals(nextOffsets));\n      task.getRunner().setEndOffsets(nextOffsets, false);\n\n      Assert.assertEquals(TaskState.SUCCESS, future.get().getStatusCode());\n\n      Assert.assertEquals(2, checkpointRequestsHash.size());\n      Assert.assertTrue(\n          checkpointRequestsHash.contains(\n              Objects.hash(\n                  DATA_SCHEMA.getDataSource(),\n                  0,\n                  new KafkaDataSourceMetadata(startPartitions),\n                  new KafkaDataSourceMetadata(new KafkaPartitions(topic, currentOffsets))\n              )\n          )\n      );\n      Assert.assertTrue(\n          checkpointRequestsHash.contains(\n              Objects.hash(\n                  DATA_SCHEMA.getDataSource(),\n                  0,\n                  new KafkaDataSourceMetadata(new KafkaPartitions(topic, currentOffsets)),\n                  new KafkaDataSourceMetadata(new KafkaPartitions(topic, nextOffsets))\n              )\n          )\n      );\n\n      \r\n      Assert.assertEquals(8, task.getRunner().getRowIngestionMeters().getProcessed());\n      Assert.assertEquals(3, task.getRunner().getRowIngestionMeters().getUnparseable());\n      Assert.assertEquals(1, task.getRunner().getRowIngestionMeters().getThrownAway());\n\n      \r\n      SegmentDescriptor desc1 = SD(task, \"2008/P1D\", 0);\n      SegmentDescriptor desc2 = SD(task, \"2009/P1D\", 0);\n      SegmentDescriptor desc3 = SD(task, \"2010/P1D\", 0);\n      SegmentDescriptor desc4 = SD(task, \"2011/P1D\", 0);\n      SegmentDescriptor desc5 = SD(task, \"2011/P1D\", 1);\n      SegmentDescriptor desc6 = SD(task, \"2012/P1D\", 0);\n      SegmentDescriptor desc7 = SD(task, \"2013/P1D\", 0);\n      Assert.assertEquals(ImmutableSet.of(desc1, desc2, desc3, desc4, desc5, desc6, desc7), publishedDescriptors());\n      Assert.assertEquals(\n          new KafkaDataSourceMetadata(new KafkaPartitions(topic, ImmutableMap.of(0, 10L, 1, 2L))),\n          metadataStorageCoordinator.getDataSourceMetadata(DATA_SCHEMA.getDataSource())\n      );\n\n      \r\n      Assert.assertEquals(ImmutableList.of(\"a\"), readSegmentColumn(\"dim1\", desc1));\n      Assert.assertEquals(ImmutableList.of(\"b\"), readSegmentColumn(\"dim1\", desc2));\n      Assert.assertEquals(ImmutableList.of(\"c\"), readSegmentColumn(\"dim1\", desc3));\n      Assert.assertTrue((ImmutableList.of(\"d\", \"e\").equals(readSegmentColumn(\"dim1\", desc4))\n                         && ImmutableList.of(\"h\").equals(readSegmentColumn(\"dim1\", desc5))) ||\n                        (ImmutableList.of(\"d\", \"h\").equals(readSegmentColumn(\"dim1\", desc4))\n                         && ImmutableList.of(\"e\").equals(readSegmentColumn(\"dim1\", desc5))));\n      Assert.assertEquals(ImmutableList.of(\"g\"), readSegmentColumn(\"dim1\", desc6));\n      Assert.assertEquals(ImmutableList.of(\"f\"), readSegmentColumn(\"dim1\", desc7));\n    }\n  }\n","date":"2018-10-12 01:03:01","endLine":682,"groupId":"3673","id":3,"instanceNumber":1,"isCurCommit":0,"methodName":"testIncrementalHandOffMaxTotalRows","params":"()","path":"/mnt/clonedata/CloneManagementServer/ManagementServer/consistResult/result-druid-10-0.7/blobInfo/CC_OUT/blobs/6d/d210ae4c590645847c1fa82033054928af3277.src","preCode":"  public void testIncrementalHandOffMaxTotalRows() throws Exception\n  {\n    if (!isIncrementalHandoffSupported) {\n      return;\n    }\n    final String baseSequenceName = \"sequence0\";\n    \r\n    maxRowsPerSegment = Integer.MAX_VALUE;\n    maxTotalRows = 3L;\n\n    \r\n    int numToAdd = records.size() - 2;\n\n    try (final KafkaProducer<byte[], byte[]> kafkaProducer = kafkaServer.newProducer()) {\n      for (int i = 0; i < numToAdd; i++) {\n        kafkaProducer.send(records.get(i)).get();\n      }\n\n      Map<String, String> consumerProps = kafkaServer.consumerProperties();\n      consumerProps.put(\"max.poll.records\", \"1\");\n\n      final KafkaPartitions startPartitions = new KafkaPartitions(topic, ImmutableMap.of(0, 0L, 1, 0L));\n      final KafkaPartitions checkpoint1 = new KafkaPartitions(topic, ImmutableMap.of(0, 3L, 1, 0L));\n      final KafkaPartitions checkpoint2 = new KafkaPartitions(topic, ImmutableMap.of(0, 10L, 1, 0L));\n\n      final KafkaPartitions endPartitions = new KafkaPartitions(topic, ImmutableMap.of(0, 10L, 1, 2L));\n      final KafkaIndexTask task = createTask(\n          null,\n          new KafkaIOConfig(\n              0,\n              baseSequenceName,\n              startPartitions,\n              endPartitions,\n              consumerProps,\n              true,\n              null,\n              null,\n              false\n          )\n      );\n      final ListenableFuture<TaskStatus> future = runTask(task);\n      while (task.getRunner().getStatus() != KafkaIndexTask.Status.PAUSED) {\n        Thread.sleep(10);\n      }\n      final Map<Integer, Long> currentOffsets = ImmutableMap.copyOf(task.getRunner().getCurrentOffsets());\n\n      Assert.assertTrue(checkpoint1.getPartitionOffsetMap().equals(currentOffsets));\n      task.getRunner().setEndOffsets(currentOffsets, false);\n\n      while (task.getRunner().getStatus() != KafkaIndexTask.Status.PAUSED) {\n        Thread.sleep(10);\n      }\n\n      \r\n      for (int i = numToAdd; i < records.size(); i++) {\n        kafkaProducer.send(records.get(i)).get();\n      }\n      final Map<Integer, Long> nextOffsets = ImmutableMap.copyOf(task.getRunner().getCurrentOffsets());\n\n      Assert.assertTrue(checkpoint2.getPartitionOffsetMap().equals(nextOffsets));\n      task.getRunner().setEndOffsets(nextOffsets, false);\n\n      Assert.assertEquals(TaskState.SUCCESS, future.get().getStatusCode());\n\n      Assert.assertEquals(2, checkpointRequestsHash.size());\n      Assert.assertTrue(\n          checkpointRequestsHash.contains(\n              Objects.hash(\n                  DATA_SCHEMA.getDataSource(),\n                  0,\n                  new KafkaDataSourceMetadata(startPartitions),\n                  new KafkaDataSourceMetadata(new KafkaPartitions(topic, currentOffsets))\n              )\n          )\n      );\n      Assert.assertTrue(\n          checkpointRequestsHash.contains(\n              Objects.hash(\n                  DATA_SCHEMA.getDataSource(),\n                  0,\n                  new KafkaDataSourceMetadata(new KafkaPartitions(topic, currentOffsets)),\n                  new KafkaDataSourceMetadata(new KafkaPartitions(topic, nextOffsets))\n              )\n          )\n      );\n\n      \r\n      Assert.assertEquals(8, task.getRunner().getRowIngestionMeters().getProcessed());\n      Assert.assertEquals(3, task.getRunner().getRowIngestionMeters().getUnparseable());\n      Assert.assertEquals(1, task.getRunner().getRowIngestionMeters().getThrownAway());\n\n      \r\n      SegmentDescriptor desc1 = SD(task, \"2008/P1D\", 0);\n      SegmentDescriptor desc2 = SD(task, \"2009/P1D\", 0);\n      SegmentDescriptor desc3 = SD(task, \"2010/P1D\", 0);\n      SegmentDescriptor desc4 = SD(task, \"2011/P1D\", 0);\n      SegmentDescriptor desc5 = SD(task, \"2011/P1D\", 1);\n      SegmentDescriptor desc6 = SD(task, \"2012/P1D\", 0);\n      SegmentDescriptor desc7 = SD(task, \"2013/P1D\", 0);\n      Assert.assertEquals(ImmutableSet.of(desc1, desc2, desc3, desc4, desc5, desc6, desc7), publishedDescriptors());\n      Assert.assertEquals(\n          new KafkaDataSourceMetadata(new KafkaPartitions(topic, ImmutableMap.of(0, 10L, 1, 2L))),\n          metadataStorageCoordinator.getDataSourceMetadata(DATA_SCHEMA.getDataSource())\n      );\n\n      \r\n      Assert.assertEquals(ImmutableList.of(\"a\"), readSegmentColumn(\"dim1\", desc1));\n      Assert.assertEquals(ImmutableList.of(\"b\"), readSegmentColumn(\"dim1\", desc2));\n      Assert.assertEquals(ImmutableList.of(\"c\"), readSegmentColumn(\"dim1\", desc3));\n      Assert.assertTrue((ImmutableList.of(\"d\", \"e\").equals(readSegmentColumn(\"dim1\", desc4))\n                         && ImmutableList.of(\"h\").equals(readSegmentColumn(\"dim1\", desc5))) ||\n                        (ImmutableList.of(\"d\", \"h\").equals(readSegmentColumn(\"dim1\", desc4))\n                         && ImmutableList.of(\"e\").equals(readSegmentColumn(\"dim1\", desc5))));\n      Assert.assertEquals(ImmutableList.of(\"g\"), readSegmentColumn(\"dim1\", desc6));\n      Assert.assertEquals(ImmutableList.of(\"f\"), readSegmentColumn(\"dim1\", desc7));\n    }\n  }\n","realPath":"extensions-core/kafka-indexing-service/src/test/java/org/apache/druid/indexing/kafka/KafkaIndexTaskTest.java","repoName":"druid","snippetEndLine":0,"snippetStartLine":0,"startLine":566,"status":"M"},{"authorDate":"2018-10-12 01:03:01","commitOrder":3,"curCode":"  public void testTimeBasedIncrementalHandOff() throws Exception\n  {\n    if (!isIncrementalHandoffSupported) {\n      return;\n    }\n    final String baseSequenceName = \"sequence0\";\n    \r\n    maxRowsPerSegment = Integer.MAX_VALUE;\n    intermediateHandoffPeriod = new Period().withSeconds(0);\n\n    \r\n    try (final KafkaProducer<byte[], byte[]> kafkaProducer = kafkaServer.newProducer()) {\n      for (ProducerRecord<byte[], byte[]> record : records.subList(0, 2)) {\n        kafkaProducer.send(record).get();\n      }\n    }\n    Map<String, Object> consumerProps = kafkaServer.consumerProperties();\n    consumerProps.put(\"max.poll.records\", \"1\");\n\n    final KafkaPartitions startPartitions = new KafkaPartitions(topic, ImmutableMap.of(0, 0L, 1, 0L));\n    \r\n    final KafkaPartitions checkpoint = new KafkaPartitions(topic, ImmutableMap.of(0, 1L, 1, 0L));\n    final KafkaPartitions endPartitions = new KafkaPartitions(topic, ImmutableMap.of(0, 2L, 1, 0L));\n    final KafkaIndexTask task = createTask(\n        null,\n        new KafkaIOConfig(\n            0,\n            baseSequenceName,\n            startPartitions,\n            endPartitions,\n            consumerProps,\n            true,\n            null,\n            null,\n            false\n        )\n    );\n    final ListenableFuture<TaskStatus> future = runTask(task);\n\n    \r\n    while (task.getRunner().getStatus() != KafkaIndexTask.Status.PAUSED) {\n      Thread.sleep(10);\n    }\n    final Map<Integer, Long> currentOffsets = ImmutableMap.copyOf(task.getRunner().getCurrentOffsets());\n    Assert.assertTrue(checkpoint.getPartitionOffsetMap().equals(currentOffsets));\n    task.getRunner().setEndOffsets(currentOffsets, false);\n    Assert.assertEquals(TaskState.SUCCESS, future.get().getStatusCode());\n\n    Assert.assertEquals(1, checkpointRequestsHash.size());\n    Assert.assertTrue(\n        checkpointRequestsHash.contains(\n            Objects.hash(\n                DATA_SCHEMA.getDataSource(),\n                0,\n                new KafkaDataSourceMetadata(startPartitions),\n                new KafkaDataSourceMetadata(new KafkaPartitions(topic, checkpoint.getPartitionOffsetMap()))\n            )\n        )\n    );\n\n    \r\n    Assert.assertEquals(2, task.getRunner().getRowIngestionMeters().getProcessed());\n    Assert.assertEquals(0, task.getRunner().getRowIngestionMeters().getUnparseable());\n    Assert.assertEquals(0, task.getRunner().getRowIngestionMeters().getThrownAway());\n\n    \r\n    SegmentDescriptor desc1 = SD(task, \"2008/P1D\", 0);\n    SegmentDescriptor desc2 = SD(task, \"2009/P1D\", 0);\n    Assert.assertEquals(ImmutableSet.of(desc1, desc2), publishedDescriptors());\n    Assert.assertEquals(\n        new KafkaDataSourceMetadata(new KafkaPartitions(topic, ImmutableMap.of(0, 2L, 1, 0L))),\n        metadataStorageCoordinator.getDataSourceMetadata(DATA_SCHEMA.getDataSource())\n    );\n\n    \r\n    Assert.assertEquals(ImmutableList.of(\"a\"), readSegmentColumn(\"dim1\", desc1));\n    Assert.assertEquals(ImmutableList.of(\"b\"), readSegmentColumn(\"dim1\", desc2));\n  }\n","date":"2018-10-12 01:03:01","endLine":762,"groupId":"11125","id":4,"instanceNumber":2,"isCurCommit":0,"methodName":"testTimeBasedIncrementalHandOff","params":"()","path":"/mnt/clonedata/CloneManagementServer/ManagementServer/consistResult/result-druid-10-0.7/blobInfo/CC_OUT/blobs/6d/d210ae4c590645847c1fa82033054928af3277.src","preCode":"  public void testTimeBasedIncrementalHandOff() throws Exception\n  {\n    if (!isIncrementalHandoffSupported) {\n      return;\n    }\n    final String baseSequenceName = \"sequence0\";\n    \r\n    maxRowsPerSegment = Integer.MAX_VALUE;\n    intermediateHandoffPeriod = new Period().withSeconds(0);\n\n    \r\n    try (final KafkaProducer<byte[], byte[]> kafkaProducer = kafkaServer.newProducer()) {\n      for (ProducerRecord<byte[], byte[]> record : records.subList(0, 2)) {\n        kafkaProducer.send(record).get();\n      }\n    }\n    Map<String, String> consumerProps = kafkaServer.consumerProperties();\n    consumerProps.put(\"max.poll.records\", \"1\");\n\n    final KafkaPartitions startPartitions = new KafkaPartitions(topic, ImmutableMap.of(0, 0L, 1, 0L));\n    \r\n    final KafkaPartitions checkpoint = new KafkaPartitions(topic, ImmutableMap.of(0, 1L, 1, 0L));\n    final KafkaPartitions endPartitions = new KafkaPartitions(topic, ImmutableMap.of(0, 2L, 1, 0L));\n    final KafkaIndexTask task = createTask(\n        null,\n        new KafkaIOConfig(\n            0,\n            baseSequenceName,\n            startPartitions,\n            endPartitions,\n            consumerProps,\n            true,\n            null,\n            null,\n            false\n        )\n    );\n    final ListenableFuture<TaskStatus> future = runTask(task);\n\n    \r\n    while (task.getRunner().getStatus() != KafkaIndexTask.Status.PAUSED) {\n      Thread.sleep(10);\n    }\n    final Map<Integer, Long> currentOffsets = ImmutableMap.copyOf(task.getRunner().getCurrentOffsets());\n    Assert.assertTrue(checkpoint.getPartitionOffsetMap().equals(currentOffsets));\n    task.getRunner().setEndOffsets(currentOffsets, false);\n    Assert.assertEquals(TaskState.SUCCESS, future.get().getStatusCode());\n\n    Assert.assertEquals(1, checkpointRequestsHash.size());\n    Assert.assertTrue(\n        checkpointRequestsHash.contains(\n            Objects.hash(\n                DATA_SCHEMA.getDataSource(),\n                0,\n                new KafkaDataSourceMetadata(startPartitions),\n                new KafkaDataSourceMetadata(new KafkaPartitions(topic, checkpoint.getPartitionOffsetMap()))\n            )\n        )\n    );\n\n    \r\n    Assert.assertEquals(2, task.getRunner().getRowIngestionMeters().getProcessed());\n    Assert.assertEquals(0, task.getRunner().getRowIngestionMeters().getUnparseable());\n    Assert.assertEquals(0, task.getRunner().getRowIngestionMeters().getThrownAway());\n\n    \r\n    SegmentDescriptor desc1 = SD(task, \"2008/P1D\", 0);\n    SegmentDescriptor desc2 = SD(task, \"2009/P1D\", 0);\n    Assert.assertEquals(ImmutableSet.of(desc1, desc2), publishedDescriptors());\n    Assert.assertEquals(\n        new KafkaDataSourceMetadata(new KafkaPartitions(topic, ImmutableMap.of(0, 2L, 1, 0L))),\n        metadataStorageCoordinator.getDataSourceMetadata(DATA_SCHEMA.getDataSource())\n    );\n\n    \r\n    Assert.assertEquals(ImmutableList.of(\"a\"), readSegmentColumn(\"dim1\", desc1));\n    Assert.assertEquals(ImmutableList.of(\"b\"), readSegmentColumn(\"dim1\", desc2));\n  }\n","realPath":"extensions-core/kafka-indexing-service/src/test/java/org/apache/druid/indexing/kafka/KafkaIndexTaskTest.java","repoName":"druid","snippetEndLine":0,"snippetStartLine":0,"startLine":685,"status":"M"}],"commitId":"ab7b4798cc2747bb68550b67f69660965a276dc8","commitMessage":"@@@Securing passwords used for SSL connections to Kafka (#6285)\n\n* Secure credentials in consumer properties\n\n* Merge master\n\n* Refactor property population into separate method\n\n* Fix property setter\n\n* Fix tests\n","date":"2018-10-12 01:03:01","modifiedFileCount":"8","status":"M","submitter":"Atul Mohan"},{"authorTime":"2018-12-22 03:49:24","codes":[{"authorDate":"2018-12-22 03:49:24","commitOrder":4,"curCode":"  public void testIncrementalHandOffMaxTotalRows() throws Exception\n  {\n    if (!isIncrementalHandoffSupported) {\n      return;\n    }\n    final String baseSequenceName = \"sequence0\";\n    \r\n    maxRowsPerSegment = Integer.MAX_VALUE;\n    maxTotalRows = 3L;\n\n    \r\n    int numToAdd = records.size() - 2;\n\n    try (final KafkaProducer<byte[], byte[]> kafkaProducer = kafkaServer.newProducer()) {\n      for (int i = 0; i < numToAdd; i++) {\n        kafkaProducer.send(records.get(i)).get();\n      }\n\n      Map<String, Object> consumerProps = kafkaServer.consumerProperties();\n      consumerProps.put(\"max.poll.records\", \"1\");\n\n      final SeekableStreamPartitions<Integer, Long> startPartitions = new SeekableStreamPartitions<>(\n          topic,\n          ImmutableMap.of(\n              0,\n              0L,\n              1,\n              0L\n          )\n      );\n      final SeekableStreamPartitions<Integer, Long> checkpoint1 = new SeekableStreamPartitions<>(\n          topic,\n          ImmutableMap.of(\n              0,\n              3L,\n              1,\n              0L\n          )\n      );\n      final SeekableStreamPartitions<Integer, Long> checkpoint2 = new SeekableStreamPartitions<>(\n          topic,\n          ImmutableMap.of(\n              0,\n              10L,\n              1,\n              0L\n          )\n      );\n\n      final SeekableStreamPartitions<Integer, Long> endPartitions = new SeekableStreamPartitions<>(\n          topic,\n          ImmutableMap.of(\n              0,\n              10L,\n              1,\n              2L\n          )\n      );\n      final KafkaIndexTask task = createTask(\n          null,\n          new KafkaIndexTaskIOConfig(\n              0,\n              baseSequenceName,\n              startPartitions,\n              endPartitions,\n              consumerProps,\n              true,\n              null,\n              null,\n              false\n          )\n      );\n      final ListenableFuture<TaskStatus> future = runTask(task);\n      while (task.getRunner().getStatus() != Status.PAUSED) {\n        Thread.sleep(10);\n      }\n      final Map<Integer, Long> currentOffsets = ImmutableMap.copyOf(task.getRunner().getCurrentOffsets());\n\n      Assert.assertTrue(checkpoint1.getPartitionSequenceNumberMap().equals(currentOffsets));\n      task.getRunner().setEndOffsets(currentOffsets, false);\n\n      while (task.getRunner().getStatus() != Status.PAUSED) {\n        Thread.sleep(10);\n      }\n\n      \r\n      for (int i = numToAdd; i < records.size(); i++) {\n        kafkaProducer.send(records.get(i)).get();\n      }\n      final Map<Integer, Long> nextOffsets = ImmutableMap.copyOf(task.getRunner().getCurrentOffsets());\n\n      Assert.assertTrue(checkpoint2.getPartitionSequenceNumberMap().equals(nextOffsets));\n      task.getRunner().setEndOffsets(nextOffsets, false);\n\n      Assert.assertEquals(TaskState.SUCCESS, future.get().getStatusCode());\n\n      Assert.assertEquals(2, checkpointRequestsHash.size());\n      Assert.assertTrue(\n          checkpointRequestsHash.contains(\n              Objects.hash(\n                  DATA_SCHEMA.getDataSource(),\n                  0,\n                  new KafkaDataSourceMetadata(startPartitions),\n                  new KafkaDataSourceMetadata(new SeekableStreamPartitions<>(topic, currentOffsets))\n              )\n          )\n      );\n      Assert.assertTrue(\n          checkpointRequestsHash.contains(\n              Objects.hash(\n                  DATA_SCHEMA.getDataSource(),\n                  0,\n                  new KafkaDataSourceMetadata(new SeekableStreamPartitions<>(topic, currentOffsets)),\n                  new KafkaDataSourceMetadata(new SeekableStreamPartitions<>(topic, nextOffsets))\n              )\n          )\n      );\n\n      \r\n      Assert.assertEquals(8, task.getRunner().getRowIngestionMeters().getProcessed());\n      Assert.assertEquals(3, task.getRunner().getRowIngestionMeters().getUnparseable());\n      Assert.assertEquals(1, task.getRunner().getRowIngestionMeters().getThrownAway());\n\n      \r\n      SegmentDescriptor desc1 = SD(task, \"2008/P1D\", 0);\n      SegmentDescriptor desc2 = SD(task, \"2009/P1D\", 0);\n      SegmentDescriptor desc3 = SD(task, \"2010/P1D\", 0);\n      SegmentDescriptor desc4 = SD(task, \"2011/P1D\", 0);\n      SegmentDescriptor desc5 = SD(task, \"2011/P1D\", 1);\n      SegmentDescriptor desc6 = SD(task, \"2012/P1D\", 0);\n      SegmentDescriptor desc7 = SD(task, \"2013/P1D\", 0);\n      Assert.assertEquals(ImmutableSet.of(desc1, desc2, desc3, desc4, desc5, desc6, desc7), publishedDescriptors());\n      Assert.assertEquals(\n          new KafkaDataSourceMetadata(new SeekableStreamPartitions<>(topic, ImmutableMap.of(0, 10L, 1, 2L))),\n          metadataStorageCoordinator.getDataSourceMetadata(DATA_SCHEMA.getDataSource())\n      );\n\n      \r\n      Assert.assertEquals(ImmutableList.of(\"a\"), readSegmentColumn(\"dim1\", desc1));\n      Assert.assertEquals(ImmutableList.of(\"b\"), readSegmentColumn(\"dim1\", desc2));\n      Assert.assertEquals(ImmutableList.of(\"c\"), readSegmentColumn(\"dim1\", desc3));\n      Assert.assertTrue((ImmutableList.of(\"d\", \"e\").equals(readSegmentColumn(\"dim1\", desc4))\n                         && ImmutableList.of(\"h\").equals(readSegmentColumn(\"dim1\", desc5))) ||\n                        (ImmutableList.of(\"d\", \"h\").equals(readSegmentColumn(\"dim1\", desc4))\n                         && ImmutableList.of(\"e\").equals(readSegmentColumn(\"dim1\", desc5))));\n      Assert.assertEquals(ImmutableList.of(\"g\"), readSegmentColumn(\"dim1\", desc6));\n      Assert.assertEquals(ImmutableList.of(\"f\"), readSegmentColumn(\"dim1\", desc7));\n    }\n  }\n","date":"2018-12-22 03:49:24","endLine":748,"groupId":"3592","id":5,"instanceNumber":1,"isCurCommit":0,"methodName":"testIncrementalHandOffMaxTotalRows","params":"()","path":"/mnt/clonedata/CloneManagementServer/ManagementServer/consistResult/result-druid-10-0.7/blobInfo/CC_OUT/blobs/72/041eb947d5fcc0208ff2a5427dfc03aa78f820.src","preCode":"  public void testIncrementalHandOffMaxTotalRows() throws Exception\n  {\n    if (!isIncrementalHandoffSupported) {\n      return;\n    }\n    final String baseSequenceName = \"sequence0\";\n    \r\n    maxRowsPerSegment = Integer.MAX_VALUE;\n    maxTotalRows = 3L;\n\n    \r\n    int numToAdd = records.size() - 2;\n\n    try (final KafkaProducer<byte[], byte[]> kafkaProducer = kafkaServer.newProducer()) {\n      for (int i = 0; i < numToAdd; i++) {\n        kafkaProducer.send(records.get(i)).get();\n      }\n\n      Map<String, Object> consumerProps = kafkaServer.consumerProperties();\n      consumerProps.put(\"max.poll.records\", \"1\");\n\n      final KafkaPartitions startPartitions = new KafkaPartitions(topic, ImmutableMap.of(0, 0L, 1, 0L));\n      final KafkaPartitions checkpoint1 = new KafkaPartitions(topic, ImmutableMap.of(0, 3L, 1, 0L));\n      final KafkaPartitions checkpoint2 = new KafkaPartitions(topic, ImmutableMap.of(0, 10L, 1, 0L));\n\n      final KafkaPartitions endPartitions = new KafkaPartitions(topic, ImmutableMap.of(0, 10L, 1, 2L));\n      final KafkaIndexTask task = createTask(\n          null,\n          new KafkaIOConfig(\n              0,\n              baseSequenceName,\n              startPartitions,\n              endPartitions,\n              consumerProps,\n              true,\n              null,\n              null,\n              false\n          )\n      );\n      final ListenableFuture<TaskStatus> future = runTask(task);\n      while (task.getRunner().getStatus() != KafkaIndexTask.Status.PAUSED) {\n        Thread.sleep(10);\n      }\n      final Map<Integer, Long> currentOffsets = ImmutableMap.copyOf(task.getRunner().getCurrentOffsets());\n\n      Assert.assertTrue(checkpoint1.getPartitionOffsetMap().equals(currentOffsets));\n      task.getRunner().setEndOffsets(currentOffsets, false);\n\n      while (task.getRunner().getStatus() != KafkaIndexTask.Status.PAUSED) {\n        Thread.sleep(10);\n      }\n\n      \r\n      for (int i = numToAdd; i < records.size(); i++) {\n        kafkaProducer.send(records.get(i)).get();\n      }\n      final Map<Integer, Long> nextOffsets = ImmutableMap.copyOf(task.getRunner().getCurrentOffsets());\n\n      Assert.assertTrue(checkpoint2.getPartitionOffsetMap().equals(nextOffsets));\n      task.getRunner().setEndOffsets(nextOffsets, false);\n\n      Assert.assertEquals(TaskState.SUCCESS, future.get().getStatusCode());\n\n      Assert.assertEquals(2, checkpointRequestsHash.size());\n      Assert.assertTrue(\n          checkpointRequestsHash.contains(\n              Objects.hash(\n                  DATA_SCHEMA.getDataSource(),\n                  0,\n                  new KafkaDataSourceMetadata(startPartitions),\n                  new KafkaDataSourceMetadata(new KafkaPartitions(topic, currentOffsets))\n              )\n          )\n      );\n      Assert.assertTrue(\n          checkpointRequestsHash.contains(\n              Objects.hash(\n                  DATA_SCHEMA.getDataSource(),\n                  0,\n                  new KafkaDataSourceMetadata(new KafkaPartitions(topic, currentOffsets)),\n                  new KafkaDataSourceMetadata(new KafkaPartitions(topic, nextOffsets))\n              )\n          )\n      );\n\n      \r\n      Assert.assertEquals(8, task.getRunner().getRowIngestionMeters().getProcessed());\n      Assert.assertEquals(3, task.getRunner().getRowIngestionMeters().getUnparseable());\n      Assert.assertEquals(1, task.getRunner().getRowIngestionMeters().getThrownAway());\n\n      \r\n      SegmentDescriptor desc1 = SD(task, \"2008/P1D\", 0);\n      SegmentDescriptor desc2 = SD(task, \"2009/P1D\", 0);\n      SegmentDescriptor desc3 = SD(task, \"2010/P1D\", 0);\n      SegmentDescriptor desc4 = SD(task, \"2011/P1D\", 0);\n      SegmentDescriptor desc5 = SD(task, \"2011/P1D\", 1);\n      SegmentDescriptor desc6 = SD(task, \"2012/P1D\", 0);\n      SegmentDescriptor desc7 = SD(task, \"2013/P1D\", 0);\n      Assert.assertEquals(ImmutableSet.of(desc1, desc2, desc3, desc4, desc5, desc6, desc7), publishedDescriptors());\n      Assert.assertEquals(\n          new KafkaDataSourceMetadata(new KafkaPartitions(topic, ImmutableMap.of(0, 10L, 1, 2L))),\n          metadataStorageCoordinator.getDataSourceMetadata(DATA_SCHEMA.getDataSource())\n      );\n\n      \r\n      Assert.assertEquals(ImmutableList.of(\"a\"), readSegmentColumn(\"dim1\", desc1));\n      Assert.assertEquals(ImmutableList.of(\"b\"), readSegmentColumn(\"dim1\", desc2));\n      Assert.assertEquals(ImmutableList.of(\"c\"), readSegmentColumn(\"dim1\", desc3));\n      Assert.assertTrue((ImmutableList.of(\"d\", \"e\").equals(readSegmentColumn(\"dim1\", desc4))\n                         && ImmutableList.of(\"h\").equals(readSegmentColumn(\"dim1\", desc5))) ||\n                        (ImmutableList.of(\"d\", \"h\").equals(readSegmentColumn(\"dim1\", desc4))\n                         && ImmutableList.of(\"e\").equals(readSegmentColumn(\"dim1\", desc5))));\n      Assert.assertEquals(ImmutableList.of(\"g\"), readSegmentColumn(\"dim1\", desc6));\n      Assert.assertEquals(ImmutableList.of(\"f\"), readSegmentColumn(\"dim1\", desc7));\n    }\n  }\n","realPath":"extensions-core/kafka-indexing-service/src/test/java/org/apache/druid/indexing/kafka/KafkaIndexTaskTest.java","repoName":"druid","snippetEndLine":0,"snippetStartLine":0,"startLine":600,"status":"M"},{"authorDate":"2018-12-22 03:49:24","commitOrder":4,"curCode":"  public void testTimeBasedIncrementalHandOff() throws Exception\n  {\n    if (!isIncrementalHandoffSupported) {\n      return;\n    }\n    final String baseSequenceName = \"sequence0\";\n    \r\n    maxRowsPerSegment = Integer.MAX_VALUE;\n    intermediateHandoffPeriod = new Period().withSeconds(0);\n\n    \r\n    try (final KafkaProducer<byte[], byte[]> kafkaProducer = kafkaServer.newProducer()) {\n      for (ProducerRecord<byte[], byte[]> record : records.subList(0, 2)) {\n        kafkaProducer.send(record).get();\n      }\n    }\n    Map<String, Object> consumerProps = kafkaServer.consumerProperties();\n    consumerProps.put(\"max.poll.records\", \"1\");\n\n    final SeekableStreamPartitions<Integer, Long> startPartitions = new SeekableStreamPartitions<>(\n        topic,\n        ImmutableMap.of(\n            0,\n            0L,\n            1,\n            0L\n        )\n    );\n    \r\n    final SeekableStreamPartitions<Integer, Long> checkpoint = new SeekableStreamPartitions<>(\n        topic,\n        ImmutableMap.of(\n            0,\n            1L,\n            1,\n            0L\n        )\n    );\n    final SeekableStreamPartitions<Integer, Long> endPartitions = new SeekableStreamPartitions<>(\n        topic,\n        ImmutableMap.of(\n            0,\n            2L,\n            1,\n            0L\n        )\n    );\n    final KafkaIndexTask task = createTask(\n        null,\n        new KafkaIndexTaskIOConfig(\n            0,\n            baseSequenceName,\n            startPartitions,\n            endPartitions,\n            consumerProps,\n            true,\n            null,\n            null,\n            false\n        )\n    );\n    final ListenableFuture<TaskStatus> future = runTask(task);\n\n    \r\n    while (task.getRunner().getStatus() != Status.PAUSED) {\n      Thread.sleep(10);\n    }\n    final Map<Integer, Long> currentOffsets = ImmutableMap.copyOf(task.getRunner().getCurrentOffsets());\n    Assert.assertTrue(checkpoint.getPartitionSequenceNumberMap().equals(currentOffsets));\n    task.getRunner().setEndOffsets(currentOffsets, false);\n    Assert.assertEquals(TaskState.SUCCESS, future.get().getStatusCode());\n\n    Assert.assertEquals(1, checkpointRequestsHash.size());\n    Assert.assertTrue(\n        checkpointRequestsHash.contains(\n            Objects.hash(\n                DATA_SCHEMA.getDataSource(),\n                0,\n                new KafkaDataSourceMetadata(startPartitions),\n                new KafkaDataSourceMetadata(new SeekableStreamPartitions<>(\n                    topic,\n                    checkpoint.getPartitionSequenceNumberMap()\n                ))\n            )\n        )\n    );\n\n    \r\n    Assert.assertEquals(2, task.getRunner().getRowIngestionMeters().getProcessed());\n    Assert.assertEquals(0, task.getRunner().getRowIngestionMeters().getUnparseable());\n    Assert.assertEquals(0, task.getRunner().getRowIngestionMeters().getThrownAway());\n\n    \r\n    SegmentDescriptor desc1 = SD(task, \"2008/P1D\", 0);\n    SegmentDescriptor desc2 = SD(task, \"2009/P1D\", 0);\n    Assert.assertEquals(ImmutableSet.of(desc1, desc2), publishedDescriptors());\n    Assert.assertEquals(\n        new KafkaDataSourceMetadata(new SeekableStreamPartitions<>(topic, ImmutableMap.of(0, 2L, 1, 0L))),\n        metadataStorageCoordinator.getDataSourceMetadata(DATA_SCHEMA.getDataSource())\n    );\n\n    \r\n    Assert.assertEquals(ImmutableList.of(\"a\"), readSegmentColumn(\"dim1\", desc1));\n    Assert.assertEquals(ImmutableList.of(\"b\"), readSegmentColumn(\"dim1\", desc2));\n  }\n","date":"2018-12-22 03:49:24","endLine":855,"groupId":"12453","id":6,"instanceNumber":2,"isCurCommit":0,"methodName":"testTimeBasedIncrementalHandOff","params":"()","path":"/mnt/clonedata/CloneManagementServer/ManagementServer/consistResult/result-druid-10-0.7/blobInfo/CC_OUT/blobs/72/041eb947d5fcc0208ff2a5427dfc03aa78f820.src","preCode":"  public void testTimeBasedIncrementalHandOff() throws Exception\n  {\n    if (!isIncrementalHandoffSupported) {\n      return;\n    }\n    final String baseSequenceName = \"sequence0\";\n    \r\n    maxRowsPerSegment = Integer.MAX_VALUE;\n    intermediateHandoffPeriod = new Period().withSeconds(0);\n\n    \r\n    try (final KafkaProducer<byte[], byte[]> kafkaProducer = kafkaServer.newProducer()) {\n      for (ProducerRecord<byte[], byte[]> record : records.subList(0, 2)) {\n        kafkaProducer.send(record).get();\n      }\n    }\n    Map<String, Object> consumerProps = kafkaServer.consumerProperties();\n    consumerProps.put(\"max.poll.records\", \"1\");\n\n    final KafkaPartitions startPartitions = new KafkaPartitions(topic, ImmutableMap.of(0, 0L, 1, 0L));\n    \r\n    final KafkaPartitions checkpoint = new KafkaPartitions(topic, ImmutableMap.of(0, 1L, 1, 0L));\n    final KafkaPartitions endPartitions = new KafkaPartitions(topic, ImmutableMap.of(0, 2L, 1, 0L));\n    final KafkaIndexTask task = createTask(\n        null,\n        new KafkaIOConfig(\n            0,\n            baseSequenceName,\n            startPartitions,\n            endPartitions,\n            consumerProps,\n            true,\n            null,\n            null,\n            false\n        )\n    );\n    final ListenableFuture<TaskStatus> future = runTask(task);\n\n    \r\n    while (task.getRunner().getStatus() != KafkaIndexTask.Status.PAUSED) {\n      Thread.sleep(10);\n    }\n    final Map<Integer, Long> currentOffsets = ImmutableMap.copyOf(task.getRunner().getCurrentOffsets());\n    Assert.assertTrue(checkpoint.getPartitionOffsetMap().equals(currentOffsets));\n    task.getRunner().setEndOffsets(currentOffsets, false);\n    Assert.assertEquals(TaskState.SUCCESS, future.get().getStatusCode());\n\n    Assert.assertEquals(1, checkpointRequestsHash.size());\n    Assert.assertTrue(\n        checkpointRequestsHash.contains(\n            Objects.hash(\n                DATA_SCHEMA.getDataSource(),\n                0,\n                new KafkaDataSourceMetadata(startPartitions),\n                new KafkaDataSourceMetadata(new KafkaPartitions(topic, checkpoint.getPartitionOffsetMap()))\n            )\n        )\n    );\n\n    \r\n    Assert.assertEquals(2, task.getRunner().getRowIngestionMeters().getProcessed());\n    Assert.assertEquals(0, task.getRunner().getRowIngestionMeters().getUnparseable());\n    Assert.assertEquals(0, task.getRunner().getRowIngestionMeters().getThrownAway());\n\n    \r\n    SegmentDescriptor desc1 = SD(task, \"2008/P1D\", 0);\n    SegmentDescriptor desc2 = SD(task, \"2009/P1D\", 0);\n    Assert.assertEquals(ImmutableSet.of(desc1, desc2), publishedDescriptors());\n    Assert.assertEquals(\n        new KafkaDataSourceMetadata(new KafkaPartitions(topic, ImmutableMap.of(0, 2L, 1, 0L))),\n        metadataStorageCoordinator.getDataSourceMetadata(DATA_SCHEMA.getDataSource())\n    );\n\n    \r\n    Assert.assertEquals(ImmutableList.of(\"a\"), readSegmentColumn(\"dim1\", desc1));\n    Assert.assertEquals(ImmutableList.of(\"b\"), readSegmentColumn(\"dim1\", desc2));\n  }\n","realPath":"extensions-core/kafka-indexing-service/src/test/java/org/apache/druid/indexing/kafka/KafkaIndexTaskTest.java","repoName":"druid","snippetEndLine":0,"snippetStartLine":0,"startLine":751,"status":"M"}],"commitId":"7c7997e8a1183a7bffad731ca94e8b4c381e8665","commitMessage":"@@@Add Kinesis Indexing Service to core Druid (#6431)\n\n* created seekablestream classes\n\n* created seekablestreamsupervisor class\n\n* first attempt to integrate kafa indexing service to use SeekableStream\n\n* seekablestream bug fixes\n\n* kafkarecordsupplier\n\n* integrated kafka indexing service with seekablestream\n\n* implemented resume/suspend and refactored some package names\n\n* moved kinesis indexing service into core druid extensions\n\n* merged some changes from kafka supervisor race condition\n\n* integrated kinesis-indexing-service with seekablestream\n\n* unite tests for kinesis-indexing-service\n\n* various bug fixes for kinesis-indexing-service\n\n* refactored kinesisindexingtask\n\n* finished up more kinesis unit tests\n\n* more bug fixes for kinesis-indexing-service\n\n* finsihed refactoring kinesis unit tests\n\n* removed KinesisParititons and KafkaPartitions to use SeekableStreamPartitions\n\n* kinesis-indexing-service code cleanup and docs\n\n* merge #6291\n\nmerge #6337\n\nmerge #6383\n\n* added more docs and reordered methods\n\n* fixd kinesis tests after merging master and added docs in seekablestream\n\n* fix various things from pr comment\n\n* improve recordsupplier and add unit tests\n\n* migrated to aws-java-sdk-kinesis\n\n* merge changes from master\n\n* fix pom files and forbiddenapi checks\n\n* checkpoint JavaType bug fix\n\n* fix pom and stuff\n\n* disable checkpointing in kinesis\n\n* fix kinesis sequence number null in closed shard\n\n* merge changes from master\n\n* fixes for kinesis tasks\n\n* capitalized <partitionType.  sequenceType>\n\n* removed abstract class loggers\n\n* conform to guava api restrictions\n\n* add docker for travis other modules test\n\n* address comments\n\n* improve RecordSupplier to supply records in batch\n\n* fix strict compile issue\n\n* add test scope for localstack dependency\n\n* kinesis indexing task refactoring\n\n* comments\n\n* github comments\n\n* minor fix\n\n* removed unneeded readme\n\n* fix deserialization bug\n\n* fix various bugs\n\n* KinesisRecordSupplier unable to catch up to earliest position in stream bug fix\n\n* minor changes to kinesis\n\n* implement deaggregate for kinesis\n\n* Merge remote-tracking branch 'upstream/master' into seekablestream\n\n* fix kinesis offset discrepancy with kafka\n\n* kinesis record supplier disable getPosition\n\n* pr comments\n\n* mock for kinesis tests and remove docker dependency for unit tests\n\n* PR comments\n\n* avg lag in kafkasupervisor #6587\n\n* refacotred SequenceMetadata in taskRunners\n\n* small fix\n\n* more small fix\n\n* recordsupplier resource leak\n\n* revert .travis.yml formatting\n\n* fix style\n\n* kinesis docs\n\n* doc part2\n\n* more docs\n\n* comments\n\n* comments*2\n\n* revert string replace changes\n\n* comments\n\n* teamcity\n\n* comments part 1\n\n* comments part 2\n\n* comments part 3\n\n* merge #6754\n\n* fix injection binding\n\n* comments\n\n* KinesisRegion refactor\n\n* comments part idk lol\n\n* can't think of a commit msg anymore\n\n* remove possiblyResetDataSourceMetadata() for IncrementalPublishingTaskRunner\n\n* commmmmmmmmmments\n\n* extra error handling in KinesisRecordSupplier getRecords\n\n* comments\n\n* quickfix\n\n* typo\n\n* oof\n","date":"2018-12-22 03:49:24","modifiedFileCount":"22","status":"M","submitter":"Joshua Sun"},{"authorTime":"2019-01-03 12:16:02","codes":[{"authorDate":"2019-01-03 12:16:02","commitOrder":5,"curCode":"  public void testIncrementalHandOffMaxTotalRows() throws Exception\n  {\n    if (!isIncrementalHandoffSupported) {\n      return;\n    }\n    final String baseSequenceName = \"sequence0\";\n    \r\n    maxRowsPerSegment = Integer.MAX_VALUE;\n    maxTotalRows = 3L;\n\n    \r\n    int numToAdd = records.size() - 2;\n\n    try (final KafkaProducer<byte[], byte[]> kafkaProducer = kafkaServer.newProducer()) {\n      for (int i = 0; i < numToAdd; i++) {\n        kafkaProducer.send(records.get(i)).get();\n      }\n\n      Map<String, Object> consumerProps = kafkaServer.consumerProperties();\n      consumerProps.put(\"max.poll.records\", \"1\");\n\n      final SeekableStreamPartitions<Integer, Long> startPartitions = new SeekableStreamPartitions<>(\n          topic,\n          ImmutableMap.of(\n              0,\n              0L,\n              1,\n              0L\n          )\n      );\n      final SeekableStreamPartitions<Integer, Long> checkpoint1 = new SeekableStreamPartitions<>(\n          topic,\n          ImmutableMap.of(\n              0,\n              3L,\n              1,\n              0L\n          )\n      );\n      final SeekableStreamPartitions<Integer, Long> checkpoint2 = new SeekableStreamPartitions<>(\n          topic,\n          ImmutableMap.of(\n              0,\n              10L,\n              1,\n              0L\n          )\n      );\n\n      final SeekableStreamPartitions<Integer, Long> endPartitions = new SeekableStreamPartitions<>(\n          topic,\n          ImmutableMap.of(\n              0,\n              10L,\n              1,\n              2L\n          )\n      );\n      final KafkaIndexTask task = createTask(\n          null,\n          new KafkaIndexTaskIOConfig(\n              0,\n              baseSequenceName,\n              startPartitions,\n              endPartitions,\n              consumerProps,\n              KafkaSupervisorIOConfig.DEFAULT_POLL_TIMEOUT_MILLIS,\n              true,\n              null,\n              null,\n              false\n          )\n      );\n      final ListenableFuture<TaskStatus> future = runTask(task);\n      while (task.getRunner().getStatus() != Status.PAUSED) {\n        Thread.sleep(10);\n      }\n      final Map<Integer, Long> currentOffsets = ImmutableMap.copyOf(task.getRunner().getCurrentOffsets());\n\n      Assert.assertTrue(checkpoint1.getPartitionSequenceNumberMap().equals(currentOffsets));\n      task.getRunner().setEndOffsets(currentOffsets, false);\n\n      while (task.getRunner().getStatus() != Status.PAUSED) {\n        Thread.sleep(10);\n      }\n\n      \r\n      for (int i = numToAdd; i < records.size(); i++) {\n        kafkaProducer.send(records.get(i)).get();\n      }\n      final Map<Integer, Long> nextOffsets = ImmutableMap.copyOf(task.getRunner().getCurrentOffsets());\n\n      Assert.assertTrue(checkpoint2.getPartitionSequenceNumberMap().equals(nextOffsets));\n      task.getRunner().setEndOffsets(nextOffsets, false);\n\n      Assert.assertEquals(TaskState.SUCCESS, future.get().getStatusCode());\n\n      Assert.assertEquals(2, checkpointRequestsHash.size());\n      Assert.assertTrue(\n          checkpointRequestsHash.contains(\n              Objects.hash(\n                  DATA_SCHEMA.getDataSource(),\n                  0,\n                  new KafkaDataSourceMetadata(startPartitions),\n                  new KafkaDataSourceMetadata(new SeekableStreamPartitions<>(topic, currentOffsets))\n              )\n          )\n      );\n      Assert.assertTrue(\n          checkpointRequestsHash.contains(\n              Objects.hash(\n                  DATA_SCHEMA.getDataSource(),\n                  0,\n                  new KafkaDataSourceMetadata(new SeekableStreamPartitions<>(topic, currentOffsets)),\n                  new KafkaDataSourceMetadata(new SeekableStreamPartitions<>(topic, nextOffsets))\n              )\n          )\n      );\n\n      \r\n      Assert.assertEquals(8, task.getRunner().getRowIngestionMeters().getProcessed());\n      Assert.assertEquals(3, task.getRunner().getRowIngestionMeters().getUnparseable());\n      Assert.assertEquals(1, task.getRunner().getRowIngestionMeters().getThrownAway());\n\n      \r\n      SegmentDescriptor desc1 = SD(task, \"2008/P1D\", 0);\n      SegmentDescriptor desc2 = SD(task, \"2009/P1D\", 0);\n      SegmentDescriptor desc3 = SD(task, \"2010/P1D\", 0);\n      SegmentDescriptor desc4 = SD(task, \"2011/P1D\", 0);\n      SegmentDescriptor desc5 = SD(task, \"2011/P1D\", 1);\n      SegmentDescriptor desc6 = SD(task, \"2012/P1D\", 0);\n      SegmentDescriptor desc7 = SD(task, \"2013/P1D\", 0);\n      Assert.assertEquals(ImmutableSet.of(desc1, desc2, desc3, desc4, desc5, desc6, desc7), publishedDescriptors());\n      Assert.assertEquals(\n          new KafkaDataSourceMetadata(new SeekableStreamPartitions<>(topic, ImmutableMap.of(0, 10L, 1, 2L))),\n          metadataStorageCoordinator.getDataSourceMetadata(DATA_SCHEMA.getDataSource())\n      );\n\n      \r\n      Assert.assertEquals(ImmutableList.of(\"a\"), readSegmentColumn(\"dim1\", desc1));\n      Assert.assertEquals(ImmutableList.of(\"b\"), readSegmentColumn(\"dim1\", desc2));\n      Assert.assertEquals(ImmutableList.of(\"c\"), readSegmentColumn(\"dim1\", desc3));\n      Assert.assertTrue((ImmutableList.of(\"d\", \"e\").equals(readSegmentColumn(\"dim1\", desc4))\n                         && ImmutableList.of(\"h\").equals(readSegmentColumn(\"dim1\", desc5))) ||\n                        (ImmutableList.of(\"d\", \"h\").equals(readSegmentColumn(\"dim1\", desc4))\n                         && ImmutableList.of(\"e\").equals(readSegmentColumn(\"dim1\", desc5))));\n      Assert.assertEquals(ImmutableList.of(\"g\"), readSegmentColumn(\"dim1\", desc6));\n      Assert.assertEquals(ImmutableList.of(\"f\"), readSegmentColumn(\"dim1\", desc7));\n    }\n  }\n","date":"2019-01-03 12:16:02","endLine":753,"groupId":"3592","id":7,"instanceNumber":1,"isCurCommit":0,"methodName":"testIncrementalHandOffMaxTotalRows","params":"()","path":"/mnt/clonedata/CloneManagementServer/ManagementServer/consistResult/result-druid-10-0.7/blobInfo/CC_OUT/blobs/f4/4877771fbd76d2e48f167fa6e25fc70e074abc.src","preCode":"  public void testIncrementalHandOffMaxTotalRows() throws Exception\n  {\n    if (!isIncrementalHandoffSupported) {\n      return;\n    }\n    final String baseSequenceName = \"sequence0\";\n    \r\n    maxRowsPerSegment = Integer.MAX_VALUE;\n    maxTotalRows = 3L;\n\n    \r\n    int numToAdd = records.size() - 2;\n\n    try (final KafkaProducer<byte[], byte[]> kafkaProducer = kafkaServer.newProducer()) {\n      for (int i = 0; i < numToAdd; i++) {\n        kafkaProducer.send(records.get(i)).get();\n      }\n\n      Map<String, Object> consumerProps = kafkaServer.consumerProperties();\n      consumerProps.put(\"max.poll.records\", \"1\");\n\n      final SeekableStreamPartitions<Integer, Long> startPartitions = new SeekableStreamPartitions<>(\n          topic,\n          ImmutableMap.of(\n              0,\n              0L,\n              1,\n              0L\n          )\n      );\n      final SeekableStreamPartitions<Integer, Long> checkpoint1 = new SeekableStreamPartitions<>(\n          topic,\n          ImmutableMap.of(\n              0,\n              3L,\n              1,\n              0L\n          )\n      );\n      final SeekableStreamPartitions<Integer, Long> checkpoint2 = new SeekableStreamPartitions<>(\n          topic,\n          ImmutableMap.of(\n              0,\n              10L,\n              1,\n              0L\n          )\n      );\n\n      final SeekableStreamPartitions<Integer, Long> endPartitions = new SeekableStreamPartitions<>(\n          topic,\n          ImmutableMap.of(\n              0,\n              10L,\n              1,\n              2L\n          )\n      );\n      final KafkaIndexTask task = createTask(\n          null,\n          new KafkaIndexTaskIOConfig(\n              0,\n              baseSequenceName,\n              startPartitions,\n              endPartitions,\n              consumerProps,\n              true,\n              null,\n              null,\n              false\n          )\n      );\n      final ListenableFuture<TaskStatus> future = runTask(task);\n      while (task.getRunner().getStatus() != Status.PAUSED) {\n        Thread.sleep(10);\n      }\n      final Map<Integer, Long> currentOffsets = ImmutableMap.copyOf(task.getRunner().getCurrentOffsets());\n\n      Assert.assertTrue(checkpoint1.getPartitionSequenceNumberMap().equals(currentOffsets));\n      task.getRunner().setEndOffsets(currentOffsets, false);\n\n      while (task.getRunner().getStatus() != Status.PAUSED) {\n        Thread.sleep(10);\n      }\n\n      \r\n      for (int i = numToAdd; i < records.size(); i++) {\n        kafkaProducer.send(records.get(i)).get();\n      }\n      final Map<Integer, Long> nextOffsets = ImmutableMap.copyOf(task.getRunner().getCurrentOffsets());\n\n      Assert.assertTrue(checkpoint2.getPartitionSequenceNumberMap().equals(nextOffsets));\n      task.getRunner().setEndOffsets(nextOffsets, false);\n\n      Assert.assertEquals(TaskState.SUCCESS, future.get().getStatusCode());\n\n      Assert.assertEquals(2, checkpointRequestsHash.size());\n      Assert.assertTrue(\n          checkpointRequestsHash.contains(\n              Objects.hash(\n                  DATA_SCHEMA.getDataSource(),\n                  0,\n                  new KafkaDataSourceMetadata(startPartitions),\n                  new KafkaDataSourceMetadata(new SeekableStreamPartitions<>(topic, currentOffsets))\n              )\n          )\n      );\n      Assert.assertTrue(\n          checkpointRequestsHash.contains(\n              Objects.hash(\n                  DATA_SCHEMA.getDataSource(),\n                  0,\n                  new KafkaDataSourceMetadata(new SeekableStreamPartitions<>(topic, currentOffsets)),\n                  new KafkaDataSourceMetadata(new SeekableStreamPartitions<>(topic, nextOffsets))\n              )\n          )\n      );\n\n      \r\n      Assert.assertEquals(8, task.getRunner().getRowIngestionMeters().getProcessed());\n      Assert.assertEquals(3, task.getRunner().getRowIngestionMeters().getUnparseable());\n      Assert.assertEquals(1, task.getRunner().getRowIngestionMeters().getThrownAway());\n\n      \r\n      SegmentDescriptor desc1 = SD(task, \"2008/P1D\", 0);\n      SegmentDescriptor desc2 = SD(task, \"2009/P1D\", 0);\n      SegmentDescriptor desc3 = SD(task, \"2010/P1D\", 0);\n      SegmentDescriptor desc4 = SD(task, \"2011/P1D\", 0);\n      SegmentDescriptor desc5 = SD(task, \"2011/P1D\", 1);\n      SegmentDescriptor desc6 = SD(task, \"2012/P1D\", 0);\n      SegmentDescriptor desc7 = SD(task, \"2013/P1D\", 0);\n      Assert.assertEquals(ImmutableSet.of(desc1, desc2, desc3, desc4, desc5, desc6, desc7), publishedDescriptors());\n      Assert.assertEquals(\n          new KafkaDataSourceMetadata(new SeekableStreamPartitions<>(topic, ImmutableMap.of(0, 10L, 1, 2L))),\n          metadataStorageCoordinator.getDataSourceMetadata(DATA_SCHEMA.getDataSource())\n      );\n\n      \r\n      Assert.assertEquals(ImmutableList.of(\"a\"), readSegmentColumn(\"dim1\", desc1));\n      Assert.assertEquals(ImmutableList.of(\"b\"), readSegmentColumn(\"dim1\", desc2));\n      Assert.assertEquals(ImmutableList.of(\"c\"), readSegmentColumn(\"dim1\", desc3));\n      Assert.assertTrue((ImmutableList.of(\"d\", \"e\").equals(readSegmentColumn(\"dim1\", desc4))\n                         && ImmutableList.of(\"h\").equals(readSegmentColumn(\"dim1\", desc5))) ||\n                        (ImmutableList.of(\"d\", \"h\").equals(readSegmentColumn(\"dim1\", desc4))\n                         && ImmutableList.of(\"e\").equals(readSegmentColumn(\"dim1\", desc5))));\n      Assert.assertEquals(ImmutableList.of(\"g\"), readSegmentColumn(\"dim1\", desc6));\n      Assert.assertEquals(ImmutableList.of(\"f\"), readSegmentColumn(\"dim1\", desc7));\n    }\n  }\n","realPath":"extensions-core/kafka-indexing-service/src/test/java/org/apache/druid/indexing/kafka/KafkaIndexTaskTest.java","repoName":"druid","snippetEndLine":0,"snippetStartLine":0,"startLine":604,"status":"M"},{"authorDate":"2019-01-03 12:16:02","commitOrder":5,"curCode":"  public void testTimeBasedIncrementalHandOff() throws Exception\n  {\n    if (!isIncrementalHandoffSupported) {\n      return;\n    }\n    final String baseSequenceName = \"sequence0\";\n    \r\n    maxRowsPerSegment = Integer.MAX_VALUE;\n    intermediateHandoffPeriod = new Period().withSeconds(0);\n\n    \r\n    try (final KafkaProducer<byte[], byte[]> kafkaProducer = kafkaServer.newProducer()) {\n      for (ProducerRecord<byte[], byte[]> record : records.subList(0, 2)) {\n        kafkaProducer.send(record).get();\n      }\n    }\n    Map<String, Object> consumerProps = kafkaServer.consumerProperties();\n    consumerProps.put(\"max.poll.records\", \"1\");\n\n    final SeekableStreamPartitions<Integer, Long> startPartitions = new SeekableStreamPartitions<>(\n        topic,\n        ImmutableMap.of(\n            0,\n            0L,\n            1,\n            0L\n        )\n    );\n    \r\n    final SeekableStreamPartitions<Integer, Long> checkpoint = new SeekableStreamPartitions<>(\n        topic,\n        ImmutableMap.of(\n            0,\n            1L,\n            1,\n            0L\n        )\n    );\n    final SeekableStreamPartitions<Integer, Long> endPartitions = new SeekableStreamPartitions<>(\n        topic,\n        ImmutableMap.of(\n            0,\n            2L,\n            1,\n            0L\n        )\n    );\n    final KafkaIndexTask task = createTask(\n        null,\n        new KafkaIndexTaskIOConfig(\n            0,\n            baseSequenceName,\n            startPartitions,\n            endPartitions,\n            consumerProps,\n            KafkaSupervisorIOConfig.DEFAULT_POLL_TIMEOUT_MILLIS,\n            true,\n            null,\n            null,\n            false\n        )\n    );\n    final ListenableFuture<TaskStatus> future = runTask(task);\n\n    \r\n    while (task.getRunner().getStatus() != Status.PAUSED) {\n      Thread.sleep(10);\n    }\n    final Map<Integer, Long> currentOffsets = ImmutableMap.copyOf(task.getRunner().getCurrentOffsets());\n    Assert.assertTrue(checkpoint.getPartitionSequenceNumberMap().equals(currentOffsets));\n    task.getRunner().setEndOffsets(currentOffsets, false);\n    Assert.assertEquals(TaskState.SUCCESS, future.get().getStatusCode());\n\n    Assert.assertEquals(1, checkpointRequestsHash.size());\n    Assert.assertTrue(\n        checkpointRequestsHash.contains(\n            Objects.hash(\n                DATA_SCHEMA.getDataSource(),\n                0,\n                new KafkaDataSourceMetadata(startPartitions),\n                new KafkaDataSourceMetadata(new SeekableStreamPartitions<>(\n                    topic,\n                    checkpoint.getPartitionSequenceNumberMap()\n                ))\n            )\n        )\n    );\n\n    \r\n    Assert.assertEquals(2, task.getRunner().getRowIngestionMeters().getProcessed());\n    Assert.assertEquals(0, task.getRunner().getRowIngestionMeters().getUnparseable());\n    Assert.assertEquals(0, task.getRunner().getRowIngestionMeters().getThrownAway());\n\n    \r\n    SegmentDescriptor desc1 = SD(task, \"2008/P1D\", 0);\n    SegmentDescriptor desc2 = SD(task, \"2009/P1D\", 0);\n    Assert.assertEquals(ImmutableSet.of(desc1, desc2), publishedDescriptors());\n    Assert.assertEquals(\n        new KafkaDataSourceMetadata(new SeekableStreamPartitions<>(topic, ImmutableMap.of(0, 2L, 1, 0L))),\n        metadataStorageCoordinator.getDataSourceMetadata(DATA_SCHEMA.getDataSource())\n    );\n\n    \r\n    Assert.assertEquals(ImmutableList.of(\"a\"), readSegmentColumn(\"dim1\", desc1));\n    Assert.assertEquals(ImmutableList.of(\"b\"), readSegmentColumn(\"dim1\", desc2));\n  }\n","date":"2019-01-03 12:16:02","endLine":861,"groupId":"12453","id":8,"instanceNumber":2,"isCurCommit":0,"methodName":"testTimeBasedIncrementalHandOff","params":"()","path":"/mnt/clonedata/CloneManagementServer/ManagementServer/consistResult/result-druid-10-0.7/blobInfo/CC_OUT/blobs/f4/4877771fbd76d2e48f167fa6e25fc70e074abc.src","preCode":"  public void testTimeBasedIncrementalHandOff() throws Exception\n  {\n    if (!isIncrementalHandoffSupported) {\n      return;\n    }\n    final String baseSequenceName = \"sequence0\";\n    \r\n    maxRowsPerSegment = Integer.MAX_VALUE;\n    intermediateHandoffPeriod = new Period().withSeconds(0);\n\n    \r\n    try (final KafkaProducer<byte[], byte[]> kafkaProducer = kafkaServer.newProducer()) {\n      for (ProducerRecord<byte[], byte[]> record : records.subList(0, 2)) {\n        kafkaProducer.send(record).get();\n      }\n    }\n    Map<String, Object> consumerProps = kafkaServer.consumerProperties();\n    consumerProps.put(\"max.poll.records\", \"1\");\n\n    final SeekableStreamPartitions<Integer, Long> startPartitions = new SeekableStreamPartitions<>(\n        topic,\n        ImmutableMap.of(\n            0,\n            0L,\n            1,\n            0L\n        )\n    );\n    \r\n    final SeekableStreamPartitions<Integer, Long> checkpoint = new SeekableStreamPartitions<>(\n        topic,\n        ImmutableMap.of(\n            0,\n            1L,\n            1,\n            0L\n        )\n    );\n    final SeekableStreamPartitions<Integer, Long> endPartitions = new SeekableStreamPartitions<>(\n        topic,\n        ImmutableMap.of(\n            0,\n            2L,\n            1,\n            0L\n        )\n    );\n    final KafkaIndexTask task = createTask(\n        null,\n        new KafkaIndexTaskIOConfig(\n            0,\n            baseSequenceName,\n            startPartitions,\n            endPartitions,\n            consumerProps,\n            true,\n            null,\n            null,\n            false\n        )\n    );\n    final ListenableFuture<TaskStatus> future = runTask(task);\n\n    \r\n    while (task.getRunner().getStatus() != Status.PAUSED) {\n      Thread.sleep(10);\n    }\n    final Map<Integer, Long> currentOffsets = ImmutableMap.copyOf(task.getRunner().getCurrentOffsets());\n    Assert.assertTrue(checkpoint.getPartitionSequenceNumberMap().equals(currentOffsets));\n    task.getRunner().setEndOffsets(currentOffsets, false);\n    Assert.assertEquals(TaskState.SUCCESS, future.get().getStatusCode());\n\n    Assert.assertEquals(1, checkpointRequestsHash.size());\n    Assert.assertTrue(\n        checkpointRequestsHash.contains(\n            Objects.hash(\n                DATA_SCHEMA.getDataSource(),\n                0,\n                new KafkaDataSourceMetadata(startPartitions),\n                new KafkaDataSourceMetadata(new SeekableStreamPartitions<>(\n                    topic,\n                    checkpoint.getPartitionSequenceNumberMap()\n                ))\n            )\n        )\n    );\n\n    \r\n    Assert.assertEquals(2, task.getRunner().getRowIngestionMeters().getProcessed());\n    Assert.assertEquals(0, task.getRunner().getRowIngestionMeters().getUnparseable());\n    Assert.assertEquals(0, task.getRunner().getRowIngestionMeters().getThrownAway());\n\n    \r\n    SegmentDescriptor desc1 = SD(task, \"2008/P1D\", 0);\n    SegmentDescriptor desc2 = SD(task, \"2009/P1D\", 0);\n    Assert.assertEquals(ImmutableSet.of(desc1, desc2), publishedDescriptors());\n    Assert.assertEquals(\n        new KafkaDataSourceMetadata(new SeekableStreamPartitions<>(topic, ImmutableMap.of(0, 2L, 1, 0L))),\n        metadataStorageCoordinator.getDataSourceMetadata(DATA_SCHEMA.getDataSource())\n    );\n\n    \r\n    Assert.assertEquals(ImmutableList.of(\"a\"), readSegmentColumn(\"dim1\", desc1));\n    Assert.assertEquals(ImmutableList.of(\"b\"), readSegmentColumn(\"dim1\", desc2));\n  }\n","realPath":"extensions-core/kafka-indexing-service/src/test/java/org/apache/druid/indexing/kafka/KafkaIndexTaskTest.java","repoName":"druid","snippetEndLine":0,"snippetStartLine":0,"startLine":756,"status":"M"}],"commitId":"6761663509025c5c88275a9752c96d417f879abe","commitMessage":"@@@make kafka poll timeout can be configured (#6773)\n\n* make kafka poll timeout can be configured\n\n* add doc\n\n* rename DEFAULT_POLL_TIMEOUT to DEFAULT_POLL_TIMEOUT_MILLIS\n","date":"2019-01-03 12:16:02","modifiedFileCount":"9","status":"M","submitter":"Mingming Qiu"},{"authorTime":"2019-02-19 03:50:08","codes":[{"authorDate":"2019-02-19 03:50:08","commitOrder":6,"curCode":"  public void testIncrementalHandOffMaxTotalRows() throws Exception\n  {\n    if (!isIncrementalHandoffSupported) {\n      return;\n    }\n    final String baseSequenceName = \"sequence0\";\n    \r\n    maxRowsPerSegment = Integer.MAX_VALUE;\n    maxTotalRows = 3L;\n\n    \r\n    int numToAdd = records.size() - 2;\n\n    try (final KafkaProducer<byte[], byte[]> kafkaProducer = kafkaServer.newProducer()) {\n      kafkaProducer.initTransactions();\n      kafkaProducer.beginTransaction();\n      for (int i = 0; i < numToAdd; i++) {\n        kafkaProducer.send(records.get(i)).get();\n      }\n      kafkaProducer.commitTransaction();\n    }\n\n    Map<String, Object> consumerProps = kafkaServer.consumerProperties();\n    consumerProps.put(\"max.poll.records\", \"1\");\n\n    final SeekableStreamPartitions<Integer, Long> startPartitions = new SeekableStreamPartitions<>(\n        topic,\n        ImmutableMap.of(\n            0,\n            0L,\n            1,\n            0L\n        )\n    );\n    final SeekableStreamPartitions<Integer, Long> checkpoint1 = new SeekableStreamPartitions<>(\n        topic,\n        ImmutableMap.of(\n            0,\n            3L,\n            1,\n            0L\n        )\n    );\n    final SeekableStreamPartitions<Integer, Long> checkpoint2 = new SeekableStreamPartitions<>(\n        topic,\n        ImmutableMap.of(\n            0,\n            10L,\n            1,\n            0L\n        )\n    );\n\n    final SeekableStreamPartitions<Integer, Long> endPartitions = new SeekableStreamPartitions<>(\n        topic,\n        ImmutableMap.of(\n            0,\n            10L,\n            1,\n            2L\n        )\n    );\n    final KafkaIndexTask task = createTask(\n        null,\n        new KafkaIndexTaskIOConfig(\n            0,\n            baseSequenceName,\n            startPartitions,\n            endPartitions,\n            consumerProps,\n            KafkaSupervisorIOConfig.DEFAULT_POLL_TIMEOUT_MILLIS,\n            true,\n            null,\n            null\n        )\n    );\n    final ListenableFuture<TaskStatus> future = runTask(task);\n    while (task.getRunner().getStatus() != Status.PAUSED) {\n      Thread.sleep(10);\n    }\n    final Map<Integer, Long> currentOffsets = ImmutableMap.copyOf(task.getRunner().getCurrentOffsets());\n\n    Assert.assertTrue(checkpoint1.getPartitionSequenceNumberMap().equals(currentOffsets));\n    task.getRunner().setEndOffsets(currentOffsets, false);\n\n    while (task.getRunner().getStatus() != Status.PAUSED) {\n      Thread.sleep(10);\n    }\n\n    \r\n    try (final KafkaProducer<byte[], byte[]> kafkaProducer = kafkaServer.newProducer()) {\n      kafkaProducer.initTransactions();\n      kafkaProducer.beginTransaction();\n      for (int i = numToAdd; i < records.size(); i++) {\n        kafkaProducer.send(records.get(i)).get();\n      }\n      kafkaProducer.commitTransaction();\n    }\n    final Map<Integer, Long> nextOffsets = ImmutableMap.copyOf(task.getRunner().getCurrentOffsets());\n\n    Assert.assertTrue(checkpoint2.getPartitionSequenceNumberMap().equals(nextOffsets));\n    task.getRunner().setEndOffsets(nextOffsets, false);\n\n    Assert.assertEquals(TaskState.SUCCESS, future.get().getStatusCode());\n\n    Assert.assertEquals(2, checkpointRequestsHash.size());\n    Assert.assertTrue(\n        checkpointRequestsHash.contains(\n            Objects.hash(\n                DATA_SCHEMA.getDataSource(),\n                0,\n                new KafkaDataSourceMetadata(startPartitions),\n                new KafkaDataSourceMetadata(new SeekableStreamPartitions<>(topic, currentOffsets))\n            )\n        )\n    );\n    Assert.assertTrue(\n        checkpointRequestsHash.contains(\n            Objects.hash(\n                DATA_SCHEMA.getDataSource(),\n                0,\n                new KafkaDataSourceMetadata(new SeekableStreamPartitions(topic, currentOffsets)),\n                new KafkaDataSourceMetadata(new SeekableStreamPartitions(topic, nextOffsets))\n            )\n        )\n    );\n\n    \r\n    Assert.assertEquals(8, task.getRunner().getRowIngestionMeters().getProcessed());\n    Assert.assertEquals(3, task.getRunner().getRowIngestionMeters().getUnparseable());\n    Assert.assertEquals(1, task.getRunner().getRowIngestionMeters().getThrownAway());\n\n    \r\n    SegmentDescriptor desc1 = SD(task, \"2008/P1D\", 0);\n    SegmentDescriptor desc2 = SD(task, \"2009/P1D\", 0);\n    SegmentDescriptor desc3 = SD(task, \"2010/P1D\", 0);\n    SegmentDescriptor desc4 = SD(task, \"2011/P1D\", 0);\n    SegmentDescriptor desc5 = SD(task, \"2011/P1D\", 1);\n    SegmentDescriptor desc6 = SD(task, \"2012/P1D\", 0);\n    SegmentDescriptor desc7 = SD(task, \"2013/P1D\", 0);\n    Assert.assertEquals(ImmutableSet.of(desc1, desc2, desc3, desc4, desc5, desc6, desc7), publishedDescriptors());\n    Assert.assertEquals(\n        new KafkaDataSourceMetadata(new SeekableStreamPartitions<>(topic, ImmutableMap.of(0, 10L, 1, 2L))),\n        metadataStorageCoordinator.getDataSourceMetadata(DATA_SCHEMA.getDataSource())\n    );\n\n    \r\n    Assert.assertEquals(ImmutableList.of(\"a\"), readSegmentColumn(\"dim1\", desc1));\n    Assert.assertEquals(ImmutableList.of(\"b\"), readSegmentColumn(\"dim1\", desc2));\n    Assert.assertEquals(ImmutableList.of(\"c\"), readSegmentColumn(\"dim1\", desc3));\n    Assert.assertTrue((ImmutableList.of(\"d\", \"e\").equals(readSegmentColumn(\"dim1\", desc4))\n                       && ImmutableList.of(\"h\").equals(readSegmentColumn(\"dim1\", desc5))) ||\n                      (ImmutableList.of(\"d\", \"h\").equals(readSegmentColumn(\"dim1\", desc4))\n                       && ImmutableList.of(\"e\").equals(readSegmentColumn(\"dim1\", desc5))));\n    Assert.assertEquals(ImmutableList.of(\"g\"), readSegmentColumn(\"dim1\", desc6));\n    Assert.assertEquals(ImmutableList.of(\"f\"), readSegmentColumn(\"dim1\", desc7));\n  }\n","date":"2019-02-19 03:50:08","endLine":756,"groupId":"3593","id":9,"instanceNumber":1,"isCurCommit":0,"methodName":"testIncrementalHandOffMaxTotalRows","params":"()","path":"/mnt/clonedata/CloneManagementServer/ManagementServer/consistResult/result-druid-10-0.7/blobInfo/CC_OUT/blobs/3d/6308cb0a96b01947db7e17b51cab010dd61129.src","preCode":"  public void testIncrementalHandOffMaxTotalRows() throws Exception\n  {\n    if (!isIncrementalHandoffSupported) {\n      return;\n    }\n    final String baseSequenceName = \"sequence0\";\n    \r\n    maxRowsPerSegment = Integer.MAX_VALUE;\n    maxTotalRows = 3L;\n\n    \r\n    int numToAdd = records.size() - 2;\n\n    try (final KafkaProducer<byte[], byte[]> kafkaProducer = kafkaServer.newProducer()) {\n      for (int i = 0; i < numToAdd; i++) {\n        kafkaProducer.send(records.get(i)).get();\n      }\n\n      Map<String, Object> consumerProps = kafkaServer.consumerProperties();\n      consumerProps.put(\"max.poll.records\", \"1\");\n\n      final SeekableStreamPartitions<Integer, Long> startPartitions = new SeekableStreamPartitions<>(\n          topic,\n          ImmutableMap.of(\n              0,\n              0L,\n              1,\n              0L\n          )\n      );\n      final SeekableStreamPartitions<Integer, Long> checkpoint1 = new SeekableStreamPartitions<>(\n          topic,\n          ImmutableMap.of(\n              0,\n              3L,\n              1,\n              0L\n          )\n      );\n      final SeekableStreamPartitions<Integer, Long> checkpoint2 = new SeekableStreamPartitions<>(\n          topic,\n          ImmutableMap.of(\n              0,\n              10L,\n              1,\n              0L\n          )\n      );\n\n      final SeekableStreamPartitions<Integer, Long> endPartitions = new SeekableStreamPartitions<>(\n          topic,\n          ImmutableMap.of(\n              0,\n              10L,\n              1,\n              2L\n          )\n      );\n      final KafkaIndexTask task = createTask(\n          null,\n          new KafkaIndexTaskIOConfig(\n              0,\n              baseSequenceName,\n              startPartitions,\n              endPartitions,\n              consumerProps,\n              KafkaSupervisorIOConfig.DEFAULT_POLL_TIMEOUT_MILLIS,\n              true,\n              null,\n              null,\n              false\n          )\n      );\n      final ListenableFuture<TaskStatus> future = runTask(task);\n      while (task.getRunner().getStatus() != Status.PAUSED) {\n        Thread.sleep(10);\n      }\n      final Map<Integer, Long> currentOffsets = ImmutableMap.copyOf(task.getRunner().getCurrentOffsets());\n\n      Assert.assertTrue(checkpoint1.getPartitionSequenceNumberMap().equals(currentOffsets));\n      task.getRunner().setEndOffsets(currentOffsets, false);\n\n      while (task.getRunner().getStatus() != Status.PAUSED) {\n        Thread.sleep(10);\n      }\n\n      \r\n      for (int i = numToAdd; i < records.size(); i++) {\n        kafkaProducer.send(records.get(i)).get();\n      }\n      final Map<Integer, Long> nextOffsets = ImmutableMap.copyOf(task.getRunner().getCurrentOffsets());\n\n      Assert.assertTrue(checkpoint2.getPartitionSequenceNumberMap().equals(nextOffsets));\n      task.getRunner().setEndOffsets(nextOffsets, false);\n\n      Assert.assertEquals(TaskState.SUCCESS, future.get().getStatusCode());\n\n      Assert.assertEquals(2, checkpointRequestsHash.size());\n      Assert.assertTrue(\n          checkpointRequestsHash.contains(\n              Objects.hash(\n                  DATA_SCHEMA.getDataSource(),\n                  0,\n                  new KafkaDataSourceMetadata(startPartitions),\n                  new KafkaDataSourceMetadata(new SeekableStreamPartitions<>(topic, currentOffsets))\n              )\n          )\n      );\n      Assert.assertTrue(\n          checkpointRequestsHash.contains(\n              Objects.hash(\n                  DATA_SCHEMA.getDataSource(),\n                  0,\n                  new KafkaDataSourceMetadata(new SeekableStreamPartitions<>(topic, currentOffsets)),\n                  new KafkaDataSourceMetadata(new SeekableStreamPartitions<>(topic, nextOffsets))\n              )\n          )\n      );\n\n      \r\n      Assert.assertEquals(8, task.getRunner().getRowIngestionMeters().getProcessed());\n      Assert.assertEquals(3, task.getRunner().getRowIngestionMeters().getUnparseable());\n      Assert.assertEquals(1, task.getRunner().getRowIngestionMeters().getThrownAway());\n\n      \r\n      SegmentDescriptor desc1 = SD(task, \"2008/P1D\", 0);\n      SegmentDescriptor desc2 = SD(task, \"2009/P1D\", 0);\n      SegmentDescriptor desc3 = SD(task, \"2010/P1D\", 0);\n      SegmentDescriptor desc4 = SD(task, \"2011/P1D\", 0);\n      SegmentDescriptor desc5 = SD(task, \"2011/P1D\", 1);\n      SegmentDescriptor desc6 = SD(task, \"2012/P1D\", 0);\n      SegmentDescriptor desc7 = SD(task, \"2013/P1D\", 0);\n      Assert.assertEquals(ImmutableSet.of(desc1, desc2, desc3, desc4, desc5, desc6, desc7), publishedDescriptors());\n      Assert.assertEquals(\n          new KafkaDataSourceMetadata(new SeekableStreamPartitions<>(topic, ImmutableMap.of(0, 10L, 1, 2L))),\n          metadataStorageCoordinator.getDataSourceMetadata(DATA_SCHEMA.getDataSource())\n      );\n\n      \r\n      Assert.assertEquals(ImmutableList.of(\"a\"), readSegmentColumn(\"dim1\", desc1));\n      Assert.assertEquals(ImmutableList.of(\"b\"), readSegmentColumn(\"dim1\", desc2));\n      Assert.assertEquals(ImmutableList.of(\"c\"), readSegmentColumn(\"dim1\", desc3));\n      Assert.assertTrue((ImmutableList.of(\"d\", \"e\").equals(readSegmentColumn(\"dim1\", desc4))\n                         && ImmutableList.of(\"h\").equals(readSegmentColumn(\"dim1\", desc5))) ||\n                        (ImmutableList.of(\"d\", \"h\").equals(readSegmentColumn(\"dim1\", desc4))\n                         && ImmutableList.of(\"e\").equals(readSegmentColumn(\"dim1\", desc5))));\n      Assert.assertEquals(ImmutableList.of(\"g\"), readSegmentColumn(\"dim1\", desc6));\n      Assert.assertEquals(ImmutableList.of(\"f\"), readSegmentColumn(\"dim1\", desc7));\n    }\n  }\n","realPath":"extensions-core/kafka-indexing-service/src/test/java/org/apache/druid/indexing/kafka/KafkaIndexTaskTest.java","repoName":"druid","snippetEndLine":0,"snippetStartLine":0,"startLine":600,"status":"M"},{"authorDate":"2019-02-19 03:50:08","commitOrder":6,"curCode":"  public void testTimeBasedIncrementalHandOff() throws Exception\n  {\n    if (!isIncrementalHandoffSupported) {\n      return;\n    }\n    final String baseSequenceName = \"sequence0\";\n    \r\n    maxRowsPerSegment = Integer.MAX_VALUE;\n    intermediateHandoffPeriod = new Period().withSeconds(0);\n\n    \r\n    insertData();\n    Map<String, Object> consumerProps = kafkaServer.consumerProperties();\n    consumerProps.put(\"max.poll.records\", \"1\");\n\n    final SeekableStreamPartitions<Integer, Long> startPartitions = new SeekableStreamPartitions<>(\n        topic,\n        ImmutableMap.of(\n            0,\n            0L,\n            1,\n            0L\n        )\n    );\n    \r\n    final SeekableStreamPartitions<Integer, Long> checkpoint = new SeekableStreamPartitions<>(\n        topic,\n        ImmutableMap.of(\n            0,\n            1L,\n            1,\n            0L\n        )\n    );\n    final SeekableStreamPartitions<Integer, Long> endPartitions = new SeekableStreamPartitions<>(\n        topic,\n        ImmutableMap.of(\n            0,\n            2L,\n            1,\n            0L\n        )\n    );\n    final KafkaIndexTask task = createTask(\n        null,\n        new KafkaIndexTaskIOConfig(\n            0,\n            baseSequenceName,\n            startPartitions,\n            endPartitions,\n            consumerProps,\n            KafkaSupervisorIOConfig.DEFAULT_POLL_TIMEOUT_MILLIS,\n            true,\n            null,\n            null\n        )\n    );\n    final ListenableFuture<TaskStatus> future = runTask(task);\n\n    \r\n    while (task.getRunner().getStatus() != Status.PAUSED) {\n      Thread.sleep(10);\n    }\n    final Map<Integer, Long> currentOffsets = ImmutableMap.copyOf(task.getRunner().getCurrentOffsets());\n    Assert.assertTrue(checkpoint.getPartitionSequenceNumberMap().equals(currentOffsets));\n    task.getRunner().setEndOffsets(currentOffsets, false);\n    Assert.assertEquals(TaskState.SUCCESS, future.get().getStatusCode());\n\n    Assert.assertEquals(1, checkpointRequestsHash.size());\n    Assert.assertTrue(\n        checkpointRequestsHash.contains(\n            Objects.hash(\n                DATA_SCHEMA.getDataSource(),\n                0,\n                new KafkaDataSourceMetadata(startPartitions),\n                new KafkaDataSourceMetadata(new SeekableStreamPartitions<>(\n                    topic,\n                    checkpoint.getPartitionSequenceNumberMap()\n                ))\n            )\n        )\n    );\n\n    \r\n    Assert.assertEquals(2, task.getRunner().getRowIngestionMeters().getProcessed());\n    Assert.assertEquals(0, task.getRunner().getRowIngestionMeters().getUnparseable());\n    Assert.assertEquals(0, task.getRunner().getRowIngestionMeters().getThrownAway());\n\n    \r\n    SegmentDescriptor desc1 = SD(task, \"2008/P1D\", 0);\n    SegmentDescriptor desc2 = SD(task, \"2009/P1D\", 0);\n    Assert.assertEquals(ImmutableSet.of(desc1, desc2), publishedDescriptors());\n    Assert.assertEquals(\n        new KafkaDataSourceMetadata(new SeekableStreamPartitions<>(topic, ImmutableMap.of(0, 2L, 1, 0L))),\n        metadataStorageCoordinator.getDataSourceMetadata(DATA_SCHEMA.getDataSource())\n    );\n\n    \r\n    Assert.assertEquals(ImmutableList.of(\"a\"), readSegmentColumn(\"dim1\", desc1));\n    Assert.assertEquals(ImmutableList.of(\"b\"), readSegmentColumn(\"dim1\", desc2));\n  }\n","date":"2019-02-19 03:50:08","endLine":859,"groupId":"6241","id":10,"instanceNumber":2,"isCurCommit":0,"methodName":"testTimeBasedIncrementalHandOff","params":"()","path":"/mnt/clonedata/CloneManagementServer/ManagementServer/consistResult/result-druid-10-0.7/blobInfo/CC_OUT/blobs/3d/6308cb0a96b01947db7e17b51cab010dd61129.src","preCode":"  public void testTimeBasedIncrementalHandOff() throws Exception\n  {\n    if (!isIncrementalHandoffSupported) {\n      return;\n    }\n    final String baseSequenceName = \"sequence0\";\n    \r\n    maxRowsPerSegment = Integer.MAX_VALUE;\n    intermediateHandoffPeriod = new Period().withSeconds(0);\n\n    \r\n    try (final KafkaProducer<byte[], byte[]> kafkaProducer = kafkaServer.newProducer()) {\n      for (ProducerRecord<byte[], byte[]> record : records.subList(0, 2)) {\n        kafkaProducer.send(record).get();\n      }\n    }\n    Map<String, Object> consumerProps = kafkaServer.consumerProperties();\n    consumerProps.put(\"max.poll.records\", \"1\");\n\n    final SeekableStreamPartitions<Integer, Long> startPartitions = new SeekableStreamPartitions<>(\n        topic,\n        ImmutableMap.of(\n            0,\n            0L,\n            1,\n            0L\n        )\n    );\n    \r\n    final SeekableStreamPartitions<Integer, Long> checkpoint = new SeekableStreamPartitions<>(\n        topic,\n        ImmutableMap.of(\n            0,\n            1L,\n            1,\n            0L\n        )\n    );\n    final SeekableStreamPartitions<Integer, Long> endPartitions = new SeekableStreamPartitions<>(\n        topic,\n        ImmutableMap.of(\n            0,\n            2L,\n            1,\n            0L\n        )\n    );\n    final KafkaIndexTask task = createTask(\n        null,\n        new KafkaIndexTaskIOConfig(\n            0,\n            baseSequenceName,\n            startPartitions,\n            endPartitions,\n            consumerProps,\n            KafkaSupervisorIOConfig.DEFAULT_POLL_TIMEOUT_MILLIS,\n            true,\n            null,\n            null,\n            false\n        )\n    );\n    final ListenableFuture<TaskStatus> future = runTask(task);\n\n    \r\n    while (task.getRunner().getStatus() != Status.PAUSED) {\n      Thread.sleep(10);\n    }\n    final Map<Integer, Long> currentOffsets = ImmutableMap.copyOf(task.getRunner().getCurrentOffsets());\n    Assert.assertTrue(checkpoint.getPartitionSequenceNumberMap().equals(currentOffsets));\n    task.getRunner().setEndOffsets(currentOffsets, false);\n    Assert.assertEquals(TaskState.SUCCESS, future.get().getStatusCode());\n\n    Assert.assertEquals(1, checkpointRequestsHash.size());\n    Assert.assertTrue(\n        checkpointRequestsHash.contains(\n            Objects.hash(\n                DATA_SCHEMA.getDataSource(),\n                0,\n                new KafkaDataSourceMetadata(startPartitions),\n                new KafkaDataSourceMetadata(new SeekableStreamPartitions<>(\n                    topic,\n                    checkpoint.getPartitionSequenceNumberMap()\n                ))\n            )\n        )\n    );\n\n    \r\n    Assert.assertEquals(2, task.getRunner().getRowIngestionMeters().getProcessed());\n    Assert.assertEquals(0, task.getRunner().getRowIngestionMeters().getUnparseable());\n    Assert.assertEquals(0, task.getRunner().getRowIngestionMeters().getThrownAway());\n\n    \r\n    SegmentDescriptor desc1 = SD(task, \"2008/P1D\", 0);\n    SegmentDescriptor desc2 = SD(task, \"2009/P1D\", 0);\n    Assert.assertEquals(ImmutableSet.of(desc1, desc2), publishedDescriptors());\n    Assert.assertEquals(\n        new KafkaDataSourceMetadata(new SeekableStreamPartitions<>(topic, ImmutableMap.of(0, 2L, 1, 0L))),\n        metadataStorageCoordinator.getDataSourceMetadata(DATA_SCHEMA.getDataSource())\n    );\n\n    \r\n    Assert.assertEquals(ImmutableList.of(\"a\"), readSegmentColumn(\"dim1\", desc1));\n    Assert.assertEquals(ImmutableList.of(\"b\"), readSegmentColumn(\"dim1\", desc2));\n  }\n","realPath":"extensions-core/kafka-indexing-service/src/test/java/org/apache/druid/indexing/kafka/KafkaIndexTaskTest.java","repoName":"druid","snippetEndLine":0,"snippetStartLine":0,"startLine":759,"status":"M"}],"commitId":"80a2ef7be46c4fc807ac2a74507b8ba8b6a44049","commitMessage":"@@@Support kafka transactional topics (#5404) (#6496)\n\n* Support kafka transactional topics\n\n* update kafka to version 2.0.0\n* Remove the skipOffsetGaps option since it's not used anymore\n* Adjust kafka consumer to use transactional semantics\n* Update tests\n\n* Remove unused import from test\n\n* Fix compilation\n\n* Invoke transaction api to fix a unit test\n\n* temporary modification of travis.yml for debugging\n\n* another attempt to get travis tasklogs\n\n* update kafka to 2.0.1 at all places\n\n* Remove druid-kafka-eight dependency from integration-tests.  remove the kafka firehose test and deprecate kafka-eight classes\n\n* Add deprecated in docs for kafka-eight and kafka-simple extensions\n\n* Remove skipOffsetGaps and code changes for transaction support\n\n* Fix indentation\n\n* remove skipOffsetGaps from kinesis\n\n* Add transaction api to KafkaRecordSupplierTest\n\n* Fix indent\n\n* Fix test\n\n* update kafka version to 2.1.0\n","date":"2019-02-19 03:50:08","modifiedFileCount":"25","status":"M","submitter":"Surekha"},{"authorTime":"2019-02-24 12:10:31","codes":[{"authorDate":"2019-02-24 12:10:31","commitOrder":7,"curCode":"  public void testIncrementalHandOffMaxTotalRows() throws Exception\n  {\n    if (!isIncrementalHandoffSupported) {\n      return;\n    }\n    final String baseSequenceName = \"sequence0\";\n    \r\n    maxRowsPerSegment = Integer.MAX_VALUE;\n    maxTotalRows = 3L;\n\n    \r\n    int numToAdd = records.size() - 2;\n\n    try (final KafkaProducer<byte[], byte[]> kafkaProducer = kafkaServer.newProducer()) {\n      kafkaProducer.initTransactions();\n      kafkaProducer.beginTransaction();\n      for (int i = 0; i < numToAdd; i++) {\n        kafkaProducer.send(records.get(i)).get();\n      }\n      kafkaProducer.commitTransaction();\n    }\n\n    Map<String, Object> consumerProps = kafkaServer.consumerProperties();\n    consumerProps.put(\"max.poll.records\", \"1\");\n\n    final SeekableStreamPartitions<Integer, Long> startPartitions = new SeekableStreamPartitions<>(\n        topic,\n        ImmutableMap.of(\n            0,\n            0L,\n            1,\n            0L\n        )\n    );\n    final SeekableStreamPartitions<Integer, Long> checkpoint1 = new SeekableStreamPartitions<>(\n        topic,\n        ImmutableMap.of(\n            0,\n            3L,\n            1,\n            0L\n        )\n    );\n    final SeekableStreamPartitions<Integer, Long> checkpoint2 = new SeekableStreamPartitions<>(\n        topic,\n        ImmutableMap.of(\n            0,\n            10L,\n            1,\n            0L\n        )\n    );\n\n    final SeekableStreamPartitions<Integer, Long> endPartitions = new SeekableStreamPartitions<>(\n        topic,\n        ImmutableMap.of(\n            0,\n            10L,\n            1,\n            2L\n        )\n    );\n    final KafkaIndexTask task = createTask(\n        null,\n        new KafkaIndexTaskIOConfig(\n            0,\n            baseSequenceName,\n            startPartitions,\n            endPartitions,\n            consumerProps,\n            KafkaSupervisorIOConfig.DEFAULT_POLL_TIMEOUT_MILLIS,\n            true,\n            null,\n            null\n        )\n    );\n    final ListenableFuture<TaskStatus> future = runTask(task);\n    while (task.getRunner().getStatus() != Status.PAUSED) {\n      Thread.sleep(10);\n    }\n    final Map<Integer, Long> currentOffsets = ImmutableMap.copyOf(task.getRunner().getCurrentOffsets());\n\n    Assert.assertTrue(checkpoint1.getPartitionSequenceNumberMap().equals(currentOffsets));\n    task.getRunner().setEndOffsets(currentOffsets, false);\n\n    while (task.getRunner().getStatus() != Status.PAUSED) {\n      Thread.sleep(10);\n    }\n\n    \r\n    try (final KafkaProducer<byte[], byte[]> kafkaProducer = kafkaServer.newProducer()) {\n      kafkaProducer.initTransactions();\n      kafkaProducer.beginTransaction();\n      for (int i = numToAdd; i < records.size(); i++) {\n        kafkaProducer.send(records.get(i)).get();\n      }\n      kafkaProducer.commitTransaction();\n    }\n    final Map<Integer, Long> nextOffsets = ImmutableMap.copyOf(task.getRunner().getCurrentOffsets());\n\n   \n    Assert.assertTrue(checkpoint2.getPartitionSequenceNumberMap().equals(nextOffsets));\n    task.getRunner().setEndOffsets(nextOffsets, false);\n\n    Assert.assertEquals(TaskState.SUCCESS, future.get().getStatusCode());\n\n    Assert.assertEquals(2, checkpointRequestsHash.size());\n    Assert.assertTrue(\n        checkpointRequestsHash.contains(\n            Objects.hash(\n                DATA_SCHEMA.getDataSource(),\n                0,\n                new KafkaDataSourceMetadata(startPartitions),\n                new KafkaDataSourceMetadata(new SeekableStreamPartitions<>(topic, currentOffsets))\n            )\n        )\n    );\n    Assert.assertTrue(\n        checkpointRequestsHash.contains(\n            Objects.hash(\n                DATA_SCHEMA.getDataSource(),\n                0,\n                new KafkaDataSourceMetadata(new SeekableStreamPartitions(topic, currentOffsets)),\n                new KafkaDataSourceMetadata(new SeekableStreamPartitions(topic, nextOffsets))\n            )\n        )\n    );\n\n    \r\n    Assert.assertEquals(8, task.getRunner().getRowIngestionMeters().getProcessed());\n    Assert.assertEquals(3, task.getRunner().getRowIngestionMeters().getUnparseable());\n    Assert.assertEquals(1, task.getRunner().getRowIngestionMeters().getThrownAway());\n    \n    \r\n    SegmentDescriptor desc1 = sd(task, \"2008/P1D\", 0);\n    SegmentDescriptor desc2 = sd(task, \"2009/P1D\", 0);\n    SegmentDescriptor desc3 = sd(task, \"2010/P1D\", 0);\n    SegmentDescriptor desc4 = sd(task, \"2011/P1D\", 0);\n    SegmentDescriptor desc5 = sd(task, \"2011/P1D\", 1);\n    SegmentDescriptor desc6 = sd(task, \"2012/P1D\", 0);\n    SegmentDescriptor desc7 = sd(task, \"2013/P1D\", 0);\n    Assert.assertEquals(ImmutableSet.of(desc1, desc2, desc3, desc4, desc5, desc6, desc7), publishedDescriptors());\n    Assert.assertEquals(\n          new KafkaDataSourceMetadata(new SeekableStreamPartitions<>(topic, ImmutableMap.of(0, 10L, 1, 2L))),\n          metadataStorageCoordinator.getDataSourceMetadata(DATA_SCHEMA.getDataSource()));\n    \n    Assert.assertEquals(ImmutableSet.of(desc1, desc2, desc3, desc4, desc5, desc6, desc7), publishedDescriptors());\n    Assert.assertEquals(\n        new KafkaDataSourceMetadata(new SeekableStreamPartitions<>(topic, ImmutableMap.of(0, 10L, 1, 2L))),\n        metadataStorageCoordinator.getDataSourceMetadata(DATA_SCHEMA.getDataSource())\n    );\n\n    \r\n    Assert.assertEquals(ImmutableList.of(\"a\"), readSegmentColumn(\"dim1\", desc1));\n    Assert.assertEquals(ImmutableList.of(\"b\"), readSegmentColumn(\"dim1\", desc2));\n    Assert.assertEquals(ImmutableList.of(\"c\"), readSegmentColumn(\"dim1\", desc3));\n    Assert.assertTrue((ImmutableList.of(\"d\", \"e\").equals(readSegmentColumn(\"dim1\", desc4))\n                       && ImmutableList.of(\"h\").equals(readSegmentColumn(\"dim1\", desc5))) ||\n                      (ImmutableList.of(\"d\", \"h\").equals(readSegmentColumn(\"dim1\", desc4))\n                       && ImmutableList.of(\"e\").equals(readSegmentColumn(\"dim1\", desc5))));\n    Assert.assertEquals(ImmutableList.of(\"g\"), readSegmentColumn(\"dim1\", desc6));\n    Assert.assertEquals(ImmutableList.of(\"f\"), readSegmentColumn(\"dim1\", desc7));\n  }\n","date":"2019-02-24 12:10:31","endLine":761,"groupId":"3593","id":11,"instanceNumber":1,"isCurCommit":0,"methodName":"testIncrementalHandOffMaxTotalRows","params":"()","path":"/mnt/clonedata/CloneManagementServer/ManagementServer/consistResult/result-druid-10-0.7/blobInfo/CC_OUT/blobs/b1/9bf8ad3716e1bda5912843b5946146e1e703d6.src","preCode":"  public void testIncrementalHandOffMaxTotalRows() throws Exception\n  {\n    if (!isIncrementalHandoffSupported) {\n      return;\n    }\n    final String baseSequenceName = \"sequence0\";\n    \r\n    maxRowsPerSegment = Integer.MAX_VALUE;\n    maxTotalRows = 3L;\n\n    \r\n    int numToAdd = records.size() - 2;\n\n    try (final KafkaProducer<byte[], byte[]> kafkaProducer = kafkaServer.newProducer()) {\n      kafkaProducer.initTransactions();\n      kafkaProducer.beginTransaction();\n      for (int i = 0; i < numToAdd; i++) {\n        kafkaProducer.send(records.get(i)).get();\n      }\n      kafkaProducer.commitTransaction();\n    }\n\n    Map<String, Object> consumerProps = kafkaServer.consumerProperties();\n    consumerProps.put(\"max.poll.records\", \"1\");\n\n    final SeekableStreamPartitions<Integer, Long> startPartitions = new SeekableStreamPartitions<>(\n        topic,\n        ImmutableMap.of(\n            0,\n            0L,\n            1,\n            0L\n        )\n    );\n    final SeekableStreamPartitions<Integer, Long> checkpoint1 = new SeekableStreamPartitions<>(\n        topic,\n        ImmutableMap.of(\n            0,\n            3L,\n            1,\n            0L\n        )\n    );\n    final SeekableStreamPartitions<Integer, Long> checkpoint2 = new SeekableStreamPartitions<>(\n        topic,\n        ImmutableMap.of(\n            0,\n            10L,\n            1,\n            0L\n        )\n    );\n\n    final SeekableStreamPartitions<Integer, Long> endPartitions = new SeekableStreamPartitions<>(\n        topic,\n        ImmutableMap.of(\n            0,\n            10L,\n            1,\n            2L\n        )\n    );\n    final KafkaIndexTask task = createTask(\n        null,\n        new KafkaIndexTaskIOConfig(\n            0,\n            baseSequenceName,\n            startPartitions,\n            endPartitions,\n            consumerProps,\n            KafkaSupervisorIOConfig.DEFAULT_POLL_TIMEOUT_MILLIS,\n            true,\n            null,\n            null\n        )\n    );\n    final ListenableFuture<TaskStatus> future = runTask(task);\n    while (task.getRunner().getStatus() != Status.PAUSED) {\n      Thread.sleep(10);\n    }\n    final Map<Integer, Long> currentOffsets = ImmutableMap.copyOf(task.getRunner().getCurrentOffsets());\n\n    Assert.assertTrue(checkpoint1.getPartitionSequenceNumberMap().equals(currentOffsets));\n    task.getRunner().setEndOffsets(currentOffsets, false);\n\n    while (task.getRunner().getStatus() != Status.PAUSED) {\n      Thread.sleep(10);\n    }\n\n    \r\n    try (final KafkaProducer<byte[], byte[]> kafkaProducer = kafkaServer.newProducer()) {\n      kafkaProducer.initTransactions();\n      kafkaProducer.beginTransaction();\n      for (int i = numToAdd; i < records.size(); i++) {\n        kafkaProducer.send(records.get(i)).get();\n      }\n      kafkaProducer.commitTransaction();\n    }\n    final Map<Integer, Long> nextOffsets = ImmutableMap.copyOf(task.getRunner().getCurrentOffsets());\n\n    Assert.assertTrue(checkpoint2.getPartitionSequenceNumberMap().equals(nextOffsets));\n    task.getRunner().setEndOffsets(nextOffsets, false);\n\n    Assert.assertEquals(TaskState.SUCCESS, future.get().getStatusCode());\n\n    Assert.assertEquals(2, checkpointRequestsHash.size());\n    Assert.assertTrue(\n        checkpointRequestsHash.contains(\n            Objects.hash(\n                DATA_SCHEMA.getDataSource(),\n                0,\n                new KafkaDataSourceMetadata(startPartitions),\n                new KafkaDataSourceMetadata(new SeekableStreamPartitions<>(topic, currentOffsets))\n            )\n        )\n    );\n    Assert.assertTrue(\n        checkpointRequestsHash.contains(\n            Objects.hash(\n                DATA_SCHEMA.getDataSource(),\n                0,\n                new KafkaDataSourceMetadata(new SeekableStreamPartitions(topic, currentOffsets)),\n                new KafkaDataSourceMetadata(new SeekableStreamPartitions(topic, nextOffsets))\n            )\n        )\n    );\n\n    \r\n    Assert.assertEquals(8, task.getRunner().getRowIngestionMeters().getProcessed());\n    Assert.assertEquals(3, task.getRunner().getRowIngestionMeters().getUnparseable());\n    Assert.assertEquals(1, task.getRunner().getRowIngestionMeters().getThrownAway());\n\n    \r\n    SegmentDescriptor desc1 = SD(task, \"2008/P1D\", 0);\n    SegmentDescriptor desc2 = SD(task, \"2009/P1D\", 0);\n    SegmentDescriptor desc3 = SD(task, \"2010/P1D\", 0);\n    SegmentDescriptor desc4 = SD(task, \"2011/P1D\", 0);\n    SegmentDescriptor desc5 = SD(task, \"2011/P1D\", 1);\n    SegmentDescriptor desc6 = SD(task, \"2012/P1D\", 0);\n    SegmentDescriptor desc7 = SD(task, \"2013/P1D\", 0);\n    Assert.assertEquals(ImmutableSet.of(desc1, desc2, desc3, desc4, desc5, desc6, desc7), publishedDescriptors());\n    Assert.assertEquals(\n        new KafkaDataSourceMetadata(new SeekableStreamPartitions<>(topic, ImmutableMap.of(0, 10L, 1, 2L))),\n        metadataStorageCoordinator.getDataSourceMetadata(DATA_SCHEMA.getDataSource())\n    );\n\n    \r\n    Assert.assertEquals(ImmutableList.of(\"a\"), readSegmentColumn(\"dim1\", desc1));\n    Assert.assertEquals(ImmutableList.of(\"b\"), readSegmentColumn(\"dim1\", desc2));\n    Assert.assertEquals(ImmutableList.of(\"c\"), readSegmentColumn(\"dim1\", desc3));\n    Assert.assertTrue((ImmutableList.of(\"d\", \"e\").equals(readSegmentColumn(\"dim1\", desc4))\n                       && ImmutableList.of(\"h\").equals(readSegmentColumn(\"dim1\", desc5))) ||\n                      (ImmutableList.of(\"d\", \"h\").equals(readSegmentColumn(\"dim1\", desc4))\n                       && ImmutableList.of(\"e\").equals(readSegmentColumn(\"dim1\", desc5))));\n    Assert.assertEquals(ImmutableList.of(\"g\"), readSegmentColumn(\"dim1\", desc6));\n    Assert.assertEquals(ImmutableList.of(\"f\"), readSegmentColumn(\"dim1\", desc7));\n  }\n","realPath":"extensions-core/kafka-indexing-service/src/test/java/org/apache/druid/indexing/kafka/KafkaIndexTaskTest.java","repoName":"druid","snippetEndLine":0,"snippetStartLine":0,"startLine":599,"status":"M"},{"authorDate":"2019-02-24 12:10:31","commitOrder":7,"curCode":"  public void testTimeBasedIncrementalHandOff() throws Exception\n  {\n    if (!isIncrementalHandoffSupported) {\n      return;\n    }\n    final String baseSequenceName = \"sequence0\";\n    \r\n    maxRowsPerSegment = Integer.MAX_VALUE;\n    intermediateHandoffPeriod = new Period().withSeconds(0);\n\n    \r\n    insertData();\n    Map<String, Object> consumerProps = kafkaServer.consumerProperties();\n    consumerProps.put(\"max.poll.records\", \"1\");\n\n    final SeekableStreamPartitions<Integer, Long> startPartitions = new SeekableStreamPartitions<>(\n        topic,\n        ImmutableMap.of(\n            0,\n            0L,\n            1,\n            0L\n        )\n    );\n    \r\n    final SeekableStreamPartitions<Integer, Long> checkpoint = new SeekableStreamPartitions<>(\n        topic,\n        ImmutableMap.of(\n            0,\n            1L,\n            1,\n            0L\n        )\n    );\n    final SeekableStreamPartitions<Integer, Long> endPartitions = new SeekableStreamPartitions<>(\n        topic,\n        ImmutableMap.of(\n            0,\n            2L,\n            1,\n            0L\n        )\n    );\n    final KafkaIndexTask task = createTask(\n        null,\n        new KafkaIndexTaskIOConfig(\n            0,\n            baseSequenceName,\n            startPartitions,\n            endPartitions,\n            consumerProps,\n            KafkaSupervisorIOConfig.DEFAULT_POLL_TIMEOUT_MILLIS,\n            true,\n            null,\n            null\n        )\n    );\n    final ListenableFuture<TaskStatus> future = runTask(task);\n\n    \r\n    while (task.getRunner().getStatus() != Status.PAUSED) {\n      Thread.sleep(10);\n    }\n    final Map<Integer, Long> currentOffsets = ImmutableMap.copyOf(task.getRunner().getCurrentOffsets());\n    Assert.assertTrue(checkpoint.getPartitionSequenceNumberMap().equals(currentOffsets));\n    task.getRunner().setEndOffsets(currentOffsets, false);\n    Assert.assertEquals(TaskState.SUCCESS, future.get().getStatusCode());\n\n    Assert.assertEquals(1, checkpointRequestsHash.size());\n    Assert.assertTrue(\n        checkpointRequestsHash.contains(\n            Objects.hash(\n                DATA_SCHEMA.getDataSource(),\n                0,\n                new KafkaDataSourceMetadata(startPartitions),\n                new KafkaDataSourceMetadata(new SeekableStreamPartitions<>(\n                    topic,\n                    checkpoint.getPartitionSequenceNumberMap()\n                ))\n            )\n        )\n    );\n\n    \r\n    Assert.assertEquals(2, task.getRunner().getRowIngestionMeters().getProcessed());\n    Assert.assertEquals(0, task.getRunner().getRowIngestionMeters().getUnparseable());\n    Assert.assertEquals(0, task.getRunner().getRowIngestionMeters().getThrownAway());\n\n    \r\n    SegmentDescriptor desc1 = sd(task, \"2008/P1D\", 0);\n    SegmentDescriptor desc2 = sd(task, \"2009/P1D\", 0);\n    Assert.assertEquals(ImmutableSet.of(desc1, desc2), publishedDescriptors());\n    Assert.assertEquals(\n        new KafkaDataSourceMetadata(new SeekableStreamPartitions<>(topic, ImmutableMap.of(0, 2L, 1, 0L))),\n        metadataStorageCoordinator.getDataSourceMetadata(DATA_SCHEMA.getDataSource())\n    );\n\n    \r\n    Assert.assertEquals(ImmutableList.of(\"a\"), readSegmentColumn(\"dim1\", desc1));\n    Assert.assertEquals(ImmutableList.of(\"b\"), readSegmentColumn(\"dim1\", desc2));\n  }\n","date":"2019-02-24 12:10:31","endLine":864,"groupId":"6241","id":12,"instanceNumber":2,"isCurCommit":0,"methodName":"testTimeBasedIncrementalHandOff","params":"()","path":"/mnt/clonedata/CloneManagementServer/ManagementServer/consistResult/result-druid-10-0.7/blobInfo/CC_OUT/blobs/b1/9bf8ad3716e1bda5912843b5946146e1e703d6.src","preCode":"  public void testTimeBasedIncrementalHandOff() throws Exception\n  {\n    if (!isIncrementalHandoffSupported) {\n      return;\n    }\n    final String baseSequenceName = \"sequence0\";\n    \r\n    maxRowsPerSegment = Integer.MAX_VALUE;\n    intermediateHandoffPeriod = new Period().withSeconds(0);\n\n    \r\n    insertData();\n    Map<String, Object> consumerProps = kafkaServer.consumerProperties();\n    consumerProps.put(\"max.poll.records\", \"1\");\n\n    final SeekableStreamPartitions<Integer, Long> startPartitions = new SeekableStreamPartitions<>(\n        topic,\n        ImmutableMap.of(\n            0,\n            0L,\n            1,\n            0L\n        )\n    );\n    \r\n    final SeekableStreamPartitions<Integer, Long> checkpoint = new SeekableStreamPartitions<>(\n        topic,\n        ImmutableMap.of(\n            0,\n            1L,\n            1,\n            0L\n        )\n    );\n    final SeekableStreamPartitions<Integer, Long> endPartitions = new SeekableStreamPartitions<>(\n        topic,\n        ImmutableMap.of(\n            0,\n            2L,\n            1,\n            0L\n        )\n    );\n    final KafkaIndexTask task = createTask(\n        null,\n        new KafkaIndexTaskIOConfig(\n            0,\n            baseSequenceName,\n            startPartitions,\n            endPartitions,\n            consumerProps,\n            KafkaSupervisorIOConfig.DEFAULT_POLL_TIMEOUT_MILLIS,\n            true,\n            null,\n            null\n        )\n    );\n    final ListenableFuture<TaskStatus> future = runTask(task);\n\n    \r\n    while (task.getRunner().getStatus() != Status.PAUSED) {\n      Thread.sleep(10);\n    }\n    final Map<Integer, Long> currentOffsets = ImmutableMap.copyOf(task.getRunner().getCurrentOffsets());\n    Assert.assertTrue(checkpoint.getPartitionSequenceNumberMap().equals(currentOffsets));\n    task.getRunner().setEndOffsets(currentOffsets, false);\n    Assert.assertEquals(TaskState.SUCCESS, future.get().getStatusCode());\n\n    Assert.assertEquals(1, checkpointRequestsHash.size());\n    Assert.assertTrue(\n        checkpointRequestsHash.contains(\n            Objects.hash(\n                DATA_SCHEMA.getDataSource(),\n                0,\n                new KafkaDataSourceMetadata(startPartitions),\n                new KafkaDataSourceMetadata(new SeekableStreamPartitions<>(\n                    topic,\n                    checkpoint.getPartitionSequenceNumberMap()\n                ))\n            )\n        )\n    );\n\n    \r\n    Assert.assertEquals(2, task.getRunner().getRowIngestionMeters().getProcessed());\n    Assert.assertEquals(0, task.getRunner().getRowIngestionMeters().getUnparseable());\n    Assert.assertEquals(0, task.getRunner().getRowIngestionMeters().getThrownAway());\n\n    \r\n    SegmentDescriptor desc1 = SD(task, \"2008/P1D\", 0);\n    SegmentDescriptor desc2 = SD(task, \"2009/P1D\", 0);\n    Assert.assertEquals(ImmutableSet.of(desc1, desc2), publishedDescriptors());\n    Assert.assertEquals(\n        new KafkaDataSourceMetadata(new SeekableStreamPartitions<>(topic, ImmutableMap.of(0, 2L, 1, 0L))),\n        metadataStorageCoordinator.getDataSourceMetadata(DATA_SCHEMA.getDataSource())\n    );\n\n    \r\n    Assert.assertEquals(ImmutableList.of(\"a\"), readSegmentColumn(\"dim1\", desc1));\n    Assert.assertEquals(ImmutableList.of(\"b\"), readSegmentColumn(\"dim1\", desc2));\n  }\n","realPath":"extensions-core/kafka-indexing-service/src/test/java/org/apache/druid/indexing/kafka/KafkaIndexTaskTest.java","repoName":"druid","snippetEndLine":0,"snippetStartLine":0,"startLine":764,"status":"M"}],"commitId":"8b803cbc22b15799fd0526de8d6d0eea155ad733","commitMessage":"@@@Added checkstyle for \"Methods starting with Capital Letters\"  (#7118)\n\n* Added checkstyle for \"Methods starting with Capital Letters\" and changed the method names violating this.\n\n* Un-abbreviate the method names in the calcite tests\n\n* Fixed checkstyle errors\n\n* Changed asserts position in the code\n","date":"2019-02-24 12:10:31","modifiedFileCount":"24","status":"M","submitter":"Himanshu Pandey"},{"authorTime":"2019-03-22 04:12:22","codes":[{"authorDate":"2019-03-22 04:12:22","commitOrder":8,"curCode":"  public void testIncrementalHandOffMaxTotalRows() throws Exception\n  {\n    if (!isIncrementalHandoffSupported) {\n      return;\n    }\n    final String baseSequenceName = \"sequence0\";\n    \r\n    maxRowsPerSegment = Integer.MAX_VALUE;\n    maxTotalRows = 3L;\n\n    \r\n    int numToAdd = records.size() - 2;\n\n    try (final KafkaProducer<byte[], byte[]> kafkaProducer = kafkaServer.newProducer()) {\n      kafkaProducer.initTransactions();\n      kafkaProducer.beginTransaction();\n      for (int i = 0; i < numToAdd; i++) {\n        kafkaProducer.send(records.get(i)).get();\n      }\n      kafkaProducer.commitTransaction();\n    }\n\n    Map<String, Object> consumerProps = kafkaServer.consumerProperties();\n    consumerProps.put(\"max.poll.records\", \"1\");\n\n    final SeekableStreamStartSequenceNumbers<Integer, Long> startPartitions = new SeekableStreamStartSequenceNumbers<>(\n        topic,\n        ImmutableMap.of(0, 0L, 1, 0L),\n        ImmutableSet.of()\n    );\n    final SeekableStreamEndSequenceNumbers<Integer, Long> checkpoint1 = new SeekableStreamEndSequenceNumbers<>(\n        topic,\n        ImmutableMap.of(0, 3L, 1, 0L)\n    );\n    final SeekableStreamEndSequenceNumbers<Integer, Long> checkpoint2 = new SeekableStreamEndSequenceNumbers<>(\n        topic,\n        ImmutableMap.of(0, 10L, 1, 0L)\n    );\n\n    final SeekableStreamEndSequenceNumbers<Integer, Long> endPartitions = new SeekableStreamEndSequenceNumbers<>(\n        topic,\n        ImmutableMap.of(0, 10L, 1, 2L)\n    );\n    final KafkaIndexTask task = createTask(\n        null,\n        new KafkaIndexTaskIOConfig(\n            0,\n            baseSequenceName,\n            startPartitions,\n            endPartitions,\n            consumerProps,\n            KafkaSupervisorIOConfig.DEFAULT_POLL_TIMEOUT_MILLIS,\n            true,\n            null,\n            null\n        )\n    );\n    final ListenableFuture<TaskStatus> future = runTask(task);\n    while (task.getRunner().getStatus() != Status.PAUSED) {\n      Thread.sleep(10);\n    }\n    final Map<Integer, Long> currentOffsets = ImmutableMap.copyOf(task.getRunner().getCurrentOffsets());\n\n    Assert.assertEquals(checkpoint1.getPartitionSequenceNumberMap(), currentOffsets);\n    task.getRunner().setEndOffsets(currentOffsets, false);\n\n    while (task.getRunner().getStatus() != Status.PAUSED) {\n      Thread.sleep(10);\n    }\n\n    \r\n    try (final KafkaProducer<byte[], byte[]> kafkaProducer = kafkaServer.newProducer()) {\n      kafkaProducer.initTransactions();\n      kafkaProducer.beginTransaction();\n      for (int i = numToAdd; i < records.size(); i++) {\n        kafkaProducer.send(records.get(i)).get();\n      }\n      kafkaProducer.commitTransaction();\n    }\n    final Map<Integer, Long> nextOffsets = ImmutableMap.copyOf(task.getRunner().getCurrentOffsets());\n\n\n    Assert.assertTrue(checkpoint2.getPartitionSequenceNumberMap().equals(nextOffsets));\n    task.getRunner().setEndOffsets(nextOffsets, false);\n\n    Assert.assertEquals(TaskState.SUCCESS, future.get().getStatusCode());\n\n    Assert.assertEquals(2, checkpointRequestsHash.size());\n    Assert.assertTrue(\n        checkpointRequestsHash.contains(\n            Objects.hash(\n                DATA_SCHEMA.getDataSource(),\n                0,\n                new KafkaDataSourceMetadata(startPartitions),\n                new KafkaDataSourceMetadata(\n                    new SeekableStreamEndSequenceNumbers<>(topic, currentOffsets)\n                )\n            )\n        )\n    );\n    Assert.assertTrue(\n        checkpointRequestsHash.contains(\n            Objects.hash(\n                DATA_SCHEMA.getDataSource(),\n                0,\n                new KafkaDataSourceMetadata(\n                    new SeekableStreamStartSequenceNumbers<>(topic, currentOffsets, ImmutableSet.of())\n                ),\n                new KafkaDataSourceMetadata(new SeekableStreamEndSequenceNumbers<>(topic, nextOffsets))\n            )\n        )\n    );\n\n    \r\n    Assert.assertEquals(8, task.getRunner().getRowIngestionMeters().getProcessed());\n    Assert.assertEquals(3, task.getRunner().getRowIngestionMeters().getUnparseable());\n    Assert.assertEquals(1, task.getRunner().getRowIngestionMeters().getThrownAway());\n\n    \r\n    SegmentDescriptor desc1 = sd(task, \"2008/P1D\", 0);\n    SegmentDescriptor desc2 = sd(task, \"2009/P1D\", 0);\n    SegmentDescriptor desc3 = sd(task, \"2010/P1D\", 0);\n    SegmentDescriptor desc4 = sd(task, \"2011/P1D\", 0);\n    SegmentDescriptor desc5 = sd(task, \"2011/P1D\", 1);\n    SegmentDescriptor desc6 = sd(task, \"2012/P1D\", 0);\n    SegmentDescriptor desc7 = sd(task, \"2013/P1D\", 0);\n    Assert.assertEquals(ImmutableSet.of(desc1, desc2, desc3, desc4, desc5, desc6, desc7), publishedDescriptors());\n    Assert.assertEquals(\n          new KafkaDataSourceMetadata(\n              new SeekableStreamEndSequenceNumbers<>(topic, ImmutableMap.of(0, 10L, 1, 2L))\n          ),\n          metadataStorageCoordinator.getDataSourceMetadata(DATA_SCHEMA.getDataSource()));\n\n    Assert.assertEquals(ImmutableSet.of(desc1, desc2, desc3, desc4, desc5, desc6, desc7), publishedDescriptors());\n    Assert.assertEquals(\n        new KafkaDataSourceMetadata(\n            new SeekableStreamEndSequenceNumbers<>(topic, ImmutableMap.of(0, 10L, 1, 2L))\n        ),\n        metadataStorageCoordinator.getDataSourceMetadata(DATA_SCHEMA.getDataSource())\n    );\n\n    \r\n    Assert.assertEquals(ImmutableList.of(\"a\"), readSegmentColumn(\"dim1\", desc1));\n    Assert.assertEquals(ImmutableList.of(\"b\"), readSegmentColumn(\"dim1\", desc2));\n    Assert.assertEquals(ImmutableList.of(\"c\"), readSegmentColumn(\"dim1\", desc3));\n    Assert.assertTrue((ImmutableList.of(\"d\", \"e\").equals(readSegmentColumn(\"dim1\", desc4))\n                       && ImmutableList.of(\"h\").equals(readSegmentColumn(\"dim1\", desc5))) ||\n                      (ImmutableList.of(\"d\", \"h\").equals(readSegmentColumn(\"dim1\", desc4))\n                       && ImmutableList.of(\"e\").equals(readSegmentColumn(\"dim1\", desc5))));\n    Assert.assertEquals(ImmutableList.of(\"g\"), readSegmentColumn(\"dim1\", desc6));\n    Assert.assertEquals(ImmutableList.of(\"f\"), readSegmentColumn(\"dim1\", desc7));\n  }\n","date":"2019-03-22 04:12:22","endLine":754,"groupId":"13977","id":13,"instanceNumber":1,"isCurCommit":0,"methodName":"testIncrementalHandOffMaxTotalRows","params":"()","path":"/mnt/clonedata/CloneManagementServer/ManagementServer/consistResult/result-druid-10-0.7/blobInfo/CC_OUT/blobs/eb/ef3fc4596a43b59efefcaf094105a8ae34ab5f.src","preCode":"  public void testIncrementalHandOffMaxTotalRows() throws Exception\n  {\n    if (!isIncrementalHandoffSupported) {\n      return;\n    }\n    final String baseSequenceName = \"sequence0\";\n    \r\n    maxRowsPerSegment = Integer.MAX_VALUE;\n    maxTotalRows = 3L;\n\n    \r\n    int numToAdd = records.size() - 2;\n\n    try (final KafkaProducer<byte[], byte[]> kafkaProducer = kafkaServer.newProducer()) {\n      kafkaProducer.initTransactions();\n      kafkaProducer.beginTransaction();\n      for (int i = 0; i < numToAdd; i++) {\n        kafkaProducer.send(records.get(i)).get();\n      }\n      kafkaProducer.commitTransaction();\n    }\n\n    Map<String, Object> consumerProps = kafkaServer.consumerProperties();\n    consumerProps.put(\"max.poll.records\", \"1\");\n\n    final SeekableStreamPartitions<Integer, Long> startPartitions = new SeekableStreamPartitions<>(\n        topic,\n        ImmutableMap.of(\n            0,\n            0L,\n            1,\n            0L\n        )\n    );\n    final SeekableStreamPartitions<Integer, Long> checkpoint1 = new SeekableStreamPartitions<>(\n        topic,\n        ImmutableMap.of(\n            0,\n            3L,\n            1,\n            0L\n        )\n    );\n    final SeekableStreamPartitions<Integer, Long> checkpoint2 = new SeekableStreamPartitions<>(\n        topic,\n        ImmutableMap.of(\n            0,\n            10L,\n            1,\n            0L\n        )\n    );\n\n    final SeekableStreamPartitions<Integer, Long> endPartitions = new SeekableStreamPartitions<>(\n        topic,\n        ImmutableMap.of(\n            0,\n            10L,\n            1,\n            2L\n        )\n    );\n    final KafkaIndexTask task = createTask(\n        null,\n        new KafkaIndexTaskIOConfig(\n            0,\n            baseSequenceName,\n            startPartitions,\n            endPartitions,\n            consumerProps,\n            KafkaSupervisorIOConfig.DEFAULT_POLL_TIMEOUT_MILLIS,\n            true,\n            null,\n            null\n        )\n    );\n    final ListenableFuture<TaskStatus> future = runTask(task);\n    while (task.getRunner().getStatus() != Status.PAUSED) {\n      Thread.sleep(10);\n    }\n    final Map<Integer, Long> currentOffsets = ImmutableMap.copyOf(task.getRunner().getCurrentOffsets());\n\n    Assert.assertTrue(checkpoint1.getPartitionSequenceNumberMap().equals(currentOffsets));\n    task.getRunner().setEndOffsets(currentOffsets, false);\n\n    while (task.getRunner().getStatus() != Status.PAUSED) {\n      Thread.sleep(10);\n    }\n\n    \r\n    try (final KafkaProducer<byte[], byte[]> kafkaProducer = kafkaServer.newProducer()) {\n      kafkaProducer.initTransactions();\n      kafkaProducer.beginTransaction();\n      for (int i = numToAdd; i < records.size(); i++) {\n        kafkaProducer.send(records.get(i)).get();\n      }\n      kafkaProducer.commitTransaction();\n    }\n    final Map<Integer, Long> nextOffsets = ImmutableMap.copyOf(task.getRunner().getCurrentOffsets());\n\n\n    Assert.assertTrue(checkpoint2.getPartitionSequenceNumberMap().equals(nextOffsets));\n    task.getRunner().setEndOffsets(nextOffsets, false);\n\n    Assert.assertEquals(TaskState.SUCCESS, future.get().getStatusCode());\n\n    Assert.assertEquals(2, checkpointRequestsHash.size());\n    Assert.assertTrue(\n        checkpointRequestsHash.contains(\n            Objects.hash(\n                DATA_SCHEMA.getDataSource(),\n                0,\n                new KafkaDataSourceMetadata(startPartitions),\n                new KafkaDataSourceMetadata(new SeekableStreamPartitions<>(topic, currentOffsets))\n            )\n        )\n    );\n    Assert.assertTrue(\n        checkpointRequestsHash.contains(\n            Objects.hash(\n                DATA_SCHEMA.getDataSource(),\n                0,\n                new KafkaDataSourceMetadata(new SeekableStreamPartitions(topic, currentOffsets)),\n                new KafkaDataSourceMetadata(new SeekableStreamPartitions(topic, nextOffsets))\n            )\n        )\n    );\n\n    \r\n    Assert.assertEquals(8, task.getRunner().getRowIngestionMeters().getProcessed());\n    Assert.assertEquals(3, task.getRunner().getRowIngestionMeters().getUnparseable());\n    Assert.assertEquals(1, task.getRunner().getRowIngestionMeters().getThrownAway());\n\n    \r\n    SegmentDescriptor desc1 = sd(task, \"2008/P1D\", 0);\n    SegmentDescriptor desc2 = sd(task, \"2009/P1D\", 0);\n    SegmentDescriptor desc3 = sd(task, \"2010/P1D\", 0);\n    SegmentDescriptor desc4 = sd(task, \"2011/P1D\", 0);\n    SegmentDescriptor desc5 = sd(task, \"2011/P1D\", 1);\n    SegmentDescriptor desc6 = sd(task, \"2012/P1D\", 0);\n    SegmentDescriptor desc7 = sd(task, \"2013/P1D\", 0);\n    Assert.assertEquals(ImmutableSet.of(desc1, desc2, desc3, desc4, desc5, desc6, desc7), publishedDescriptors());\n    Assert.assertEquals(\n          new KafkaDataSourceMetadata(new SeekableStreamPartitions<>(topic, ImmutableMap.of(0, 10L, 1, 2L))),\n          metadataStorageCoordinator.getDataSourceMetadata(DATA_SCHEMA.getDataSource()));\n\n    Assert.assertEquals(ImmutableSet.of(desc1, desc2, desc3, desc4, desc5, desc6, desc7), publishedDescriptors());\n    Assert.assertEquals(\n        new KafkaDataSourceMetadata(new SeekableStreamPartitions<>(topic, ImmutableMap.of(0, 10L, 1, 2L))),\n        metadataStorageCoordinator.getDataSourceMetadata(DATA_SCHEMA.getDataSource())\n    );\n\n    \r\n    Assert.assertEquals(ImmutableList.of(\"a\"), readSegmentColumn(\"dim1\", desc1));\n    Assert.assertEquals(ImmutableList.of(\"b\"), readSegmentColumn(\"dim1\", desc2));\n    Assert.assertEquals(ImmutableList.of(\"c\"), readSegmentColumn(\"dim1\", desc3));\n    Assert.assertTrue((ImmutableList.of(\"d\", \"e\").equals(readSegmentColumn(\"dim1\", desc4))\n                       && ImmutableList.of(\"h\").equals(readSegmentColumn(\"dim1\", desc5))) ||\n                      (ImmutableList.of(\"d\", \"h\").equals(readSegmentColumn(\"dim1\", desc4))\n                       && ImmutableList.of(\"e\").equals(readSegmentColumn(\"dim1\", desc5))));\n    Assert.assertEquals(ImmutableList.of(\"g\"), readSegmentColumn(\"dim1\", desc6));\n    Assert.assertEquals(ImmutableList.of(\"f\"), readSegmentColumn(\"dim1\", desc7));\n  }\n","realPath":"extensions-core/kafka-indexing-service/src/test/java/org/apache/druid/indexing/kafka/KafkaIndexTaskTest.java","repoName":"druid","snippetEndLine":0,"snippetStartLine":0,"startLine":603,"status":"M"},{"authorDate":"2019-03-22 04:12:22","commitOrder":8,"curCode":"  public void testTimeBasedIncrementalHandOff() throws Exception\n  {\n    if (!isIncrementalHandoffSupported) {\n      return;\n    }\n    final String baseSequenceName = \"sequence0\";\n    \r\n    maxRowsPerSegment = Integer.MAX_VALUE;\n    intermediateHandoffPeriod = new Period().withSeconds(0);\n\n    \r\n    insertData();\n    Map<String, Object> consumerProps = kafkaServer.consumerProperties();\n    consumerProps.put(\"max.poll.records\", \"1\");\n\n    final SeekableStreamStartSequenceNumbers<Integer, Long> startPartitions = new SeekableStreamStartSequenceNumbers<>(\n        topic,\n        ImmutableMap.of(0, 0L, 1, 0L),\n        ImmutableSet.of()\n    );\n    \r\n    final SeekableStreamEndSequenceNumbers<Integer, Long> checkpoint = new SeekableStreamEndSequenceNumbers<>(\n        topic,\n        ImmutableMap.of(0, 1L, 1, 0L)\n    );\n    final SeekableStreamEndSequenceNumbers<Integer, Long> endPartitions = new SeekableStreamEndSequenceNumbers<>(\n        topic,\n        ImmutableMap.of(0, 2L, 1, 0L)\n    );\n    final KafkaIndexTask task = createTask(\n        null,\n        new KafkaIndexTaskIOConfig(\n            0,\n            baseSequenceName,\n            startPartitions,\n            endPartitions,\n            consumerProps,\n            KafkaSupervisorIOConfig.DEFAULT_POLL_TIMEOUT_MILLIS,\n            true,\n            null,\n            null\n        )\n    );\n    final ListenableFuture<TaskStatus> future = runTask(task);\n\n    \r\n    while (task.getRunner().getStatus() != Status.PAUSED) {\n      Thread.sleep(10);\n    }\n    final Map<Integer, Long> currentOffsets = ImmutableMap.copyOf(task.getRunner().getCurrentOffsets());\n    Assert.assertTrue(checkpoint.getPartitionSequenceNumberMap().equals(currentOffsets));\n    task.getRunner().setEndOffsets(currentOffsets, false);\n    Assert.assertEquals(TaskState.SUCCESS, future.get().getStatusCode());\n\n    Assert.assertEquals(1, checkpointRequestsHash.size());\n    Assert.assertTrue(\n        checkpointRequestsHash.contains(\n            Objects.hash(\n                DATA_SCHEMA.getDataSource(),\n                0,\n                new KafkaDataSourceMetadata(startPartitions),\n                new KafkaDataSourceMetadata(\n                    new SeekableStreamEndSequenceNumbers<>(topic, checkpoint.getPartitionSequenceNumberMap())\n                )\n            )\n        )\n    );\n\n    \r\n    Assert.assertEquals(2, task.getRunner().getRowIngestionMeters().getProcessed());\n    Assert.assertEquals(0, task.getRunner().getRowIngestionMeters().getUnparseable());\n    Assert.assertEquals(0, task.getRunner().getRowIngestionMeters().getThrownAway());\n\n    \r\n    SegmentDescriptor desc1 = sd(task, \"2008/P1D\", 0);\n    SegmentDescriptor desc2 = sd(task, \"2009/P1D\", 0);\n    Assert.assertEquals(ImmutableSet.of(desc1, desc2), publishedDescriptors());\n    Assert.assertEquals(\n        new KafkaDataSourceMetadata(\n            new SeekableStreamEndSequenceNumbers<>(topic, ImmutableMap.of(0, 2L, 1, 0L))\n        ),\n        metadataStorageCoordinator.getDataSourceMetadata(DATA_SCHEMA.getDataSource())\n    );\n\n    \r\n    Assert.assertEquals(ImmutableList.of(\"a\"), readSegmentColumn(\"dim1\", desc1));\n    Assert.assertEquals(ImmutableList.of(\"b\"), readSegmentColumn(\"dim1\", desc2));\n  }\n","date":"2019-03-22 04:12:22","endLine":844,"groupId":"13979","id":14,"instanceNumber":2,"isCurCommit":0,"methodName":"testTimeBasedIncrementalHandOff","params":"()","path":"/mnt/clonedata/CloneManagementServer/ManagementServer/consistResult/result-druid-10-0.7/blobInfo/CC_OUT/blobs/eb/ef3fc4596a43b59efefcaf094105a8ae34ab5f.src","preCode":"  public void testTimeBasedIncrementalHandOff() throws Exception\n  {\n    if (!isIncrementalHandoffSupported) {\n      return;\n    }\n    final String baseSequenceName = \"sequence0\";\n    \r\n    maxRowsPerSegment = Integer.MAX_VALUE;\n    intermediateHandoffPeriod = new Period().withSeconds(0);\n\n    \r\n    insertData();\n    Map<String, Object> consumerProps = kafkaServer.consumerProperties();\n    consumerProps.put(\"max.poll.records\", \"1\");\n\n    final SeekableStreamPartitions<Integer, Long> startPartitions = new SeekableStreamPartitions<>(\n        topic,\n        ImmutableMap.of(\n            0,\n            0L,\n            1,\n            0L\n        )\n    );\n    \r\n    final SeekableStreamPartitions<Integer, Long> checkpoint = new SeekableStreamPartitions<>(\n        topic,\n        ImmutableMap.of(\n            0,\n            1L,\n            1,\n            0L\n        )\n    );\n    final SeekableStreamPartitions<Integer, Long> endPartitions = new SeekableStreamPartitions<>(\n        topic,\n        ImmutableMap.of(\n            0,\n            2L,\n            1,\n            0L\n        )\n    );\n    final KafkaIndexTask task = createTask(\n        null,\n        new KafkaIndexTaskIOConfig(\n            0,\n            baseSequenceName,\n            startPartitions,\n            endPartitions,\n            consumerProps,\n            KafkaSupervisorIOConfig.DEFAULT_POLL_TIMEOUT_MILLIS,\n            true,\n            null,\n            null\n        )\n    );\n    final ListenableFuture<TaskStatus> future = runTask(task);\n\n    \r\n    while (task.getRunner().getStatus() != Status.PAUSED) {\n      Thread.sleep(10);\n    }\n    final Map<Integer, Long> currentOffsets = ImmutableMap.copyOf(task.getRunner().getCurrentOffsets());\n    Assert.assertTrue(checkpoint.getPartitionSequenceNumberMap().equals(currentOffsets));\n    task.getRunner().setEndOffsets(currentOffsets, false);\n    Assert.assertEquals(TaskState.SUCCESS, future.get().getStatusCode());\n\n    Assert.assertEquals(1, checkpointRequestsHash.size());\n    Assert.assertTrue(\n        checkpointRequestsHash.contains(\n            Objects.hash(\n                DATA_SCHEMA.getDataSource(),\n                0,\n                new KafkaDataSourceMetadata(startPartitions),\n                new KafkaDataSourceMetadata(new SeekableStreamPartitions<>(\n                    topic,\n                    checkpoint.getPartitionSequenceNumberMap()\n                ))\n            )\n        )\n    );\n\n    \r\n    Assert.assertEquals(2, task.getRunner().getRowIngestionMeters().getProcessed());\n    Assert.assertEquals(0, task.getRunner().getRowIngestionMeters().getUnparseable());\n    Assert.assertEquals(0, task.getRunner().getRowIngestionMeters().getThrownAway());\n\n    \r\n    SegmentDescriptor desc1 = sd(task, \"2008/P1D\", 0);\n    SegmentDescriptor desc2 = sd(task, \"2009/P1D\", 0);\n    Assert.assertEquals(ImmutableSet.of(desc1, desc2), publishedDescriptors());\n    Assert.assertEquals(\n        new KafkaDataSourceMetadata(new SeekableStreamPartitions<>(topic, ImmutableMap.of(0, 2L, 1, 0L))),\n        metadataStorageCoordinator.getDataSourceMetadata(DATA_SCHEMA.getDataSource())\n    );\n\n    \r\n    Assert.assertEquals(ImmutableList.of(\"a\"), readSegmentColumn(\"dim1\", desc1));\n    Assert.assertEquals(ImmutableList.of(\"b\"), readSegmentColumn(\"dim1\", desc2));\n  }\n","realPath":"extensions-core/kafka-indexing-service/src/test/java/org/apache/druid/indexing/kafka/KafkaIndexTaskTest.java","repoName":"druid","snippetEndLine":0,"snippetStartLine":0,"startLine":757,"status":"M"}],"commitId":"0c5dcf5586e33607849e397209f3eb0b10661f1e","commitMessage":"@@@Fix exclusivity for start offset in kinesis indexing service & check exclusivity properly in IndexerSQLMetadataStorageCoordinator (#7291)\n\n* Fix exclusivity for start offset in kinesis indexing service\n\n* some adjustment\n\n* Fix SeekableStreamDataSourceMetadata\n\n* Add missing javadocs\n\n* Add missing comments and unit test\n\n* fix SeekableStreamStartSequenceNumbers.plus and add comments\n\n* remove extra exclusivePartitions in KafkaIOConfig and fix downgrade issue\n\n* Add javadocs\n\n* fix compilation\n\n* fix test\n\n* remove unused variable\n","date":"2019-03-22 04:12:22","modifiedFileCount":"30","status":"M","submitter":"Jihoon Son"},{"authorTime":"2019-05-24 00:25:35","codes":[{"authorDate":"2019-05-24 00:25:35","commitOrder":9,"curCode":"  public void testIncrementalHandOffMaxTotalRows() throws Exception\n  {\n    final String baseSequenceName = \"sequence0\";\n    \r\n    maxRowsPerSegment = Integer.MAX_VALUE;\n    maxTotalRows = 3L;\n\n    \r\n    int numToAdd = records.size() - 2;\n\n    try (final KafkaProducer<byte[], byte[]> kafkaProducer = kafkaServer.newProducer()) {\n      kafkaProducer.initTransactions();\n      kafkaProducer.beginTransaction();\n      for (int i = 0; i < numToAdd; i++) {\n        kafkaProducer.send(records.get(i)).get();\n      }\n      kafkaProducer.commitTransaction();\n    }\n\n    Map<String, Object> consumerProps = kafkaServer.consumerProperties();\n    consumerProps.put(\"max.poll.records\", \"1\");\n\n    final SeekableStreamStartSequenceNumbers<Integer, Long> startPartitions = new SeekableStreamStartSequenceNumbers<>(\n        topic,\n        ImmutableMap.of(0, 0L, 1, 0L),\n        ImmutableSet.of()\n    );\n    final SeekableStreamEndSequenceNumbers<Integer, Long> checkpoint1 = new SeekableStreamEndSequenceNumbers<>(\n        topic,\n        ImmutableMap.of(0, 3L, 1, 0L)\n    );\n    final SeekableStreamEndSequenceNumbers<Integer, Long> checkpoint2 = new SeekableStreamEndSequenceNumbers<>(\n        topic,\n        ImmutableMap.of(0, 10L, 1, 0L)\n    );\n\n    final SeekableStreamEndSequenceNumbers<Integer, Long> endPartitions = new SeekableStreamEndSequenceNumbers<>(\n        topic,\n        ImmutableMap.of(0, 10L, 1, 2L)\n    );\n    final KafkaIndexTask task = createTask(\n        null,\n        new KafkaIndexTaskIOConfig(\n            0,\n            baseSequenceName,\n            startPartitions,\n            endPartitions,\n            consumerProps,\n            KafkaSupervisorIOConfig.DEFAULT_POLL_TIMEOUT_MILLIS,\n            true,\n            null,\n            null\n        )\n    );\n    final ListenableFuture<TaskStatus> future = runTask(task);\n    while (task.getRunner().getStatus() != Status.PAUSED) {\n      Thread.sleep(10);\n    }\n    final Map<Integer, Long> currentOffsets = ImmutableMap.copyOf(task.getRunner().getCurrentOffsets());\n\n    Assert.assertEquals(checkpoint1.getPartitionSequenceNumberMap(), currentOffsets);\n    task.getRunner().setEndOffsets(currentOffsets, false);\n\n    while (task.getRunner().getStatus() != Status.PAUSED) {\n      Thread.sleep(10);\n    }\n\n    \r\n    try (final KafkaProducer<byte[], byte[]> kafkaProducer = kafkaServer.newProducer()) {\n      kafkaProducer.initTransactions();\n      kafkaProducer.beginTransaction();\n      for (int i = numToAdd; i < records.size(); i++) {\n        kafkaProducer.send(records.get(i)).get();\n      }\n      kafkaProducer.commitTransaction();\n    }\n    final Map<Integer, Long> nextOffsets = ImmutableMap.copyOf(task.getRunner().getCurrentOffsets());\n\n\n    Assert.assertTrue(checkpoint2.getPartitionSequenceNumberMap().equals(nextOffsets));\n    task.getRunner().setEndOffsets(nextOffsets, false);\n\n    Assert.assertEquals(TaskState.SUCCESS, future.get().getStatusCode());\n\n    Assert.assertEquals(2, checkpointRequestsHash.size());\n    Assert.assertTrue(\n        checkpointRequestsHash.contains(\n            Objects.hash(\n                DATA_SCHEMA.getDataSource(),\n                0,\n                new KafkaDataSourceMetadata(startPartitions),\n                new KafkaDataSourceMetadata(\n                    new SeekableStreamEndSequenceNumbers<>(topic, currentOffsets)\n                )\n            )\n        )\n    );\n    Assert.assertTrue(\n        checkpointRequestsHash.contains(\n            Objects.hash(\n                DATA_SCHEMA.getDataSource(),\n                0,\n                new KafkaDataSourceMetadata(\n                    new SeekableStreamStartSequenceNumbers<>(topic, currentOffsets, ImmutableSet.of())\n                ),\n                new KafkaDataSourceMetadata(new SeekableStreamEndSequenceNumbers<>(topic, nextOffsets))\n            )\n        )\n    );\n\n    \r\n    Assert.assertEquals(8, task.getRunner().getRowIngestionMeters().getProcessed());\n    Assert.assertEquals(3, task.getRunner().getRowIngestionMeters().getUnparseable());\n    Assert.assertEquals(1, task.getRunner().getRowIngestionMeters().getThrownAway());\n\n    \r\n    SegmentDescriptor desc1 = sd(task, \"2008/P1D\", 0);\n    SegmentDescriptor desc2 = sd(task, \"2009/P1D\", 0);\n    SegmentDescriptor desc3 = sd(task, \"2010/P1D\", 0);\n    SegmentDescriptor desc4 = sd(task, \"2011/P1D\", 0);\n    SegmentDescriptor desc5 = sd(task, \"2011/P1D\", 1);\n    SegmentDescriptor desc6 = sd(task, \"2012/P1D\", 0);\n    SegmentDescriptor desc7 = sd(task, \"2013/P1D\", 0);\n    Assert.assertEquals(ImmutableSet.of(desc1, desc2, desc3, desc4, desc5, desc6, desc7), publishedDescriptors());\n    Assert.assertEquals(\n        new KafkaDataSourceMetadata(\n            new SeekableStreamEndSequenceNumbers<>(topic, ImmutableMap.of(0, 10L, 1, 2L))\n        ),\n        metadataStorageCoordinator.getDataSourceMetadata(DATA_SCHEMA.getDataSource())\n    );\n\n    Assert.assertEquals(ImmutableSet.of(desc1, desc2, desc3, desc4, desc5, desc6, desc7), publishedDescriptors());\n    Assert.assertEquals(\n        new KafkaDataSourceMetadata(\n            new SeekableStreamEndSequenceNumbers<>(topic, ImmutableMap.of(0, 10L, 1, 2L))\n        ),\n        metadataStorageCoordinator.getDataSourceMetadata(DATA_SCHEMA.getDataSource())\n    );\n\n    \r\n    Assert.assertEquals(ImmutableList.of(\"a\"), readSegmentColumn(\"dim1\", desc1));\n    Assert.assertEquals(ImmutableList.of(\"b\"), readSegmentColumn(\"dim1\", desc2));\n    Assert.assertEquals(ImmutableList.of(\"c\"), readSegmentColumn(\"dim1\", desc3));\n    Assert.assertTrue((ImmutableList.of(\"d\", \"e\").equals(readSegmentColumn(\"dim1\", desc4))\n                       && ImmutableList.of(\"h\").equals(readSegmentColumn(\"dim1\", desc5))) ||\n                      (ImmutableList.of(\"d\", \"h\").equals(readSegmentColumn(\"dim1\", desc4))\n                       && ImmutableList.of(\"e\").equals(readSegmentColumn(\"dim1\", desc5))));\n    Assert.assertEquals(ImmutableList.of(\"g\"), readSegmentColumn(\"dim1\", desc6));\n    Assert.assertEquals(ImmutableList.of(\"f\"), readSegmentColumn(\"dim1\", desc7));\n  }\n","date":"2019-05-24 00:25:35","endLine":736,"groupId":"13977","id":15,"instanceNumber":1,"isCurCommit":0,"methodName":"testIncrementalHandOffMaxTotalRows","params":"()","path":"/mnt/clonedata/CloneManagementServer/ManagementServer/consistResult/result-druid-10-0.7/blobInfo/CC_OUT/blobs/78/3af47ca8bb71623d67c9415e3f7a2ce8d26cb8.src","preCode":"  public void testIncrementalHandOffMaxTotalRows() throws Exception\n  {\n    if (!isIncrementalHandoffSupported) {\n      return;\n    }\n    final String baseSequenceName = \"sequence0\";\n    \r\n    maxRowsPerSegment = Integer.MAX_VALUE;\n    maxTotalRows = 3L;\n\n    \r\n    int numToAdd = records.size() - 2;\n\n    try (final KafkaProducer<byte[], byte[]> kafkaProducer = kafkaServer.newProducer()) {\n      kafkaProducer.initTransactions();\n      kafkaProducer.beginTransaction();\n      for (int i = 0; i < numToAdd; i++) {\n        kafkaProducer.send(records.get(i)).get();\n      }\n      kafkaProducer.commitTransaction();\n    }\n\n    Map<String, Object> consumerProps = kafkaServer.consumerProperties();\n    consumerProps.put(\"max.poll.records\", \"1\");\n\n    final SeekableStreamStartSequenceNumbers<Integer, Long> startPartitions = new SeekableStreamStartSequenceNumbers<>(\n        topic,\n        ImmutableMap.of(0, 0L, 1, 0L),\n        ImmutableSet.of()\n    );\n    final SeekableStreamEndSequenceNumbers<Integer, Long> checkpoint1 = new SeekableStreamEndSequenceNumbers<>(\n        topic,\n        ImmutableMap.of(0, 3L, 1, 0L)\n    );\n    final SeekableStreamEndSequenceNumbers<Integer, Long> checkpoint2 = new SeekableStreamEndSequenceNumbers<>(\n        topic,\n        ImmutableMap.of(0, 10L, 1, 0L)\n    );\n\n    final SeekableStreamEndSequenceNumbers<Integer, Long> endPartitions = new SeekableStreamEndSequenceNumbers<>(\n        topic,\n        ImmutableMap.of(0, 10L, 1, 2L)\n    );\n    final KafkaIndexTask task = createTask(\n        null,\n        new KafkaIndexTaskIOConfig(\n            0,\n            baseSequenceName,\n            startPartitions,\n            endPartitions,\n            consumerProps,\n            KafkaSupervisorIOConfig.DEFAULT_POLL_TIMEOUT_MILLIS,\n            true,\n            null,\n            null\n        )\n    );\n    final ListenableFuture<TaskStatus> future = runTask(task);\n    while (task.getRunner().getStatus() != Status.PAUSED) {\n      Thread.sleep(10);\n    }\n    final Map<Integer, Long> currentOffsets = ImmutableMap.copyOf(task.getRunner().getCurrentOffsets());\n\n    Assert.assertEquals(checkpoint1.getPartitionSequenceNumberMap(), currentOffsets);\n    task.getRunner().setEndOffsets(currentOffsets, false);\n\n    while (task.getRunner().getStatus() != Status.PAUSED) {\n      Thread.sleep(10);\n    }\n\n    \r\n    try (final KafkaProducer<byte[], byte[]> kafkaProducer = kafkaServer.newProducer()) {\n      kafkaProducer.initTransactions();\n      kafkaProducer.beginTransaction();\n      for (int i = numToAdd; i < records.size(); i++) {\n        kafkaProducer.send(records.get(i)).get();\n      }\n      kafkaProducer.commitTransaction();\n    }\n    final Map<Integer, Long> nextOffsets = ImmutableMap.copyOf(task.getRunner().getCurrentOffsets());\n\n\n    Assert.assertTrue(checkpoint2.getPartitionSequenceNumberMap().equals(nextOffsets));\n    task.getRunner().setEndOffsets(nextOffsets, false);\n\n    Assert.assertEquals(TaskState.SUCCESS, future.get().getStatusCode());\n\n    Assert.assertEquals(2, checkpointRequestsHash.size());\n    Assert.assertTrue(\n        checkpointRequestsHash.contains(\n            Objects.hash(\n                DATA_SCHEMA.getDataSource(),\n                0,\n                new KafkaDataSourceMetadata(startPartitions),\n                new KafkaDataSourceMetadata(\n                    new SeekableStreamEndSequenceNumbers<>(topic, currentOffsets)\n                )\n            )\n        )\n    );\n    Assert.assertTrue(\n        checkpointRequestsHash.contains(\n            Objects.hash(\n                DATA_SCHEMA.getDataSource(),\n                0,\n                new KafkaDataSourceMetadata(\n                    new SeekableStreamStartSequenceNumbers<>(topic, currentOffsets, ImmutableSet.of())\n                ),\n                new KafkaDataSourceMetadata(new SeekableStreamEndSequenceNumbers<>(topic, nextOffsets))\n            )\n        )\n    );\n\n    \r\n    Assert.assertEquals(8, task.getRunner().getRowIngestionMeters().getProcessed());\n    Assert.assertEquals(3, task.getRunner().getRowIngestionMeters().getUnparseable());\n    Assert.assertEquals(1, task.getRunner().getRowIngestionMeters().getThrownAway());\n\n    \r\n    SegmentDescriptor desc1 = sd(task, \"2008/P1D\", 0);\n    SegmentDescriptor desc2 = sd(task, \"2009/P1D\", 0);\n    SegmentDescriptor desc3 = sd(task, \"2010/P1D\", 0);\n    SegmentDescriptor desc4 = sd(task, \"2011/P1D\", 0);\n    SegmentDescriptor desc5 = sd(task, \"2011/P1D\", 1);\n    SegmentDescriptor desc6 = sd(task, \"2012/P1D\", 0);\n    SegmentDescriptor desc7 = sd(task, \"2013/P1D\", 0);\n    Assert.assertEquals(ImmutableSet.of(desc1, desc2, desc3, desc4, desc5, desc6, desc7), publishedDescriptors());\n    Assert.assertEquals(\n        new KafkaDataSourceMetadata(\n            new SeekableStreamEndSequenceNumbers<>(topic, ImmutableMap.of(0, 10L, 1, 2L))\n        ),\n        metadataStorageCoordinator.getDataSourceMetadata(DATA_SCHEMA.getDataSource())\n    );\n\n    Assert.assertEquals(ImmutableSet.of(desc1, desc2, desc3, desc4, desc5, desc6, desc7), publishedDescriptors());\n    Assert.assertEquals(\n        new KafkaDataSourceMetadata(\n            new SeekableStreamEndSequenceNumbers<>(topic, ImmutableMap.of(0, 10L, 1, 2L))\n        ),\n        metadataStorageCoordinator.getDataSourceMetadata(DATA_SCHEMA.getDataSource())\n    );\n\n    \r\n    Assert.assertEquals(ImmutableList.of(\"a\"), readSegmentColumn(\"dim1\", desc1));\n    Assert.assertEquals(ImmutableList.of(\"b\"), readSegmentColumn(\"dim1\", desc2));\n    Assert.assertEquals(ImmutableList.of(\"c\"), readSegmentColumn(\"dim1\", desc3));\n    Assert.assertTrue((ImmutableList.of(\"d\", \"e\").equals(readSegmentColumn(\"dim1\", desc4))\n                       && ImmutableList.of(\"h\").equals(readSegmentColumn(\"dim1\", desc5))) ||\n                      (ImmutableList.of(\"d\", \"h\").equals(readSegmentColumn(\"dim1\", desc4))\n                       && ImmutableList.of(\"e\").equals(readSegmentColumn(\"dim1\", desc5))));\n    Assert.assertEquals(ImmutableList.of(\"g\"), readSegmentColumn(\"dim1\", desc6));\n    Assert.assertEquals(ImmutableList.of(\"f\"), readSegmentColumn(\"dim1\", desc7));\n  }\n","realPath":"extensions-core/kafka-indexing-service/src/test/java/org/apache/druid/indexing/kafka/KafkaIndexTaskTest.java","repoName":"druid","snippetEndLine":0,"snippetStartLine":0,"startLine":587,"status":"M"},{"authorDate":"2019-05-24 00:25:35","commitOrder":9,"curCode":"  public void testTimeBasedIncrementalHandOff() throws Exception\n  {\n    final String baseSequenceName = \"sequence0\";\n    \r\n    maxRowsPerSegment = Integer.MAX_VALUE;\n    intermediateHandoffPeriod = new Period().withSeconds(0);\n\n    \r\n    insertData();\n    Map<String, Object> consumerProps = kafkaServer.consumerProperties();\n    consumerProps.put(\"max.poll.records\", \"1\");\n\n    final SeekableStreamStartSequenceNumbers<Integer, Long> startPartitions = new SeekableStreamStartSequenceNumbers<>(\n        topic,\n        ImmutableMap.of(0, 0L, 1, 0L),\n        ImmutableSet.of()\n    );\n    \r\n    final SeekableStreamEndSequenceNumbers<Integer, Long> checkpoint = new SeekableStreamEndSequenceNumbers<>(\n        topic,\n        ImmutableMap.of(0, 1L, 1, 0L)\n    );\n    final SeekableStreamEndSequenceNumbers<Integer, Long> endPartitions = new SeekableStreamEndSequenceNumbers<>(\n        topic,\n        ImmutableMap.of(0, 2L, 1, 0L)\n    );\n    final KafkaIndexTask task = createTask(\n        null,\n        new KafkaIndexTaskIOConfig(\n            0,\n            baseSequenceName,\n            startPartitions,\n            endPartitions,\n            consumerProps,\n            KafkaSupervisorIOConfig.DEFAULT_POLL_TIMEOUT_MILLIS,\n            true,\n            null,\n            null\n        )\n    );\n    final ListenableFuture<TaskStatus> future = runTask(task);\n\n    \r\n    while (task.getRunner().getStatus() != Status.PAUSED) {\n      Thread.sleep(10);\n    }\n    final Map<Integer, Long> currentOffsets = ImmutableMap.copyOf(task.getRunner().getCurrentOffsets());\n    Assert.assertTrue(checkpoint.getPartitionSequenceNumberMap().equals(currentOffsets));\n    task.getRunner().setEndOffsets(currentOffsets, false);\n    Assert.assertEquals(TaskState.SUCCESS, future.get().getStatusCode());\n\n    Assert.assertEquals(1, checkpointRequestsHash.size());\n    Assert.assertTrue(\n        checkpointRequestsHash.contains(\n            Objects.hash(\n                DATA_SCHEMA.getDataSource(),\n                0,\n                new KafkaDataSourceMetadata(startPartitions),\n                new KafkaDataSourceMetadata(\n                    new SeekableStreamEndSequenceNumbers<>(topic, checkpoint.getPartitionSequenceNumberMap())\n                )\n            )\n        )\n    );\n\n    \r\n    Assert.assertEquals(2, task.getRunner().getRowIngestionMeters().getProcessed());\n    Assert.assertEquals(0, task.getRunner().getRowIngestionMeters().getUnparseable());\n    Assert.assertEquals(0, task.getRunner().getRowIngestionMeters().getThrownAway());\n\n    \r\n    SegmentDescriptor desc1 = sd(task, \"2008/P1D\", 0);\n    SegmentDescriptor desc2 = sd(task, \"2009/P1D\", 0);\n    Assert.assertEquals(ImmutableSet.of(desc1, desc2), publishedDescriptors());\n    Assert.assertEquals(\n        new KafkaDataSourceMetadata(\n            new SeekableStreamEndSequenceNumbers<>(topic, ImmutableMap.of(0, 2L, 1, 0L))\n        ),\n        metadataStorageCoordinator.getDataSourceMetadata(DATA_SCHEMA.getDataSource())\n    );\n\n    \r\n    Assert.assertEquals(ImmutableList.of(\"a\"), readSegmentColumn(\"dim1\", desc1));\n    Assert.assertEquals(ImmutableList.of(\"b\"), readSegmentColumn(\"dim1\", desc2));\n  }\n","date":"2019-05-24 00:25:35","endLine":823,"groupId":"13979","id":16,"instanceNumber":2,"isCurCommit":0,"methodName":"testTimeBasedIncrementalHandOff","params":"()","path":"/mnt/clonedata/CloneManagementServer/ManagementServer/consistResult/result-druid-10-0.7/blobInfo/CC_OUT/blobs/78/3af47ca8bb71623d67c9415e3f7a2ce8d26cb8.src","preCode":"  public void testTimeBasedIncrementalHandOff() throws Exception\n  {\n    if (!isIncrementalHandoffSupported) {\n      return;\n    }\n    final String baseSequenceName = \"sequence0\";\n    \r\n    maxRowsPerSegment = Integer.MAX_VALUE;\n    intermediateHandoffPeriod = new Period().withSeconds(0);\n\n    \r\n    insertData();\n    Map<String, Object> consumerProps = kafkaServer.consumerProperties();\n    consumerProps.put(\"max.poll.records\", \"1\");\n\n    final SeekableStreamStartSequenceNumbers<Integer, Long> startPartitions = new SeekableStreamStartSequenceNumbers<>(\n        topic,\n        ImmutableMap.of(0, 0L, 1, 0L),\n        ImmutableSet.of()\n    );\n    \r\n    final SeekableStreamEndSequenceNumbers<Integer, Long> checkpoint = new SeekableStreamEndSequenceNumbers<>(\n        topic,\n        ImmutableMap.of(0, 1L, 1, 0L)\n    );\n    final SeekableStreamEndSequenceNumbers<Integer, Long> endPartitions = new SeekableStreamEndSequenceNumbers<>(\n        topic,\n        ImmutableMap.of(0, 2L, 1, 0L)\n    );\n    final KafkaIndexTask task = createTask(\n        null,\n        new KafkaIndexTaskIOConfig(\n            0,\n            baseSequenceName,\n            startPartitions,\n            endPartitions,\n            consumerProps,\n            KafkaSupervisorIOConfig.DEFAULT_POLL_TIMEOUT_MILLIS,\n            true,\n            null,\n            null\n        )\n    );\n    final ListenableFuture<TaskStatus> future = runTask(task);\n\n    \r\n    while (task.getRunner().getStatus() != Status.PAUSED) {\n      Thread.sleep(10);\n    }\n    final Map<Integer, Long> currentOffsets = ImmutableMap.copyOf(task.getRunner().getCurrentOffsets());\n    Assert.assertTrue(checkpoint.getPartitionSequenceNumberMap().equals(currentOffsets));\n    task.getRunner().setEndOffsets(currentOffsets, false);\n    Assert.assertEquals(TaskState.SUCCESS, future.get().getStatusCode());\n\n    Assert.assertEquals(1, checkpointRequestsHash.size());\n    Assert.assertTrue(\n        checkpointRequestsHash.contains(\n            Objects.hash(\n                DATA_SCHEMA.getDataSource(),\n                0,\n                new KafkaDataSourceMetadata(startPartitions),\n                new KafkaDataSourceMetadata(\n                    new SeekableStreamEndSequenceNumbers<>(topic, checkpoint.getPartitionSequenceNumberMap())\n                )\n            )\n        )\n    );\n\n    \r\n    Assert.assertEquals(2, task.getRunner().getRowIngestionMeters().getProcessed());\n    Assert.assertEquals(0, task.getRunner().getRowIngestionMeters().getUnparseable());\n    Assert.assertEquals(0, task.getRunner().getRowIngestionMeters().getThrownAway());\n\n    \r\n    SegmentDescriptor desc1 = sd(task, \"2008/P1D\", 0);\n    SegmentDescriptor desc2 = sd(task, \"2009/P1D\", 0);\n    Assert.assertEquals(ImmutableSet.of(desc1, desc2), publishedDescriptors());\n    Assert.assertEquals(\n        new KafkaDataSourceMetadata(\n            new SeekableStreamEndSequenceNumbers<>(topic, ImmutableMap.of(0, 2L, 1, 0L))\n        ),\n        metadataStorageCoordinator.getDataSourceMetadata(DATA_SCHEMA.getDataSource())\n    );\n\n    \r\n    Assert.assertEquals(ImmutableList.of(\"a\"), readSegmentColumn(\"dim1\", desc1));\n    Assert.assertEquals(ImmutableList.of(\"b\"), readSegmentColumn(\"dim1\", desc2));\n  }\n","realPath":"extensions-core/kafka-indexing-service/src/test/java/org/apache/druid/indexing/kafka/KafkaIndexTaskTest.java","repoName":"druid","snippetEndLine":0,"snippetStartLine":0,"startLine":739,"status":"M"}],"commitId":"eff2be4f8f9d7aad0f01516f5425f3ebccaa006c","commitMessage":"@@@Remove LegacyKafkaIndexTaskRunner (#7735)\n\n","date":"2019-05-24 00:25:35","modifiedFileCount":"4","status":"M","submitter":"Jihoon Son"},{"authorTime":"2019-07-07 00:33:12","codes":[{"authorDate":"2019-07-07 00:33:12","commitOrder":10,"curCode":"  public void testIncrementalHandOffMaxTotalRows() throws Exception\n  {\n    final String baseSequenceName = \"sequence0\";\n    \r\n    maxRowsPerSegment = Integer.MAX_VALUE;\n    maxTotalRows = 3L;\n\n    \r\n    int numToAdd = records.size() - 2;\n\n    try (final KafkaProducer<byte[], byte[]> kafkaProducer = kafkaServer.newProducer()) {\n      kafkaProducer.initTransactions();\n      kafkaProducer.beginTransaction();\n      for (int i = 0; i < numToAdd; i++) {\n        kafkaProducer.send(records.get(i)).get();\n      }\n      kafkaProducer.commitTransaction();\n    }\n\n    Map<String, Object> consumerProps = kafkaServer.consumerProperties();\n    consumerProps.put(\"max.poll.records\", \"1\");\n\n    final SeekableStreamStartSequenceNumbers<Integer, Long> startPartitions = new SeekableStreamStartSequenceNumbers<>(\n        topic,\n        ImmutableMap.of(0, 0L, 1, 0L),\n        ImmutableSet.of()\n    );\n    final SeekableStreamEndSequenceNumbers<Integer, Long> checkpoint1 = new SeekableStreamEndSequenceNumbers<>(\n        topic,\n        ImmutableMap.of(0, 3L, 1, 0L)\n    );\n    final SeekableStreamEndSequenceNumbers<Integer, Long> checkpoint2 = new SeekableStreamEndSequenceNumbers<>(\n        topic,\n        ImmutableMap.of(0, 10L, 1, 0L)\n    );\n\n    final SeekableStreamEndSequenceNumbers<Integer, Long> endPartitions = new SeekableStreamEndSequenceNumbers<>(\n        topic,\n        ImmutableMap.of(0, 10L, 1, 2L)\n    );\n    final KafkaIndexTask task = createTask(\n        null,\n        new KafkaIndexTaskIOConfig(\n            0,\n            baseSequenceName,\n            startPartitions,\n            endPartitions,\n            consumerProps,\n            KafkaSupervisorIOConfig.DEFAULT_POLL_TIMEOUT_MILLIS,\n            true,\n            null,\n            null\n        )\n    );\n    final ListenableFuture<TaskStatus> future = runTask(task);\n    while (task.getRunner().getStatus() != Status.PAUSED) {\n      Thread.sleep(10);\n    }\n    final Map<Integer, Long> currentOffsets = ImmutableMap.copyOf(task.getRunner().getCurrentOffsets());\n\n    Assert.assertEquals(checkpoint1.getPartitionSequenceNumberMap(), currentOffsets);\n    task.getRunner().setEndOffsets(currentOffsets, false);\n\n    while (task.getRunner().getStatus() != Status.PAUSED) {\n      Thread.sleep(10);\n    }\n\n    \r\n    try (final KafkaProducer<byte[], byte[]> kafkaProducer = kafkaServer.newProducer()) {\n      kafkaProducer.initTransactions();\n      kafkaProducer.beginTransaction();\n      for (int i = numToAdd; i < records.size(); i++) {\n        kafkaProducer.send(records.get(i)).get();\n      }\n      kafkaProducer.commitTransaction();\n    }\n    final Map<Integer, Long> nextOffsets = ImmutableMap.copyOf(task.getRunner().getCurrentOffsets());\n\n\n    Assert.assertEquals(checkpoint2.getPartitionSequenceNumberMap(), nextOffsets);\n    task.getRunner().setEndOffsets(nextOffsets, false);\n\n    Assert.assertEquals(TaskState.SUCCESS, future.get().getStatusCode());\n\n    Assert.assertEquals(2, checkpointRequestsHash.size());\n    Assert.assertTrue(\n        checkpointRequestsHash.contains(\n            Objects.hash(\n                DATA_SCHEMA.getDataSource(),\n                0,\n                new KafkaDataSourceMetadata(startPartitions),\n                new KafkaDataSourceMetadata(\n                    new SeekableStreamEndSequenceNumbers<>(topic, currentOffsets)\n                )\n            )\n        )\n    );\n    Assert.assertTrue(\n        checkpointRequestsHash.contains(\n            Objects.hash(\n                DATA_SCHEMA.getDataSource(),\n                0,\n                new KafkaDataSourceMetadata(\n                    new SeekableStreamStartSequenceNumbers<>(topic, currentOffsets, ImmutableSet.of())\n                ),\n                new KafkaDataSourceMetadata(new SeekableStreamEndSequenceNumbers<>(topic, nextOffsets))\n            )\n        )\n    );\n\n    \r\n    Assert.assertEquals(8, task.getRunner().getRowIngestionMeters().getProcessed());\n    Assert.assertEquals(3, task.getRunner().getRowIngestionMeters().getUnparseable());\n    Assert.assertEquals(1, task.getRunner().getRowIngestionMeters().getThrownAway());\n\n    \r\n    SegmentDescriptor desc1 = sd(task, \"2008/P1D\", 0);\n    SegmentDescriptor desc2 = sd(task, \"2009/P1D\", 0);\n    SegmentDescriptor desc3 = sd(task, \"2010/P1D\", 0);\n    SegmentDescriptor desc4 = sd(task, \"2011/P1D\", 0);\n    SegmentDescriptor desc5 = sd(task, \"2011/P1D\", 1);\n    SegmentDescriptor desc6 = sd(task, \"2012/P1D\", 0);\n    SegmentDescriptor desc7 = sd(task, \"2013/P1D\", 0);\n    Assert.assertEquals(ImmutableSet.of(desc1, desc2, desc3, desc4, desc5, desc6, desc7), publishedDescriptors());\n    Assert.assertEquals(\n        new KafkaDataSourceMetadata(\n            new SeekableStreamEndSequenceNumbers<>(topic, ImmutableMap.of(0, 10L, 1, 2L))\n        ),\n        metadataStorageCoordinator.getDataSourceMetadata(DATA_SCHEMA.getDataSource())\n    );\n\n    Assert.assertEquals(ImmutableSet.of(desc1, desc2, desc3, desc4, desc5, desc6, desc7), publishedDescriptors());\n    Assert.assertEquals(\n        new KafkaDataSourceMetadata(\n            new SeekableStreamEndSequenceNumbers<>(topic, ImmutableMap.of(0, 10L, 1, 2L))\n        ),\n        metadataStorageCoordinator.getDataSourceMetadata(DATA_SCHEMA.getDataSource())\n    );\n\n    \r\n    Assert.assertEquals(ImmutableList.of(\"a\"), readSegmentColumn(\"dim1\", desc1));\n    Assert.assertEquals(ImmutableList.of(\"b\"), readSegmentColumn(\"dim1\", desc2));\n    Assert.assertEquals(ImmutableList.of(\"c\"), readSegmentColumn(\"dim1\", desc3));\n    Assert.assertTrue((ImmutableList.of(\"d\", \"e\").equals(readSegmentColumn(\"dim1\", desc4))\n                       && ImmutableList.of(\"h\").equals(readSegmentColumn(\"dim1\", desc5))) ||\n                      (ImmutableList.of(\"d\", \"h\").equals(readSegmentColumn(\"dim1\", desc4))\n                       && ImmutableList.of(\"e\").equals(readSegmentColumn(\"dim1\", desc5))));\n    Assert.assertEquals(ImmutableList.of(\"g\"), readSegmentColumn(\"dim1\", desc6));\n    Assert.assertEquals(ImmutableList.of(\"f\"), readSegmentColumn(\"dim1\", desc7));\n  }\n","date":"2019-07-07 00:33:12","endLine":733,"groupId":"23394","id":17,"instanceNumber":1,"isCurCommit":0,"methodName":"testIncrementalHandOffMaxTotalRows","params":"()","path":"/mnt/clonedata/CloneManagementServer/ManagementServer/consistResult/result-druid-10-0.7/blobInfo/CC_OUT/blobs/c9/506d6ef203fbcd3e0ff362c5c0ee77be304a4d.src","preCode":"  public void testIncrementalHandOffMaxTotalRows() throws Exception\n  {\n    final String baseSequenceName = \"sequence0\";\n    \r\n    maxRowsPerSegment = Integer.MAX_VALUE;\n    maxTotalRows = 3L;\n\n    \r\n    int numToAdd = records.size() - 2;\n\n    try (final KafkaProducer<byte[], byte[]> kafkaProducer = kafkaServer.newProducer()) {\n      kafkaProducer.initTransactions();\n      kafkaProducer.beginTransaction();\n      for (int i = 0; i < numToAdd; i++) {\n        kafkaProducer.send(records.get(i)).get();\n      }\n      kafkaProducer.commitTransaction();\n    }\n\n    Map<String, Object> consumerProps = kafkaServer.consumerProperties();\n    consumerProps.put(\"max.poll.records\", \"1\");\n\n    final SeekableStreamStartSequenceNumbers<Integer, Long> startPartitions = new SeekableStreamStartSequenceNumbers<>(\n        topic,\n        ImmutableMap.of(0, 0L, 1, 0L),\n        ImmutableSet.of()\n    );\n    final SeekableStreamEndSequenceNumbers<Integer, Long> checkpoint1 = new SeekableStreamEndSequenceNumbers<>(\n        topic,\n        ImmutableMap.of(0, 3L, 1, 0L)\n    );\n    final SeekableStreamEndSequenceNumbers<Integer, Long> checkpoint2 = new SeekableStreamEndSequenceNumbers<>(\n        topic,\n        ImmutableMap.of(0, 10L, 1, 0L)\n    );\n\n    final SeekableStreamEndSequenceNumbers<Integer, Long> endPartitions = new SeekableStreamEndSequenceNumbers<>(\n        topic,\n        ImmutableMap.of(0, 10L, 1, 2L)\n    );\n    final KafkaIndexTask task = createTask(\n        null,\n        new KafkaIndexTaskIOConfig(\n            0,\n            baseSequenceName,\n            startPartitions,\n            endPartitions,\n            consumerProps,\n            KafkaSupervisorIOConfig.DEFAULT_POLL_TIMEOUT_MILLIS,\n            true,\n            null,\n            null\n        )\n    );\n    final ListenableFuture<TaskStatus> future = runTask(task);\n    while (task.getRunner().getStatus() != Status.PAUSED) {\n      Thread.sleep(10);\n    }\n    final Map<Integer, Long> currentOffsets = ImmutableMap.copyOf(task.getRunner().getCurrentOffsets());\n\n    Assert.assertEquals(checkpoint1.getPartitionSequenceNumberMap(), currentOffsets);\n    task.getRunner().setEndOffsets(currentOffsets, false);\n\n    while (task.getRunner().getStatus() != Status.PAUSED) {\n      Thread.sleep(10);\n    }\n\n    \r\n    try (final KafkaProducer<byte[], byte[]> kafkaProducer = kafkaServer.newProducer()) {\n      kafkaProducer.initTransactions();\n      kafkaProducer.beginTransaction();\n      for (int i = numToAdd; i < records.size(); i++) {\n        kafkaProducer.send(records.get(i)).get();\n      }\n      kafkaProducer.commitTransaction();\n    }\n    final Map<Integer, Long> nextOffsets = ImmutableMap.copyOf(task.getRunner().getCurrentOffsets());\n\n\n    Assert.assertTrue(checkpoint2.getPartitionSequenceNumberMap().equals(nextOffsets));\n    task.getRunner().setEndOffsets(nextOffsets, false);\n\n    Assert.assertEquals(TaskState.SUCCESS, future.get().getStatusCode());\n\n    Assert.assertEquals(2, checkpointRequestsHash.size());\n    Assert.assertTrue(\n        checkpointRequestsHash.contains(\n            Objects.hash(\n                DATA_SCHEMA.getDataSource(),\n                0,\n                new KafkaDataSourceMetadata(startPartitions),\n                new KafkaDataSourceMetadata(\n                    new SeekableStreamEndSequenceNumbers<>(topic, currentOffsets)\n                )\n            )\n        )\n    );\n    Assert.assertTrue(\n        checkpointRequestsHash.contains(\n            Objects.hash(\n                DATA_SCHEMA.getDataSource(),\n                0,\n                new KafkaDataSourceMetadata(\n                    new SeekableStreamStartSequenceNumbers<>(topic, currentOffsets, ImmutableSet.of())\n                ),\n                new KafkaDataSourceMetadata(new SeekableStreamEndSequenceNumbers<>(topic, nextOffsets))\n            )\n        )\n    );\n\n    \r\n    Assert.assertEquals(8, task.getRunner().getRowIngestionMeters().getProcessed());\n    Assert.assertEquals(3, task.getRunner().getRowIngestionMeters().getUnparseable());\n    Assert.assertEquals(1, task.getRunner().getRowIngestionMeters().getThrownAway());\n\n    \r\n    SegmentDescriptor desc1 = sd(task, \"2008/P1D\", 0);\n    SegmentDescriptor desc2 = sd(task, \"2009/P1D\", 0);\n    SegmentDescriptor desc3 = sd(task, \"2010/P1D\", 0);\n    SegmentDescriptor desc4 = sd(task, \"2011/P1D\", 0);\n    SegmentDescriptor desc5 = sd(task, \"2011/P1D\", 1);\n    SegmentDescriptor desc6 = sd(task, \"2012/P1D\", 0);\n    SegmentDescriptor desc7 = sd(task, \"2013/P1D\", 0);\n    Assert.assertEquals(ImmutableSet.of(desc1, desc2, desc3, desc4, desc5, desc6, desc7), publishedDescriptors());\n    Assert.assertEquals(\n        new KafkaDataSourceMetadata(\n            new SeekableStreamEndSequenceNumbers<>(topic, ImmutableMap.of(0, 10L, 1, 2L))\n        ),\n        metadataStorageCoordinator.getDataSourceMetadata(DATA_SCHEMA.getDataSource())\n    );\n\n    Assert.assertEquals(ImmutableSet.of(desc1, desc2, desc3, desc4, desc5, desc6, desc7), publishedDescriptors());\n    Assert.assertEquals(\n        new KafkaDataSourceMetadata(\n            new SeekableStreamEndSequenceNumbers<>(topic, ImmutableMap.of(0, 10L, 1, 2L))\n        ),\n        metadataStorageCoordinator.getDataSourceMetadata(DATA_SCHEMA.getDataSource())\n    );\n\n    \r\n    Assert.assertEquals(ImmutableList.of(\"a\"), readSegmentColumn(\"dim1\", desc1));\n    Assert.assertEquals(ImmutableList.of(\"b\"), readSegmentColumn(\"dim1\", desc2));\n    Assert.assertEquals(ImmutableList.of(\"c\"), readSegmentColumn(\"dim1\", desc3));\n    Assert.assertTrue((ImmutableList.of(\"d\", \"e\").equals(readSegmentColumn(\"dim1\", desc4))\n                       && ImmutableList.of(\"h\").equals(readSegmentColumn(\"dim1\", desc5))) ||\n                      (ImmutableList.of(\"d\", \"h\").equals(readSegmentColumn(\"dim1\", desc4))\n                       && ImmutableList.of(\"e\").equals(readSegmentColumn(\"dim1\", desc5))));\n    Assert.assertEquals(ImmutableList.of(\"g\"), readSegmentColumn(\"dim1\", desc6));\n    Assert.assertEquals(ImmutableList.of(\"f\"), readSegmentColumn(\"dim1\", desc7));\n  }\n","realPath":"extensions-core/kafka-indexing-service/src/test/java/org/apache/druid/indexing/kafka/KafkaIndexTaskTest.java","repoName":"druid","snippetEndLine":0,"snippetStartLine":0,"startLine":584,"status":"M"},{"authorDate":"2019-07-07 00:33:12","commitOrder":10,"curCode":"  public void testTimeBasedIncrementalHandOff() throws Exception\n  {\n    final String baseSequenceName = \"sequence0\";\n    \r\n    maxRowsPerSegment = Integer.MAX_VALUE;\n    intermediateHandoffPeriod = new Period().withSeconds(0);\n\n    \r\n    insertData();\n    Map<String, Object> consumerProps = kafkaServer.consumerProperties();\n    consumerProps.put(\"max.poll.records\", \"1\");\n\n    final SeekableStreamStartSequenceNumbers<Integer, Long> startPartitions = new SeekableStreamStartSequenceNumbers<>(\n        topic,\n        ImmutableMap.of(0, 0L, 1, 0L),\n        ImmutableSet.of()\n    );\n    \r\n    final SeekableStreamEndSequenceNumbers<Integer, Long> checkpoint = new SeekableStreamEndSequenceNumbers<>(\n        topic,\n        ImmutableMap.of(0, 1L, 1, 0L)\n    );\n    final SeekableStreamEndSequenceNumbers<Integer, Long> endPartitions = new SeekableStreamEndSequenceNumbers<>(\n        topic,\n        ImmutableMap.of(0, 2L, 1, 0L)\n    );\n    final KafkaIndexTask task = createTask(\n        null,\n        new KafkaIndexTaskIOConfig(\n            0,\n            baseSequenceName,\n            startPartitions,\n            endPartitions,\n            consumerProps,\n            KafkaSupervisorIOConfig.DEFAULT_POLL_TIMEOUT_MILLIS,\n            true,\n            null,\n            null\n        )\n    );\n    final ListenableFuture<TaskStatus> future = runTask(task);\n\n    \r\n    while (task.getRunner().getStatus() != Status.PAUSED) {\n      Thread.sleep(10);\n    }\n    final Map<Integer, Long> currentOffsets = ImmutableMap.copyOf(task.getRunner().getCurrentOffsets());\n    Assert.assertEquals(checkpoint.getPartitionSequenceNumberMap(), currentOffsets);\n    task.getRunner().setEndOffsets(currentOffsets, false);\n    Assert.assertEquals(TaskState.SUCCESS, future.get().getStatusCode());\n\n    Assert.assertEquals(1, checkpointRequestsHash.size());\n    Assert.assertTrue(\n        checkpointRequestsHash.contains(\n            Objects.hash(\n                DATA_SCHEMA.getDataSource(),\n                0,\n                new KafkaDataSourceMetadata(startPartitions),\n                new KafkaDataSourceMetadata(\n                    new SeekableStreamEndSequenceNumbers<>(topic, checkpoint.getPartitionSequenceNumberMap())\n                )\n            )\n        )\n    );\n\n    \r\n    Assert.assertEquals(2, task.getRunner().getRowIngestionMeters().getProcessed());\n    Assert.assertEquals(0, task.getRunner().getRowIngestionMeters().getUnparseable());\n    Assert.assertEquals(0, task.getRunner().getRowIngestionMeters().getThrownAway());\n\n    \r\n    SegmentDescriptor desc1 = sd(task, \"2008/P1D\", 0);\n    SegmentDescriptor desc2 = sd(task, \"2009/P1D\", 0);\n    Assert.assertEquals(ImmutableSet.of(desc1, desc2), publishedDescriptors());\n    Assert.assertEquals(\n        new KafkaDataSourceMetadata(\n            new SeekableStreamEndSequenceNumbers<>(topic, ImmutableMap.of(0, 2L, 1, 0L))\n        ),\n        metadataStorageCoordinator.getDataSourceMetadata(DATA_SCHEMA.getDataSource())\n    );\n\n    \r\n    Assert.assertEquals(ImmutableList.of(\"a\"), readSegmentColumn(\"dim1\", desc1));\n    Assert.assertEquals(ImmutableList.of(\"b\"), readSegmentColumn(\"dim1\", desc2));\n  }\n","date":"2019-07-07 00:33:12","endLine":820,"groupId":"2270","id":18,"instanceNumber":2,"isCurCommit":0,"methodName":"testTimeBasedIncrementalHandOff","params":"()","path":"/mnt/clonedata/CloneManagementServer/ManagementServer/consistResult/result-druid-10-0.7/blobInfo/CC_OUT/blobs/c9/506d6ef203fbcd3e0ff362c5c0ee77be304a4d.src","preCode":"  public void testTimeBasedIncrementalHandOff() throws Exception\n  {\n    final String baseSequenceName = \"sequence0\";\n    \r\n    maxRowsPerSegment = Integer.MAX_VALUE;\n    intermediateHandoffPeriod = new Period().withSeconds(0);\n\n    \r\n    insertData();\n    Map<String, Object> consumerProps = kafkaServer.consumerProperties();\n    consumerProps.put(\"max.poll.records\", \"1\");\n\n    final SeekableStreamStartSequenceNumbers<Integer, Long> startPartitions = new SeekableStreamStartSequenceNumbers<>(\n        topic,\n        ImmutableMap.of(0, 0L, 1, 0L),\n        ImmutableSet.of()\n    );\n    \r\n    final SeekableStreamEndSequenceNumbers<Integer, Long> checkpoint = new SeekableStreamEndSequenceNumbers<>(\n        topic,\n        ImmutableMap.of(0, 1L, 1, 0L)\n    );\n    final SeekableStreamEndSequenceNumbers<Integer, Long> endPartitions = new SeekableStreamEndSequenceNumbers<>(\n        topic,\n        ImmutableMap.of(0, 2L, 1, 0L)\n    );\n    final KafkaIndexTask task = createTask(\n        null,\n        new KafkaIndexTaskIOConfig(\n            0,\n            baseSequenceName,\n            startPartitions,\n            endPartitions,\n            consumerProps,\n            KafkaSupervisorIOConfig.DEFAULT_POLL_TIMEOUT_MILLIS,\n            true,\n            null,\n            null\n        )\n    );\n    final ListenableFuture<TaskStatus> future = runTask(task);\n\n    \r\n    while (task.getRunner().getStatus() != Status.PAUSED) {\n      Thread.sleep(10);\n    }\n    final Map<Integer, Long> currentOffsets = ImmutableMap.copyOf(task.getRunner().getCurrentOffsets());\n    Assert.assertTrue(checkpoint.getPartitionSequenceNumberMap().equals(currentOffsets));\n    task.getRunner().setEndOffsets(currentOffsets, false);\n    Assert.assertEquals(TaskState.SUCCESS, future.get().getStatusCode());\n\n    Assert.assertEquals(1, checkpointRequestsHash.size());\n    Assert.assertTrue(\n        checkpointRequestsHash.contains(\n            Objects.hash(\n                DATA_SCHEMA.getDataSource(),\n                0,\n                new KafkaDataSourceMetadata(startPartitions),\n                new KafkaDataSourceMetadata(\n                    new SeekableStreamEndSequenceNumbers<>(topic, checkpoint.getPartitionSequenceNumberMap())\n                )\n            )\n        )\n    );\n\n    \r\n    Assert.assertEquals(2, task.getRunner().getRowIngestionMeters().getProcessed());\n    Assert.assertEquals(0, task.getRunner().getRowIngestionMeters().getUnparseable());\n    Assert.assertEquals(0, task.getRunner().getRowIngestionMeters().getThrownAway());\n\n    \r\n    SegmentDescriptor desc1 = sd(task, \"2008/P1D\", 0);\n    SegmentDescriptor desc2 = sd(task, \"2009/P1D\", 0);\n    Assert.assertEquals(ImmutableSet.of(desc1, desc2), publishedDescriptors());\n    Assert.assertEquals(\n        new KafkaDataSourceMetadata(\n            new SeekableStreamEndSequenceNumbers<>(topic, ImmutableMap.of(0, 2L, 1, 0L))\n        ),\n        metadataStorageCoordinator.getDataSourceMetadata(DATA_SCHEMA.getDataSource())\n    );\n\n    \r\n    Assert.assertEquals(ImmutableList.of(\"a\"), readSegmentColumn(\"dim1\", desc1));\n    Assert.assertEquals(ImmutableList.of(\"b\"), readSegmentColumn(\"dim1\", desc2));\n  }\n","realPath":"extensions-core/kafka-indexing-service/src/test/java/org/apache/druid/indexing/kafka/KafkaIndexTaskTest.java","repoName":"druid","snippetEndLine":0,"snippetStartLine":0,"startLine":736,"status":"M"}],"commitId":"1166bbcb75d432817715fdd429737f86730b5591","commitMessage":"@@@Remove static imports from tests (#8036)\n\nMake static imports forbidden in tests and remove all occurrences to be\nconsistent with the non-test code.\n\nAlso.  various changes to files affected by above:\n- Reformat to adhere to druid style guide\n- Fix various IntelliJ warnings\n- Fix various SonarLint warnings (e.g..  the expected/actual args to\n  Assert.assertEquals() were flipped)","date":"2019-07-07 00:33:12","modifiedFileCount":"98","status":"M","submitter":"Chi Cao Minh"},{"authorTime":"2019-07-25 08:35:46","codes":[{"authorDate":"2019-07-25 08:35:46","commitOrder":11,"curCode":"  public void testIncrementalHandOffMaxTotalRows() throws Exception\n  {\n    final String baseSequenceName = \"sequence0\";\n    \r\n    maxRowsPerSegment = Integer.MAX_VALUE;\n    maxTotalRows = 3L;\n\n    \r\n    int numToAdd = records.size() - 2;\n\n    try (final KafkaProducer<byte[], byte[]> kafkaProducer = kafkaServer.newProducer()) {\n      kafkaProducer.initTransactions();\n      kafkaProducer.beginTransaction();\n      for (int i = 0; i < numToAdd; i++) {\n        kafkaProducer.send(records.get(i)).get();\n      }\n      kafkaProducer.commitTransaction();\n    }\n\n    Map<String, Object> consumerProps = kafkaServer.consumerProperties();\n    consumerProps.put(\"max.poll.records\", \"1\");\n\n    final SeekableStreamStartSequenceNumbers<Integer, Long> startPartitions = new SeekableStreamStartSequenceNumbers<>(\n        topic,\n        ImmutableMap.of(0, 0L, 1, 0L),\n        ImmutableSet.of()\n    );\n    final SeekableStreamEndSequenceNumbers<Integer, Long> checkpoint1 = new SeekableStreamEndSequenceNumbers<>(\n        topic,\n        ImmutableMap.of(0, 3L, 1, 0L)\n    );\n    final SeekableStreamEndSequenceNumbers<Integer, Long> checkpoint2 = new SeekableStreamEndSequenceNumbers<>(\n        topic,\n        ImmutableMap.of(0, 10L, 1, 0L)\n    );\n\n    final SeekableStreamEndSequenceNumbers<Integer, Long> endPartitions = new SeekableStreamEndSequenceNumbers<>(\n        topic,\n        ImmutableMap.of(0, 10L, 1, 2L)\n    );\n    final KafkaIndexTask task = createTask(\n        null,\n        new KafkaIndexTaskIOConfig(\n            0,\n            baseSequenceName,\n            startPartitions,\n            endPartitions,\n            consumerProps,\n            KafkaSupervisorIOConfig.DEFAULT_POLL_TIMEOUT_MILLIS,\n            true,\n            null,\n            null\n        )\n    );\n    final ListenableFuture<TaskStatus> future = runTask(task);\n    while (task.getRunner().getStatus() != Status.PAUSED) {\n      Thread.sleep(10);\n    }\n    final Map<Integer, Long> currentOffsets = ImmutableMap.copyOf(task.getRunner().getCurrentOffsets());\n\n    Assert.assertEquals(checkpoint1.getPartitionSequenceNumberMap(), currentOffsets);\n    task.getRunner().setEndOffsets(currentOffsets, false);\n\n    while (task.getRunner().getStatus() != Status.PAUSED) {\n      Thread.sleep(10);\n    }\n\n    \r\n    try (final KafkaProducer<byte[], byte[]> kafkaProducer = kafkaServer.newProducer()) {\n      kafkaProducer.initTransactions();\n      kafkaProducer.beginTransaction();\n      for (int i = numToAdd; i < records.size(); i++) {\n        kafkaProducer.send(records.get(i)).get();\n      }\n      kafkaProducer.commitTransaction();\n    }\n    final Map<Integer, Long> nextOffsets = ImmutableMap.copyOf(task.getRunner().getCurrentOffsets());\n\n\n    Assert.assertEquals(checkpoint2.getPartitionSequenceNumberMap(), nextOffsets);\n    task.getRunner().setEndOffsets(nextOffsets, false);\n\n    Assert.assertEquals(TaskState.SUCCESS, future.get().getStatusCode());\n\n    Assert.assertEquals(2, checkpointRequestsHash.size());\n    Assert.assertTrue(\n        checkpointRequestsHash.contains(\n            Objects.hash(\n                DATA_SCHEMA.getDataSource(),\n                0,\n                new KafkaDataSourceMetadata(startPartitions),\n                new KafkaDataSourceMetadata(\n                    new SeekableStreamEndSequenceNumbers<>(topic, currentOffsets)\n                )\n            )\n        )\n    );\n    Assert.assertTrue(\n        checkpointRequestsHash.contains(\n            Objects.hash(\n                DATA_SCHEMA.getDataSource(),\n                0,\n                new KafkaDataSourceMetadata(\n                    new SeekableStreamStartSequenceNumbers<>(topic, currentOffsets, ImmutableSet.of())\n                ),\n                new KafkaDataSourceMetadata(new SeekableStreamEndSequenceNumbers<>(topic, nextOffsets))\n            )\n        )\n    );\n\n    \r\n    Assert.assertEquals(8, task.getRunner().getRowIngestionMeters().getProcessed());\n    Assert.assertEquals(3, task.getRunner().getRowIngestionMeters().getUnparseable());\n    Assert.assertEquals(1, task.getRunner().getRowIngestionMeters().getThrownAway());\n\n    \r\n    SegmentDescriptor desc1 = sd(\"2008/P1D\", 0);\n    SegmentDescriptor desc2 = sd(\"2009/P1D\", 0);\n    SegmentDescriptor desc3 = sd(\"2010/P1D\", 0);\n    SegmentDescriptor desc4 = sd(\"2011/P1D\", 0);\n    SegmentDescriptor desc5 = sd(\"2011/P1D\", 1);\n    SegmentDescriptor desc6 = sd(\"2012/P1D\", 0);\n    SegmentDescriptor desc7 = sd(\"2013/P1D\", 0);\n    assertEqualsExceptVersion(ImmutableList.of(desc1, desc2, desc3, desc4, desc5, desc6, desc7), publishedDescriptors());\n    Assert.assertEquals(\n        new KafkaDataSourceMetadata(\n            new SeekableStreamEndSequenceNumbers<>(topic, ImmutableMap.of(0, 10L, 1, 2L))\n        ),\n        metadataStorageCoordinator.getDataSourceMetadata(DATA_SCHEMA.getDataSource())\n    );\n\n    assertEqualsExceptVersion(ImmutableList.of(desc1, desc2, desc3, desc4, desc5, desc6, desc7), publishedDescriptors());\n    Assert.assertEquals(\n        new KafkaDataSourceMetadata(\n            new SeekableStreamEndSequenceNumbers<>(topic, ImmutableMap.of(0, 10L, 1, 2L))\n        ),\n        metadataStorageCoordinator.getDataSourceMetadata(DATA_SCHEMA.getDataSource())\n    );\n\n    \r\n    final List<SegmentDescriptor> publishedDescriptors = publishedDescriptors();\n    Assert.assertEquals(ImmutableList.of(\"a\"), readSegmentColumn(\"dim1\", publishedDescriptors.get(0)));\n    Assert.assertEquals(ImmutableList.of(\"b\"), readSegmentColumn(\"dim1\", publishedDescriptors.get(1)));\n    Assert.assertEquals(ImmutableList.of(\"c\"), readSegmentColumn(\"dim1\", publishedDescriptors.get(2)));\n    Assert.assertTrue((ImmutableList.of(\"d\", \"e\").equals(readSegmentColumn(\"dim1\", publishedDescriptors.get(3)))\n                       && ImmutableList.of(\"h\").equals(readSegmentColumn(\"dim1\", publishedDescriptors.get(4)))) ||\n                      (ImmutableList.of(\"d\", \"h\").equals(readSegmentColumn(\"dim1\", publishedDescriptors.get(3)))\n                       && ImmutableList.of(\"e\").equals(readSegmentColumn(\"dim1\", publishedDescriptors.get(4)))));\n    Assert.assertEquals(ImmutableList.of(\"g\"), readSegmentColumn(\"dim1\", publishedDescriptors.get(5)));\n    Assert.assertEquals(ImmutableList.of(\"f\"), readSegmentColumn(\"dim1\", publishedDescriptors.get(6)));\n  }\n","date":"2019-07-25 08:35:46","endLine":759,"groupId":"11118","id":19,"instanceNumber":1,"isCurCommit":0,"methodName":"testIncrementalHandOffMaxTotalRows","params":"()","path":"/mnt/clonedata/CloneManagementServer/ManagementServer/consistResult/result-druid-10-0.7/blobInfo/CC_OUT/blobs/87/f522eae18ccef1c23a5fd9ae035dbbe3410dfc.src","preCode":"  public void testIncrementalHandOffMaxTotalRows() throws Exception\n  {\n    final String baseSequenceName = \"sequence0\";\n    \r\n    maxRowsPerSegment = Integer.MAX_VALUE;\n    maxTotalRows = 3L;\n\n    \r\n    int numToAdd = records.size() - 2;\n\n    try (final KafkaProducer<byte[], byte[]> kafkaProducer = kafkaServer.newProducer()) {\n      kafkaProducer.initTransactions();\n      kafkaProducer.beginTransaction();\n      for (int i = 0; i < numToAdd; i++) {\n        kafkaProducer.send(records.get(i)).get();\n      }\n      kafkaProducer.commitTransaction();\n    }\n\n    Map<String, Object> consumerProps = kafkaServer.consumerProperties();\n    consumerProps.put(\"max.poll.records\", \"1\");\n\n    final SeekableStreamStartSequenceNumbers<Integer, Long> startPartitions = new SeekableStreamStartSequenceNumbers<>(\n        topic,\n        ImmutableMap.of(0, 0L, 1, 0L),\n        ImmutableSet.of()\n    );\n    final SeekableStreamEndSequenceNumbers<Integer, Long> checkpoint1 = new SeekableStreamEndSequenceNumbers<>(\n        topic,\n        ImmutableMap.of(0, 3L, 1, 0L)\n    );\n    final SeekableStreamEndSequenceNumbers<Integer, Long> checkpoint2 = new SeekableStreamEndSequenceNumbers<>(\n        topic,\n        ImmutableMap.of(0, 10L, 1, 0L)\n    );\n\n    final SeekableStreamEndSequenceNumbers<Integer, Long> endPartitions = new SeekableStreamEndSequenceNumbers<>(\n        topic,\n        ImmutableMap.of(0, 10L, 1, 2L)\n    );\n    final KafkaIndexTask task = createTask(\n        null,\n        new KafkaIndexTaskIOConfig(\n            0,\n            baseSequenceName,\n            startPartitions,\n            endPartitions,\n            consumerProps,\n            KafkaSupervisorIOConfig.DEFAULT_POLL_TIMEOUT_MILLIS,\n            true,\n            null,\n            null\n        )\n    );\n    final ListenableFuture<TaskStatus> future = runTask(task);\n    while (task.getRunner().getStatus() != Status.PAUSED) {\n      Thread.sleep(10);\n    }\n    final Map<Integer, Long> currentOffsets = ImmutableMap.copyOf(task.getRunner().getCurrentOffsets());\n\n    Assert.assertEquals(checkpoint1.getPartitionSequenceNumberMap(), currentOffsets);\n    task.getRunner().setEndOffsets(currentOffsets, false);\n\n    while (task.getRunner().getStatus() != Status.PAUSED) {\n      Thread.sleep(10);\n    }\n\n    \r\n    try (final KafkaProducer<byte[], byte[]> kafkaProducer = kafkaServer.newProducer()) {\n      kafkaProducer.initTransactions();\n      kafkaProducer.beginTransaction();\n      for (int i = numToAdd; i < records.size(); i++) {\n        kafkaProducer.send(records.get(i)).get();\n      }\n      kafkaProducer.commitTransaction();\n    }\n    final Map<Integer, Long> nextOffsets = ImmutableMap.copyOf(task.getRunner().getCurrentOffsets());\n\n\n    Assert.assertEquals(checkpoint2.getPartitionSequenceNumberMap(), nextOffsets);\n    task.getRunner().setEndOffsets(nextOffsets, false);\n\n    Assert.assertEquals(TaskState.SUCCESS, future.get().getStatusCode());\n\n    Assert.assertEquals(2, checkpointRequestsHash.size());\n    Assert.assertTrue(\n        checkpointRequestsHash.contains(\n            Objects.hash(\n                DATA_SCHEMA.getDataSource(),\n                0,\n                new KafkaDataSourceMetadata(startPartitions),\n                new KafkaDataSourceMetadata(\n                    new SeekableStreamEndSequenceNumbers<>(topic, currentOffsets)\n                )\n            )\n        )\n    );\n    Assert.assertTrue(\n        checkpointRequestsHash.contains(\n            Objects.hash(\n                DATA_SCHEMA.getDataSource(),\n                0,\n                new KafkaDataSourceMetadata(\n                    new SeekableStreamStartSequenceNumbers<>(topic, currentOffsets, ImmutableSet.of())\n                ),\n                new KafkaDataSourceMetadata(new SeekableStreamEndSequenceNumbers<>(topic, nextOffsets))\n            )\n        )\n    );\n\n    \r\n    Assert.assertEquals(8, task.getRunner().getRowIngestionMeters().getProcessed());\n    Assert.assertEquals(3, task.getRunner().getRowIngestionMeters().getUnparseable());\n    Assert.assertEquals(1, task.getRunner().getRowIngestionMeters().getThrownAway());\n\n    \r\n    SegmentDescriptor desc1 = sd(task, \"2008/P1D\", 0);\n    SegmentDescriptor desc2 = sd(task, \"2009/P1D\", 0);\n    SegmentDescriptor desc3 = sd(task, \"2010/P1D\", 0);\n    SegmentDescriptor desc4 = sd(task, \"2011/P1D\", 0);\n    SegmentDescriptor desc5 = sd(task, \"2011/P1D\", 1);\n    SegmentDescriptor desc6 = sd(task, \"2012/P1D\", 0);\n    SegmentDescriptor desc7 = sd(task, \"2013/P1D\", 0);\n    Assert.assertEquals(ImmutableSet.of(desc1, desc2, desc3, desc4, desc5, desc6, desc7), publishedDescriptors());\n    Assert.assertEquals(\n        new KafkaDataSourceMetadata(\n            new SeekableStreamEndSequenceNumbers<>(topic, ImmutableMap.of(0, 10L, 1, 2L))\n        ),\n        metadataStorageCoordinator.getDataSourceMetadata(DATA_SCHEMA.getDataSource())\n    );\n\n    Assert.assertEquals(ImmutableSet.of(desc1, desc2, desc3, desc4, desc5, desc6, desc7), publishedDescriptors());\n    Assert.assertEquals(\n        new KafkaDataSourceMetadata(\n            new SeekableStreamEndSequenceNumbers<>(topic, ImmutableMap.of(0, 10L, 1, 2L))\n        ),\n        metadataStorageCoordinator.getDataSourceMetadata(DATA_SCHEMA.getDataSource())\n    );\n\n    \r\n    Assert.assertEquals(ImmutableList.of(\"a\"), readSegmentColumn(\"dim1\", desc1));\n    Assert.assertEquals(ImmutableList.of(\"b\"), readSegmentColumn(\"dim1\", desc2));\n    Assert.assertEquals(ImmutableList.of(\"c\"), readSegmentColumn(\"dim1\", desc3));\n    Assert.assertTrue((ImmutableList.of(\"d\", \"e\").equals(readSegmentColumn(\"dim1\", desc4))\n                       && ImmutableList.of(\"h\").equals(readSegmentColumn(\"dim1\", desc5))) ||\n                      (ImmutableList.of(\"d\", \"h\").equals(readSegmentColumn(\"dim1\", desc4))\n                       && ImmutableList.of(\"e\").equals(readSegmentColumn(\"dim1\", desc5))));\n    Assert.assertEquals(ImmutableList.of(\"g\"), readSegmentColumn(\"dim1\", desc6));\n    Assert.assertEquals(ImmutableList.of(\"f\"), readSegmentColumn(\"dim1\", desc7));\n  }\n","realPath":"extensions-core/kafka-indexing-service/src/test/java/org/apache/druid/indexing/kafka/KafkaIndexTaskTest.java","repoName":"druid","snippetEndLine":0,"snippetStartLine":0,"startLine":609,"status":"M"},{"authorDate":"2019-07-25 08:35:46","commitOrder":11,"curCode":"  public void testTimeBasedIncrementalHandOff() throws Exception\n  {\n    final String baseSequenceName = \"sequence0\";\n    \r\n    maxRowsPerSegment = Integer.MAX_VALUE;\n    intermediateHandoffPeriod = new Period().withSeconds(0);\n\n    \r\n    insertData();\n    Map<String, Object> consumerProps = kafkaServer.consumerProperties();\n    consumerProps.put(\"max.poll.records\", \"1\");\n\n    final SeekableStreamStartSequenceNumbers<Integer, Long> startPartitions = new SeekableStreamStartSequenceNumbers<>(\n        topic,\n        ImmutableMap.of(0, 0L, 1, 0L),\n        ImmutableSet.of()\n    );\n    \r\n    final SeekableStreamEndSequenceNumbers<Integer, Long> checkpoint = new SeekableStreamEndSequenceNumbers<>(\n        topic,\n        ImmutableMap.of(0, 1L, 1, 0L)\n    );\n    final SeekableStreamEndSequenceNumbers<Integer, Long> endPartitions = new SeekableStreamEndSequenceNumbers<>(\n        topic,\n        ImmutableMap.of(0, 2L, 1, 0L)\n    );\n    final KafkaIndexTask task = createTask(\n        null,\n        new KafkaIndexTaskIOConfig(\n            0,\n            baseSequenceName,\n            startPartitions,\n            endPartitions,\n            consumerProps,\n            KafkaSupervisorIOConfig.DEFAULT_POLL_TIMEOUT_MILLIS,\n            true,\n            null,\n            null\n        )\n    );\n    final ListenableFuture<TaskStatus> future = runTask(task);\n\n    \r\n    while (task.getRunner().getStatus() != Status.PAUSED) {\n      Thread.sleep(10);\n    }\n    final Map<Integer, Long> currentOffsets = ImmutableMap.copyOf(task.getRunner().getCurrentOffsets());\n    Assert.assertEquals(checkpoint.getPartitionSequenceNumberMap(), currentOffsets);\n    task.getRunner().setEndOffsets(currentOffsets, false);\n    Assert.assertEquals(TaskState.SUCCESS, future.get().getStatusCode());\n\n    Assert.assertEquals(1, checkpointRequestsHash.size());\n    Assert.assertTrue(\n        checkpointRequestsHash.contains(\n            Objects.hash(\n                DATA_SCHEMA.getDataSource(),\n                0,\n                new KafkaDataSourceMetadata(startPartitions),\n                new KafkaDataSourceMetadata(\n                    new SeekableStreamEndSequenceNumbers<>(topic, checkpoint.getPartitionSequenceNumberMap())\n                )\n            )\n        )\n    );\n\n    \r\n    Assert.assertEquals(2, task.getRunner().getRowIngestionMeters().getProcessed());\n    Assert.assertEquals(0, task.getRunner().getRowIngestionMeters().getUnparseable());\n    Assert.assertEquals(0, task.getRunner().getRowIngestionMeters().getThrownAway());\n\n    \r\n    SegmentDescriptor desc1 = sd(\"2008/P1D\", 0);\n    SegmentDescriptor desc2 = sd(\"2009/P1D\", 0);\n    assertEqualsExceptVersion(ImmutableList.of(desc1, desc2), publishedDescriptors());\n    Assert.assertEquals(\n        new KafkaDataSourceMetadata(\n            new SeekableStreamEndSequenceNumbers<>(topic, ImmutableMap.of(0, 2L, 1, 0L))\n        ),\n        metadataStorageCoordinator.getDataSourceMetadata(DATA_SCHEMA.getDataSource())\n    );\n\n    \r\n    final List<SegmentDescriptor> publishedDescriptors = publishedDescriptors();\n    Assert.assertEquals(ImmutableList.of(\"a\"), readSegmentColumn(\"dim1\", publishedDescriptors.get(0)));\n    Assert.assertEquals(ImmutableList.of(\"b\"), readSegmentColumn(\"dim1\", publishedDescriptors.get(1)));\n  }\n","date":"2019-07-25 08:35:46","endLine":847,"groupId":"2270","id":20,"instanceNumber":2,"isCurCommit":0,"methodName":"testTimeBasedIncrementalHandOff","params":"()","path":"/mnt/clonedata/CloneManagementServer/ManagementServer/consistResult/result-druid-10-0.7/blobInfo/CC_OUT/blobs/87/f522eae18ccef1c23a5fd9ae035dbbe3410dfc.src","preCode":"  public void testTimeBasedIncrementalHandOff() throws Exception\n  {\n    final String baseSequenceName = \"sequence0\";\n    \r\n    maxRowsPerSegment = Integer.MAX_VALUE;\n    intermediateHandoffPeriod = new Period().withSeconds(0);\n\n    \r\n    insertData();\n    Map<String, Object> consumerProps = kafkaServer.consumerProperties();\n    consumerProps.put(\"max.poll.records\", \"1\");\n\n    final SeekableStreamStartSequenceNumbers<Integer, Long> startPartitions = new SeekableStreamStartSequenceNumbers<>(\n        topic,\n        ImmutableMap.of(0, 0L, 1, 0L),\n        ImmutableSet.of()\n    );\n    \r\n    final SeekableStreamEndSequenceNumbers<Integer, Long> checkpoint = new SeekableStreamEndSequenceNumbers<>(\n        topic,\n        ImmutableMap.of(0, 1L, 1, 0L)\n    );\n    final SeekableStreamEndSequenceNumbers<Integer, Long> endPartitions = new SeekableStreamEndSequenceNumbers<>(\n        topic,\n        ImmutableMap.of(0, 2L, 1, 0L)\n    );\n    final KafkaIndexTask task = createTask(\n        null,\n        new KafkaIndexTaskIOConfig(\n            0,\n            baseSequenceName,\n            startPartitions,\n            endPartitions,\n            consumerProps,\n            KafkaSupervisorIOConfig.DEFAULT_POLL_TIMEOUT_MILLIS,\n            true,\n            null,\n            null\n        )\n    );\n    final ListenableFuture<TaskStatus> future = runTask(task);\n\n    \r\n    while (task.getRunner().getStatus() != Status.PAUSED) {\n      Thread.sleep(10);\n    }\n    final Map<Integer, Long> currentOffsets = ImmutableMap.copyOf(task.getRunner().getCurrentOffsets());\n    Assert.assertEquals(checkpoint.getPartitionSequenceNumberMap(), currentOffsets);\n    task.getRunner().setEndOffsets(currentOffsets, false);\n    Assert.assertEquals(TaskState.SUCCESS, future.get().getStatusCode());\n\n    Assert.assertEquals(1, checkpointRequestsHash.size());\n    Assert.assertTrue(\n        checkpointRequestsHash.contains(\n            Objects.hash(\n                DATA_SCHEMA.getDataSource(),\n                0,\n                new KafkaDataSourceMetadata(startPartitions),\n                new KafkaDataSourceMetadata(\n                    new SeekableStreamEndSequenceNumbers<>(topic, checkpoint.getPartitionSequenceNumberMap())\n                )\n            )\n        )\n    );\n\n    \r\n    Assert.assertEquals(2, task.getRunner().getRowIngestionMeters().getProcessed());\n    Assert.assertEquals(0, task.getRunner().getRowIngestionMeters().getUnparseable());\n    Assert.assertEquals(0, task.getRunner().getRowIngestionMeters().getThrownAway());\n\n    \r\n    SegmentDescriptor desc1 = sd(task, \"2008/P1D\", 0);\n    SegmentDescriptor desc2 = sd(task, \"2009/P1D\", 0);\n    Assert.assertEquals(ImmutableSet.of(desc1, desc2), publishedDescriptors());\n    Assert.assertEquals(\n        new KafkaDataSourceMetadata(\n            new SeekableStreamEndSequenceNumbers<>(topic, ImmutableMap.of(0, 2L, 1, 0L))\n        ),\n        metadataStorageCoordinator.getDataSourceMetadata(DATA_SCHEMA.getDataSource())\n    );\n\n    \r\n    Assert.assertEquals(ImmutableList.of(\"a\"), readSegmentColumn(\"dim1\", desc1));\n    Assert.assertEquals(ImmutableList.of(\"b\"), readSegmentColumn(\"dim1\", desc2));\n  }\n","realPath":"extensions-core/kafka-indexing-service/src/test/java/org/apache/druid/indexing/kafka/KafkaIndexTaskTest.java","repoName":"druid","snippetEndLine":0,"snippetStartLine":0,"startLine":762,"status":"M"}],"commitId":"db149462073d59e7563f0d3834e69d44a2bb4011","commitMessage":"@@@Add support minor compaction with segment locking (#7547)\n\n* Segment locking\n\n* Allow both timeChunk and segment lock in the same gruop\n\n* fix it test\n\n* Fix adding same chunk to atomicUpdateGroup\n\n* resolving todos\n\n* Fix segments to lock\n\n* fix segments to lock\n\n* fix kill task\n\n* resolving todos\n\n* resolving todos\n\n* fix teamcity\n\n* remove unused class\n\n* fix single map\n\n* resolving todos\n\n* fix build\n\n* fix SQLMetadataSegmentManager\n\n* fix findInputSegments\n\n* adding more tests\n\n* fixing task lock checks\n\n* add SegmentTransactionalOverwriteAction\n\n* changing publisher\n\n* fixing something\n\n* fix for perfect rollup\n\n* fix test\n\n* adjust package-lock.json\n\n* fix test\n\n* fix style\n\n* adding javadocs\n\n* remove unused classes\n\n* add more javadocs\n\n* unused import\n\n* fix test\n\n* fix test\n\n* Support forceTimeChunk context and force timeChunk lock for parallel index task if intervals are missing\n\n* fix travis\n\n* fix travis\n\n* unused import\n\n* spotbug\n\n* revert getMaxVersion\n\n* address comments\n\n* fix tc\n\n* add missing error handling\n\n* fix backward compatibility\n\n* unused import\n\n* Fix perf of versionedIntervalTimeline\n\n* fix timeline\n\n* fix tc\n\n* remove remaining todos\n\n* add comment for parallel index\n\n* fix javadoc and typos\n\n* typo\n\n* address comments\n","date":"2019-07-25 08:35:46","modifiedFileCount":"130","status":"M","submitter":"Jihoon Son"},{"authorTime":"2019-08-22 01:58:22","codes":[{"authorDate":"2019-08-22 01:58:22","commitOrder":12,"curCode":"  public void testIncrementalHandOffMaxTotalRows() throws Exception\n  {\n    final String baseSequenceName = \"sequence0\";\n    \r\n    maxRowsPerSegment = Integer.MAX_VALUE;\n    maxTotalRows = 3L;\n\n    \r\n    int numToAdd = records.size() - 2;\n\n    try (final KafkaProducer<byte[], byte[]> kafkaProducer = kafkaServer.newProducer()) {\n      kafkaProducer.initTransactions();\n      kafkaProducer.beginTransaction();\n      for (int i = 0; i < numToAdd; i++) {\n        kafkaProducer.send(records.get(i)).get();\n      }\n      kafkaProducer.commitTransaction();\n    }\n\n    Map<String, Object> consumerProps = kafkaServer.consumerProperties();\n    consumerProps.put(\"max.poll.records\", \"1\");\n\n    final SeekableStreamStartSequenceNumbers<Integer, Long> startPartitions = new SeekableStreamStartSequenceNumbers<>(\n        topic,\n        ImmutableMap.of(0, 0L, 1, 0L),\n        ImmutableSet.of()\n    );\n    final SeekableStreamEndSequenceNumbers<Integer, Long> checkpoint1 = new SeekableStreamEndSequenceNumbers<>(\n        topic,\n        ImmutableMap.of(0, 3L, 1, 0L)\n    );\n    final SeekableStreamEndSequenceNumbers<Integer, Long> checkpoint2 = new SeekableStreamEndSequenceNumbers<>(\n        topic,\n        ImmutableMap.of(0, 10L, 1, 0L)\n    );\n\n    final SeekableStreamEndSequenceNumbers<Integer, Long> endPartitions = new SeekableStreamEndSequenceNumbers<>(\n        topic,\n        ImmutableMap.of(0, 10L, 1, 2L)\n    );\n    final KafkaIndexTask task = createTask(\n        null,\n        new KafkaIndexTaskIOConfig(\n            0,\n            baseSequenceName,\n            startPartitions,\n            endPartitions,\n            consumerProps,\n            KafkaSupervisorIOConfig.DEFAULT_POLL_TIMEOUT_MILLIS,\n            true,\n            null,\n            null\n        )\n    );\n    final ListenableFuture<TaskStatus> future = runTask(task);\n    while (task.getRunner().getStatus() != Status.PAUSED) {\n      Thread.sleep(10);\n    }\n    final Map<Integer, Long> currentOffsets = ImmutableMap.copyOf(task.getRunner().getCurrentOffsets());\n\n    Assert.assertEquals(checkpoint1.getPartitionSequenceNumberMap(), currentOffsets);\n    task.getRunner().setEndOffsets(currentOffsets, false);\n\n    while (task.getRunner().getStatus() != Status.PAUSED) {\n      Thread.sleep(10);\n    }\n\n    \r\n    try (final KafkaProducer<byte[], byte[]> kafkaProducer = kafkaServer.newProducer()) {\n      kafkaProducer.initTransactions();\n      kafkaProducer.beginTransaction();\n      for (int i = numToAdd; i < records.size(); i++) {\n        kafkaProducer.send(records.get(i)).get();\n      }\n      kafkaProducer.commitTransaction();\n    }\n    final Map<Integer, Long> nextOffsets = ImmutableMap.copyOf(task.getRunner().getCurrentOffsets());\n\n\n    Assert.assertEquals(checkpoint2.getPartitionSequenceNumberMap(), nextOffsets);\n    task.getRunner().setEndOffsets(nextOffsets, false);\n\n    Assert.assertEquals(TaskState.SUCCESS, future.get().getStatusCode());\n\n    Assert.assertEquals(2, checkpointRequestsHash.size());\n    Assert.assertTrue(\n        checkpointRequestsHash.contains(\n            Objects.hash(\n                DATA_SCHEMA.getDataSource(),\n                0,\n                new KafkaDataSourceMetadata(startPartitions)\n            )\n        )\n    );\n    Assert.assertTrue(\n        checkpointRequestsHash.contains(\n            Objects.hash(\n                DATA_SCHEMA.getDataSource(),\n                0,\n                new KafkaDataSourceMetadata(\n                    new SeekableStreamStartSequenceNumbers<>(topic, currentOffsets, ImmutableSet.of())\n                )\n            )\n        )\n    );\n\n    \r\n    Assert.assertEquals(8, task.getRunner().getRowIngestionMeters().getProcessed());\n    Assert.assertEquals(3, task.getRunner().getRowIngestionMeters().getUnparseable());\n    Assert.assertEquals(1, task.getRunner().getRowIngestionMeters().getThrownAway());\n\n    \r\n    SegmentDescriptor desc1 = sd(\"2008/P1D\", 0);\n    SegmentDescriptor desc2 = sd(\"2009/P1D\", 0);\n    SegmentDescriptor desc3 = sd(\"2010/P1D\", 0);\n    SegmentDescriptor desc4 = sd(\"2011/P1D\", 0);\n    SegmentDescriptor desc5 = sd(\"2011/P1D\", 1);\n    SegmentDescriptor desc6 = sd(\"2012/P1D\", 0);\n    SegmentDescriptor desc7 = sd(\"2013/P1D\", 0);\n    assertEqualsExceptVersion(ImmutableList.of(desc1, desc2, desc3, desc4, desc5, desc6, desc7), publishedDescriptors());\n    Assert.assertEquals(\n        new KafkaDataSourceMetadata(\n            new SeekableStreamEndSequenceNumbers<>(topic, ImmutableMap.of(0, 10L, 1, 2L))\n        ),\n        metadataStorageCoordinator.getDataSourceMetadata(DATA_SCHEMA.getDataSource())\n    );\n\n    assertEqualsExceptVersion(ImmutableList.of(desc1, desc2, desc3, desc4, desc5, desc6, desc7), publishedDescriptors());\n    Assert.assertEquals(\n        new KafkaDataSourceMetadata(\n            new SeekableStreamEndSequenceNumbers<>(topic, ImmutableMap.of(0, 10L, 1, 2L))\n        ),\n        metadataStorageCoordinator.getDataSourceMetadata(DATA_SCHEMA.getDataSource())\n    );\n\n    \r\n    final List<SegmentDescriptor> publishedDescriptors = publishedDescriptors();\n    Assert.assertEquals(ImmutableList.of(\"a\"), readSegmentColumn(\"dim1\", publishedDescriptors.get(0)));\n    Assert.assertEquals(ImmutableList.of(\"b\"), readSegmentColumn(\"dim1\", publishedDescriptors.get(1)));\n    Assert.assertEquals(ImmutableList.of(\"c\"), readSegmentColumn(\"dim1\", publishedDescriptors.get(2)));\n    Assert.assertTrue((ImmutableList.of(\"d\", \"e\").equals(readSegmentColumn(\"dim1\", publishedDescriptors.get(3)))\n                       && ImmutableList.of(\"h\").equals(readSegmentColumn(\"dim1\", publishedDescriptors.get(4)))) ||\n                      (ImmutableList.of(\"d\", \"h\").equals(readSegmentColumn(\"dim1\", publishedDescriptors.get(3)))\n                       && ImmutableList.of(\"e\").equals(readSegmentColumn(\"dim1\", publishedDescriptors.get(4)))));\n    Assert.assertEquals(ImmutableList.of(\"g\"), readSegmentColumn(\"dim1\", publishedDescriptors.get(5)));\n    Assert.assertEquals(ImmutableList.of(\"f\"), readSegmentColumn(\"dim1\", publishedDescriptors.get(6)));\n  }\n","date":"2019-08-22 01:58:22","endLine":756,"groupId":"15102","id":21,"instanceNumber":1,"isCurCommit":0,"methodName":"testIncrementalHandOffMaxTotalRows","params":"()","path":"/mnt/clonedata/CloneManagementServer/ManagementServer/consistResult/result-druid-10-0.7/blobInfo/CC_OUT/blobs/14/55d2ab29c9b3d7854f443cccacf03a9f645d06.src","preCode":"  public void testIncrementalHandOffMaxTotalRows() throws Exception\n  {\n    final String baseSequenceName = \"sequence0\";\n    \r\n    maxRowsPerSegment = Integer.MAX_VALUE;\n    maxTotalRows = 3L;\n\n    \r\n    int numToAdd = records.size() - 2;\n\n    try (final KafkaProducer<byte[], byte[]> kafkaProducer = kafkaServer.newProducer()) {\n      kafkaProducer.initTransactions();\n      kafkaProducer.beginTransaction();\n      for (int i = 0; i < numToAdd; i++) {\n        kafkaProducer.send(records.get(i)).get();\n      }\n      kafkaProducer.commitTransaction();\n    }\n\n    Map<String, Object> consumerProps = kafkaServer.consumerProperties();\n    consumerProps.put(\"max.poll.records\", \"1\");\n\n    final SeekableStreamStartSequenceNumbers<Integer, Long> startPartitions = new SeekableStreamStartSequenceNumbers<>(\n        topic,\n        ImmutableMap.of(0, 0L, 1, 0L),\n        ImmutableSet.of()\n    );\n    final SeekableStreamEndSequenceNumbers<Integer, Long> checkpoint1 = new SeekableStreamEndSequenceNumbers<>(\n        topic,\n        ImmutableMap.of(0, 3L, 1, 0L)\n    );\n    final SeekableStreamEndSequenceNumbers<Integer, Long> checkpoint2 = new SeekableStreamEndSequenceNumbers<>(\n        topic,\n        ImmutableMap.of(0, 10L, 1, 0L)\n    );\n\n    final SeekableStreamEndSequenceNumbers<Integer, Long> endPartitions = new SeekableStreamEndSequenceNumbers<>(\n        topic,\n        ImmutableMap.of(0, 10L, 1, 2L)\n    );\n    final KafkaIndexTask task = createTask(\n        null,\n        new KafkaIndexTaskIOConfig(\n            0,\n            baseSequenceName,\n            startPartitions,\n            endPartitions,\n            consumerProps,\n            KafkaSupervisorIOConfig.DEFAULT_POLL_TIMEOUT_MILLIS,\n            true,\n            null,\n            null\n        )\n    );\n    final ListenableFuture<TaskStatus> future = runTask(task);\n    while (task.getRunner().getStatus() != Status.PAUSED) {\n      Thread.sleep(10);\n    }\n    final Map<Integer, Long> currentOffsets = ImmutableMap.copyOf(task.getRunner().getCurrentOffsets());\n\n    Assert.assertEquals(checkpoint1.getPartitionSequenceNumberMap(), currentOffsets);\n    task.getRunner().setEndOffsets(currentOffsets, false);\n\n    while (task.getRunner().getStatus() != Status.PAUSED) {\n      Thread.sleep(10);\n    }\n\n    \r\n    try (final KafkaProducer<byte[], byte[]> kafkaProducer = kafkaServer.newProducer()) {\n      kafkaProducer.initTransactions();\n      kafkaProducer.beginTransaction();\n      for (int i = numToAdd; i < records.size(); i++) {\n        kafkaProducer.send(records.get(i)).get();\n      }\n      kafkaProducer.commitTransaction();\n    }\n    final Map<Integer, Long> nextOffsets = ImmutableMap.copyOf(task.getRunner().getCurrentOffsets());\n\n\n    Assert.assertEquals(checkpoint2.getPartitionSequenceNumberMap(), nextOffsets);\n    task.getRunner().setEndOffsets(nextOffsets, false);\n\n    Assert.assertEquals(TaskState.SUCCESS, future.get().getStatusCode());\n\n    Assert.assertEquals(2, checkpointRequestsHash.size());\n    Assert.assertTrue(\n        checkpointRequestsHash.contains(\n            Objects.hash(\n                DATA_SCHEMA.getDataSource(),\n                0,\n                new KafkaDataSourceMetadata(startPartitions),\n                new KafkaDataSourceMetadata(\n                    new SeekableStreamEndSequenceNumbers<>(topic, currentOffsets)\n                )\n            )\n        )\n    );\n    Assert.assertTrue(\n        checkpointRequestsHash.contains(\n            Objects.hash(\n                DATA_SCHEMA.getDataSource(),\n                0,\n                new KafkaDataSourceMetadata(\n                    new SeekableStreamStartSequenceNumbers<>(topic, currentOffsets, ImmutableSet.of())\n                ),\n                new KafkaDataSourceMetadata(new SeekableStreamEndSequenceNumbers<>(topic, nextOffsets))\n            )\n        )\n    );\n\n    \r\n    Assert.assertEquals(8, task.getRunner().getRowIngestionMeters().getProcessed());\n    Assert.assertEquals(3, task.getRunner().getRowIngestionMeters().getUnparseable());\n    Assert.assertEquals(1, task.getRunner().getRowIngestionMeters().getThrownAway());\n\n    \r\n    SegmentDescriptor desc1 = sd(\"2008/P1D\", 0);\n    SegmentDescriptor desc2 = sd(\"2009/P1D\", 0);\n    SegmentDescriptor desc3 = sd(\"2010/P1D\", 0);\n    SegmentDescriptor desc4 = sd(\"2011/P1D\", 0);\n    SegmentDescriptor desc5 = sd(\"2011/P1D\", 1);\n    SegmentDescriptor desc6 = sd(\"2012/P1D\", 0);\n    SegmentDescriptor desc7 = sd(\"2013/P1D\", 0);\n    assertEqualsExceptVersion(ImmutableList.of(desc1, desc2, desc3, desc4, desc5, desc6, desc7), publishedDescriptors());\n    Assert.assertEquals(\n        new KafkaDataSourceMetadata(\n            new SeekableStreamEndSequenceNumbers<>(topic, ImmutableMap.of(0, 10L, 1, 2L))\n        ),\n        metadataStorageCoordinator.getDataSourceMetadata(DATA_SCHEMA.getDataSource())\n    );\n\n    assertEqualsExceptVersion(ImmutableList.of(desc1, desc2, desc3, desc4, desc5, desc6, desc7), publishedDescriptors());\n    Assert.assertEquals(\n        new KafkaDataSourceMetadata(\n            new SeekableStreamEndSequenceNumbers<>(topic, ImmutableMap.of(0, 10L, 1, 2L))\n        ),\n        metadataStorageCoordinator.getDataSourceMetadata(DATA_SCHEMA.getDataSource())\n    );\n\n    \r\n    final List<SegmentDescriptor> publishedDescriptors = publishedDescriptors();\n    Assert.assertEquals(ImmutableList.of(\"a\"), readSegmentColumn(\"dim1\", publishedDescriptors.get(0)));\n    Assert.assertEquals(ImmutableList.of(\"b\"), readSegmentColumn(\"dim1\", publishedDescriptors.get(1)));\n    Assert.assertEquals(ImmutableList.of(\"c\"), readSegmentColumn(\"dim1\", publishedDescriptors.get(2)));\n    Assert.assertTrue((ImmutableList.of(\"d\", \"e\").equals(readSegmentColumn(\"dim1\", publishedDescriptors.get(3)))\n                       && ImmutableList.of(\"h\").equals(readSegmentColumn(\"dim1\", publishedDescriptors.get(4)))) ||\n                      (ImmutableList.of(\"d\", \"h\").equals(readSegmentColumn(\"dim1\", publishedDescriptors.get(3)))\n                       && ImmutableList.of(\"e\").equals(readSegmentColumn(\"dim1\", publishedDescriptors.get(4)))));\n    Assert.assertEquals(ImmutableList.of(\"g\"), readSegmentColumn(\"dim1\", publishedDescriptors.get(5)));\n    Assert.assertEquals(ImmutableList.of(\"f\"), readSegmentColumn(\"dim1\", publishedDescriptors.get(6)));\n  }\n","realPath":"extensions-core/kafka-indexing-service/src/test/java/org/apache/druid/indexing/kafka/KafkaIndexTaskTest.java","repoName":"druid","snippetEndLine":0,"snippetStartLine":0,"startLine":610,"status":"M"},{"authorDate":"2019-08-22 01:58:22","commitOrder":12,"curCode":"  public void testTimeBasedIncrementalHandOff() throws Exception\n  {\n    final String baseSequenceName = \"sequence0\";\n    \r\n    maxRowsPerSegment = Integer.MAX_VALUE;\n    intermediateHandoffPeriod = new Period().withSeconds(0);\n\n    \r\n    insertData();\n    Map<String, Object> consumerProps = kafkaServer.consumerProperties();\n    consumerProps.put(\"max.poll.records\", \"1\");\n\n    final SeekableStreamStartSequenceNumbers<Integer, Long> startPartitions = new SeekableStreamStartSequenceNumbers<>(\n        topic,\n        ImmutableMap.of(0, 0L, 1, 0L),\n        ImmutableSet.of()\n    );\n    \r\n    final SeekableStreamEndSequenceNumbers<Integer, Long> checkpoint = new SeekableStreamEndSequenceNumbers<>(\n        topic,\n        ImmutableMap.of(0, 1L, 1, 0L)\n    );\n    final SeekableStreamEndSequenceNumbers<Integer, Long> endPartitions = new SeekableStreamEndSequenceNumbers<>(\n        topic,\n        ImmutableMap.of(0, 2L, 1, 0L)\n    );\n    final KafkaIndexTask task = createTask(\n        null,\n        new KafkaIndexTaskIOConfig(\n            0,\n            baseSequenceName,\n            startPartitions,\n            endPartitions,\n            consumerProps,\n            KafkaSupervisorIOConfig.DEFAULT_POLL_TIMEOUT_MILLIS,\n            true,\n            null,\n            null\n        )\n    );\n    final ListenableFuture<TaskStatus> future = runTask(task);\n\n    \r\n    while (task.getRunner().getStatus() != Status.PAUSED) {\n      Thread.sleep(10);\n    }\n    final Map<Integer, Long> currentOffsets = ImmutableMap.copyOf(task.getRunner().getCurrentOffsets());\n    Assert.assertEquals(checkpoint.getPartitionSequenceNumberMap(), currentOffsets);\n    task.getRunner().setEndOffsets(currentOffsets, false);\n    Assert.assertEquals(TaskState.SUCCESS, future.get().getStatusCode());\n\n    Assert.assertEquals(1, checkpointRequestsHash.size());\n    Assert.assertTrue(\n        checkpointRequestsHash.contains(\n            Objects.hash(\n                DATA_SCHEMA.getDataSource(),\n                0,\n                new KafkaDataSourceMetadata(startPartitions)\n            )\n        )\n    );\n\n    \r\n    Assert.assertEquals(2, task.getRunner().getRowIngestionMeters().getProcessed());\n    Assert.assertEquals(0, task.getRunner().getRowIngestionMeters().getUnparseable());\n    Assert.assertEquals(0, task.getRunner().getRowIngestionMeters().getThrownAway());\n\n    \r\n    SegmentDescriptor desc1 = sd(\"2008/P1D\", 0);\n    SegmentDescriptor desc2 = sd(\"2009/P1D\", 0);\n    assertEqualsExceptVersion(ImmutableList.of(desc1, desc2), publishedDescriptors());\n    Assert.assertEquals(\n        new KafkaDataSourceMetadata(\n            new SeekableStreamEndSequenceNumbers<>(topic, ImmutableMap.of(0, 2L, 1, 0L))\n        ),\n        metadataStorageCoordinator.getDataSourceMetadata(DATA_SCHEMA.getDataSource())\n    );\n\n    \r\n    final List<SegmentDescriptor> publishedDescriptors = publishedDescriptors();\n    Assert.assertEquals(ImmutableList.of(\"a\"), readSegmentColumn(\"dim1\", publishedDescriptors.get(0)));\n    Assert.assertEquals(ImmutableList.of(\"b\"), readSegmentColumn(\"dim1\", publishedDescriptors.get(1)));\n  }\n","date":"2019-08-22 01:58:22","endLine":841,"groupId":"17791","id":22,"instanceNumber":2,"isCurCommit":0,"methodName":"testTimeBasedIncrementalHandOff","params":"()","path":"/mnt/clonedata/CloneManagementServer/ManagementServer/consistResult/result-druid-10-0.7/blobInfo/CC_OUT/blobs/14/55d2ab29c9b3d7854f443cccacf03a9f645d06.src","preCode":"  public void testTimeBasedIncrementalHandOff() throws Exception\n  {\n    final String baseSequenceName = \"sequence0\";\n    \r\n    maxRowsPerSegment = Integer.MAX_VALUE;\n    intermediateHandoffPeriod = new Period().withSeconds(0);\n\n    \r\n    insertData();\n    Map<String, Object> consumerProps = kafkaServer.consumerProperties();\n    consumerProps.put(\"max.poll.records\", \"1\");\n\n    final SeekableStreamStartSequenceNumbers<Integer, Long> startPartitions = new SeekableStreamStartSequenceNumbers<>(\n        topic,\n        ImmutableMap.of(0, 0L, 1, 0L),\n        ImmutableSet.of()\n    );\n    \r\n    final SeekableStreamEndSequenceNumbers<Integer, Long> checkpoint = new SeekableStreamEndSequenceNumbers<>(\n        topic,\n        ImmutableMap.of(0, 1L, 1, 0L)\n    );\n    final SeekableStreamEndSequenceNumbers<Integer, Long> endPartitions = new SeekableStreamEndSequenceNumbers<>(\n        topic,\n        ImmutableMap.of(0, 2L, 1, 0L)\n    );\n    final KafkaIndexTask task = createTask(\n        null,\n        new KafkaIndexTaskIOConfig(\n            0,\n            baseSequenceName,\n            startPartitions,\n            endPartitions,\n            consumerProps,\n            KafkaSupervisorIOConfig.DEFAULT_POLL_TIMEOUT_MILLIS,\n            true,\n            null,\n            null\n        )\n    );\n    final ListenableFuture<TaskStatus> future = runTask(task);\n\n    \r\n    while (task.getRunner().getStatus() != Status.PAUSED) {\n      Thread.sleep(10);\n    }\n    final Map<Integer, Long> currentOffsets = ImmutableMap.copyOf(task.getRunner().getCurrentOffsets());\n    Assert.assertEquals(checkpoint.getPartitionSequenceNumberMap(), currentOffsets);\n    task.getRunner().setEndOffsets(currentOffsets, false);\n    Assert.assertEquals(TaskState.SUCCESS, future.get().getStatusCode());\n\n    Assert.assertEquals(1, checkpointRequestsHash.size());\n    Assert.assertTrue(\n        checkpointRequestsHash.contains(\n            Objects.hash(\n                DATA_SCHEMA.getDataSource(),\n                0,\n                new KafkaDataSourceMetadata(startPartitions),\n                new KafkaDataSourceMetadata(\n                    new SeekableStreamEndSequenceNumbers<>(topic, checkpoint.getPartitionSequenceNumberMap())\n                )\n            )\n        )\n    );\n\n    \r\n    Assert.assertEquals(2, task.getRunner().getRowIngestionMeters().getProcessed());\n    Assert.assertEquals(0, task.getRunner().getRowIngestionMeters().getUnparseable());\n    Assert.assertEquals(0, task.getRunner().getRowIngestionMeters().getThrownAway());\n\n    \r\n    SegmentDescriptor desc1 = sd(\"2008/P1D\", 0);\n    SegmentDescriptor desc2 = sd(\"2009/P1D\", 0);\n    assertEqualsExceptVersion(ImmutableList.of(desc1, desc2), publishedDescriptors());\n    Assert.assertEquals(\n        new KafkaDataSourceMetadata(\n            new SeekableStreamEndSequenceNumbers<>(topic, ImmutableMap.of(0, 2L, 1, 0L))\n        ),\n        metadataStorageCoordinator.getDataSourceMetadata(DATA_SCHEMA.getDataSource())\n    );\n\n    \r\n    final List<SegmentDescriptor> publishedDescriptors = publishedDescriptors();\n    Assert.assertEquals(ImmutableList.of(\"a\"), readSegmentColumn(\"dim1\", publishedDescriptors.get(0)));\n    Assert.assertEquals(ImmutableList.of(\"b\"), readSegmentColumn(\"dim1\", publishedDescriptors.get(1)));\n  }\n","realPath":"extensions-core/kafka-indexing-service/src/test/java/org/apache/druid/indexing/kafka/KafkaIndexTaskTest.java","repoName":"druid","snippetEndLine":0,"snippetStartLine":0,"startLine":759,"status":"M"}],"commitId":"22d6384d364a851fd2b5cecafea9d72b004cb03b","commitMessage":"@@@Fix unrealistic test variables in KafkaSupervisorTest and tidy up unused variable in checkpointing process (#7319)\n\n* Fix unrealistic test arguments in KafkaSupervisorTest\n\n* remove currentCheckpoint from checkpoint action\n\n* rename variable\n","date":"2019-08-22 01:58:22","modifiedFileCount":"12","status":"M","submitter":"Jihoon Son"},{"authorTime":"2019-11-07 03:07:04","codes":[{"authorDate":"2019-11-07 03:07:04","commitOrder":13,"curCode":"  public void testIncrementalHandOffMaxTotalRows() throws Exception\n  {\n    final String baseSequenceName = \"sequence0\";\n    \r\n    maxRowsPerSegment = Integer.MAX_VALUE;\n    maxTotalRows = 3L;\n\n    \r\n    int numToAdd = records.size() - 2;\n\n    try (final KafkaProducer<byte[], byte[]> kafkaProducer = kafkaServer.newProducer()) {\n      kafkaProducer.initTransactions();\n      kafkaProducer.beginTransaction();\n      for (int i = 0; i < numToAdd; i++) {\n        kafkaProducer.send(records.get(i)).get();\n      }\n      kafkaProducer.commitTransaction();\n    }\n\n    Map<String, Object> consumerProps = kafkaServer.consumerProperties();\n    consumerProps.put(\"max.poll.records\", \"1\");\n\n    final SeekableStreamStartSequenceNumbers<Integer, Long> startPartitions = new SeekableStreamStartSequenceNumbers<>(\n        topic,\n        ImmutableMap.of(0, 0L, 1, 0L),\n        ImmutableSet.of()\n    );\n    final SeekableStreamEndSequenceNumbers<Integer, Long> checkpoint1 = new SeekableStreamEndSequenceNumbers<>(\n        topic,\n        ImmutableMap.of(0, 3L, 1, 0L)\n    );\n    final SeekableStreamEndSequenceNumbers<Integer, Long> checkpoint2 = new SeekableStreamEndSequenceNumbers<>(\n        topic,\n        ImmutableMap.of(0, 10L, 1, 0L)\n    );\n\n    final SeekableStreamEndSequenceNumbers<Integer, Long> endPartitions = new SeekableStreamEndSequenceNumbers<>(\n        topic,\n        ImmutableMap.of(0, 10L, 1, 2L)\n    );\n    final KafkaIndexTask task = createTask(\n        null,\n        new KafkaIndexTaskIOConfig(\n            0,\n            baseSequenceName,\n            startPartitions,\n            endPartitions,\n            consumerProps,\n            KafkaSupervisorIOConfig.DEFAULT_POLL_TIMEOUT_MILLIS,\n            true,\n            null,\n            null\n        )\n    );\n    final ListenableFuture<TaskStatus> future = runTask(task);\n    while (task.getRunner().getStatus() != Status.PAUSED) {\n      Thread.sleep(10);\n    }\n    final Map<Integer, Long> currentOffsets = ImmutableMap.copyOf(task.getRunner().getCurrentOffsets());\n\n    Assert.assertEquals(checkpoint1.getPartitionSequenceNumberMap(), currentOffsets);\n    task.getRunner().setEndOffsets(currentOffsets, false);\n\n    while (task.getRunner().getStatus() != Status.PAUSED) {\n      Thread.sleep(10);\n    }\n\n    \r\n    try (final KafkaProducer<byte[], byte[]> kafkaProducer = kafkaServer.newProducer()) {\n      kafkaProducer.initTransactions();\n      kafkaProducer.beginTransaction();\n      for (int i = numToAdd; i < records.size(); i++) {\n        kafkaProducer.send(records.get(i)).get();\n      }\n      kafkaProducer.commitTransaction();\n    }\n    final Map<Integer, Long> nextOffsets = ImmutableMap.copyOf(task.getRunner().getCurrentOffsets());\n\n\n    Assert.assertEquals(checkpoint2.getPartitionSequenceNumberMap(), nextOffsets);\n    task.getRunner().setEndOffsets(nextOffsets, false);\n\n    Assert.assertEquals(TaskState.SUCCESS, future.get().getStatusCode());\n\n    Assert.assertEquals(2, checkpointRequestsHash.size());\n    Assert.assertTrue(\n        checkpointRequestsHash.contains(\n            Objects.hash(\n                DATA_SCHEMA.getDataSource(),\n                0,\n                new KafkaDataSourceMetadata(startPartitions)\n            )\n        )\n    );\n    Assert.assertTrue(\n        checkpointRequestsHash.contains(\n            Objects.hash(\n                DATA_SCHEMA.getDataSource(),\n                0,\n                new KafkaDataSourceMetadata(\n                    new SeekableStreamStartSequenceNumbers<>(topic, currentOffsets, ImmutableSet.of())\n                )\n            )\n        )\n    );\n\n    \r\n    Assert.assertEquals(8, task.getRunner().getRowIngestionMeters().getProcessed());\n    Assert.assertEquals(3, task.getRunner().getRowIngestionMeters().getUnparseable());\n    Assert.assertEquals(1, task.getRunner().getRowIngestionMeters().getThrownAway());\n\n    \r\n    assertEqualsExceptVersion(\n        ImmutableList.of(\n            sdd(\"2008/P1D\", 0, ImmutableList.of(\"a\")),\n            sdd(\"2009/P1D\", 0, ImmutableList.of(\"b\")),\n            sdd(\"2010/P1D\", 0, ImmutableList.of(\"c\")),\n            sdd(\"2011/P1D\", 0, ImmutableList.of(\"d\", \"e\"), ImmutableList.of(\"d\", \"h\")),\n            sdd(\"2011/P1D\", 1, ImmutableList.of(\"h\"), ImmutableList.of(\"e\")),\n            sdd(\"2012/P1D\", 0, ImmutableList.of(\"g\")),\n            sdd(\"2013/P1D\", 0, ImmutableList.of(\"f\"))\n        ),\n        publishedDescriptors()\n    );\n    Assert.assertEquals(\n        new KafkaDataSourceMetadata(\n            new SeekableStreamEndSequenceNumbers<>(topic, ImmutableMap.of(0, 10L, 1, 2L))\n        ),\n        metadataStorageCoordinator.getDataSourceMetadata(DATA_SCHEMA.getDataSource())\n    );\n\n    Assert.assertEquals(\n        new KafkaDataSourceMetadata(\n            new SeekableStreamEndSequenceNumbers<>(topic, ImmutableMap.of(0, 10L, 1, 2L))\n        ),\n        metadataStorageCoordinator.getDataSourceMetadata(DATA_SCHEMA.getDataSource())\n    );\n  }\n","date":"2019-11-07 03:07:04","endLine":704,"groupId":"15102","id":23,"instanceNumber":1,"isCurCommit":0,"methodName":"testIncrementalHandOffMaxTotalRows","params":"()","path":"/mnt/clonedata/CloneManagementServer/ManagementServer/consistResult/result-druid-10-0.7/blobInfo/CC_OUT/blobs/c8/35bb95038567e2b9b3112cb98971f6d2418766.src","preCode":"  public void testIncrementalHandOffMaxTotalRows() throws Exception\n  {\n    final String baseSequenceName = \"sequence0\";\n    \r\n    maxRowsPerSegment = Integer.MAX_VALUE;\n    maxTotalRows = 3L;\n\n    \r\n    int numToAdd = records.size() - 2;\n\n    try (final KafkaProducer<byte[], byte[]> kafkaProducer = kafkaServer.newProducer()) {\n      kafkaProducer.initTransactions();\n      kafkaProducer.beginTransaction();\n      for (int i = 0; i < numToAdd; i++) {\n        kafkaProducer.send(records.get(i)).get();\n      }\n      kafkaProducer.commitTransaction();\n    }\n\n    Map<String, Object> consumerProps = kafkaServer.consumerProperties();\n    consumerProps.put(\"max.poll.records\", \"1\");\n\n    final SeekableStreamStartSequenceNumbers<Integer, Long> startPartitions = new SeekableStreamStartSequenceNumbers<>(\n        topic,\n        ImmutableMap.of(0, 0L, 1, 0L),\n        ImmutableSet.of()\n    );\n    final SeekableStreamEndSequenceNumbers<Integer, Long> checkpoint1 = new SeekableStreamEndSequenceNumbers<>(\n        topic,\n        ImmutableMap.of(0, 3L, 1, 0L)\n    );\n    final SeekableStreamEndSequenceNumbers<Integer, Long> checkpoint2 = new SeekableStreamEndSequenceNumbers<>(\n        topic,\n        ImmutableMap.of(0, 10L, 1, 0L)\n    );\n\n    final SeekableStreamEndSequenceNumbers<Integer, Long> endPartitions = new SeekableStreamEndSequenceNumbers<>(\n        topic,\n        ImmutableMap.of(0, 10L, 1, 2L)\n    );\n    final KafkaIndexTask task = createTask(\n        null,\n        new KafkaIndexTaskIOConfig(\n            0,\n            baseSequenceName,\n            startPartitions,\n            endPartitions,\n            consumerProps,\n            KafkaSupervisorIOConfig.DEFAULT_POLL_TIMEOUT_MILLIS,\n            true,\n            null,\n            null\n        )\n    );\n    final ListenableFuture<TaskStatus> future = runTask(task);\n    while (task.getRunner().getStatus() != Status.PAUSED) {\n      Thread.sleep(10);\n    }\n    final Map<Integer, Long> currentOffsets = ImmutableMap.copyOf(task.getRunner().getCurrentOffsets());\n\n    Assert.assertEquals(checkpoint1.getPartitionSequenceNumberMap(), currentOffsets);\n    task.getRunner().setEndOffsets(currentOffsets, false);\n\n    while (task.getRunner().getStatus() != Status.PAUSED) {\n      Thread.sleep(10);\n    }\n\n    \r\n    try (final KafkaProducer<byte[], byte[]> kafkaProducer = kafkaServer.newProducer()) {\n      kafkaProducer.initTransactions();\n      kafkaProducer.beginTransaction();\n      for (int i = numToAdd; i < records.size(); i++) {\n        kafkaProducer.send(records.get(i)).get();\n      }\n      kafkaProducer.commitTransaction();\n    }\n    final Map<Integer, Long> nextOffsets = ImmutableMap.copyOf(task.getRunner().getCurrentOffsets());\n\n\n    Assert.assertEquals(checkpoint2.getPartitionSequenceNumberMap(), nextOffsets);\n    task.getRunner().setEndOffsets(nextOffsets, false);\n\n    Assert.assertEquals(TaskState.SUCCESS, future.get().getStatusCode());\n\n    Assert.assertEquals(2, checkpointRequestsHash.size());\n    Assert.assertTrue(\n        checkpointRequestsHash.contains(\n            Objects.hash(\n                DATA_SCHEMA.getDataSource(),\n                0,\n                new KafkaDataSourceMetadata(startPartitions)\n            )\n        )\n    );\n    Assert.assertTrue(\n        checkpointRequestsHash.contains(\n            Objects.hash(\n                DATA_SCHEMA.getDataSource(),\n                0,\n                new KafkaDataSourceMetadata(\n                    new SeekableStreamStartSequenceNumbers<>(topic, currentOffsets, ImmutableSet.of())\n                )\n            )\n        )\n    );\n\n    \r\n    Assert.assertEquals(8, task.getRunner().getRowIngestionMeters().getProcessed());\n    Assert.assertEquals(3, task.getRunner().getRowIngestionMeters().getUnparseable());\n    Assert.assertEquals(1, task.getRunner().getRowIngestionMeters().getThrownAway());\n\n    \r\n    SegmentDescriptor desc1 = sd(\"2008/P1D\", 0);\n    SegmentDescriptor desc2 = sd(\"2009/P1D\", 0);\n    SegmentDescriptor desc3 = sd(\"2010/P1D\", 0);\n    SegmentDescriptor desc4 = sd(\"2011/P1D\", 0);\n    SegmentDescriptor desc5 = sd(\"2011/P1D\", 1);\n    SegmentDescriptor desc6 = sd(\"2012/P1D\", 0);\n    SegmentDescriptor desc7 = sd(\"2013/P1D\", 0);\n    assertEqualsExceptVersion(\n        ImmutableList.of(desc1, desc2, desc3, desc4, desc5, desc6, desc7),\n        publishedDescriptors()\n    );\n    Assert.assertEquals(\n        new KafkaDataSourceMetadata(\n            new SeekableStreamEndSequenceNumbers<>(topic, ImmutableMap.of(0, 10L, 1, 2L))\n        ),\n        metadataStorageCoordinator.getDataSourceMetadata(DATA_SCHEMA.getDataSource())\n    );\n\n    assertEqualsExceptVersion(\n        ImmutableList.of(desc1, desc2, desc3, desc4, desc5, desc6, desc7),\n        publishedDescriptors()\n    );\n    Assert.assertEquals(\n        new KafkaDataSourceMetadata(\n            new SeekableStreamEndSequenceNumbers<>(topic, ImmutableMap.of(0, 10L, 1, 2L))\n        ),\n        metadataStorageCoordinator.getDataSourceMetadata(DATA_SCHEMA.getDataSource())\n    );\n\n    \r\n    final List<SegmentDescriptor> publishedDescriptors = publishedDescriptors();\n    Assert.assertEquals(ImmutableList.of(\"a\"), readSegmentColumn(\"dim1\", publishedDescriptors.get(0)));\n    Assert.assertEquals(ImmutableList.of(\"b\"), readSegmentColumn(\"dim1\", publishedDescriptors.get(1)));\n    Assert.assertEquals(ImmutableList.of(\"c\"), readSegmentColumn(\"dim1\", publishedDescriptors.get(2)));\n    Assert.assertTrue((ImmutableList.of(\"d\", \"e\").equals(readSegmentColumn(\"dim1\", publishedDescriptors.get(3)))\n                       && ImmutableList.of(\"h\").equals(readSegmentColumn(\"dim1\", publishedDescriptors.get(4)))) ||\n                      (ImmutableList.of(\"d\", \"h\").equals(readSegmentColumn(\"dim1\", publishedDescriptors.get(3)))\n                       && ImmutableList.of(\"e\").equals(readSegmentColumn(\"dim1\", publishedDescriptors.get(4)))));\n    Assert.assertEquals(ImmutableList.of(\"g\"), readSegmentColumn(\"dim1\", publishedDescriptors.get(5)));\n    Assert.assertEquals(ImmutableList.of(\"f\"), readSegmentColumn(\"dim1\", publishedDescriptors.get(6)));\n  }\n","realPath":"extensions-core/kafka-indexing-service/src/test/java/org/apache/druid/indexing/kafka/KafkaIndexTaskTest.java","repoName":"druid","snippetEndLine":0,"snippetStartLine":0,"startLine":567,"status":"M"},{"authorDate":"2019-11-07 03:07:04","commitOrder":13,"curCode":"  public void testTimeBasedIncrementalHandOff() throws Exception\n  {\n    final String baseSequenceName = \"sequence0\";\n    \r\n    maxRowsPerSegment = Integer.MAX_VALUE;\n    intermediateHandoffPeriod = new Period().withSeconds(0);\n\n    \r\n    insertData();\n    Map<String, Object> consumerProps = kafkaServer.consumerProperties();\n    consumerProps.put(\"max.poll.records\", \"1\");\n\n    final SeekableStreamStartSequenceNumbers<Integer, Long> startPartitions = new SeekableStreamStartSequenceNumbers<>(\n        topic,\n        ImmutableMap.of(0, 0L, 1, 0L),\n        ImmutableSet.of()\n    );\n    \r\n    final SeekableStreamEndSequenceNumbers<Integer, Long> checkpoint = new SeekableStreamEndSequenceNumbers<>(\n        topic,\n        ImmutableMap.of(0, 1L, 1, 0L)\n    );\n    final SeekableStreamEndSequenceNumbers<Integer, Long> endPartitions = new SeekableStreamEndSequenceNumbers<>(\n        topic,\n        ImmutableMap.of(0, 2L, 1, 0L)\n    );\n    final KafkaIndexTask task = createTask(\n        null,\n        new KafkaIndexTaskIOConfig(\n            0,\n            baseSequenceName,\n            startPartitions,\n            endPartitions,\n            consumerProps,\n            KafkaSupervisorIOConfig.DEFAULT_POLL_TIMEOUT_MILLIS,\n            true,\n            null,\n            null\n        )\n    );\n    final ListenableFuture<TaskStatus> future = runTask(task);\n\n    \r\n    while (task.getRunner().getStatus() != Status.PAUSED) {\n      Thread.sleep(10);\n    }\n    final Map<Integer, Long> currentOffsets = ImmutableMap.copyOf(task.getRunner().getCurrentOffsets());\n    Assert.assertEquals(checkpoint.getPartitionSequenceNumberMap(), currentOffsets);\n    task.getRunner().setEndOffsets(currentOffsets, false);\n    Assert.assertEquals(TaskState.SUCCESS, future.get().getStatusCode());\n\n    Assert.assertEquals(1, checkpointRequestsHash.size());\n    Assert.assertTrue(\n        checkpointRequestsHash.contains(\n            Objects.hash(\n                DATA_SCHEMA.getDataSource(),\n                0,\n                new KafkaDataSourceMetadata(startPartitions)\n            )\n        )\n    );\n\n    \r\n    Assert.assertEquals(2, task.getRunner().getRowIngestionMeters().getProcessed());\n    Assert.assertEquals(0, task.getRunner().getRowIngestionMeters().getUnparseable());\n    Assert.assertEquals(0, task.getRunner().getRowIngestionMeters().getThrownAway());\n\n    \r\n    assertEqualsExceptVersion(\n        ImmutableList.of(\n            sdd(\"2008/P1D\", 0, ImmutableList.of(\"a\")),\n            sdd(\"2009/P1D\", 0, ImmutableList.of(\"b\"))\n        ),\n        publishedDescriptors()\n    );\n    Assert.assertEquals(\n        new KafkaDataSourceMetadata(\n            new SeekableStreamEndSequenceNumbers<>(topic, ImmutableMap.of(0, 2L, 1, 0L))\n        ),\n        metadataStorageCoordinator.getDataSourceMetadata(DATA_SCHEMA.getDataSource())\n    );\n  }\n","date":"2019-11-07 03:07:04","endLine":788,"groupId":"17791","id":24,"instanceNumber":2,"isCurCommit":0,"methodName":"testTimeBasedIncrementalHandOff","params":"()","path":"/mnt/clonedata/CloneManagementServer/ManagementServer/consistResult/result-druid-10-0.7/blobInfo/CC_OUT/blobs/c8/35bb95038567e2b9b3112cb98971f6d2418766.src","preCode":"  public void testTimeBasedIncrementalHandOff() throws Exception\n  {\n    final String baseSequenceName = \"sequence0\";\n    \r\n    maxRowsPerSegment = Integer.MAX_VALUE;\n    intermediateHandoffPeriod = new Period().withSeconds(0);\n\n    \r\n    insertData();\n    Map<String, Object> consumerProps = kafkaServer.consumerProperties();\n    consumerProps.put(\"max.poll.records\", \"1\");\n\n    final SeekableStreamStartSequenceNumbers<Integer, Long> startPartitions = new SeekableStreamStartSequenceNumbers<>(\n        topic,\n        ImmutableMap.of(0, 0L, 1, 0L),\n        ImmutableSet.of()\n    );\n    \r\n    final SeekableStreamEndSequenceNumbers<Integer, Long> checkpoint = new SeekableStreamEndSequenceNumbers<>(\n        topic,\n        ImmutableMap.of(0, 1L, 1, 0L)\n    );\n    final SeekableStreamEndSequenceNumbers<Integer, Long> endPartitions = new SeekableStreamEndSequenceNumbers<>(\n        topic,\n        ImmutableMap.of(0, 2L, 1, 0L)\n    );\n    final KafkaIndexTask task = createTask(\n        null,\n        new KafkaIndexTaskIOConfig(\n            0,\n            baseSequenceName,\n            startPartitions,\n            endPartitions,\n            consumerProps,\n            KafkaSupervisorIOConfig.DEFAULT_POLL_TIMEOUT_MILLIS,\n            true,\n            null,\n            null\n        )\n    );\n    final ListenableFuture<TaskStatus> future = runTask(task);\n\n    \r\n    while (task.getRunner().getStatus() != Status.PAUSED) {\n      Thread.sleep(10);\n    }\n    final Map<Integer, Long> currentOffsets = ImmutableMap.copyOf(task.getRunner().getCurrentOffsets());\n    Assert.assertEquals(checkpoint.getPartitionSequenceNumberMap(), currentOffsets);\n    task.getRunner().setEndOffsets(currentOffsets, false);\n    Assert.assertEquals(TaskState.SUCCESS, future.get().getStatusCode());\n\n    Assert.assertEquals(1, checkpointRequestsHash.size());\n    Assert.assertTrue(\n        checkpointRequestsHash.contains(\n            Objects.hash(\n                DATA_SCHEMA.getDataSource(),\n                0,\n                new KafkaDataSourceMetadata(startPartitions)\n            )\n        )\n    );\n\n    \r\n    Assert.assertEquals(2, task.getRunner().getRowIngestionMeters().getProcessed());\n    Assert.assertEquals(0, task.getRunner().getRowIngestionMeters().getUnparseable());\n    Assert.assertEquals(0, task.getRunner().getRowIngestionMeters().getThrownAway());\n\n    \r\n    SegmentDescriptor desc1 = sd(\"2008/P1D\", 0);\n    SegmentDescriptor desc2 = sd(\"2009/P1D\", 0);\n    assertEqualsExceptVersion(ImmutableList.of(desc1, desc2), publishedDescriptors());\n    Assert.assertEquals(\n        new KafkaDataSourceMetadata(\n            new SeekableStreamEndSequenceNumbers<>(topic, ImmutableMap.of(0, 2L, 1, 0L))\n        ),\n        metadataStorageCoordinator.getDataSourceMetadata(DATA_SCHEMA.getDataSource())\n    );\n\n    \r\n    final List<SegmentDescriptor> publishedDescriptors = publishedDescriptors();\n    Assert.assertEquals(ImmutableList.of(\"a\"), readSegmentColumn(\"dim1\", publishedDescriptors.get(0)));\n    Assert.assertEquals(ImmutableList.of(\"b\"), readSegmentColumn(\"dim1\", publishedDescriptors.get(1)));\n  }\n","realPath":"extensions-core/kafka-indexing-service/src/test/java/org/apache/druid/indexing/kafka/KafkaIndexTaskTest.java","repoName":"druid","snippetEndLine":0,"snippetStartLine":0,"startLine":707,"status":"M"}],"commitId":"5c0fc0a13ab4d259b430bf50b322f631504c4529","commitMessage":"@@@Fix ambiguity about IndexerSQLMetadataStorageCoordinator.getUsedSegmentsForInterval() returning only non-overshadowed or all used segments (#8564)\n\n* IndexerSQLMetadataStorageCoordinator.getTimelineForIntervalsWithHandle() don't fetch abutting intervals; simplify getUsedSegmentsForIntervals()\n\n* Add VersionedIntervalTimeline.findNonOvershadowedObjectsInInterval() method; Propagate the decision about whether only visible segmetns or visible and overshadowed segments should be returned from IndexerMetadataStorageCoordinator's methods to the user logic; Rename SegmentListUsedAction to RetrieveUsedSegmentsAction.  SegmetnListUnusedAction to RetrieveUnusedSegmentsAction.  and UsedSegmentLister to UsedSegmentsRetriever\n\n* Fix tests\n\n* More fixes\n\n* Add javadoc notes about returning Collection instead of Set. Add JacksonUtils.readValue() to reduce boilerplate code\n\n* Fix KinesisIndexTaskTest.  factor out common parts from KinesisIndexTaskTest and KafkaIndexTaskTest into SeekableStreamIndexTaskTestBase\n\n* More test fixes\n\n* More test fixes\n\n* Add a comment to VersionedIntervalTimelineTestBase\n\n* Fix tests\n\n* Set DataSegment.size(0) in more tests\n\n* Specify DataSegment.size(0) in more places in tests\n\n* Fix more tests\n\n* Fix DruidSchemaTest\n\n* Set DataSegment's size in more tests and benchmarks\n\n* Fix HdfsDataSegmentPusherTest\n\n* Doc changes addressing comments\n\n* Extended doc for visibility\n\n* Typo\n\n* Typo 2\n\n* Address comment\n","date":"2019-11-07 03:07:04","modifiedFileCount":"88","status":"M","submitter":"Roman Leventov"},{"authorTime":"2019-11-21 06:51:25","codes":[{"authorDate":"2019-11-21 06:51:25","commitOrder":14,"curCode":"  public void testIncrementalHandOffMaxTotalRows() throws Exception\n  {\n    final String baseSequenceName = \"sequence0\";\n    \r\n    maxRowsPerSegment = Integer.MAX_VALUE;\n    maxTotalRows = 3L;\n\n    \r\n    int numToAdd = records.size() - 2;\n\n    try (final KafkaProducer<byte[], byte[]> kafkaProducer = kafkaServer.newProducer()) {\n      kafkaProducer.initTransactions();\n      kafkaProducer.beginTransaction();\n      for (int i = 0; i < numToAdd; i++) {\n        kafkaProducer.send(records.get(i)).get();\n      }\n      kafkaProducer.commitTransaction();\n    }\n\n    Map<String, Object> consumerProps = kafkaServer.consumerProperties();\n    consumerProps.put(\"max.poll.records\", \"1\");\n\n    final SeekableStreamStartSequenceNumbers<Integer, Long> startPartitions = new SeekableStreamStartSequenceNumbers<>(\n        topic,\n        ImmutableMap.of(0, 0L, 1, 0L),\n        ImmutableSet.of()\n    );\n    final SeekableStreamEndSequenceNumbers<Integer, Long> checkpoint1 = new SeekableStreamEndSequenceNumbers<>(\n        topic,\n        ImmutableMap.of(0, 3L, 1, 0L)\n    );\n    final SeekableStreamEndSequenceNumbers<Integer, Long> checkpoint2 = new SeekableStreamEndSequenceNumbers<>(\n        topic,\n        ImmutableMap.of(0, 10L, 1, 0L)\n    );\n\n    final SeekableStreamEndSequenceNumbers<Integer, Long> endPartitions = new SeekableStreamEndSequenceNumbers<>(\n        topic,\n        ImmutableMap.of(0, 10L, 1, 2L)\n    );\n    final KafkaIndexTask task = createTask(\n        null,\n        new KafkaIndexTaskIOConfig(\n            0,\n            baseSequenceName,\n            startPartitions,\n            endPartitions,\n            consumerProps,\n            KafkaSupervisorIOConfig.DEFAULT_POLL_TIMEOUT_MILLIS,\n            true,\n            null,\n            null,\n            INPUT_FORMAT\n        )\n    );\n    final ListenableFuture<TaskStatus> future = runTask(task);\n    while (task.getRunner().getStatus() != Status.PAUSED) {\n      Thread.sleep(10);\n    }\n    final Map<Integer, Long> currentOffsets = ImmutableMap.copyOf(task.getRunner().getCurrentOffsets());\n\n    Assert.assertEquals(checkpoint1.getPartitionSequenceNumberMap(), currentOffsets);\n    task.getRunner().setEndOffsets(currentOffsets, false);\n\n    while (task.getRunner().getStatus() != Status.PAUSED) {\n      Thread.sleep(10);\n    }\n\n    \r\n    try (final KafkaProducer<byte[], byte[]> kafkaProducer = kafkaServer.newProducer()) {\n      kafkaProducer.initTransactions();\n      kafkaProducer.beginTransaction();\n      for (int i = numToAdd; i < records.size(); i++) {\n        kafkaProducer.send(records.get(i)).get();\n      }\n      kafkaProducer.commitTransaction();\n    }\n    final Map<Integer, Long> nextOffsets = ImmutableMap.copyOf(task.getRunner().getCurrentOffsets());\n\n\n    Assert.assertEquals(checkpoint2.getPartitionSequenceNumberMap(), nextOffsets);\n    task.getRunner().setEndOffsets(nextOffsets, false);\n\n    Assert.assertEquals(TaskState.SUCCESS, future.get().getStatusCode());\n\n    Assert.assertEquals(2, checkpointRequestsHash.size());\n    Assert.assertTrue(\n        checkpointRequestsHash.contains(\n            Objects.hash(\n                NEW_DATA_SCHEMA.getDataSource(),\n                0,\n                new KafkaDataSourceMetadata(startPartitions)\n            )\n        )\n    );\n    Assert.assertTrue(\n        checkpointRequestsHash.contains(\n            Objects.hash(\n                NEW_DATA_SCHEMA.getDataSource(),\n                0,\n                new KafkaDataSourceMetadata(\n                    new SeekableStreamStartSequenceNumbers<>(topic, currentOffsets, ImmutableSet.of())\n                )\n            )\n        )\n    );\n\n    \r\n    Assert.assertEquals(8, task.getRunner().getRowIngestionMeters().getProcessed());\n    Assert.assertEquals(3, task.getRunner().getRowIngestionMeters().getUnparseable());\n    Assert.assertEquals(1, task.getRunner().getRowIngestionMeters().getThrownAway());\n\n    \r\n    assertEqualsExceptVersion(\n        ImmutableList.of(\n            sdd(\"2008/P1D\", 0, ImmutableList.of(\"a\")),\n            sdd(\"2009/P1D\", 0, ImmutableList.of(\"b\")),\n            sdd(\"2010/P1D\", 0, ImmutableList.of(\"c\")),\n            sdd(\"2011/P1D\", 0, ImmutableList.of(\"d\", \"e\"), ImmutableList.of(\"d\", \"h\")),\n            sdd(\"2011/P1D\", 1, ImmutableList.of(\"h\"), ImmutableList.of(\"e\")),\n            sdd(\"2012/P1D\", 0, ImmutableList.of(\"g\")),\n            sdd(\"2013/P1D\", 0, ImmutableList.of(\"f\"))\n        ),\n        publishedDescriptors()\n    );\n    Assert.assertEquals(\n        new KafkaDataSourceMetadata(\n            new SeekableStreamEndSequenceNumbers<>(topic, ImmutableMap.of(0, 10L, 1, 2L))\n        ),\n        metadataStorageCoordinator.getDataSourceMetadata(NEW_DATA_SCHEMA.getDataSource())\n    );\n\n    Assert.assertEquals(\n        new KafkaDataSourceMetadata(\n            new SeekableStreamEndSequenceNumbers<>(topic, ImmutableMap.of(0, 10L, 1, 2L))\n        ),\n        metadataStorageCoordinator.getDataSourceMetadata(NEW_DATA_SCHEMA.getDataSource())\n    );\n  }\n","date":"2019-11-21 06:51:25","endLine":756,"groupId":"16538","id":25,"instanceNumber":1,"isCurCommit":0,"methodName":"testIncrementalHandOffMaxTotalRows","params":"()","path":"/mnt/clonedata/CloneManagementServer/ManagementServer/consistResult/result-druid-10-0.7/blobInfo/CC_OUT/blobs/e8/bde11469c6f5c09cd43b1c5d562e5b78e0f593.src","preCode":"  public void testIncrementalHandOffMaxTotalRows() throws Exception\n  {\n    final String baseSequenceName = \"sequence0\";\n    \r\n    maxRowsPerSegment = Integer.MAX_VALUE;\n    maxTotalRows = 3L;\n\n    \r\n    int numToAdd = records.size() - 2;\n\n    try (final KafkaProducer<byte[], byte[]> kafkaProducer = kafkaServer.newProducer()) {\n      kafkaProducer.initTransactions();\n      kafkaProducer.beginTransaction();\n      for (int i = 0; i < numToAdd; i++) {\n        kafkaProducer.send(records.get(i)).get();\n      }\n      kafkaProducer.commitTransaction();\n    }\n\n    Map<String, Object> consumerProps = kafkaServer.consumerProperties();\n    consumerProps.put(\"max.poll.records\", \"1\");\n\n    final SeekableStreamStartSequenceNumbers<Integer, Long> startPartitions = new SeekableStreamStartSequenceNumbers<>(\n        topic,\n        ImmutableMap.of(0, 0L, 1, 0L),\n        ImmutableSet.of()\n    );\n    final SeekableStreamEndSequenceNumbers<Integer, Long> checkpoint1 = new SeekableStreamEndSequenceNumbers<>(\n        topic,\n        ImmutableMap.of(0, 3L, 1, 0L)\n    );\n    final SeekableStreamEndSequenceNumbers<Integer, Long> checkpoint2 = new SeekableStreamEndSequenceNumbers<>(\n        topic,\n        ImmutableMap.of(0, 10L, 1, 0L)\n    );\n\n    final SeekableStreamEndSequenceNumbers<Integer, Long> endPartitions = new SeekableStreamEndSequenceNumbers<>(\n        topic,\n        ImmutableMap.of(0, 10L, 1, 2L)\n    );\n    final KafkaIndexTask task = createTask(\n        null,\n        new KafkaIndexTaskIOConfig(\n            0,\n            baseSequenceName,\n            startPartitions,\n            endPartitions,\n            consumerProps,\n            KafkaSupervisorIOConfig.DEFAULT_POLL_TIMEOUT_MILLIS,\n            true,\n            null,\n            null\n        )\n    );\n    final ListenableFuture<TaskStatus> future = runTask(task);\n    while (task.getRunner().getStatus() != Status.PAUSED) {\n      Thread.sleep(10);\n    }\n    final Map<Integer, Long> currentOffsets = ImmutableMap.copyOf(task.getRunner().getCurrentOffsets());\n\n    Assert.assertEquals(checkpoint1.getPartitionSequenceNumberMap(), currentOffsets);\n    task.getRunner().setEndOffsets(currentOffsets, false);\n\n    while (task.getRunner().getStatus() != Status.PAUSED) {\n      Thread.sleep(10);\n    }\n\n    \r\n    try (final KafkaProducer<byte[], byte[]> kafkaProducer = kafkaServer.newProducer()) {\n      kafkaProducer.initTransactions();\n      kafkaProducer.beginTransaction();\n      for (int i = numToAdd; i < records.size(); i++) {\n        kafkaProducer.send(records.get(i)).get();\n      }\n      kafkaProducer.commitTransaction();\n    }\n    final Map<Integer, Long> nextOffsets = ImmutableMap.copyOf(task.getRunner().getCurrentOffsets());\n\n\n    Assert.assertEquals(checkpoint2.getPartitionSequenceNumberMap(), nextOffsets);\n    task.getRunner().setEndOffsets(nextOffsets, false);\n\n    Assert.assertEquals(TaskState.SUCCESS, future.get().getStatusCode());\n\n    Assert.assertEquals(2, checkpointRequestsHash.size());\n    Assert.assertTrue(\n        checkpointRequestsHash.contains(\n            Objects.hash(\n                DATA_SCHEMA.getDataSource(),\n                0,\n                new KafkaDataSourceMetadata(startPartitions)\n            )\n        )\n    );\n    Assert.assertTrue(\n        checkpointRequestsHash.contains(\n            Objects.hash(\n                DATA_SCHEMA.getDataSource(),\n                0,\n                new KafkaDataSourceMetadata(\n                    new SeekableStreamStartSequenceNumbers<>(topic, currentOffsets, ImmutableSet.of())\n                )\n            )\n        )\n    );\n\n    \r\n    Assert.assertEquals(8, task.getRunner().getRowIngestionMeters().getProcessed());\n    Assert.assertEquals(3, task.getRunner().getRowIngestionMeters().getUnparseable());\n    Assert.assertEquals(1, task.getRunner().getRowIngestionMeters().getThrownAway());\n\n    \r\n    assertEqualsExceptVersion(\n        ImmutableList.of(\n            sdd(\"2008/P1D\", 0, ImmutableList.of(\"a\")),\n            sdd(\"2009/P1D\", 0, ImmutableList.of(\"b\")),\n            sdd(\"2010/P1D\", 0, ImmutableList.of(\"c\")),\n            sdd(\"2011/P1D\", 0, ImmutableList.of(\"d\", \"e\"), ImmutableList.of(\"d\", \"h\")),\n            sdd(\"2011/P1D\", 1, ImmutableList.of(\"h\"), ImmutableList.of(\"e\")),\n            sdd(\"2012/P1D\", 0, ImmutableList.of(\"g\")),\n            sdd(\"2013/P1D\", 0, ImmutableList.of(\"f\"))\n        ),\n        publishedDescriptors()\n    );\n    Assert.assertEquals(\n        new KafkaDataSourceMetadata(\n            new SeekableStreamEndSequenceNumbers<>(topic, ImmutableMap.of(0, 10L, 1, 2L))\n        ),\n        metadataStorageCoordinator.getDataSourceMetadata(DATA_SCHEMA.getDataSource())\n    );\n\n    Assert.assertEquals(\n        new KafkaDataSourceMetadata(\n            new SeekableStreamEndSequenceNumbers<>(topic, ImmutableMap.of(0, 10L, 1, 2L))\n        ),\n        metadataStorageCoordinator.getDataSourceMetadata(DATA_SCHEMA.getDataSource())\n    );\n  }\n","realPath":"extensions-core/kafka-indexing-service/src/test/java/org/apache/druid/indexing/kafka/KafkaIndexTaskTest.java","repoName":"druid","snippetEndLine":0,"snippetStartLine":0,"startLine":618,"status":"M"},{"authorDate":"2019-11-21 06:51:25","commitOrder":14,"curCode":"  public void testTimeBasedIncrementalHandOff() throws Exception\n  {\n    final String baseSequenceName = \"sequence0\";\n    \r\n    maxRowsPerSegment = Integer.MAX_VALUE;\n    intermediateHandoffPeriod = new Period().withSeconds(0);\n\n    \r\n    insertData();\n    Map<String, Object> consumerProps = kafkaServer.consumerProperties();\n    consumerProps.put(\"max.poll.records\", \"1\");\n\n    final SeekableStreamStartSequenceNumbers<Integer, Long> startPartitions = new SeekableStreamStartSequenceNumbers<>(\n        topic,\n        ImmutableMap.of(0, 0L, 1, 0L),\n        ImmutableSet.of()\n    );\n    \r\n    final SeekableStreamEndSequenceNumbers<Integer, Long> checkpoint = new SeekableStreamEndSequenceNumbers<>(\n        topic,\n        ImmutableMap.of(0, 1L, 1, 0L)\n    );\n    final SeekableStreamEndSequenceNumbers<Integer, Long> endPartitions = new SeekableStreamEndSequenceNumbers<>(\n        topic,\n        ImmutableMap.of(0, 2L, 1, 0L)\n    );\n    final KafkaIndexTask task = createTask(\n        null,\n        new KafkaIndexTaskIOConfig(\n            0,\n            baseSequenceName,\n            startPartitions,\n            endPartitions,\n            consumerProps,\n            KafkaSupervisorIOConfig.DEFAULT_POLL_TIMEOUT_MILLIS,\n            true,\n            null,\n            null,\n            INPUT_FORMAT\n        )\n    );\n    final ListenableFuture<TaskStatus> future = runTask(task);\n\n    \r\n    while (task.getRunner().getStatus() != Status.PAUSED) {\n      Thread.sleep(10);\n    }\n    final Map<Integer, Long> currentOffsets = ImmutableMap.copyOf(task.getRunner().getCurrentOffsets());\n    Assert.assertEquals(checkpoint.getPartitionSequenceNumberMap(), currentOffsets);\n    task.getRunner().setEndOffsets(currentOffsets, false);\n    Assert.assertEquals(TaskState.SUCCESS, future.get().getStatusCode());\n\n    Assert.assertEquals(1, checkpointRequestsHash.size());\n    Assert.assertTrue(\n        checkpointRequestsHash.contains(\n            Objects.hash(\n                NEW_DATA_SCHEMA.getDataSource(),\n                0,\n                new KafkaDataSourceMetadata(startPartitions)\n            )\n        )\n    );\n\n    \r\n    Assert.assertEquals(2, task.getRunner().getRowIngestionMeters().getProcessed());\n    Assert.assertEquals(0, task.getRunner().getRowIngestionMeters().getUnparseable());\n    Assert.assertEquals(0, task.getRunner().getRowIngestionMeters().getThrownAway());\n\n    \r\n    assertEqualsExceptVersion(\n        ImmutableList.of(\n            sdd(\"2008/P1D\", 0, ImmutableList.of(\"a\")),\n            sdd(\"2009/P1D\", 0, ImmutableList.of(\"b\"))\n        ),\n        publishedDescriptors()\n    );\n    Assert.assertEquals(\n        new KafkaDataSourceMetadata(\n            new SeekableStreamEndSequenceNumbers<>(topic, ImmutableMap.of(0, 2L, 1, 0L))\n        ),\n        metadataStorageCoordinator.getDataSourceMetadata(NEW_DATA_SCHEMA.getDataSource())\n    );\n  }\n","date":"2019-11-21 06:51:25","endLine":841,"groupId":"7016","id":26,"instanceNumber":2,"isCurCommit":0,"methodName":"testTimeBasedIncrementalHandOff","params":"()","path":"/mnt/clonedata/CloneManagementServer/ManagementServer/consistResult/result-druid-10-0.7/blobInfo/CC_OUT/blobs/e8/bde11469c6f5c09cd43b1c5d562e5b78e0f593.src","preCode":"  public void testTimeBasedIncrementalHandOff() throws Exception\n  {\n    final String baseSequenceName = \"sequence0\";\n    \r\n    maxRowsPerSegment = Integer.MAX_VALUE;\n    intermediateHandoffPeriod = new Period().withSeconds(0);\n\n    \r\n    insertData();\n    Map<String, Object> consumerProps = kafkaServer.consumerProperties();\n    consumerProps.put(\"max.poll.records\", \"1\");\n\n    final SeekableStreamStartSequenceNumbers<Integer, Long> startPartitions = new SeekableStreamStartSequenceNumbers<>(\n        topic,\n        ImmutableMap.of(0, 0L, 1, 0L),\n        ImmutableSet.of()\n    );\n    \r\n    final SeekableStreamEndSequenceNumbers<Integer, Long> checkpoint = new SeekableStreamEndSequenceNumbers<>(\n        topic,\n        ImmutableMap.of(0, 1L, 1, 0L)\n    );\n    final SeekableStreamEndSequenceNumbers<Integer, Long> endPartitions = new SeekableStreamEndSequenceNumbers<>(\n        topic,\n        ImmutableMap.of(0, 2L, 1, 0L)\n    );\n    final KafkaIndexTask task = createTask(\n        null,\n        new KafkaIndexTaskIOConfig(\n            0,\n            baseSequenceName,\n            startPartitions,\n            endPartitions,\n            consumerProps,\n            KafkaSupervisorIOConfig.DEFAULT_POLL_TIMEOUT_MILLIS,\n            true,\n            null,\n            null\n        )\n    );\n    final ListenableFuture<TaskStatus> future = runTask(task);\n\n    \r\n    while (task.getRunner().getStatus() != Status.PAUSED) {\n      Thread.sleep(10);\n    }\n    final Map<Integer, Long> currentOffsets = ImmutableMap.copyOf(task.getRunner().getCurrentOffsets());\n    Assert.assertEquals(checkpoint.getPartitionSequenceNumberMap(), currentOffsets);\n    task.getRunner().setEndOffsets(currentOffsets, false);\n    Assert.assertEquals(TaskState.SUCCESS, future.get().getStatusCode());\n\n    Assert.assertEquals(1, checkpointRequestsHash.size());\n    Assert.assertTrue(\n        checkpointRequestsHash.contains(\n            Objects.hash(\n                DATA_SCHEMA.getDataSource(),\n                0,\n                new KafkaDataSourceMetadata(startPartitions)\n            )\n        )\n    );\n\n    \r\n    Assert.assertEquals(2, task.getRunner().getRowIngestionMeters().getProcessed());\n    Assert.assertEquals(0, task.getRunner().getRowIngestionMeters().getUnparseable());\n    Assert.assertEquals(0, task.getRunner().getRowIngestionMeters().getThrownAway());\n\n    \r\n    assertEqualsExceptVersion(\n        ImmutableList.of(\n            sdd(\"2008/P1D\", 0, ImmutableList.of(\"a\")),\n            sdd(\"2009/P1D\", 0, ImmutableList.of(\"b\"))\n        ),\n        publishedDescriptors()\n    );\n    Assert.assertEquals(\n        new KafkaDataSourceMetadata(\n            new SeekableStreamEndSequenceNumbers<>(topic, ImmutableMap.of(0, 2L, 1, 0L))\n        ),\n        metadataStorageCoordinator.getDataSourceMetadata(DATA_SCHEMA.getDataSource())\n    );\n  }\n","realPath":"extensions-core/kafka-indexing-service/src/test/java/org/apache/druid/indexing/kafka/KafkaIndexTaskTest.java","repoName":"druid","snippetEndLine":0,"snippetStartLine":0,"startLine":759,"status":"M"}],"commitId":"ac6d703814ccb5b258c586b63e0bc33d669e0f57","commitMessage":"@@@Support inputFormat and inputSource for sampler (#8901)\n\n* Support inputFormat and inputSource for sampler\n\n* Cleanup javadocs and names\n\n* fix style\n\n* fix timed shutoff input source reader\n\n* fix timed shutoff input source reader again\n\n* tidy up timed shutoff reader\n\n* unused imports\n\n* fix tc\n","date":"2019-11-21 06:51:25","modifiedFileCount":"66","status":"M","submitter":"Jihoon Son"},{"authorTime":"2020-01-28 03:24:29","codes":[{"authorDate":"2020-01-28 03:24:29","commitOrder":15,"curCode":"  public void testIncrementalHandOffMaxTotalRows() throws Exception\n  {\n    final String baseSequenceName = \"sequence0\";\n    \r\n    maxRowsPerSegment = Integer.MAX_VALUE;\n    maxTotalRows = 3L;\n\n    \r\n    int numToAdd = records.size() - 2;\n\n    try (final KafkaProducer<byte[], byte[]> kafkaProducer = kafkaServer.newProducer()) {\n      kafkaProducer.initTransactions();\n      kafkaProducer.beginTransaction();\n      for (int i = 0; i < numToAdd; i++) {\n        kafkaProducer.send(records.get(i)).get();\n      }\n      kafkaProducer.commitTransaction();\n    }\n\n    Map<String, Object> consumerProps = kafkaServer.consumerProperties();\n    consumerProps.put(\"max.poll.records\", \"1\");\n\n    final SeekableStreamStartSequenceNumbers<Integer, Long> startPartitions = new SeekableStreamStartSequenceNumbers<>(\n        topic,\n        ImmutableMap.of(0, 0L, 1, 0L),\n        ImmutableSet.of()\n    );\n    final SeekableStreamEndSequenceNumbers<Integer, Long> checkpoint1 = new SeekableStreamEndSequenceNumbers<>(\n        topic,\n        ImmutableMap.of(0, 3L, 1, 0L)\n    );\n    final SeekableStreamEndSequenceNumbers<Integer, Long> checkpoint2 = new SeekableStreamEndSequenceNumbers<>(\n        topic,\n        ImmutableMap.of(0, 10L, 1, 0L)\n    );\n\n    final SeekableStreamEndSequenceNumbers<Integer, Long> endPartitions = new SeekableStreamEndSequenceNumbers<>(\n        topic,\n        ImmutableMap.of(0, 10L, 1, 2L)\n    );\n    final KafkaIndexTask task = createTask(\n        null,\n        new KafkaIndexTaskIOConfig(\n            0,\n            baseSequenceName,\n            startPartitions,\n            endPartitions,\n            consumerProps,\n            KafkaSupervisorIOConfig.DEFAULT_POLL_TIMEOUT_MILLIS,\n            true,\n            null,\n            null,\n            INPUT_FORMAT\n        )\n    );\n    final ListenableFuture<TaskStatus> future = runTask(task);\n    while (task.getRunner().getStatus() != Status.PAUSED) {\n      Thread.sleep(10);\n    }\n    final Map<Integer, Long> currentOffsets = ImmutableMap.copyOf(task.getRunner().getCurrentOffsets());\n\n    Assert.assertEquals(checkpoint1.getPartitionSequenceNumberMap(), currentOffsets);\n    task.getRunner().setEndOffsets(currentOffsets, false);\n\n    while (task.getRunner().getStatus() != Status.PAUSED) {\n      Thread.sleep(10);\n    }\n\n    \r\n    try (final KafkaProducer<byte[], byte[]> kafkaProducer = kafkaServer.newProducer()) {\n      kafkaProducer.initTransactions();\n      kafkaProducer.beginTransaction();\n      for (int i = numToAdd; i < records.size(); i++) {\n        kafkaProducer.send(records.get(i)).get();\n      }\n      kafkaProducer.commitTransaction();\n    }\n    final Map<Integer, Long> nextOffsets = ImmutableMap.copyOf(task.getRunner().getCurrentOffsets());\n\n\n    Assert.assertEquals(checkpoint2.getPartitionSequenceNumberMap(), nextOffsets);\n    task.getRunner().setEndOffsets(nextOffsets, false);\n\n    Assert.assertEquals(TaskState.SUCCESS, future.get().getStatusCode());\n\n    Assert.assertEquals(2, checkpointRequestsHash.size());\n    Assert.assertTrue(\n        checkpointRequestsHash.contains(\n            Objects.hash(\n                NEW_DATA_SCHEMA.getDataSource(),\n                0,\n                new KafkaDataSourceMetadata(startPartitions)\n            )\n        )\n    );\n    Assert.assertTrue(\n        checkpointRequestsHash.contains(\n            Objects.hash(\n                NEW_DATA_SCHEMA.getDataSource(),\n                0,\n                new KafkaDataSourceMetadata(\n                    new SeekableStreamStartSequenceNumbers<>(topic, currentOffsets, ImmutableSet.of())\n                )\n            )\n        )\n    );\n\n    \r\n    Assert.assertEquals(8, task.getRunner().getRowIngestionMeters().getProcessed());\n    Assert.assertEquals(3, task.getRunner().getRowIngestionMeters().getUnparseable());\n    Assert.assertEquals(1, task.getRunner().getRowIngestionMeters().getThrownAway());\n\n    \r\n    assertEqualsExceptVersion(\n        ImmutableList.of(\n            sdd(\"2008/P1D\", 0, ImmutableList.of(\"a\")),\n            sdd(\"2009/P1D\", 0, ImmutableList.of(\"b\")),\n            sdd(\"2010/P1D\", 0, ImmutableList.of(\"c\")),\n            sdd(\"2011/P1D\", 0, ImmutableList.of(\"d\", \"e\"), ImmutableList.of(\"d\", \"h\")),\n            sdd(\"2011/P1D\", 1, ImmutableList.of(\"h\"), ImmutableList.of(\"e\")),\n            sdd(\"2012/P1D\", 0, ImmutableList.of(\"g\")),\n            sdd(\"2013/P1D\", 0, ImmutableList.of(\"f\"))\n        ),\n        publishedDescriptors()\n    );\n    Assert.assertEquals(\n        new KafkaDataSourceMetadata(\n            new SeekableStreamEndSequenceNumbers<>(topic, ImmutableMap.of(0, 10L, 1, 2L))\n        ),\n        newDataSchemaMetadata()\n    );\n\n    Assert.assertEquals(\n        new KafkaDataSourceMetadata(\n            new SeekableStreamEndSequenceNumbers<>(topic, ImmutableMap.of(0, 10L, 1, 2L))\n        ),\n        newDataSchemaMetadata()\n    );\n  }\n","date":"2020-01-28 03:24:29","endLine":754,"groupId":"102511","id":27,"instanceNumber":1,"isCurCommit":0,"methodName":"testIncrementalHandOffMaxTotalRows","params":"()","path":"/mnt/clonedata/CloneManagementServer/ManagementServer/consistResult/result-druid-10-0.7/blobInfo/CC_OUT/blobs/d4/468bd39e032386fe654402d0006ecea542747e.src","preCode":"  public void testIncrementalHandOffMaxTotalRows() throws Exception\n  {\n    final String baseSequenceName = \"sequence0\";\n    \r\n    maxRowsPerSegment = Integer.MAX_VALUE;\n    maxTotalRows = 3L;\n\n    \r\n    int numToAdd = records.size() - 2;\n\n    try (final KafkaProducer<byte[], byte[]> kafkaProducer = kafkaServer.newProducer()) {\n      kafkaProducer.initTransactions();\n      kafkaProducer.beginTransaction();\n      for (int i = 0; i < numToAdd; i++) {\n        kafkaProducer.send(records.get(i)).get();\n      }\n      kafkaProducer.commitTransaction();\n    }\n\n    Map<String, Object> consumerProps = kafkaServer.consumerProperties();\n    consumerProps.put(\"max.poll.records\", \"1\");\n\n    final SeekableStreamStartSequenceNumbers<Integer, Long> startPartitions = new SeekableStreamStartSequenceNumbers<>(\n        topic,\n        ImmutableMap.of(0, 0L, 1, 0L),\n        ImmutableSet.of()\n    );\n    final SeekableStreamEndSequenceNumbers<Integer, Long> checkpoint1 = new SeekableStreamEndSequenceNumbers<>(\n        topic,\n        ImmutableMap.of(0, 3L, 1, 0L)\n    );\n    final SeekableStreamEndSequenceNumbers<Integer, Long> checkpoint2 = new SeekableStreamEndSequenceNumbers<>(\n        topic,\n        ImmutableMap.of(0, 10L, 1, 0L)\n    );\n\n    final SeekableStreamEndSequenceNumbers<Integer, Long> endPartitions = new SeekableStreamEndSequenceNumbers<>(\n        topic,\n        ImmutableMap.of(0, 10L, 1, 2L)\n    );\n    final KafkaIndexTask task = createTask(\n        null,\n        new KafkaIndexTaskIOConfig(\n            0,\n            baseSequenceName,\n            startPartitions,\n            endPartitions,\n            consumerProps,\n            KafkaSupervisorIOConfig.DEFAULT_POLL_TIMEOUT_MILLIS,\n            true,\n            null,\n            null,\n            INPUT_FORMAT\n        )\n    );\n    final ListenableFuture<TaskStatus> future = runTask(task);\n    while (task.getRunner().getStatus() != Status.PAUSED) {\n      Thread.sleep(10);\n    }\n    final Map<Integer, Long> currentOffsets = ImmutableMap.copyOf(task.getRunner().getCurrentOffsets());\n\n    Assert.assertEquals(checkpoint1.getPartitionSequenceNumberMap(), currentOffsets);\n    task.getRunner().setEndOffsets(currentOffsets, false);\n\n    while (task.getRunner().getStatus() != Status.PAUSED) {\n      Thread.sleep(10);\n    }\n\n    \r\n    try (final KafkaProducer<byte[], byte[]> kafkaProducer = kafkaServer.newProducer()) {\n      kafkaProducer.initTransactions();\n      kafkaProducer.beginTransaction();\n      for (int i = numToAdd; i < records.size(); i++) {\n        kafkaProducer.send(records.get(i)).get();\n      }\n      kafkaProducer.commitTransaction();\n    }\n    final Map<Integer, Long> nextOffsets = ImmutableMap.copyOf(task.getRunner().getCurrentOffsets());\n\n\n    Assert.assertEquals(checkpoint2.getPartitionSequenceNumberMap(), nextOffsets);\n    task.getRunner().setEndOffsets(nextOffsets, false);\n\n    Assert.assertEquals(TaskState.SUCCESS, future.get().getStatusCode());\n\n    Assert.assertEquals(2, checkpointRequestsHash.size());\n    Assert.assertTrue(\n        checkpointRequestsHash.contains(\n            Objects.hash(\n                NEW_DATA_SCHEMA.getDataSource(),\n                0,\n                new KafkaDataSourceMetadata(startPartitions)\n            )\n        )\n    );\n    Assert.assertTrue(\n        checkpointRequestsHash.contains(\n            Objects.hash(\n                NEW_DATA_SCHEMA.getDataSource(),\n                0,\n                new KafkaDataSourceMetadata(\n                    new SeekableStreamStartSequenceNumbers<>(topic, currentOffsets, ImmutableSet.of())\n                )\n            )\n        )\n    );\n\n    \r\n    Assert.assertEquals(8, task.getRunner().getRowIngestionMeters().getProcessed());\n    Assert.assertEquals(3, task.getRunner().getRowIngestionMeters().getUnparseable());\n    Assert.assertEquals(1, task.getRunner().getRowIngestionMeters().getThrownAway());\n\n    \r\n    assertEqualsExceptVersion(\n        ImmutableList.of(\n            sdd(\"2008/P1D\", 0, ImmutableList.of(\"a\")),\n            sdd(\"2009/P1D\", 0, ImmutableList.of(\"b\")),\n            sdd(\"2010/P1D\", 0, ImmutableList.of(\"c\")),\n            sdd(\"2011/P1D\", 0, ImmutableList.of(\"d\", \"e\"), ImmutableList.of(\"d\", \"h\")),\n            sdd(\"2011/P1D\", 1, ImmutableList.of(\"h\"), ImmutableList.of(\"e\")),\n            sdd(\"2012/P1D\", 0, ImmutableList.of(\"g\")),\n            sdd(\"2013/P1D\", 0, ImmutableList.of(\"f\"))\n        ),\n        publishedDescriptors()\n    );\n    Assert.assertEquals(\n        new KafkaDataSourceMetadata(\n            new SeekableStreamEndSequenceNumbers<>(topic, ImmutableMap.of(0, 10L, 1, 2L))\n        ),\n        metadataStorageCoordinator.getDataSourceMetadata(NEW_DATA_SCHEMA.getDataSource())\n    );\n\n    Assert.assertEquals(\n        new KafkaDataSourceMetadata(\n            new SeekableStreamEndSequenceNumbers<>(topic, ImmutableMap.of(0, 10L, 1, 2L))\n        ),\n        metadataStorageCoordinator.getDataSourceMetadata(NEW_DATA_SCHEMA.getDataSource())\n    );\n  }\n","realPath":"extensions-core/kafka-indexing-service/src/test/java/org/apache/druid/indexing/kafka/KafkaIndexTaskTest.java","repoName":"druid","snippetEndLine":0,"snippetStartLine":0,"startLine":616,"status":"M"},{"authorDate":"2020-01-28 03:24:29","commitOrder":15,"curCode":"  public void testTimeBasedIncrementalHandOff() throws Exception\n  {\n    final String baseSequenceName = \"sequence0\";\n    \r\n    maxRowsPerSegment = Integer.MAX_VALUE;\n    intermediateHandoffPeriod = new Period().withSeconds(0);\n\n    \r\n    insertData();\n    Map<String, Object> consumerProps = kafkaServer.consumerProperties();\n    consumerProps.put(\"max.poll.records\", \"1\");\n\n    final SeekableStreamStartSequenceNumbers<Integer, Long> startPartitions = new SeekableStreamStartSequenceNumbers<>(\n        topic,\n        ImmutableMap.of(0, 0L, 1, 0L),\n        ImmutableSet.of()\n    );\n    \r\n    final SeekableStreamEndSequenceNumbers<Integer, Long> checkpoint = new SeekableStreamEndSequenceNumbers<>(\n        topic,\n        ImmutableMap.of(0, 1L, 1, 0L)\n    );\n    final SeekableStreamEndSequenceNumbers<Integer, Long> endPartitions = new SeekableStreamEndSequenceNumbers<>(\n        topic,\n        ImmutableMap.of(0, 2L, 1, 0L)\n    );\n    final KafkaIndexTask task = createTask(\n        null,\n        new KafkaIndexTaskIOConfig(\n            0,\n            baseSequenceName,\n            startPartitions,\n            endPartitions,\n            consumerProps,\n            KafkaSupervisorIOConfig.DEFAULT_POLL_TIMEOUT_MILLIS,\n            true,\n            null,\n            null,\n            INPUT_FORMAT\n        )\n    );\n    final ListenableFuture<TaskStatus> future = runTask(task);\n\n    \r\n    while (task.getRunner().getStatus() != Status.PAUSED) {\n      Thread.sleep(10);\n    }\n    final Map<Integer, Long> currentOffsets = ImmutableMap.copyOf(task.getRunner().getCurrentOffsets());\n    Assert.assertEquals(checkpoint.getPartitionSequenceNumberMap(), currentOffsets);\n    task.getRunner().setEndOffsets(currentOffsets, false);\n    Assert.assertEquals(TaskState.SUCCESS, future.get().getStatusCode());\n\n    Assert.assertEquals(1, checkpointRequestsHash.size());\n    Assert.assertTrue(\n        checkpointRequestsHash.contains(\n            Objects.hash(\n                NEW_DATA_SCHEMA.getDataSource(),\n                0,\n                new KafkaDataSourceMetadata(startPartitions)\n            )\n        )\n    );\n\n    \r\n    Assert.assertEquals(2, task.getRunner().getRowIngestionMeters().getProcessed());\n    Assert.assertEquals(0, task.getRunner().getRowIngestionMeters().getUnparseable());\n    Assert.assertEquals(0, task.getRunner().getRowIngestionMeters().getThrownAway());\n\n    \r\n    assertEqualsExceptVersion(\n        ImmutableList.of(\n            sdd(\"2008/P1D\", 0, ImmutableList.of(\"a\")),\n            sdd(\"2009/P1D\", 0, ImmutableList.of(\"b\"))\n        ),\n        publishedDescriptors()\n    );\n    Assert.assertEquals(\n        new KafkaDataSourceMetadata(\n            new SeekableStreamEndSequenceNumbers<>(topic, ImmutableMap.of(0, 2L, 1, 0L))\n        ),\n        newDataSchemaMetadata()\n    );\n  }\n","date":"2020-01-28 03:24:29","endLine":839,"groupId":"102511","id":28,"instanceNumber":2,"isCurCommit":0,"methodName":"testTimeBasedIncrementalHandOff","params":"()","path":"/mnt/clonedata/CloneManagementServer/ManagementServer/consistResult/result-druid-10-0.7/blobInfo/CC_OUT/blobs/d4/468bd39e032386fe654402d0006ecea542747e.src","preCode":"  public void testTimeBasedIncrementalHandOff() throws Exception\n  {\n    final String baseSequenceName = \"sequence0\";\n    \r\n    maxRowsPerSegment = Integer.MAX_VALUE;\n    intermediateHandoffPeriod = new Period().withSeconds(0);\n\n    \r\n    insertData();\n    Map<String, Object> consumerProps = kafkaServer.consumerProperties();\n    consumerProps.put(\"max.poll.records\", \"1\");\n\n    final SeekableStreamStartSequenceNumbers<Integer, Long> startPartitions = new SeekableStreamStartSequenceNumbers<>(\n        topic,\n        ImmutableMap.of(0, 0L, 1, 0L),\n        ImmutableSet.of()\n    );\n    \r\n    final SeekableStreamEndSequenceNumbers<Integer, Long> checkpoint = new SeekableStreamEndSequenceNumbers<>(\n        topic,\n        ImmutableMap.of(0, 1L, 1, 0L)\n    );\n    final SeekableStreamEndSequenceNumbers<Integer, Long> endPartitions = new SeekableStreamEndSequenceNumbers<>(\n        topic,\n        ImmutableMap.of(0, 2L, 1, 0L)\n    );\n    final KafkaIndexTask task = createTask(\n        null,\n        new KafkaIndexTaskIOConfig(\n            0,\n            baseSequenceName,\n            startPartitions,\n            endPartitions,\n            consumerProps,\n            KafkaSupervisorIOConfig.DEFAULT_POLL_TIMEOUT_MILLIS,\n            true,\n            null,\n            null,\n            INPUT_FORMAT\n        )\n    );\n    final ListenableFuture<TaskStatus> future = runTask(task);\n\n    \r\n    while (task.getRunner().getStatus() != Status.PAUSED) {\n      Thread.sleep(10);\n    }\n    final Map<Integer, Long> currentOffsets = ImmutableMap.copyOf(task.getRunner().getCurrentOffsets());\n    Assert.assertEquals(checkpoint.getPartitionSequenceNumberMap(), currentOffsets);\n    task.getRunner().setEndOffsets(currentOffsets, false);\n    Assert.assertEquals(TaskState.SUCCESS, future.get().getStatusCode());\n\n    Assert.assertEquals(1, checkpointRequestsHash.size());\n    Assert.assertTrue(\n        checkpointRequestsHash.contains(\n            Objects.hash(\n                NEW_DATA_SCHEMA.getDataSource(),\n                0,\n                new KafkaDataSourceMetadata(startPartitions)\n            )\n        )\n    );\n\n    \r\n    Assert.assertEquals(2, task.getRunner().getRowIngestionMeters().getProcessed());\n    Assert.assertEquals(0, task.getRunner().getRowIngestionMeters().getUnparseable());\n    Assert.assertEquals(0, task.getRunner().getRowIngestionMeters().getThrownAway());\n\n    \r\n    assertEqualsExceptVersion(\n        ImmutableList.of(\n            sdd(\"2008/P1D\", 0, ImmutableList.of(\"a\")),\n            sdd(\"2009/P1D\", 0, ImmutableList.of(\"b\"))\n        ),\n        publishedDescriptors()\n    );\n    Assert.assertEquals(\n        new KafkaDataSourceMetadata(\n            new SeekableStreamEndSequenceNumbers<>(topic, ImmutableMap.of(0, 2L, 1, 0L))\n        ),\n        metadataStorageCoordinator.getDataSourceMetadata(NEW_DATA_SCHEMA.getDataSource())\n    );\n  }\n","realPath":"extensions-core/kafka-indexing-service/src/test/java/org/apache/druid/indexing/kafka/KafkaIndexTaskTest.java","repoName":"druid","snippetEndLine":0,"snippetStartLine":0,"startLine":757,"status":"M"}],"commitId":"b9186f8f9ff2ff52aceda42bc5f24ffd47a7d17e","commitMessage":"@@@Reconcile terminology and method naming to 'used/unused segments'; Rename MetadataSegmentManager to MetadataSegmentsManager (#7306)\n\n* Reconcile terminology and method naming to 'used/unused segments'; Don't use terms 'enable/disable data source'; Rename MetadataSegmentManager to MetadataSegments; Make REST API methods which mark segments as used/unused to return server error instead of an empty response in case of error\n\n* Fix brace\n\n* Import order\n\n* Rename withKillDataSourceWhitelist to withSpecificDataSourcesToKill\n\n* Fix tests\n\n* Fix tests by adding proper methods without interval parameters to IndexerMetadataStorageCoordinator instead of hacking with Intervals.ETERNITY\n\n* More aligned names of DruidCoordinatorHelpers.  rename several CoordinatorDynamicConfig parameters\n\n* Rename ClientCompactTaskQuery to ClientCompactionTaskQuery for consistency with CompactionTask; ClientCompactQueryTuningConfig to ClientCompactionTaskQueryTuningConfig\n\n* More variable and method renames\n\n* Rename MetadataSegments to SegmentsMetadata\n\n* Javadoc update\n\n* Simplify SegmentsMetadata.getUnusedSegmentIntervals().  more javadocs\n\n* Update Javadoc of VersionedIntervalTimeline.iterateAllObjects()\n\n* Reorder imports\n\n* Rename SegmentsMetadata.tryMark... methods to mark... and make them to return boolean and the numbers of segments changed and relay exceptions to callers\n\n* Complete merge\n\n* Add CollectionUtils.newTreeSet(); Refactor DruidCoordinatorRuntimeParams creation in tests\n\n* Remove MetadataSegmentManager\n\n* Rename millisLagSinceCoordinatorBecomesLeaderBeforeCanMarkAsUnusedOvershadowedSegments to leadingTimeMillisBeforeCanMarkAsUnusedOvershadowedSegments\n\n* Fix tests.  refactor DruidCluster creation in tests into DruidClusterBuilder\n\n* Fix inspections\n\n* Fix SQLMetadataSegmentManagerEmptyTest and rename it to SqlSegmentsMetadataEmptyTest\n\n* Rename SegmentsAndMetadata to SegmentsAndCommitMetadata to reduce the similarity with SegmentsMetadata; Rename some methods\n\n* Rename DruidCoordinatorHelper to CoordinatorDuty.  refactor DruidCoordinator\n\n* Unused import\n\n* Optimize imports\n\n* Rename IndexerSQLMetadataStorageCoordinator.getDataSourceMetadata() to retrieveDataSourceMetadata()\n\n* Unused import\n\n* Update terminology in datasource-view.tsx\n\n* Fix label in datasource-view.spec.tsx.snap\n\n* Fix lint errors in datasource-view.tsx\n\n* Doc improvements\n\n* Another attempt to please TSLint\n\n* Another attempt to please TSLint\n\n* Style fixes\n\n* Fix IndexerSQLMetadataStorageCoordinator.createUsedSegmentsSqlQueryForIntervals() (wrong merge)\n\n* Try to fix docs build issue\n\n* Javadoc and spelling fixes\n\n* Rename SegmentsMetadata to SegmentsMetadataManager.  address other comments\n\n* Address more comments\n","date":"2020-01-28 03:24:29","modifiedFileCount":"127","status":"M","submitter":"Roman Leventov"}]
