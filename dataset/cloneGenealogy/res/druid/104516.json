[{"authorTime":"2021-03-03 03:23:52","codes":[{"authorDate":"2021-03-03 03:23:52","commitOrder":18,"curCode":"  public void testCreateIngestionSchemaWithMaxTotalRows() throws IOException, SegmentLoadingException\n  {\n    final ParallelIndexTuningConfig tuningConfig = new ParallelIndexTuningConfig(\n        null,\n        null,\n        null,\n        500000,\n        1000000L,\n        null,\n        1000000L,\n        null,\n        null,\n        null,\n        new IndexSpec(\n            new RoaringBitmapSerdeFactory(true),\n            CompressionStrategy.LZ4,\n            CompressionStrategy.LZF,\n            LongEncodingStrategy.LONGS\n        ),\n        null,\n        null,\n        false,\n        false,\n        5000L,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null\n    );\n    final List<ParallelIndexIngestionSpec> ingestionSpecs = CompactionTask.createIngestionSchema(\n        toolbox,\n        LockGranularity.TIME_CHUNK,\n        new SegmentProvider(DATA_SOURCE, new CompactionIntervalSpec(COMPACTION_INTERVAL, null)),\n        new PartitionConfigurationManager(tuningConfig),\n        null,\n        null,\n        null,\n        COORDINATOR_CLIENT,\n        segmentLoaderFactory,\n        RETRY_POLICY_FACTORY\n    );\n    final List<DimensionsSpec> expectedDimensionsSpec = getExpectedDimensionsSpecForAutoGeneration();\n\n    ingestionSpecs.sort(\n        (s1, s2) -> Comparators.intervalsByStartThenEnd().compare(\n            s1.getDataSchema().getGranularitySpec().inputIntervals().get(0),\n            s2.getDataSchema().getGranularitySpec().inputIntervals().get(0)\n        )\n    );\n    Assert.assertEquals(6, ingestionSpecs.size());\n    assertIngestionSchema(\n        ingestionSpecs,\n        expectedDimensionsSpec,\n        AGGREGATORS,\n        SEGMENT_INTERVALS,\n        tuningConfig,\n        Granularities.MONTH,\n        Granularities.NONE\n    );\n  }\n","date":"2021-03-03 03:23:52","endLine":785,"groupId":"14702","id":1,"instanceNumber":1,"isCurCommit":0,"methodName":"testCreateIngestionSchemaWithMaxTotalRows","params":"()","path":"/mnt/clonedata/CloneManagementServer/ManagementServer/consistResult/result-druid-10-0.7/blobInfo/CC_OUT/blobs/b0/deee546550836611f13834cf7230e1115b793c.src","preCode":"  public void testCreateIngestionSchemaWithMaxTotalRows() throws IOException, SegmentLoadingException\n  {\n    final ParallelIndexTuningConfig tuningConfig = new ParallelIndexTuningConfig(\n        null,\n        null,\n        null,\n        500000,\n        1000000L,\n        null,\n        1000000L,\n        null,\n        null,\n        null,\n        new IndexSpec(\n            new RoaringBitmapSerdeFactory(true),\n            CompressionStrategy.LZ4,\n            CompressionStrategy.LZF,\n            LongEncodingStrategy.LONGS\n        ),\n        null,\n        null,\n        false,\n        false,\n        5000L,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null\n    );\n    final List<ParallelIndexIngestionSpec> ingestionSpecs = CompactionTask.createIngestionSchema(\n        toolbox,\n        LockGranularity.TIME_CHUNK,\n        new SegmentProvider(DATA_SOURCE, new CompactionIntervalSpec(COMPACTION_INTERVAL, null)),\n        new PartitionConfigurationManager(tuningConfig),\n        null,\n        null,\n        null,\n        COORDINATOR_CLIENT,\n        segmentLoaderFactory,\n        RETRY_POLICY_FACTORY\n    );\n    final List<DimensionsSpec> expectedDimensionsSpec = getExpectedDimensionsSpecForAutoGeneration();\n\n    ingestionSpecs.sort(\n        (s1, s2) -> Comparators.intervalsByStartThenEnd().compare(\n            s1.getDataSchema().getGranularitySpec().inputIntervals().get(0),\n            s2.getDataSchema().getGranularitySpec().inputIntervals().get(0)\n        )\n    );\n    Assert.assertEquals(6, ingestionSpecs.size());\n    assertIngestionSchema(\n        ingestionSpecs,\n        expectedDimensionsSpec,\n        AGGREGATORS,\n        SEGMENT_INTERVALS,\n        tuningConfig,\n        Granularities.MONTH,\n        Granularities.NONE\n    );\n  }\n","realPath":"indexing-service/src/test/java/org/apache/druid/indexing/common/task/CompactionTaskTest.java","repoName":"druid","snippetEndLine":0,"snippetStartLine":0,"startLine":717,"status":"MB"},{"authorDate":"2021-03-03 03:23:52","commitOrder":18,"curCode":"  public void testNullGranularitySpec() throws IOException, SegmentLoadingException\n  {\n    final List<ParallelIndexIngestionSpec> ingestionSpecs = CompactionTask.createIngestionSchema(\n        toolbox,\n        LockGranularity.TIME_CHUNK,\n        new SegmentProvider(DATA_SOURCE, new CompactionIntervalSpec(COMPACTION_INTERVAL, null)),\n        new PartitionConfigurationManager(TUNING_CONFIG),\n        null,\n        null,\n        null,\n        COORDINATOR_CLIENT,\n        segmentLoaderFactory,\n        RETRY_POLICY_FACTORY\n    );\n    final List<DimensionsSpec> expectedDimensionsSpec = getExpectedDimensionsSpecForAutoGeneration();\n\n    ingestionSpecs.sort(\n        (s1, s2) -> Comparators.intervalsByStartThenEnd().compare(\n            s1.getDataSchema().getGranularitySpec().inputIntervals().get(0),\n            s2.getDataSchema().getGranularitySpec().inputIntervals().get(0)\n        )\n    );\n    Assert.assertEquals(6, ingestionSpecs.size());\n    assertIngestionSchema(\n        ingestionSpecs,\n        expectedDimensionsSpec,\n        AGGREGATORS,\n        SEGMENT_INTERVALS,\n        Granularities.MONTH,\n        Granularities.NONE\n    );\n  }\n","date":"2021-03-03 03:23:52","endLine":1201,"groupId":"14702","id":2,"instanceNumber":2,"isCurCommit":0,"methodName":"testNullGranularitySpec","params":"()","path":"/mnt/clonedata/CloneManagementServer/ManagementServer/consistResult/result-druid-10-0.7/blobInfo/CC_OUT/blobs/b0/deee546550836611f13834cf7230e1115b793c.src","preCode":"  public void testNullGranularitySpec() throws IOException, SegmentLoadingException\n  {\n    final List<ParallelIndexIngestionSpec> ingestionSpecs = CompactionTask.createIngestionSchema(\n        toolbox,\n        LockGranularity.TIME_CHUNK,\n        new SegmentProvider(DATA_SOURCE, new CompactionIntervalSpec(COMPACTION_INTERVAL, null)),\n        new PartitionConfigurationManager(TUNING_CONFIG),\n        null,\n        null,\n        null,\n        COORDINATOR_CLIENT,\n        segmentLoaderFactory,\n        RETRY_POLICY_FACTORY\n    );\n    final List<DimensionsSpec> expectedDimensionsSpec = getExpectedDimensionsSpecForAutoGeneration();\n\n    ingestionSpecs.sort(\n        (s1, s2) -> Comparators.intervalsByStartThenEnd().compare(\n            s1.getDataSchema().getGranularitySpec().inputIntervals().get(0),\n            s2.getDataSchema().getGranularitySpec().inputIntervals().get(0)\n        )\n    );\n    Assert.assertEquals(6, ingestionSpecs.size());\n    assertIngestionSchema(\n        ingestionSpecs,\n        expectedDimensionsSpec,\n        AGGREGATORS,\n        SEGMENT_INTERVALS,\n        Granularities.MONTH,\n        Granularities.NONE\n    );\n  }\n","realPath":"indexing-service/src/test/java/org/apache/druid/indexing/common/task/CompactionTaskTest.java","repoName":"druid","snippetEndLine":0,"snippetStartLine":0,"startLine":1170,"status":"B"}],"commitId":"b7b0ee83627dd7887392e8f9d6fb5cb29465c28c","commitMessage":"@@@Add query granularity to compaction task (#10900)\n\n* add query granularity to compaction task\n\n* fix checkstyle\n\n* fix checkstyle\n\n* fix test\n\n* fix test\n\n* add tests\n\n* fix test\n\n* fix test\n\n* cleanup\n\n* rename class\n\n* fix test\n\n* fix test\n\n* add test\n\n* fix test","date":"2021-03-03 03:23:52","modifiedFileCount":"15","status":"M","submitter":"Maytas Monsereenusorn"},{"authorTime":"2021-03-03 03:23:52","codes":[{"authorDate":"2021-04-09 12:03:00","commitOrder":19,"curCode":"  public void testCreateIngestionSchemaWithMaxTotalRows() throws IOException, SegmentLoadingException\n  {\n    final CompactionTask.CompactionTuningConfig tuningConfig = new CompactionTask.CompactionTuningConfig(\n        null,\n        null,\n        null,\n        500000,\n        1000000L,\n        null,\n        1000000L,\n        null,\n        null,\n        null,\n        new IndexSpec(\n            new RoaringBitmapSerdeFactory(true),\n            CompressionStrategy.LZ4,\n            CompressionStrategy.LZF,\n            LongEncodingStrategy.LONGS\n        ),\n        null,\n        null,\n        false,\n        false,\n        5000L,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null\n    );\n    final List<ParallelIndexIngestionSpec> ingestionSpecs = CompactionTask.createIngestionSchema(\n        toolbox,\n        LockGranularity.TIME_CHUNK,\n        new SegmentProvider(DATA_SOURCE, new CompactionIntervalSpec(COMPACTION_INTERVAL, null)),\n        new PartitionConfigurationManager(tuningConfig),\n        null,\n        null,\n        null,\n        COORDINATOR_CLIENT,\n        segmentLoaderFactory,\n        RETRY_POLICY_FACTORY\n    );\n    final List<DimensionsSpec> expectedDimensionsSpec = getExpectedDimensionsSpecForAutoGeneration();\n\n    ingestionSpecs.sort(\n        (s1, s2) -> Comparators.intervalsByStartThenEnd().compare(\n            s1.getDataSchema().getGranularitySpec().inputIntervals().get(0),\n            s2.getDataSchema().getGranularitySpec().inputIntervals().get(0)\n        )\n    );\n    Assert.assertEquals(6, ingestionSpecs.size());\n    assertIngestionSchema(\n        ingestionSpecs,\n        expectedDimensionsSpec,\n        AGGREGATORS,\n        SEGMENT_INTERVALS,\n        tuningConfig,\n        Granularities.MONTH,\n        Granularities.NONE\n    );\n  }\n","date":"2021-04-09 12:03:00","endLine":1014,"groupId":"14702","id":3,"instanceNumber":1,"isCurCommit":0,"methodName":"testCreateIngestionSchemaWithMaxTotalRows","params":"()","path":"/mnt/clonedata/CloneManagementServer/ManagementServer/consistResult/result-druid-10-0.7/blobInfo/CC_OUT/blobs/f9/f3a66cab3a866f1891788d2026f7aedad17f30.src","preCode":"  public void testCreateIngestionSchemaWithMaxTotalRows() throws IOException, SegmentLoadingException\n  {\n    final ParallelIndexTuningConfig tuningConfig = new ParallelIndexTuningConfig(\n        null,\n        null,\n        null,\n        500000,\n        1000000L,\n        null,\n        1000000L,\n        null,\n        null,\n        null,\n        new IndexSpec(\n            new RoaringBitmapSerdeFactory(true),\n            CompressionStrategy.LZ4,\n            CompressionStrategy.LZF,\n            LongEncodingStrategy.LONGS\n        ),\n        null,\n        null,\n        false,\n        false,\n        5000L,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null\n    );\n    final List<ParallelIndexIngestionSpec> ingestionSpecs = CompactionTask.createIngestionSchema(\n        toolbox,\n        LockGranularity.TIME_CHUNK,\n        new SegmentProvider(DATA_SOURCE, new CompactionIntervalSpec(COMPACTION_INTERVAL, null)),\n        new PartitionConfigurationManager(tuningConfig),\n        null,\n        null,\n        null,\n        COORDINATOR_CLIENT,\n        segmentLoaderFactory,\n        RETRY_POLICY_FACTORY\n    );\n    final List<DimensionsSpec> expectedDimensionsSpec = getExpectedDimensionsSpecForAutoGeneration();\n\n    ingestionSpecs.sort(\n        (s1, s2) -> Comparators.intervalsByStartThenEnd().compare(\n            s1.getDataSchema().getGranularitySpec().inputIntervals().get(0),\n            s2.getDataSchema().getGranularitySpec().inputIntervals().get(0)\n        )\n    );\n    Assert.assertEquals(6, ingestionSpecs.size());\n    assertIngestionSchema(\n        ingestionSpecs,\n        expectedDimensionsSpec,\n        AGGREGATORS,\n        SEGMENT_INTERVALS,\n        tuningConfig,\n        Granularities.MONTH,\n        Granularities.NONE\n    );\n  }\n","realPath":"indexing-service/src/test/java/org/apache/druid/indexing/common/task/CompactionTaskTest.java","repoName":"druid","snippetEndLine":0,"snippetStartLine":0,"startLine":945,"status":"M"},{"authorDate":"2021-03-03 03:23:52","commitOrder":19,"curCode":"  public void testNullGranularitySpec() throws IOException, SegmentLoadingException\n  {\n    final List<ParallelIndexIngestionSpec> ingestionSpecs = CompactionTask.createIngestionSchema(\n        toolbox,\n        LockGranularity.TIME_CHUNK,\n        new SegmentProvider(DATA_SOURCE, new CompactionIntervalSpec(COMPACTION_INTERVAL, null)),\n        new PartitionConfigurationManager(TUNING_CONFIG),\n        null,\n        null,\n        null,\n        COORDINATOR_CLIENT,\n        segmentLoaderFactory,\n        RETRY_POLICY_FACTORY\n    );\n    final List<DimensionsSpec> expectedDimensionsSpec = getExpectedDimensionsSpecForAutoGeneration();\n\n    ingestionSpecs.sort(\n        (s1, s2) -> Comparators.intervalsByStartThenEnd().compare(\n            s1.getDataSchema().getGranularitySpec().inputIntervals().get(0),\n            s2.getDataSchema().getGranularitySpec().inputIntervals().get(0)\n        )\n    );\n    Assert.assertEquals(6, ingestionSpecs.size());\n    assertIngestionSchema(\n        ingestionSpecs,\n        expectedDimensionsSpec,\n        AGGREGATORS,\n        SEGMENT_INTERVALS,\n        Granularities.MONTH,\n        Granularities.NONE\n    );\n  }\n","date":"2021-03-03 03:23:52","endLine":1201,"groupId":"14702","id":4,"instanceNumber":2,"isCurCommit":0,"methodName":"testNullGranularitySpec","params":"()","path":"/mnt/clonedata/CloneManagementServer/ManagementServer/consistResult/result-druid-10-0.7/blobInfo/CC_OUT/blobs/b0/deee546550836611f13834cf7230e1115b793c.src","preCode":"  public void testNullGranularitySpec() throws IOException, SegmentLoadingException\n  {\n    final List<ParallelIndexIngestionSpec> ingestionSpecs = CompactionTask.createIngestionSchema(\n        toolbox,\n        LockGranularity.TIME_CHUNK,\n        new SegmentProvider(DATA_SOURCE, new CompactionIntervalSpec(COMPACTION_INTERVAL, null)),\n        new PartitionConfigurationManager(TUNING_CONFIG),\n        null,\n        null,\n        null,\n        COORDINATOR_CLIENT,\n        segmentLoaderFactory,\n        RETRY_POLICY_FACTORY\n    );\n    final List<DimensionsSpec> expectedDimensionsSpec = getExpectedDimensionsSpecForAutoGeneration();\n\n    ingestionSpecs.sort(\n        (s1, s2) -> Comparators.intervalsByStartThenEnd().compare(\n            s1.getDataSchema().getGranularitySpec().inputIntervals().get(0),\n            s2.getDataSchema().getGranularitySpec().inputIntervals().get(0)\n        )\n    );\n    Assert.assertEquals(6, ingestionSpecs.size());\n    assertIngestionSchema(\n        ingestionSpecs,\n        expectedDimensionsSpec,\n        AGGREGATORS,\n        SEGMENT_INTERVALS,\n        Granularities.MONTH,\n        Granularities.NONE\n    );\n  }\n","realPath":"indexing-service/src/test/java/org/apache/druid/indexing/common/task/CompactionTaskTest.java","repoName":"druid","snippetEndLine":0,"snippetStartLine":0,"startLine":1170,"status":"N"}],"commitId":"8264203cee688607091232897749e959e7706010","commitMessage":"@@@Allow client to configure batch ingestion task to wait to complete until segments are confirmed to be available by other (#10676)\n\n* Add ability to wait for segment availability for batch jobs\n\n* IT updates\n\n* fix queries in legacy hadoop IT\n\n* Fix broken indexing integration tests\n\n* address an lgtm flag\n\n* spell checker still flagging for hadoop doc. adding under that file header too\n\n* fix compaction IT\n\n* Updates to wait for availability method\n\n* improve unit testing for patch\n\n* fix bad indentation\n\n* refactor waitForSegmentAvailability\n\n* Fixes based off of review comments\n\n* cleanup to get compile after merging with master\n\n* fix failing test after previous logic update\n\n* add back code that must have gotten deleted during conflict resolution\n\n* update some logging code\n\n* fixes to get compilation working after merge with master\n\n* reset interrupt flag in catch block after code review pointed it out\n\n* small changes following self-review\n\n* fixup some issues brought on by merge with master\n\n* small changes after review\n\n* cleanup a little bit after merge with master\n\n* Fix potential resource leak in AbstractBatchIndexTask\n\n* syntax fix\n\n* Add a Compcation TuningConfig type\n\n* add docs stipulating the lack of support by Compaction tasks for the new config\n\n* Fixup compilation errors after merge with master\n\n* Remove erreneous newline","date":"2021-04-09 12:03:00","modifiedFileCount":"106","status":"M","submitter":"Lucas Capistrant"},{"authorTime":"2021-04-09 15:12:28","codes":[{"authorDate":"2021-04-09 15:12:28","commitOrder":20,"curCode":"  public void testCreateIngestionSchemaWithMaxTotalRows() throws IOException, SegmentLoadingException\n  {\n    final CompactionTask.CompactionTuningConfig tuningConfig = new CompactionTask.CompactionTuningConfig(\n        null,\n        null,\n        null,\n        500000,\n        1000000L,\n        null,\n        1000000L,\n        null,\n        null,\n        null,\n        new IndexSpec(\n            new RoaringBitmapSerdeFactory(true),\n            CompressionStrategy.LZ4,\n            CompressionStrategy.LZF,\n            LongEncodingStrategy.LONGS\n        ),\n        null,\n        null,\n        false,\n        false,\n        5000L,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null\n    );\n    final List<ParallelIndexIngestionSpec> ingestionSpecs = CompactionTask.createIngestionSchema(\n        toolbox,\n        LockGranularity.TIME_CHUNK,\n        new SegmentProvider(DATA_SOURCE, new CompactionIntervalSpec(COMPACTION_INTERVAL, null)),\n        new PartitionConfigurationManager(tuningConfig),\n        null,\n        null,\n        null,\n        COORDINATOR_CLIENT,\n        segmentLoaderFactory,\n        RETRY_POLICY_FACTORY,\n        IOConfig.DEFAULT_DROP_EXISTING\n    );\n    final List<DimensionsSpec> expectedDimensionsSpec = getExpectedDimensionsSpecForAutoGeneration();\n\n    ingestionSpecs.sort(\n        (s1, s2) -> Comparators.intervalsByStartThenEnd().compare(\n            s1.getDataSchema().getGranularitySpec().inputIntervals().get(0),\n            s2.getDataSchema().getGranularitySpec().inputIntervals().get(0)\n        )\n    );\n    Assert.assertEquals(6, ingestionSpecs.size());\n    assertIngestionSchema(\n        ingestionSpecs,\n        expectedDimensionsSpec,\n        AGGREGATORS,\n        SEGMENT_INTERVALS,\n        tuningConfig,\n        Granularities.MONTH,\n        Granularities.NONE,\n        IOConfig.DEFAULT_DROP_EXISTING\n    );\n  }\n","date":"2021-04-09 15:12:28","endLine":1021,"groupId":"14702","id":5,"instanceNumber":1,"isCurCommit":0,"methodName":"testCreateIngestionSchemaWithMaxTotalRows","params":"()","path":"/mnt/clonedata/CloneManagementServer/ManagementServer/consistResult/result-druid-10-0.7/blobInfo/CC_OUT/blobs/c4/faa5b2754ed36e4f3b71a6caa2cd64ceba3565.src","preCode":"  public void testCreateIngestionSchemaWithMaxTotalRows() throws IOException, SegmentLoadingException\n  {\n    final CompactionTask.CompactionTuningConfig tuningConfig = new CompactionTask.CompactionTuningConfig(\n        null,\n        null,\n        null,\n        500000,\n        1000000L,\n        null,\n        1000000L,\n        null,\n        null,\n        null,\n        new IndexSpec(\n            new RoaringBitmapSerdeFactory(true),\n            CompressionStrategy.LZ4,\n            CompressionStrategy.LZF,\n            LongEncodingStrategy.LONGS\n        ),\n        null,\n        null,\n        false,\n        false,\n        5000L,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null\n    );\n    final List<ParallelIndexIngestionSpec> ingestionSpecs = CompactionTask.createIngestionSchema(\n        toolbox,\n        LockGranularity.TIME_CHUNK,\n        new SegmentProvider(DATA_SOURCE, new CompactionIntervalSpec(COMPACTION_INTERVAL, null)),\n        new PartitionConfigurationManager(tuningConfig),\n        null,\n        null,\n        null,\n        COORDINATOR_CLIENT,\n        segmentLoaderFactory,\n        RETRY_POLICY_FACTORY\n    );\n    final List<DimensionsSpec> expectedDimensionsSpec = getExpectedDimensionsSpecForAutoGeneration();\n\n    ingestionSpecs.sort(\n        (s1, s2) -> Comparators.intervalsByStartThenEnd().compare(\n            s1.getDataSchema().getGranularitySpec().inputIntervals().get(0),\n            s2.getDataSchema().getGranularitySpec().inputIntervals().get(0)\n        )\n    );\n    Assert.assertEquals(6, ingestionSpecs.size());\n    assertIngestionSchema(\n        ingestionSpecs,\n        expectedDimensionsSpec,\n        AGGREGATORS,\n        SEGMENT_INTERVALS,\n        tuningConfig,\n        Granularities.MONTH,\n        Granularities.NONE\n    );\n  }\n","realPath":"indexing-service/src/test/java/org/apache/druid/indexing/common/task/CompactionTaskTest.java","repoName":"druid","snippetEndLine":0,"snippetStartLine":0,"startLine":950,"status":"M"},{"authorDate":"2021-04-09 15:12:28","commitOrder":20,"curCode":"  public void testNullGranularitySpec() throws IOException, SegmentLoadingException\n  {\n    final List<ParallelIndexIngestionSpec> ingestionSpecs = CompactionTask.createIngestionSchema(\n        toolbox,\n        LockGranularity.TIME_CHUNK,\n        new SegmentProvider(DATA_SOURCE, new CompactionIntervalSpec(COMPACTION_INTERVAL, null)),\n        new PartitionConfigurationManager(TUNING_CONFIG),\n        null,\n        null,\n        null,\n        COORDINATOR_CLIENT,\n        segmentLoaderFactory,\n        RETRY_POLICY_FACTORY,\n        IOConfig.DEFAULT_DROP_EXISTING\n    );\n    final List<DimensionsSpec> expectedDimensionsSpec = getExpectedDimensionsSpecForAutoGeneration();\n\n    ingestionSpecs.sort(\n        (s1, s2) -> Comparators.intervalsByStartThenEnd().compare(\n            s1.getDataSchema().getGranularitySpec().inputIntervals().get(0),\n            s2.getDataSchema().getGranularitySpec().inputIntervals().get(0)\n        )\n    );\n    Assert.assertEquals(6, ingestionSpecs.size());\n    assertIngestionSchema(\n        ingestionSpecs,\n        expectedDimensionsSpec,\n        AGGREGATORS,\n        SEGMENT_INTERVALS,\n        Granularities.MONTH,\n        Granularities.NONE,\n        IOConfig.DEFAULT_DROP_EXISTING\n    );\n  }\n","date":"2021-04-09 15:12:28","endLine":1456,"groupId":"14702","id":6,"instanceNumber":2,"isCurCommit":0,"methodName":"testNullGranularitySpec","params":"()","path":"/mnt/clonedata/CloneManagementServer/ManagementServer/consistResult/result-druid-10-0.7/blobInfo/CC_OUT/blobs/c4/faa5b2754ed36e4f3b71a6caa2cd64ceba3565.src","preCode":"  public void testNullGranularitySpec() throws IOException, SegmentLoadingException\n  {\n    final List<ParallelIndexIngestionSpec> ingestionSpecs = CompactionTask.createIngestionSchema(\n        toolbox,\n        LockGranularity.TIME_CHUNK,\n        new SegmentProvider(DATA_SOURCE, new CompactionIntervalSpec(COMPACTION_INTERVAL, null)),\n        new PartitionConfigurationManager(TUNING_CONFIG),\n        null,\n        null,\n        null,\n        COORDINATOR_CLIENT,\n        segmentLoaderFactory,\n        RETRY_POLICY_FACTORY\n    );\n    final List<DimensionsSpec> expectedDimensionsSpec = getExpectedDimensionsSpecForAutoGeneration();\n\n    ingestionSpecs.sort(\n        (s1, s2) -> Comparators.intervalsByStartThenEnd().compare(\n            s1.getDataSchema().getGranularitySpec().inputIntervals().get(0),\n            s2.getDataSchema().getGranularitySpec().inputIntervals().get(0)\n        )\n    );\n    Assert.assertEquals(6, ingestionSpecs.size());\n    assertIngestionSchema(\n        ingestionSpecs,\n        expectedDimensionsSpec,\n        AGGREGATORS,\n        SEGMENT_INTERVALS,\n        Granularities.MONTH,\n        Granularities.NONE\n    );\n  }\n","realPath":"indexing-service/src/test/java/org/apache/druid/indexing/common/task/CompactionTaskTest.java","repoName":"druid","snippetEndLine":0,"snippetStartLine":0,"startLine":1423,"status":"M"}],"commitId":"4576152e4a0213d17048a330e7089aa9d89f3972","commitMessage":"@@@Make dropExisting flag for Compaction configurable and add warning documentations (#11070)\n\n* Make dropExisting flag for Compaction configurable\n\n* fix checkstyle\n\n* fix checkstyle\n\n* fix test\n\n* add tests\n\n* fix spelling\n\n* fix docs\n\n* add IT\n\n* fix test\n\n* fix doc\n\n* fix doc","date":"2021-04-09 15:12:28","modifiedFileCount":"20","status":"M","submitter":"Maytas Monsereenusorn"},{"authorTime":"2021-07-21 02:44:19","codes":[{"authorDate":"2021-07-21 02:44:19","commitOrder":21,"curCode":"  public void testCreateIngestionSchemaWithMaxTotalRows() throws IOException, SegmentLoadingException\n  {\n    final CompactionTask.CompactionTuningConfig tuningConfig = new CompactionTask.CompactionTuningConfig(\n        null,\n        null,\n        null,\n        500000,\n        1000000L,\n        null,\n        1000000L,\n        null,\n        null,\n        null,\n        new IndexSpec(\n            new RoaringBitmapSerdeFactory(true),\n            CompressionStrategy.LZ4,\n            CompressionStrategy.LZF,\n            LongEncodingStrategy.LONGS\n        ),\n        null,\n        null,\n        false,\n        false,\n        5000L,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null\n    );\n    final List<ParallelIndexIngestionSpec> ingestionSpecs = CompactionTask.createIngestionSchema(\n        toolbox,\n        LockGranularity.TIME_CHUNK,\n        new SegmentProvider(DATA_SOURCE, new CompactionIntervalSpec(COMPACTION_INTERVAL, null)),\n        new PartitionConfigurationManager(tuningConfig),\n        null,\n        null,\n        null,\n        COORDINATOR_CLIENT,\n        segmentCacheManagerFactory,\n        RETRY_POLICY_FACTORY,\n        IOConfig.DEFAULT_DROP_EXISTING\n    );\n    final List<DimensionsSpec> expectedDimensionsSpec = getExpectedDimensionsSpecForAutoGeneration();\n\n    ingestionSpecs.sort(\n        (s1, s2) -> Comparators.intervalsByStartThenEnd().compare(\n            s1.getDataSchema().getGranularitySpec().inputIntervals().get(0),\n            s2.getDataSchema().getGranularitySpec().inputIntervals().get(0)\n        )\n    );\n    Assert.assertEquals(6, ingestionSpecs.size());\n    assertIngestionSchema(\n        ingestionSpecs,\n        expectedDimensionsSpec,\n        AGGREGATORS,\n        SEGMENT_INTERVALS,\n        tuningConfig,\n        Granularities.MONTH,\n        Granularities.NONE,\n        IOConfig.DEFAULT_DROP_EXISTING\n    );\n  }\n","date":"2021-07-21 02:44:19","endLine":1021,"groupId":"104516","id":7,"instanceNumber":1,"isCurCommit":0,"methodName":"testCreateIngestionSchemaWithMaxTotalRows","params":"()","path":"/mnt/clonedata/CloneManagementServer/ManagementServer/consistResult/result-druid-10-0.7/blobInfo/CC_OUT/blobs/ac/9e3d011aef9e7655bf6426661366cead8b80e9.src","preCode":"  public void testCreateIngestionSchemaWithMaxTotalRows() throws IOException, SegmentLoadingException\n  {\n    final CompactionTask.CompactionTuningConfig tuningConfig = new CompactionTask.CompactionTuningConfig(\n        null,\n        null,\n        null,\n        500000,\n        1000000L,\n        null,\n        1000000L,\n        null,\n        null,\n        null,\n        new IndexSpec(\n            new RoaringBitmapSerdeFactory(true),\n            CompressionStrategy.LZ4,\n            CompressionStrategy.LZF,\n            LongEncodingStrategy.LONGS\n        ),\n        null,\n        null,\n        false,\n        false,\n        5000L,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null\n    );\n    final List<ParallelIndexIngestionSpec> ingestionSpecs = CompactionTask.createIngestionSchema(\n        toolbox,\n        LockGranularity.TIME_CHUNK,\n        new SegmentProvider(DATA_SOURCE, new CompactionIntervalSpec(COMPACTION_INTERVAL, null)),\n        new PartitionConfigurationManager(tuningConfig),\n        null,\n        null,\n        null,\n        COORDINATOR_CLIENT,\n        segmentLoaderFactory,\n        RETRY_POLICY_FACTORY,\n        IOConfig.DEFAULT_DROP_EXISTING\n    );\n    final List<DimensionsSpec> expectedDimensionsSpec = getExpectedDimensionsSpecForAutoGeneration();\n\n    ingestionSpecs.sort(\n        (s1, s2) -> Comparators.intervalsByStartThenEnd().compare(\n            s1.getDataSchema().getGranularitySpec().inputIntervals().get(0),\n            s2.getDataSchema().getGranularitySpec().inputIntervals().get(0)\n        )\n    );\n    Assert.assertEquals(6, ingestionSpecs.size());\n    assertIngestionSchema(\n        ingestionSpecs,\n        expectedDimensionsSpec,\n        AGGREGATORS,\n        SEGMENT_INTERVALS,\n        tuningConfig,\n        Granularities.MONTH,\n        Granularities.NONE,\n        IOConfig.DEFAULT_DROP_EXISTING\n    );\n  }\n","realPath":"indexing-service/src/test/java/org/apache/druid/indexing/common/task/CompactionTaskTest.java","repoName":"druid","snippetEndLine":0,"snippetStartLine":0,"startLine":950,"status":"M"},{"authorDate":"2021-07-21 02:44:19","commitOrder":21,"curCode":"  public void testNullGranularitySpec() throws IOException, SegmentLoadingException\n  {\n    final List<ParallelIndexIngestionSpec> ingestionSpecs = CompactionTask.createIngestionSchema(\n        toolbox,\n        LockGranularity.TIME_CHUNK,\n        new SegmentProvider(DATA_SOURCE, new CompactionIntervalSpec(COMPACTION_INTERVAL, null)),\n        new PartitionConfigurationManager(TUNING_CONFIG),\n        null,\n        null,\n        null,\n        COORDINATOR_CLIENT,\n        segmentCacheManagerFactory,\n        RETRY_POLICY_FACTORY,\n        IOConfig.DEFAULT_DROP_EXISTING\n    );\n    final List<DimensionsSpec> expectedDimensionsSpec = getExpectedDimensionsSpecForAutoGeneration();\n\n    ingestionSpecs.sort(\n        (s1, s2) -> Comparators.intervalsByStartThenEnd().compare(\n            s1.getDataSchema().getGranularitySpec().inputIntervals().get(0),\n            s2.getDataSchema().getGranularitySpec().inputIntervals().get(0)\n        )\n    );\n    Assert.assertEquals(6, ingestionSpecs.size());\n    assertIngestionSchema(\n        ingestionSpecs,\n        expectedDimensionsSpec,\n        AGGREGATORS,\n        SEGMENT_INTERVALS,\n        Granularities.MONTH,\n        Granularities.NONE,\n        IOConfig.DEFAULT_DROP_EXISTING\n    );\n  }\n","date":"2021-07-21 02:44:19","endLine":1456,"groupId":"104516","id":8,"instanceNumber":2,"isCurCommit":0,"methodName":"testNullGranularitySpec","params":"()","path":"/mnt/clonedata/CloneManagementServer/ManagementServer/consistResult/result-druid-10-0.7/blobInfo/CC_OUT/blobs/ac/9e3d011aef9e7655bf6426661366cead8b80e9.src","preCode":"  public void testNullGranularitySpec() throws IOException, SegmentLoadingException\n  {\n    final List<ParallelIndexIngestionSpec> ingestionSpecs = CompactionTask.createIngestionSchema(\n        toolbox,\n        LockGranularity.TIME_CHUNK,\n        new SegmentProvider(DATA_SOURCE, new CompactionIntervalSpec(COMPACTION_INTERVAL, null)),\n        new PartitionConfigurationManager(TUNING_CONFIG),\n        null,\n        null,\n        null,\n        COORDINATOR_CLIENT,\n        segmentLoaderFactory,\n        RETRY_POLICY_FACTORY,\n        IOConfig.DEFAULT_DROP_EXISTING\n    );\n    final List<DimensionsSpec> expectedDimensionsSpec = getExpectedDimensionsSpecForAutoGeneration();\n\n    ingestionSpecs.sort(\n        (s1, s2) -> Comparators.intervalsByStartThenEnd().compare(\n            s1.getDataSchema().getGranularitySpec().inputIntervals().get(0),\n            s2.getDataSchema().getGranularitySpec().inputIntervals().get(0)\n        )\n    );\n    Assert.assertEquals(6, ingestionSpecs.size());\n    assertIngestionSchema(\n        ingestionSpecs,\n        expectedDimensionsSpec,\n        AGGREGATORS,\n        SEGMENT_INTERVALS,\n        Granularities.MONTH,\n        Granularities.NONE,\n        IOConfig.DEFAULT_DROP_EXISTING\n    );\n  }\n","realPath":"indexing-service/src/test/java/org/apache/druid/indexing/common/task/CompactionTaskTest.java","repoName":"druid","snippetEndLine":0,"snippetStartLine":0,"startLine":1423,"status":"M"}],"commitId":"94c1671eaf7b050972602fdedcb1971cdbde692d","commitMessage":"@@@Split SegmentLoader into SegmentLoader and SegmentCacheManager (#11466)\n\nThis PR splits current SegmentLoader into SegmentLoader and SegmentCacheManager.\n\nSegmentLoader - this class is responsible for building the segment object but does not expose any methods for downloading.  cache space management.  etc. Default implementation delegates the download operations to SegmentCacheManager and only contains the logic for building segments once downloaded. . This class will be used in SegmentManager to construct Segment objects.\n\nSegmentCacheManager - this class manages the segment cache on the local disk. It fetches the segment files to the local disk.  can clean up the cache.  and in the future.  support reserve and release on cache space. [See https://github.com/Make SegmentLoader extensible and customizable #11398]. This class will be used in ingestion tasks such as compaction.  re-indexing where segment files need to be downloaded locally.","date":"2021-07-21 02:44:19","modifiedFileCount":"41","status":"M","submitter":"Abhishek Agarwal"}]
