[{"authorTime":"2018-08-31 00:56:26","codes":[{"authorDate":"2018-08-31 00:56:26","commitOrder":1,"curCode":"  public void testRunTwoTasksTwoPartitions() throws Exception\n  {\n    final KafkaIndexTask task1 = createTask(\n        null,\n        new KafkaIOConfig(\n            0,\n            \"sequence0\",\n            new KafkaPartitions(topic, ImmutableMap.of(0, 2L)),\n            new KafkaPartitions(topic, ImmutableMap.of(0, 5L)),\n            kafkaServer.consumerProperties(),\n            true,\n            null,\n            null,\n            false\n        )\n    );\n    final KafkaIndexTask task2 = createTask(\n        null,\n        new KafkaIOConfig(\n            1,\n            \"sequence1\",\n            new KafkaPartitions(topic, ImmutableMap.of(1, 0L)),\n            new KafkaPartitions(topic, ImmutableMap.of(1, 1L)),\n            kafkaServer.consumerProperties(),\n            true,\n            null,\n            null,\n            false\n        )\n    );\n\n    final ListenableFuture<TaskStatus> future1 = runTask(task1);\n    final ListenableFuture<TaskStatus> future2 = runTask(task2);\n\n    \r\n    try (final KafkaProducer<byte[], byte[]> kafkaProducer = kafkaServer.newProducer()) {\n      for (ProducerRecord<byte[], byte[]> record : records) {\n        kafkaProducer.send(record).get();\n      }\n    }\n\n    \r\n    Assert.assertEquals(TaskState.SUCCESS, future1.get().getStatusCode());\n    Assert.assertEquals(TaskState.SUCCESS, future2.get().getStatusCode());\n\n    \r\n    Assert.assertEquals(3, task1.getRunner().getRowIngestionMeters().getProcessed());\n    Assert.assertEquals(0, task1.getRunner().getRowIngestionMeters().getUnparseable());\n    Assert.assertEquals(0, task1.getRunner().getRowIngestionMeters().getThrownAway());\n    Assert.assertEquals(1, task2.getRunner().getRowIngestionMeters().getProcessed());\n    Assert.assertEquals(0, task2.getRunner().getRowIngestionMeters().getUnparseable());\n    Assert.assertEquals(0, task2.getRunner().getRowIngestionMeters().getThrownAway());\n\n    \r\n    SegmentDescriptor desc1 = SD(task1, \"2010/P1D\", 0);\n    SegmentDescriptor desc2 = SD(task1, \"2011/P1D\", 0);\n    SegmentDescriptor desc3 = SD(task2, \"2012/P1D\", 0);\n    Assert.assertEquals(ImmutableSet.of(desc1, desc2, desc3), publishedDescriptors());\n    Assert.assertEquals(\n        new KafkaDataSourceMetadata(new KafkaPartitions(topic, ImmutableMap.of(0, 5L, 1, 1L))),\n        metadataStorageCoordinator.getDataSourceMetadata(DATA_SCHEMA.getDataSource())\n    );\n\n    \r\n    Assert.assertEquals(ImmutableList.of(\"c\"), readSegmentColumn(\"dim1\", desc1));\n    Assert.assertEquals(ImmutableList.of(\"d\", \"e\"), readSegmentColumn(\"dim1\", desc2));\n    Assert.assertEquals(ImmutableList.of(\"g\"), readSegmentColumn(\"dim1\", desc3));\n  }\n","date":"2018-08-31 00:56:26","endLine":1499,"groupId":"11378","id":1,"instanceNumber":1,"isCurCommit":0,"methodName":"testRunTwoTasksTwoPartitions","params":"()","path":"/mnt/clonedata/CloneManagementServer/ManagementServer/consistResult/result-druid-10-0.7/blobInfo/CC_OUT/blobs/a5/d450fc656ab61dcc37bbc18c28d1a253ebd3b3.src","preCode":"  public void testRunTwoTasksTwoPartitions() throws Exception\n  {\n    final KafkaIndexTask task1 = createTask(\n        null,\n        new KafkaIOConfig(\n            0,\n            \"sequence0\",\n            new KafkaPartitions(topic, ImmutableMap.of(0, 2L)),\n            new KafkaPartitions(topic, ImmutableMap.of(0, 5L)),\n            kafkaServer.consumerProperties(),\n            true,\n            null,\n            null,\n            false\n        )\n    );\n    final KafkaIndexTask task2 = createTask(\n        null,\n        new KafkaIOConfig(\n            1,\n            \"sequence1\",\n            new KafkaPartitions(topic, ImmutableMap.of(1, 0L)),\n            new KafkaPartitions(topic, ImmutableMap.of(1, 1L)),\n            kafkaServer.consumerProperties(),\n            true,\n            null,\n            null,\n            false\n        )\n    );\n\n    final ListenableFuture<TaskStatus> future1 = runTask(task1);\n    final ListenableFuture<TaskStatus> future2 = runTask(task2);\n\n    \r\n    try (final KafkaProducer<byte[], byte[]> kafkaProducer = kafkaServer.newProducer()) {\n      for (ProducerRecord<byte[], byte[]> record : records) {\n        kafkaProducer.send(record).get();\n      }\n    }\n\n    \r\n    Assert.assertEquals(TaskState.SUCCESS, future1.get().getStatusCode());\n    Assert.assertEquals(TaskState.SUCCESS, future2.get().getStatusCode());\n\n    \r\n    Assert.assertEquals(3, task1.getRunner().getRowIngestionMeters().getProcessed());\n    Assert.assertEquals(0, task1.getRunner().getRowIngestionMeters().getUnparseable());\n    Assert.assertEquals(0, task1.getRunner().getRowIngestionMeters().getThrownAway());\n    Assert.assertEquals(1, task2.getRunner().getRowIngestionMeters().getProcessed());\n    Assert.assertEquals(0, task2.getRunner().getRowIngestionMeters().getUnparseable());\n    Assert.assertEquals(0, task2.getRunner().getRowIngestionMeters().getThrownAway());\n\n    \r\n    SegmentDescriptor desc1 = SD(task1, \"2010/P1D\", 0);\n    SegmentDescriptor desc2 = SD(task1, \"2011/P1D\", 0);\n    SegmentDescriptor desc3 = SD(task2, \"2012/P1D\", 0);\n    Assert.assertEquals(ImmutableSet.of(desc1, desc2, desc3), publishedDescriptors());\n    Assert.assertEquals(\n        new KafkaDataSourceMetadata(new KafkaPartitions(topic, ImmutableMap.of(0, 5L, 1, 1L))),\n        metadataStorageCoordinator.getDataSourceMetadata(DATA_SCHEMA.getDataSource())\n    );\n\n    \r\n    Assert.assertEquals(ImmutableList.of(\"c\"), readSegmentColumn(\"dim1\", desc1));\n    Assert.assertEquals(ImmutableList.of(\"d\", \"e\"), readSegmentColumn(\"dim1\", desc2));\n    Assert.assertEquals(ImmutableList.of(\"g\"), readSegmentColumn(\"dim1\", desc3));\n  }\n","realPath":"extensions-core/kafka-indexing-service/src/test/java/org/apache/druid/indexing/kafka/KafkaIndexTaskTest.java","repoName":"druid","snippetEndLine":0,"snippetStartLine":0,"startLine":1432,"status":"B"},{"authorDate":"2018-08-31 00:56:26","commitOrder":1,"curCode":"  public void testRestore() throws Exception\n  {\n    final KafkaIndexTask task1 = createTask(\n        null,\n        new KafkaIOConfig(\n            0,\n            \"sequence0\",\n            new KafkaPartitions(topic, ImmutableMap.of(0, 2L)),\n            new KafkaPartitions(topic, ImmutableMap.of(0, 5L)),\n            kafkaServer.consumerProperties(),\n            true,\n            null,\n            null,\n            false\n        )\n    );\n\n    final ListenableFuture<TaskStatus> future1 = runTask(task1);\n\n    \r\n    try (final KafkaProducer<byte[], byte[]> kafkaProducer = kafkaServer.newProducer()) {\n      for (ProducerRecord<byte[], byte[]> record : Iterables.limit(records, 4)) {\n        kafkaProducer.send(record).get();\n      }\n    }\n\n    while (countEvents(task1) != 2) {\n      Thread.sleep(25);\n    }\n\n    Assert.assertEquals(2, countEvents(task1));\n\n    \r\n    task1.stopGracefully();\n    unlockAppenderatorBasePersistDirForTask(task1);\n\n    Assert.assertEquals(TaskState.SUCCESS, future1.get().getStatusCode());\n\n    \r\n    final KafkaIndexTask task2 = createTask(\n        task1.getId(),\n        new KafkaIOConfig(\n            0,\n            \"sequence0\",\n            new KafkaPartitions(topic, ImmutableMap.of(0, 2L)),\n            new KafkaPartitions(topic, ImmutableMap.of(0, 5L)),\n            kafkaServer.consumerProperties(),\n            true,\n            null,\n            null,\n            false\n        )\n    );\n\n    final ListenableFuture<TaskStatus> future2 = runTask(task2);\n\n    \r\n    try (final KafkaProducer<byte[], byte[]> kafkaProducer = kafkaServer.newProducer()) {\n      for (ProducerRecord<byte[], byte[]> record : Iterables.skip(records, 4)) {\n        kafkaProducer.send(record).get();\n      }\n    }\n\n    \r\n    Assert.assertEquals(TaskState.SUCCESS, future2.get().getStatusCode());\n\n    \r\n    Assert.assertEquals(2, task1.getRunner().getRowIngestionMeters().getProcessed());\n    Assert.assertEquals(0, task1.getRunner().getRowIngestionMeters().getUnparseable());\n    Assert.assertEquals(0, task1.getRunner().getRowIngestionMeters().getThrownAway());\n    Assert.assertEquals(1, task2.getRunner().getRowIngestionMeters().getProcessed());\n    Assert.assertEquals(0, task2.getRunner().getRowIngestionMeters().getUnparseable());\n    Assert.assertEquals(0, task2.getRunner().getRowIngestionMeters().getThrownAway());\n\n    \r\n    SegmentDescriptor desc1 = SD(task1, \"2010/P1D\", 0);\n    SegmentDescriptor desc2 = SD(task1, \"2011/P1D\", 0);\n    Assert.assertEquals(ImmutableSet.of(desc1, desc2), publishedDescriptors());\n    Assert.assertEquals(\n        new KafkaDataSourceMetadata(new KafkaPartitions(topic, ImmutableMap.of(0, 5L))),\n        metadataStorageCoordinator.getDataSourceMetadata(DATA_SCHEMA.getDataSource())\n    );\n\n    \r\n    Assert.assertEquals(ImmutableList.of(\"c\"), readSegmentColumn(\"dim1\", desc1));\n    Assert.assertEquals(ImmutableList.of(\"d\", \"e\"), readSegmentColumn(\"dim1\", desc2));\n  }\n","date":"2018-08-31 00:56:26","endLine":1588,"groupId":"8576","id":2,"instanceNumber":2,"isCurCommit":0,"methodName":"testRestore","params":"()","path":"/mnt/clonedata/CloneManagementServer/ManagementServer/consistResult/result-druid-10-0.7/blobInfo/CC_OUT/blobs/a5/d450fc656ab61dcc37bbc18c28d1a253ebd3b3.src","preCode":"  public void testRestore() throws Exception\n  {\n    final KafkaIndexTask task1 = createTask(\n        null,\n        new KafkaIOConfig(\n            0,\n            \"sequence0\",\n            new KafkaPartitions(topic, ImmutableMap.of(0, 2L)),\n            new KafkaPartitions(topic, ImmutableMap.of(0, 5L)),\n            kafkaServer.consumerProperties(),\n            true,\n            null,\n            null,\n            false\n        )\n    );\n\n    final ListenableFuture<TaskStatus> future1 = runTask(task1);\n\n    \r\n    try (final KafkaProducer<byte[], byte[]> kafkaProducer = kafkaServer.newProducer()) {\n      for (ProducerRecord<byte[], byte[]> record : Iterables.limit(records, 4)) {\n        kafkaProducer.send(record).get();\n      }\n    }\n\n    while (countEvents(task1) != 2) {\n      Thread.sleep(25);\n    }\n\n    Assert.assertEquals(2, countEvents(task1));\n\n    \r\n    task1.stopGracefully();\n    unlockAppenderatorBasePersistDirForTask(task1);\n\n    Assert.assertEquals(TaskState.SUCCESS, future1.get().getStatusCode());\n\n    \r\n    final KafkaIndexTask task2 = createTask(\n        task1.getId(),\n        new KafkaIOConfig(\n            0,\n            \"sequence0\",\n            new KafkaPartitions(topic, ImmutableMap.of(0, 2L)),\n            new KafkaPartitions(topic, ImmutableMap.of(0, 5L)),\n            kafkaServer.consumerProperties(),\n            true,\n            null,\n            null,\n            false\n        )\n    );\n\n    final ListenableFuture<TaskStatus> future2 = runTask(task2);\n\n    \r\n    try (final KafkaProducer<byte[], byte[]> kafkaProducer = kafkaServer.newProducer()) {\n      for (ProducerRecord<byte[], byte[]> record : Iterables.skip(records, 4)) {\n        kafkaProducer.send(record).get();\n      }\n    }\n\n    \r\n    Assert.assertEquals(TaskState.SUCCESS, future2.get().getStatusCode());\n\n    \r\n    Assert.assertEquals(2, task1.getRunner().getRowIngestionMeters().getProcessed());\n    Assert.assertEquals(0, task1.getRunner().getRowIngestionMeters().getUnparseable());\n    Assert.assertEquals(0, task1.getRunner().getRowIngestionMeters().getThrownAway());\n    Assert.assertEquals(1, task2.getRunner().getRowIngestionMeters().getProcessed());\n    Assert.assertEquals(0, task2.getRunner().getRowIngestionMeters().getUnparseable());\n    Assert.assertEquals(0, task2.getRunner().getRowIngestionMeters().getThrownAway());\n\n    \r\n    SegmentDescriptor desc1 = SD(task1, \"2010/P1D\", 0);\n    SegmentDescriptor desc2 = SD(task1, \"2011/P1D\", 0);\n    Assert.assertEquals(ImmutableSet.of(desc1, desc2), publishedDescriptors());\n    Assert.assertEquals(\n        new KafkaDataSourceMetadata(new KafkaPartitions(topic, ImmutableMap.of(0, 5L))),\n        metadataStorageCoordinator.getDataSourceMetadata(DATA_SCHEMA.getDataSource())\n    );\n\n    \r\n    Assert.assertEquals(ImmutableList.of(\"c\"), readSegmentColumn(\"dim1\", desc1));\n    Assert.assertEquals(ImmutableList.of(\"d\", \"e\"), readSegmentColumn(\"dim1\", desc2));\n  }\n","realPath":"extensions-core/kafka-indexing-service/src/test/java/org/apache/druid/indexing/kafka/KafkaIndexTaskTest.java","repoName":"druid","snippetEndLine":0,"snippetStartLine":0,"startLine":1502,"status":"B"}],"commitId":"431d3d8497f9079857c3baa7ae7ab4cb44a22355","commitMessage":"@@@Rename io.druid to org.apache.druid. (#6266)\n\n* Rename io.druid to org.apache.druid.\n\n* Fix META-INF files and remove some benchmark results.\n\n* MonitorsConfig update for metrics package migration.\n\n* Reorder some dimensions in inner queries for some reason.\n\n* Fix protobuf tests.\n","date":"2018-08-31 00:56:26","modifiedFileCount":"5","status":"B","submitter":"Gian Merlino"},{"authorTime":"2018-12-22 03:49:24","codes":[{"authorDate":"2018-12-22 03:49:24","commitOrder":2,"curCode":"  public void testRunTwoTasksTwoPartitions() throws Exception\n  {\n    final KafkaIndexTask task1 = createTask(\n        null,\n        new KafkaIndexTaskIOConfig(\n            0,\n            \"sequence0\",\n            new SeekableStreamPartitions<>(topic, ImmutableMap.of(0, 2L)),\n            new SeekableStreamPartitions<>(topic, ImmutableMap.of(0, 5L)),\n            kafkaServer.consumerProperties(),\n            true,\n            null,\n            null,\n            false\n        )\n    );\n    final KafkaIndexTask task2 = createTask(\n        null,\n        new KafkaIndexTaskIOConfig(\n            1,\n            \"sequence1\",\n            new SeekableStreamPartitions<>(topic, ImmutableMap.of(1, 0L)),\n            new SeekableStreamPartitions<>(topic, ImmutableMap.of(1, 1L)),\n            kafkaServer.consumerProperties(),\n            true,\n            null,\n            null,\n            false\n        )\n    );\n\n    final ListenableFuture<TaskStatus> future1 = runTask(task1);\n    final ListenableFuture<TaskStatus> future2 = runTask(task2);\n\n    \r\n    try (final KafkaProducer<byte[], byte[]> kafkaProducer = kafkaServer.newProducer()) {\n      for (ProducerRecord<byte[], byte[]> record : records) {\n        kafkaProducer.send(record).get();\n      }\n    }\n\n    \r\n    Assert.assertEquals(TaskState.SUCCESS, future1.get().getStatusCode());\n    Assert.assertEquals(TaskState.SUCCESS, future2.get().getStatusCode());\n\n    \r\n    Assert.assertEquals(3, task1.getRunner().getRowIngestionMeters().getProcessed());\n    Assert.assertEquals(0, task1.getRunner().getRowIngestionMeters().getUnparseable());\n    Assert.assertEquals(0, task1.getRunner().getRowIngestionMeters().getThrownAway());\n    Assert.assertEquals(1, task2.getRunner().getRowIngestionMeters().getProcessed());\n    Assert.assertEquals(0, task2.getRunner().getRowIngestionMeters().getUnparseable());\n    Assert.assertEquals(0, task2.getRunner().getRowIngestionMeters().getThrownAway());\n\n    \r\n    SegmentDescriptor desc1 = SD(task1, \"2010/P1D\", 0);\n    SegmentDescriptor desc2 = SD(task1, \"2011/P1D\", 0);\n    SegmentDescriptor desc3 = SD(task2, \"2012/P1D\", 0);\n    Assert.assertEquals(ImmutableSet.of(desc1, desc2, desc3), publishedDescriptors());\n    Assert.assertEquals(\n        new KafkaDataSourceMetadata(new SeekableStreamPartitions<>(topic, ImmutableMap.of(0, 5L, 1, 1L))),\n        metadataStorageCoordinator.getDataSourceMetadata(DATA_SCHEMA.getDataSource())\n    );\n\n    \r\n    Assert.assertEquals(ImmutableList.of(\"c\"), readSegmentColumn(\"dim1\", desc1));\n    Assert.assertEquals(ImmutableList.of(\"d\", \"e\"), readSegmentColumn(\"dim1\", desc2));\n    Assert.assertEquals(ImmutableList.of(\"g\"), readSegmentColumn(\"dim1\", desc3));\n  }\n","date":"2018-12-22 03:49:24","endLine":1789,"groupId":"6247","id":3,"instanceNumber":1,"isCurCommit":0,"methodName":"testRunTwoTasksTwoPartitions","params":"()","path":"/mnt/clonedata/CloneManagementServer/ManagementServer/consistResult/result-druid-10-0.7/blobInfo/CC_OUT/blobs/72/041eb947d5fcc0208ff2a5427dfc03aa78f820.src","preCode":"  public void testRunTwoTasksTwoPartitions() throws Exception\n  {\n    final KafkaIndexTask task1 = createTask(\n        null,\n        new KafkaIOConfig(\n            0,\n            \"sequence0\",\n            new KafkaPartitions(topic, ImmutableMap.of(0, 2L)),\n            new KafkaPartitions(topic, ImmutableMap.of(0, 5L)),\n            kafkaServer.consumerProperties(),\n            true,\n            null,\n            null,\n            false\n        )\n    );\n    final KafkaIndexTask task2 = createTask(\n        null,\n        new KafkaIOConfig(\n            1,\n            \"sequence1\",\n            new KafkaPartitions(topic, ImmutableMap.of(1, 0L)),\n            new KafkaPartitions(topic, ImmutableMap.of(1, 1L)),\n            kafkaServer.consumerProperties(),\n            true,\n            null,\n            null,\n            false\n        )\n    );\n\n    final ListenableFuture<TaskStatus> future1 = runTask(task1);\n    final ListenableFuture<TaskStatus> future2 = runTask(task2);\n\n    \r\n    try (final KafkaProducer<byte[], byte[]> kafkaProducer = kafkaServer.newProducer()) {\n      for (ProducerRecord<byte[], byte[]> record : records) {\n        kafkaProducer.send(record).get();\n      }\n    }\n\n    \r\n    Assert.assertEquals(TaskState.SUCCESS, future1.get().getStatusCode());\n    Assert.assertEquals(TaskState.SUCCESS, future2.get().getStatusCode());\n\n    \r\n    Assert.assertEquals(3, task1.getRunner().getRowIngestionMeters().getProcessed());\n    Assert.assertEquals(0, task1.getRunner().getRowIngestionMeters().getUnparseable());\n    Assert.assertEquals(0, task1.getRunner().getRowIngestionMeters().getThrownAway());\n    Assert.assertEquals(1, task2.getRunner().getRowIngestionMeters().getProcessed());\n    Assert.assertEquals(0, task2.getRunner().getRowIngestionMeters().getUnparseable());\n    Assert.assertEquals(0, task2.getRunner().getRowIngestionMeters().getThrownAway());\n\n    \r\n    SegmentDescriptor desc1 = SD(task1, \"2010/P1D\", 0);\n    SegmentDescriptor desc2 = SD(task1, \"2011/P1D\", 0);\n    SegmentDescriptor desc3 = SD(task2, \"2012/P1D\", 0);\n    Assert.assertEquals(ImmutableSet.of(desc1, desc2, desc3), publishedDescriptors());\n    Assert.assertEquals(\n        new KafkaDataSourceMetadata(new KafkaPartitions(topic, ImmutableMap.of(0, 5L, 1, 1L))),\n        metadataStorageCoordinator.getDataSourceMetadata(DATA_SCHEMA.getDataSource())\n    );\n\n    \r\n    Assert.assertEquals(ImmutableList.of(\"c\"), readSegmentColumn(\"dim1\", desc1));\n    Assert.assertEquals(ImmutableList.of(\"d\", \"e\"), readSegmentColumn(\"dim1\", desc2));\n    Assert.assertEquals(ImmutableList.of(\"g\"), readSegmentColumn(\"dim1\", desc3));\n  }\n","realPath":"extensions-core/kafka-indexing-service/src/test/java/org/apache/druid/indexing/kafka/KafkaIndexTaskTest.java","repoName":"druid","snippetEndLine":0,"snippetStartLine":0,"startLine":1722,"status":"M"},{"authorDate":"2018-12-22 03:49:24","commitOrder":2,"curCode":"  public void testRestore() throws Exception\n  {\n    final KafkaIndexTask task1 = createTask(\n        null,\n        new KafkaIndexTaskIOConfig(\n            0,\n            \"sequence0\",\n            new SeekableStreamPartitions<>(topic, ImmutableMap.of(0, 2L)),\n            new SeekableStreamPartitions<>(topic, ImmutableMap.of(0, 5L)),\n            kafkaServer.consumerProperties(),\n            true,\n            null,\n            null,\n            false\n        )\n    );\n\n    final ListenableFuture<TaskStatus> future1 = runTask(task1);\n\n    \r\n    try (final KafkaProducer<byte[], byte[]> kafkaProducer = kafkaServer.newProducer()) {\n      for (ProducerRecord<byte[], byte[]> record : Iterables.limit(records, 4)) {\n        kafkaProducer.send(record).get();\n      }\n    }\n\n    while (countEvents(task1) != 2) {\n      Thread.sleep(25);\n    }\n\n    Assert.assertEquals(2, countEvents(task1));\n\n    \r\n    task1.stopGracefully();\n    unlockAppenderatorBasePersistDirForTask(task1);\n\n    Assert.assertEquals(TaskState.SUCCESS, future1.get().getStatusCode());\n\n    \r\n    final KafkaIndexTask task2 = createTask(\n        task1.getId(),\n        new KafkaIndexTaskIOConfig(\n            0,\n            \"sequence0\",\n            new SeekableStreamPartitions<>(topic, ImmutableMap.of(0, 2L)),\n            new SeekableStreamPartitions<>(topic, ImmutableMap.of(0, 5L)),\n            kafkaServer.consumerProperties(),\n            true,\n            null,\n            null,\n            false\n        )\n    );\n\n    final ListenableFuture<TaskStatus> future2 = runTask(task2);\n\n    \r\n    try (final KafkaProducer<byte[], byte[]> kafkaProducer = kafkaServer.newProducer()) {\n      for (ProducerRecord<byte[], byte[]> record : Iterables.skip(records, 4)) {\n        kafkaProducer.send(record).get();\n      }\n    }\n\n    \r\n    Assert.assertEquals(TaskState.SUCCESS, future2.get().getStatusCode());\n\n    \r\n    Assert.assertEquals(2, task1.getRunner().getRowIngestionMeters().getProcessed());\n    Assert.assertEquals(0, task1.getRunner().getRowIngestionMeters().getUnparseable());\n    Assert.assertEquals(0, task1.getRunner().getRowIngestionMeters().getThrownAway());\n    Assert.assertEquals(1, task2.getRunner().getRowIngestionMeters().getProcessed());\n    Assert.assertEquals(0, task2.getRunner().getRowIngestionMeters().getUnparseable());\n    Assert.assertEquals(0, task2.getRunner().getRowIngestionMeters().getThrownAway());\n\n    \r\n    SegmentDescriptor desc1 = SD(task1, \"2010/P1D\", 0);\n    SegmentDescriptor desc2 = SD(task1, \"2011/P1D\", 0);\n    Assert.assertEquals(ImmutableSet.of(desc1, desc2), publishedDescriptors());\n    Assert.assertEquals(\n        new KafkaDataSourceMetadata(new SeekableStreamPartitions<>(topic, ImmutableMap.of(0, 5L))),\n        metadataStorageCoordinator.getDataSourceMetadata(DATA_SCHEMA.getDataSource())\n    );\n\n    \r\n    Assert.assertEquals(ImmutableList.of(\"c\"), readSegmentColumn(\"dim1\", desc1));\n    Assert.assertEquals(ImmutableList.of(\"d\", \"e\"), readSegmentColumn(\"dim1\", desc2));\n  }\n","date":"2018-12-22 03:49:24","endLine":1878,"groupId":"0","id":4,"instanceNumber":2,"isCurCommit":0,"methodName":"testRestore","params":"()","path":"/mnt/clonedata/CloneManagementServer/ManagementServer/consistResult/result-druid-10-0.7/blobInfo/CC_OUT/blobs/72/041eb947d5fcc0208ff2a5427dfc03aa78f820.src","preCode":"  public void testRestore() throws Exception\n  {\n    final KafkaIndexTask task1 = createTask(\n        null,\n        new KafkaIOConfig(\n            0,\n            \"sequence0\",\n            new KafkaPartitions(topic, ImmutableMap.of(0, 2L)),\n            new KafkaPartitions(topic, ImmutableMap.of(0, 5L)),\n            kafkaServer.consumerProperties(),\n            true,\n            null,\n            null,\n            false\n        )\n    );\n\n    final ListenableFuture<TaskStatus> future1 = runTask(task1);\n\n    \r\n    try (final KafkaProducer<byte[], byte[]> kafkaProducer = kafkaServer.newProducer()) {\n      for (ProducerRecord<byte[], byte[]> record : Iterables.limit(records, 4)) {\n        kafkaProducer.send(record).get();\n      }\n    }\n\n    while (countEvents(task1) != 2) {\n      Thread.sleep(25);\n    }\n\n    Assert.assertEquals(2, countEvents(task1));\n\n    \r\n    task1.stopGracefully();\n    unlockAppenderatorBasePersistDirForTask(task1);\n\n    Assert.assertEquals(TaskState.SUCCESS, future1.get().getStatusCode());\n\n    \r\n    final KafkaIndexTask task2 = createTask(\n        task1.getId(),\n        new KafkaIOConfig(\n            0,\n            \"sequence0\",\n            new KafkaPartitions(topic, ImmutableMap.of(0, 2L)),\n            new KafkaPartitions(topic, ImmutableMap.of(0, 5L)),\n            kafkaServer.consumerProperties(),\n            true,\n            null,\n            null,\n            false\n        )\n    );\n\n    final ListenableFuture<TaskStatus> future2 = runTask(task2);\n\n    \r\n    try (final KafkaProducer<byte[], byte[]> kafkaProducer = kafkaServer.newProducer()) {\n      for (ProducerRecord<byte[], byte[]> record : Iterables.skip(records, 4)) {\n        kafkaProducer.send(record).get();\n      }\n    }\n\n    \r\n    Assert.assertEquals(TaskState.SUCCESS, future2.get().getStatusCode());\n\n    \r\n    Assert.assertEquals(2, task1.getRunner().getRowIngestionMeters().getProcessed());\n    Assert.assertEquals(0, task1.getRunner().getRowIngestionMeters().getUnparseable());\n    Assert.assertEquals(0, task1.getRunner().getRowIngestionMeters().getThrownAway());\n    Assert.assertEquals(1, task2.getRunner().getRowIngestionMeters().getProcessed());\n    Assert.assertEquals(0, task2.getRunner().getRowIngestionMeters().getUnparseable());\n    Assert.assertEquals(0, task2.getRunner().getRowIngestionMeters().getThrownAway());\n\n    \r\n    SegmentDescriptor desc1 = SD(task1, \"2010/P1D\", 0);\n    SegmentDescriptor desc2 = SD(task1, \"2011/P1D\", 0);\n    Assert.assertEquals(ImmutableSet.of(desc1, desc2), publishedDescriptors());\n    Assert.assertEquals(\n        new KafkaDataSourceMetadata(new KafkaPartitions(topic, ImmutableMap.of(0, 5L))),\n        metadataStorageCoordinator.getDataSourceMetadata(DATA_SCHEMA.getDataSource())\n    );\n\n    \r\n    Assert.assertEquals(ImmutableList.of(\"c\"), readSegmentColumn(\"dim1\", desc1));\n    Assert.assertEquals(ImmutableList.of(\"d\", \"e\"), readSegmentColumn(\"dim1\", desc2));\n  }\n","realPath":"extensions-core/kafka-indexing-service/src/test/java/org/apache/druid/indexing/kafka/KafkaIndexTaskTest.java","repoName":"druid","snippetEndLine":0,"snippetStartLine":0,"startLine":1792,"status":"M"}],"commitId":"7c7997e8a1183a7bffad731ca94e8b4c381e8665","commitMessage":"@@@Add Kinesis Indexing Service to core Druid (#6431)\n\n* created seekablestream classes\n\n* created seekablestreamsupervisor class\n\n* first attempt to integrate kafa indexing service to use SeekableStream\n\n* seekablestream bug fixes\n\n* kafkarecordsupplier\n\n* integrated kafka indexing service with seekablestream\n\n* implemented resume/suspend and refactored some package names\n\n* moved kinesis indexing service into core druid extensions\n\n* merged some changes from kafka supervisor race condition\n\n* integrated kinesis-indexing-service with seekablestream\n\n* unite tests for kinesis-indexing-service\n\n* various bug fixes for kinesis-indexing-service\n\n* refactored kinesisindexingtask\n\n* finished up more kinesis unit tests\n\n* more bug fixes for kinesis-indexing-service\n\n* finsihed refactoring kinesis unit tests\n\n* removed KinesisParititons and KafkaPartitions to use SeekableStreamPartitions\n\n* kinesis-indexing-service code cleanup and docs\n\n* merge #6291\n\nmerge #6337\n\nmerge #6383\n\n* added more docs and reordered methods\n\n* fixd kinesis tests after merging master and added docs in seekablestream\n\n* fix various things from pr comment\n\n* improve recordsupplier and add unit tests\n\n* migrated to aws-java-sdk-kinesis\n\n* merge changes from master\n\n* fix pom files and forbiddenapi checks\n\n* checkpoint JavaType bug fix\n\n* fix pom and stuff\n\n* disable checkpointing in kinesis\n\n* fix kinesis sequence number null in closed shard\n\n* merge changes from master\n\n* fixes for kinesis tasks\n\n* capitalized <partitionType.  sequenceType>\n\n* removed abstract class loggers\n\n* conform to guava api restrictions\n\n* add docker for travis other modules test\n\n* address comments\n\n* improve RecordSupplier to supply records in batch\n\n* fix strict compile issue\n\n* add test scope for localstack dependency\n\n* kinesis indexing task refactoring\n\n* comments\n\n* github comments\n\n* minor fix\n\n* removed unneeded readme\n\n* fix deserialization bug\n\n* fix various bugs\n\n* KinesisRecordSupplier unable to catch up to earliest position in stream bug fix\n\n* minor changes to kinesis\n\n* implement deaggregate for kinesis\n\n* Merge remote-tracking branch 'upstream/master' into seekablestream\n\n* fix kinesis offset discrepancy with kafka\n\n* kinesis record supplier disable getPosition\n\n* pr comments\n\n* mock for kinesis tests and remove docker dependency for unit tests\n\n* PR comments\n\n* avg lag in kafkasupervisor #6587\n\n* refacotred SequenceMetadata in taskRunners\n\n* small fix\n\n* more small fix\n\n* recordsupplier resource leak\n\n* revert .travis.yml formatting\n\n* fix style\n\n* kinesis docs\n\n* doc part2\n\n* more docs\n\n* comments\n\n* comments*2\n\n* revert string replace changes\n\n* comments\n\n* teamcity\n\n* comments part 1\n\n* comments part 2\n\n* comments part 3\n\n* merge #6754\n\n* fix injection binding\n\n* comments\n\n* KinesisRegion refactor\n\n* comments part idk lol\n\n* can't think of a commit msg anymore\n\n* remove possiblyResetDataSourceMetadata() for IncrementalPublishingTaskRunner\n\n* commmmmmmmmmments\n\n* extra error handling in KinesisRecordSupplier getRecords\n\n* comments\n\n* quickfix\n\n* typo\n\n* oof\n","date":"2018-12-22 03:49:24","modifiedFileCount":"22","status":"M","submitter":"Joshua Sun"},{"authorTime":"2019-01-03 12:16:02","codes":[{"authorDate":"2019-01-03 12:16:02","commitOrder":3,"curCode":"  public void testRunTwoTasksTwoPartitions() throws Exception\n  {\n    final KafkaIndexTask task1 = createTask(\n        null,\n        new KafkaIndexTaskIOConfig(\n            0,\n            \"sequence0\",\n            new SeekableStreamPartitions<>(topic, ImmutableMap.of(0, 2L)),\n            new SeekableStreamPartitions<>(topic, ImmutableMap.of(0, 5L)),\n            kafkaServer.consumerProperties(),\n            KafkaSupervisorIOConfig.DEFAULT_POLL_TIMEOUT_MILLIS,\n            true,\n            null,\n            null,\n            false\n        )\n    );\n    final KafkaIndexTask task2 = createTask(\n        null,\n        new KafkaIndexTaskIOConfig(\n            1,\n            \"sequence1\",\n            new SeekableStreamPartitions<>(topic, ImmutableMap.of(1, 0L)),\n            new SeekableStreamPartitions<>(topic, ImmutableMap.of(1, 1L)),\n            kafkaServer.consumerProperties(),\n            KafkaSupervisorIOConfig.DEFAULT_POLL_TIMEOUT_MILLIS,\n            true,\n            null,\n            null,\n            false\n        )\n    );\n\n    final ListenableFuture<TaskStatus> future1 = runTask(task1);\n    final ListenableFuture<TaskStatus> future2 = runTask(task2);\n\n    \r\n    try (final KafkaProducer<byte[], byte[]> kafkaProducer = kafkaServer.newProducer()) {\n      for (ProducerRecord<byte[], byte[]> record : records) {\n        kafkaProducer.send(record).get();\n      }\n    }\n\n    \r\n    Assert.assertEquals(TaskState.SUCCESS, future1.get().getStatusCode());\n    Assert.assertEquals(TaskState.SUCCESS, future2.get().getStatusCode());\n\n    \r\n    Assert.assertEquals(3, task1.getRunner().getRowIngestionMeters().getProcessed());\n    Assert.assertEquals(0, task1.getRunner().getRowIngestionMeters().getUnparseable());\n    Assert.assertEquals(0, task1.getRunner().getRowIngestionMeters().getThrownAway());\n    Assert.assertEquals(1, task2.getRunner().getRowIngestionMeters().getProcessed());\n    Assert.assertEquals(0, task2.getRunner().getRowIngestionMeters().getUnparseable());\n    Assert.assertEquals(0, task2.getRunner().getRowIngestionMeters().getThrownAway());\n\n    \r\n    SegmentDescriptor desc1 = SD(task1, \"2010/P1D\", 0);\n    SegmentDescriptor desc2 = SD(task1, \"2011/P1D\", 0);\n    SegmentDescriptor desc3 = SD(task2, \"2012/P1D\", 0);\n    Assert.assertEquals(ImmutableSet.of(desc1, desc2, desc3), publishedDescriptors());\n    Assert.assertEquals(\n        new KafkaDataSourceMetadata(new SeekableStreamPartitions<>(topic, ImmutableMap.of(0, 5L, 1, 1L))),\n        metadataStorageCoordinator.getDataSourceMetadata(DATA_SCHEMA.getDataSource())\n    );\n\n    \r\n    Assert.assertEquals(ImmutableList.of(\"c\"), readSegmentColumn(\"dim1\", desc1));\n    Assert.assertEquals(ImmutableList.of(\"d\", \"e\"), readSegmentColumn(\"dim1\", desc2));\n    Assert.assertEquals(ImmutableList.of(\"g\"), readSegmentColumn(\"dim1\", desc3));\n  }\n","date":"2019-01-03 12:16:02","endLine":1814,"groupId":"6247","id":5,"instanceNumber":1,"isCurCommit":0,"methodName":"testRunTwoTasksTwoPartitions","params":"()","path":"/mnt/clonedata/CloneManagementServer/ManagementServer/consistResult/result-druid-10-0.7/blobInfo/CC_OUT/blobs/f4/4877771fbd76d2e48f167fa6e25fc70e074abc.src","preCode":"  public void testRunTwoTasksTwoPartitions() throws Exception\n  {\n    final KafkaIndexTask task1 = createTask(\n        null,\n        new KafkaIndexTaskIOConfig(\n            0,\n            \"sequence0\",\n            new SeekableStreamPartitions<>(topic, ImmutableMap.of(0, 2L)),\n            new SeekableStreamPartitions<>(topic, ImmutableMap.of(0, 5L)),\n            kafkaServer.consumerProperties(),\n            true,\n            null,\n            null,\n            false\n        )\n    );\n    final KafkaIndexTask task2 = createTask(\n        null,\n        new KafkaIndexTaskIOConfig(\n            1,\n            \"sequence1\",\n            new SeekableStreamPartitions<>(topic, ImmutableMap.of(1, 0L)),\n            new SeekableStreamPartitions<>(topic, ImmutableMap.of(1, 1L)),\n            kafkaServer.consumerProperties(),\n            true,\n            null,\n            null,\n            false\n        )\n    );\n\n    final ListenableFuture<TaskStatus> future1 = runTask(task1);\n    final ListenableFuture<TaskStatus> future2 = runTask(task2);\n\n    \r\n    try (final KafkaProducer<byte[], byte[]> kafkaProducer = kafkaServer.newProducer()) {\n      for (ProducerRecord<byte[], byte[]> record : records) {\n        kafkaProducer.send(record).get();\n      }\n    }\n\n    \r\n    Assert.assertEquals(TaskState.SUCCESS, future1.get().getStatusCode());\n    Assert.assertEquals(TaskState.SUCCESS, future2.get().getStatusCode());\n\n    \r\n    Assert.assertEquals(3, task1.getRunner().getRowIngestionMeters().getProcessed());\n    Assert.assertEquals(0, task1.getRunner().getRowIngestionMeters().getUnparseable());\n    Assert.assertEquals(0, task1.getRunner().getRowIngestionMeters().getThrownAway());\n    Assert.assertEquals(1, task2.getRunner().getRowIngestionMeters().getProcessed());\n    Assert.assertEquals(0, task2.getRunner().getRowIngestionMeters().getUnparseable());\n    Assert.assertEquals(0, task2.getRunner().getRowIngestionMeters().getThrownAway());\n\n    \r\n    SegmentDescriptor desc1 = SD(task1, \"2010/P1D\", 0);\n    SegmentDescriptor desc2 = SD(task1, \"2011/P1D\", 0);\n    SegmentDescriptor desc3 = SD(task2, \"2012/P1D\", 0);\n    Assert.assertEquals(ImmutableSet.of(desc1, desc2, desc3), publishedDescriptors());\n    Assert.assertEquals(\n        new KafkaDataSourceMetadata(new SeekableStreamPartitions<>(topic, ImmutableMap.of(0, 5L, 1, 1L))),\n        metadataStorageCoordinator.getDataSourceMetadata(DATA_SCHEMA.getDataSource())\n    );\n\n    \r\n    Assert.assertEquals(ImmutableList.of(\"c\"), readSegmentColumn(\"dim1\", desc1));\n    Assert.assertEquals(ImmutableList.of(\"d\", \"e\"), readSegmentColumn(\"dim1\", desc2));\n    Assert.assertEquals(ImmutableList.of(\"g\"), readSegmentColumn(\"dim1\", desc3));\n  }\n","realPath":"extensions-core/kafka-indexing-service/src/test/java/org/apache/druid/indexing/kafka/KafkaIndexTaskTest.java","repoName":"druid","snippetEndLine":0,"snippetStartLine":0,"startLine":1745,"status":"M"},{"authorDate":"2019-01-03 12:16:02","commitOrder":3,"curCode":"  public void testRestore() throws Exception\n  {\n    final KafkaIndexTask task1 = createTask(\n        null,\n        new KafkaIndexTaskIOConfig(\n            0,\n            \"sequence0\",\n            new SeekableStreamPartitions<>(topic, ImmutableMap.of(0, 2L)),\n            new SeekableStreamPartitions<>(topic, ImmutableMap.of(0, 5L)),\n            kafkaServer.consumerProperties(),\n            KafkaSupervisorIOConfig.DEFAULT_POLL_TIMEOUT_MILLIS,\n            true,\n            null,\n            null,\n            false\n        )\n    );\n\n    final ListenableFuture<TaskStatus> future1 = runTask(task1);\n\n    \r\n    try (final KafkaProducer<byte[], byte[]> kafkaProducer = kafkaServer.newProducer()) {\n      for (ProducerRecord<byte[], byte[]> record : Iterables.limit(records, 4)) {\n        kafkaProducer.send(record).get();\n      }\n    }\n\n    while (countEvents(task1) != 2) {\n      Thread.sleep(25);\n    }\n\n    Assert.assertEquals(2, countEvents(task1));\n\n    \r\n    task1.stopGracefully();\n    unlockAppenderatorBasePersistDirForTask(task1);\n\n    Assert.assertEquals(TaskState.SUCCESS, future1.get().getStatusCode());\n\n    \r\n    final KafkaIndexTask task2 = createTask(\n        task1.getId(),\n        new KafkaIndexTaskIOConfig(\n            0,\n            \"sequence0\",\n            new SeekableStreamPartitions<>(topic, ImmutableMap.of(0, 2L)),\n            new SeekableStreamPartitions<>(topic, ImmutableMap.of(0, 5L)),\n            kafkaServer.consumerProperties(),\n            KafkaSupervisorIOConfig.DEFAULT_POLL_TIMEOUT_MILLIS,\n            true,\n            null,\n            null,\n            false\n        )\n    );\n\n    final ListenableFuture<TaskStatus> future2 = runTask(task2);\n\n    \r\n    try (final KafkaProducer<byte[], byte[]> kafkaProducer = kafkaServer.newProducer()) {\n      for (ProducerRecord<byte[], byte[]> record : Iterables.skip(records, 4)) {\n        kafkaProducer.send(record).get();\n      }\n    }\n\n    \r\n    Assert.assertEquals(TaskState.SUCCESS, future2.get().getStatusCode());\n\n    \r\n    Assert.assertEquals(2, task1.getRunner().getRowIngestionMeters().getProcessed());\n    Assert.assertEquals(0, task1.getRunner().getRowIngestionMeters().getUnparseable());\n    Assert.assertEquals(0, task1.getRunner().getRowIngestionMeters().getThrownAway());\n    Assert.assertEquals(1, task2.getRunner().getRowIngestionMeters().getProcessed());\n    Assert.assertEquals(0, task2.getRunner().getRowIngestionMeters().getUnparseable());\n    Assert.assertEquals(0, task2.getRunner().getRowIngestionMeters().getThrownAway());\n\n    \r\n    SegmentDescriptor desc1 = SD(task1, \"2010/P1D\", 0);\n    SegmentDescriptor desc2 = SD(task1, \"2011/P1D\", 0);\n    Assert.assertEquals(ImmutableSet.of(desc1, desc2), publishedDescriptors());\n    Assert.assertEquals(\n        new KafkaDataSourceMetadata(new SeekableStreamPartitions<>(topic, ImmutableMap.of(0, 5L))),\n        metadataStorageCoordinator.getDataSourceMetadata(DATA_SCHEMA.getDataSource())\n    );\n\n    \r\n    Assert.assertEquals(ImmutableList.of(\"c\"), readSegmentColumn(\"dim1\", desc1));\n    Assert.assertEquals(ImmutableList.of(\"d\", \"e\"), readSegmentColumn(\"dim1\", desc2));\n  }\n","date":"2019-01-03 12:16:02","endLine":1905,"groupId":"20856","id":6,"instanceNumber":2,"isCurCommit":0,"methodName":"testRestore","params":"()","path":"/mnt/clonedata/CloneManagementServer/ManagementServer/consistResult/result-druid-10-0.7/blobInfo/CC_OUT/blobs/f4/4877771fbd76d2e48f167fa6e25fc70e074abc.src","preCode":"  public void testRestore() throws Exception\n  {\n    final KafkaIndexTask task1 = createTask(\n        null,\n        new KafkaIndexTaskIOConfig(\n            0,\n            \"sequence0\",\n            new SeekableStreamPartitions<>(topic, ImmutableMap.of(0, 2L)),\n            new SeekableStreamPartitions<>(topic, ImmutableMap.of(0, 5L)),\n            kafkaServer.consumerProperties(),\n            true,\n            null,\n            null,\n            false\n        )\n    );\n\n    final ListenableFuture<TaskStatus> future1 = runTask(task1);\n\n    \r\n    try (final KafkaProducer<byte[], byte[]> kafkaProducer = kafkaServer.newProducer()) {\n      for (ProducerRecord<byte[], byte[]> record : Iterables.limit(records, 4)) {\n        kafkaProducer.send(record).get();\n      }\n    }\n\n    while (countEvents(task1) != 2) {\n      Thread.sleep(25);\n    }\n\n    Assert.assertEquals(2, countEvents(task1));\n\n    \r\n    task1.stopGracefully();\n    unlockAppenderatorBasePersistDirForTask(task1);\n\n    Assert.assertEquals(TaskState.SUCCESS, future1.get().getStatusCode());\n\n    \r\n    final KafkaIndexTask task2 = createTask(\n        task1.getId(),\n        new KafkaIndexTaskIOConfig(\n            0,\n            \"sequence0\",\n            new SeekableStreamPartitions<>(topic, ImmutableMap.of(0, 2L)),\n            new SeekableStreamPartitions<>(topic, ImmutableMap.of(0, 5L)),\n            kafkaServer.consumerProperties(),\n            true,\n            null,\n            null,\n            false\n        )\n    );\n\n    final ListenableFuture<TaskStatus> future2 = runTask(task2);\n\n    \r\n    try (final KafkaProducer<byte[], byte[]> kafkaProducer = kafkaServer.newProducer()) {\n      for (ProducerRecord<byte[], byte[]> record : Iterables.skip(records, 4)) {\n        kafkaProducer.send(record).get();\n      }\n    }\n\n    \r\n    Assert.assertEquals(TaskState.SUCCESS, future2.get().getStatusCode());\n\n    \r\n    Assert.assertEquals(2, task1.getRunner().getRowIngestionMeters().getProcessed());\n    Assert.assertEquals(0, task1.getRunner().getRowIngestionMeters().getUnparseable());\n    Assert.assertEquals(0, task1.getRunner().getRowIngestionMeters().getThrownAway());\n    Assert.assertEquals(1, task2.getRunner().getRowIngestionMeters().getProcessed());\n    Assert.assertEquals(0, task2.getRunner().getRowIngestionMeters().getUnparseable());\n    Assert.assertEquals(0, task2.getRunner().getRowIngestionMeters().getThrownAway());\n\n    \r\n    SegmentDescriptor desc1 = SD(task1, \"2010/P1D\", 0);\n    SegmentDescriptor desc2 = SD(task1, \"2011/P1D\", 0);\n    Assert.assertEquals(ImmutableSet.of(desc1, desc2), publishedDescriptors());\n    Assert.assertEquals(\n        new KafkaDataSourceMetadata(new SeekableStreamPartitions<>(topic, ImmutableMap.of(0, 5L))),\n        metadataStorageCoordinator.getDataSourceMetadata(DATA_SCHEMA.getDataSource())\n    );\n\n    \r\n    Assert.assertEquals(ImmutableList.of(\"c\"), readSegmentColumn(\"dim1\", desc1));\n    Assert.assertEquals(ImmutableList.of(\"d\", \"e\"), readSegmentColumn(\"dim1\", desc2));\n  }\n","realPath":"extensions-core/kafka-indexing-service/src/test/java/org/apache/druid/indexing/kafka/KafkaIndexTaskTest.java","repoName":"druid","snippetEndLine":0,"snippetStartLine":0,"startLine":1817,"status":"M"}],"commitId":"6761663509025c5c88275a9752c96d417f879abe","commitMessage":"@@@make kafka poll timeout can be configured (#6773)\n\n* make kafka poll timeout can be configured\n\n* add doc\n\n* rename DEFAULT_POLL_TIMEOUT to DEFAULT_POLL_TIMEOUT_MILLIS\n","date":"2019-01-03 12:16:02","modifiedFileCount":"9","status":"M","submitter":"Mingming Qiu"},{"authorTime":"2019-01-26 07:43:06","codes":[{"authorDate":"2019-01-03 12:16:02","commitOrder":4,"curCode":"  public void testRunTwoTasksTwoPartitions() throws Exception\n  {\n    final KafkaIndexTask task1 = createTask(\n        null,\n        new KafkaIndexTaskIOConfig(\n            0,\n            \"sequence0\",\n            new SeekableStreamPartitions<>(topic, ImmutableMap.of(0, 2L)),\n            new SeekableStreamPartitions<>(topic, ImmutableMap.of(0, 5L)),\n            kafkaServer.consumerProperties(),\n            KafkaSupervisorIOConfig.DEFAULT_POLL_TIMEOUT_MILLIS,\n            true,\n            null,\n            null,\n            false\n        )\n    );\n    final KafkaIndexTask task2 = createTask(\n        null,\n        new KafkaIndexTaskIOConfig(\n            1,\n            \"sequence1\",\n            new SeekableStreamPartitions<>(topic, ImmutableMap.of(1, 0L)),\n            new SeekableStreamPartitions<>(topic, ImmutableMap.of(1, 1L)),\n            kafkaServer.consumerProperties(),\n            KafkaSupervisorIOConfig.DEFAULT_POLL_TIMEOUT_MILLIS,\n            true,\n            null,\n            null,\n            false\n        )\n    );\n\n    final ListenableFuture<TaskStatus> future1 = runTask(task1);\n    final ListenableFuture<TaskStatus> future2 = runTask(task2);\n\n    \r\n    try (final KafkaProducer<byte[], byte[]> kafkaProducer = kafkaServer.newProducer()) {\n      for (ProducerRecord<byte[], byte[]> record : records) {\n        kafkaProducer.send(record).get();\n      }\n    }\n\n    \r\n    Assert.assertEquals(TaskState.SUCCESS, future1.get().getStatusCode());\n    Assert.assertEquals(TaskState.SUCCESS, future2.get().getStatusCode());\n\n    \r\n    Assert.assertEquals(3, task1.getRunner().getRowIngestionMeters().getProcessed());\n    Assert.assertEquals(0, task1.getRunner().getRowIngestionMeters().getUnparseable());\n    Assert.assertEquals(0, task1.getRunner().getRowIngestionMeters().getThrownAway());\n    Assert.assertEquals(1, task2.getRunner().getRowIngestionMeters().getProcessed());\n    Assert.assertEquals(0, task2.getRunner().getRowIngestionMeters().getUnparseable());\n    Assert.assertEquals(0, task2.getRunner().getRowIngestionMeters().getThrownAway());\n\n    \r\n    SegmentDescriptor desc1 = SD(task1, \"2010/P1D\", 0);\n    SegmentDescriptor desc2 = SD(task1, \"2011/P1D\", 0);\n    SegmentDescriptor desc3 = SD(task2, \"2012/P1D\", 0);\n    Assert.assertEquals(ImmutableSet.of(desc1, desc2, desc3), publishedDescriptors());\n    Assert.assertEquals(\n        new KafkaDataSourceMetadata(new SeekableStreamPartitions<>(topic, ImmutableMap.of(0, 5L, 1, 1L))),\n        metadataStorageCoordinator.getDataSourceMetadata(DATA_SCHEMA.getDataSource())\n    );\n\n    \r\n    Assert.assertEquals(ImmutableList.of(\"c\"), readSegmentColumn(\"dim1\", desc1));\n    Assert.assertEquals(ImmutableList.of(\"d\", \"e\"), readSegmentColumn(\"dim1\", desc2));\n    Assert.assertEquals(ImmutableList.of(\"g\"), readSegmentColumn(\"dim1\", desc3));\n  }\n","date":"2019-01-03 12:16:02","endLine":1814,"groupId":"6247","id":7,"instanceNumber":1,"isCurCommit":0,"methodName":"testRunTwoTasksTwoPartitions","params":"()","path":"/mnt/clonedata/CloneManagementServer/ManagementServer/consistResult/result-druid-10-0.7/blobInfo/CC_OUT/blobs/f4/4877771fbd76d2e48f167fa6e25fc70e074abc.src","preCode":"  public void testRunTwoTasksTwoPartitions() throws Exception\n  {\n    final KafkaIndexTask task1 = createTask(\n        null,\n        new KafkaIndexTaskIOConfig(\n            0,\n            \"sequence0\",\n            new SeekableStreamPartitions<>(topic, ImmutableMap.of(0, 2L)),\n            new SeekableStreamPartitions<>(topic, ImmutableMap.of(0, 5L)),\n            kafkaServer.consumerProperties(),\n            KafkaSupervisorIOConfig.DEFAULT_POLL_TIMEOUT_MILLIS,\n            true,\n            null,\n            null,\n            false\n        )\n    );\n    final KafkaIndexTask task2 = createTask(\n        null,\n        new KafkaIndexTaskIOConfig(\n            1,\n            \"sequence1\",\n            new SeekableStreamPartitions<>(topic, ImmutableMap.of(1, 0L)),\n            new SeekableStreamPartitions<>(topic, ImmutableMap.of(1, 1L)),\n            kafkaServer.consumerProperties(),\n            KafkaSupervisorIOConfig.DEFAULT_POLL_TIMEOUT_MILLIS,\n            true,\n            null,\n            null,\n            false\n        )\n    );\n\n    final ListenableFuture<TaskStatus> future1 = runTask(task1);\n    final ListenableFuture<TaskStatus> future2 = runTask(task2);\n\n    \r\n    try (final KafkaProducer<byte[], byte[]> kafkaProducer = kafkaServer.newProducer()) {\n      for (ProducerRecord<byte[], byte[]> record : records) {\n        kafkaProducer.send(record).get();\n      }\n    }\n\n    \r\n    Assert.assertEquals(TaskState.SUCCESS, future1.get().getStatusCode());\n    Assert.assertEquals(TaskState.SUCCESS, future2.get().getStatusCode());\n\n    \r\n    Assert.assertEquals(3, task1.getRunner().getRowIngestionMeters().getProcessed());\n    Assert.assertEquals(0, task1.getRunner().getRowIngestionMeters().getUnparseable());\n    Assert.assertEquals(0, task1.getRunner().getRowIngestionMeters().getThrownAway());\n    Assert.assertEquals(1, task2.getRunner().getRowIngestionMeters().getProcessed());\n    Assert.assertEquals(0, task2.getRunner().getRowIngestionMeters().getUnparseable());\n    Assert.assertEquals(0, task2.getRunner().getRowIngestionMeters().getThrownAway());\n\n    \r\n    SegmentDescriptor desc1 = SD(task1, \"2010/P1D\", 0);\n    SegmentDescriptor desc2 = SD(task1, \"2011/P1D\", 0);\n    SegmentDescriptor desc3 = SD(task2, \"2012/P1D\", 0);\n    Assert.assertEquals(ImmutableSet.of(desc1, desc2, desc3), publishedDescriptors());\n    Assert.assertEquals(\n        new KafkaDataSourceMetadata(new SeekableStreamPartitions<>(topic, ImmutableMap.of(0, 5L, 1, 1L))),\n        metadataStorageCoordinator.getDataSourceMetadata(DATA_SCHEMA.getDataSource())\n    );\n\n    \r\n    Assert.assertEquals(ImmutableList.of(\"c\"), readSegmentColumn(\"dim1\", desc1));\n    Assert.assertEquals(ImmutableList.of(\"d\", \"e\"), readSegmentColumn(\"dim1\", desc2));\n    Assert.assertEquals(ImmutableList.of(\"g\"), readSegmentColumn(\"dim1\", desc3));\n  }\n","realPath":"extensions-core/kafka-indexing-service/src/test/java/org/apache/druid/indexing/kafka/KafkaIndexTaskTest.java","repoName":"druid","snippetEndLine":0,"snippetStartLine":0,"startLine":1745,"status":"N"},{"authorDate":"2019-01-26 07:43:06","commitOrder":4,"curCode":"  public void testRestore() throws Exception\n  {\n    final KafkaIndexTask task1 = createTask(\n        null,\n        new KafkaIndexTaskIOConfig(\n            0,\n            \"sequence0\",\n            new SeekableStreamPartitions<>(topic, ImmutableMap.of(0, 2L)),\n            new SeekableStreamPartitions<>(topic, ImmutableMap.of(0, 5L)),\n            kafkaServer.consumerProperties(),\n            KafkaSupervisorIOConfig.DEFAULT_POLL_TIMEOUT_MILLIS,\n            true,\n            null,\n            null,\n            false\n        )\n    );\n\n    final ListenableFuture<TaskStatus> future1 = runTask(task1);\n\n    \r\n    try (final KafkaProducer<byte[], byte[]> kafkaProducer = kafkaServer.newProducer()) {\n      for (ProducerRecord<byte[], byte[]> record : Iterables.limit(records, 4)) {\n        kafkaProducer.send(record).get();\n      }\n    }\n\n    while (countEvents(task1) != 2) {\n      Thread.sleep(25);\n    }\n\n    Assert.assertEquals(2, countEvents(task1));\n\n    \r\n    task1.stopGracefully(toolboxFactory.build(task1).getConfig());\n    unlockAppenderatorBasePersistDirForTask(task1);\n\n    Assert.assertEquals(TaskState.SUCCESS, future1.get().getStatusCode());\n\n    \r\n    final KafkaIndexTask task2 = createTask(\n        task1.getId(),\n        new KafkaIndexTaskIOConfig(\n            0,\n            \"sequence0\",\n            new SeekableStreamPartitions<>(topic, ImmutableMap.of(0, 2L)),\n            new SeekableStreamPartitions<>(topic, ImmutableMap.of(0, 5L)),\n            kafkaServer.consumerProperties(),\n            KafkaSupervisorIOConfig.DEFAULT_POLL_TIMEOUT_MILLIS,\n            true,\n            null,\n            null,\n            false\n        )\n    );\n\n    final ListenableFuture<TaskStatus> future2 = runTask(task2);\n\n    \r\n    try (final KafkaProducer<byte[], byte[]> kafkaProducer = kafkaServer.newProducer()) {\n      for (ProducerRecord<byte[], byte[]> record : Iterables.skip(records, 4)) {\n        kafkaProducer.send(record).get();\n      }\n    }\n\n    \r\n    Assert.assertEquals(TaskState.SUCCESS, future2.get().getStatusCode());\n\n    \r\n    Assert.assertEquals(2, task1.getRunner().getRowIngestionMeters().getProcessed());\n    Assert.assertEquals(0, task1.getRunner().getRowIngestionMeters().getUnparseable());\n    Assert.assertEquals(0, task1.getRunner().getRowIngestionMeters().getThrownAway());\n    Assert.assertEquals(1, task2.getRunner().getRowIngestionMeters().getProcessed());\n    Assert.assertEquals(0, task2.getRunner().getRowIngestionMeters().getUnparseable());\n    Assert.assertEquals(0, task2.getRunner().getRowIngestionMeters().getThrownAway());\n\n    \r\n    SegmentDescriptor desc1 = SD(task1, \"2010/P1D\", 0);\n    SegmentDescriptor desc2 = SD(task1, \"2011/P1D\", 0);\n    Assert.assertEquals(ImmutableSet.of(desc1, desc2), publishedDescriptors());\n    Assert.assertEquals(\n        new KafkaDataSourceMetadata(new SeekableStreamPartitions<>(topic, ImmutableMap.of(0, 5L))),\n        metadataStorageCoordinator.getDataSourceMetadata(DATA_SCHEMA.getDataSource())\n    );\n\n    \r\n    Assert.assertEquals(ImmutableList.of(\"c\"), readSegmentColumn(\"dim1\", desc1));\n    Assert.assertEquals(ImmutableList.of(\"d\", \"e\"), readSegmentColumn(\"dim1\", desc2));\n  }\n","date":"2019-01-26 07:43:06","endLine":1905,"groupId":"20856","id":8,"instanceNumber":2,"isCurCommit":0,"methodName":"testRestore","params":"()","path":"/mnt/clonedata/CloneManagementServer/ManagementServer/consistResult/result-druid-10-0.7/blobInfo/CC_OUT/blobs/7b/f83ddd3a031288880dcd0c141cd2f31ea868d6.src","preCode":"  public void testRestore() throws Exception\n  {\n    final KafkaIndexTask task1 = createTask(\n        null,\n        new KafkaIndexTaskIOConfig(\n            0,\n            \"sequence0\",\n            new SeekableStreamPartitions<>(topic, ImmutableMap.of(0, 2L)),\n            new SeekableStreamPartitions<>(topic, ImmutableMap.of(0, 5L)),\n            kafkaServer.consumerProperties(),\n            KafkaSupervisorIOConfig.DEFAULT_POLL_TIMEOUT_MILLIS,\n            true,\n            null,\n            null,\n            false\n        )\n    );\n\n    final ListenableFuture<TaskStatus> future1 = runTask(task1);\n\n    \r\n    try (final KafkaProducer<byte[], byte[]> kafkaProducer = kafkaServer.newProducer()) {\n      for (ProducerRecord<byte[], byte[]> record : Iterables.limit(records, 4)) {\n        kafkaProducer.send(record).get();\n      }\n    }\n\n    while (countEvents(task1) != 2) {\n      Thread.sleep(25);\n    }\n\n    Assert.assertEquals(2, countEvents(task1));\n\n    \r\n    task1.stopGracefully();\n    unlockAppenderatorBasePersistDirForTask(task1);\n\n    Assert.assertEquals(TaskState.SUCCESS, future1.get().getStatusCode());\n\n    \r\n    final KafkaIndexTask task2 = createTask(\n        task1.getId(),\n        new KafkaIndexTaskIOConfig(\n            0,\n            \"sequence0\",\n            new SeekableStreamPartitions<>(topic, ImmutableMap.of(0, 2L)),\n            new SeekableStreamPartitions<>(topic, ImmutableMap.of(0, 5L)),\n            kafkaServer.consumerProperties(),\n            KafkaSupervisorIOConfig.DEFAULT_POLL_TIMEOUT_MILLIS,\n            true,\n            null,\n            null,\n            false\n        )\n    );\n\n    final ListenableFuture<TaskStatus> future2 = runTask(task2);\n\n    \r\n    try (final KafkaProducer<byte[], byte[]> kafkaProducer = kafkaServer.newProducer()) {\n      for (ProducerRecord<byte[], byte[]> record : Iterables.skip(records, 4)) {\n        kafkaProducer.send(record).get();\n      }\n    }\n\n    \r\n    Assert.assertEquals(TaskState.SUCCESS, future2.get().getStatusCode());\n\n    \r\n    Assert.assertEquals(2, task1.getRunner().getRowIngestionMeters().getProcessed());\n    Assert.assertEquals(0, task1.getRunner().getRowIngestionMeters().getUnparseable());\n    Assert.assertEquals(0, task1.getRunner().getRowIngestionMeters().getThrownAway());\n    Assert.assertEquals(1, task2.getRunner().getRowIngestionMeters().getProcessed());\n    Assert.assertEquals(0, task2.getRunner().getRowIngestionMeters().getUnparseable());\n    Assert.assertEquals(0, task2.getRunner().getRowIngestionMeters().getThrownAway());\n\n    \r\n    SegmentDescriptor desc1 = SD(task1, \"2010/P1D\", 0);\n    SegmentDescriptor desc2 = SD(task1, \"2011/P1D\", 0);\n    Assert.assertEquals(ImmutableSet.of(desc1, desc2), publishedDescriptors());\n    Assert.assertEquals(\n        new KafkaDataSourceMetadata(new SeekableStreamPartitions<>(topic, ImmutableMap.of(0, 5L))),\n        metadataStorageCoordinator.getDataSourceMetadata(DATA_SCHEMA.getDataSource())\n    );\n\n    \r\n    Assert.assertEquals(ImmutableList.of(\"c\"), readSegmentColumn(\"dim1\", desc1));\n    Assert.assertEquals(ImmutableList.of(\"d\", \"e\"), readSegmentColumn(\"dim1\", desc2));\n  }\n","realPath":"extensions-core/kafka-indexing-service/src/test/java/org/apache/druid/indexing/kafka/KafkaIndexTaskTest.java","repoName":"druid","snippetEndLine":0,"snippetStartLine":0,"startLine":1817,"status":"M"}],"commitId":"8492d94f599da1f7851add2a0e7500515abd881d","commitMessage":"@@@Kill Hadoop MR task on kill of Hadoop ingestion task  (#6828)\n\n* KillTask from overlord UI now makes sure that it terminates the underlying MR job.  thus saving unnecessary compute\n\nRun in jobby is now split into 2\n 1. submitAndGetHadoopJobId followed by 2. run\n  submitAndGetHadoopJobId is responsible for submitting the job and returning the jobId as a string.  run monitors this job for completion\n\nJobHelper writes this jobId in the path provided by HadoopIndexTask which in turn is provided by the ForkingTaskRunner\n\nHadoopIndexTask reads this path when kill task is clicked to get hte jobId and fire the kill command via the yarn api. This is taken care in the stopGracefully method which is called in SingleTaskBackgroundRunner. Have enabled `canRestore` method to return `true` for HadoopIndexTask in order for the stopGracefully method to be called\n\nHadoop*Job files have been changed to incorporate the changes to jobby\n\n* Addressing PR comments\n\n* Addressing PR comments - Fix taskDir\n\n* Addressing PR comments - For changing the contract of Task.stopGracefully()\n`SingleTaskBackgroundRunner` calls stopGracefully in stop() and then checks for canRestore condition to return the status of the task\n\n* Addressing PR comments\n 1. Formatting\n 2. Removing `submitAndGetHadoopJobId` from `Jobby` and calling writeJobIdToFile in the job itself\n\n* Addressing PR comments\n 1. POM change. Moving hadoop dependency to indexing-hadoop\n\n* Addressing PR comments\n 1. stopGracefully now accepts TaskConfig as a param\n     Handling isRestoreOnRestart in stopGracefully for `AppenderatorDriverRealtimeIndexTask.  RealtimeIndexTask.  SeekableStreamIndexTask`\n     Changing tests to make TaskConfig param isRestoreOnRestart to true\n","date":"2019-01-26 07:43:06","modifiedFileCount":"20","status":"M","submitter":"Ankit Kothari"},{"authorTime":"2019-02-19 03:50:08","codes":[{"authorDate":"2019-02-19 03:50:08","commitOrder":5,"curCode":"  public void testRunTwoTasksTwoPartitions() throws Exception\n  {\n    final KafkaIndexTask task1 = createTask(\n        null,\n        new KafkaIndexTaskIOConfig(\n            0,\n            \"sequence0\",\n            new SeekableStreamPartitions<>(topic, ImmutableMap.of(0, 2L)),\n            new SeekableStreamPartitions<>(topic, ImmutableMap.of(0, 5L)),\n            kafkaServer.consumerProperties(),\n            KafkaSupervisorIOConfig.DEFAULT_POLL_TIMEOUT_MILLIS,\n            true,\n            null,\n            null\n        )\n    );\n    final KafkaIndexTask task2 = createTask(\n        null,\n        new KafkaIndexTaskIOConfig(\n            1,\n            \"sequence1\",\n            new SeekableStreamPartitions<>(topic, ImmutableMap.of(1, 0L)),\n            new SeekableStreamPartitions<>(topic, ImmutableMap.of(1, 1L)),\n            kafkaServer.consumerProperties(),\n            KafkaSupervisorIOConfig.DEFAULT_POLL_TIMEOUT_MILLIS,\n            true,\n            null,\n            null\n        )\n    );\n\n    final ListenableFuture<TaskStatus> future1 = runTask(task1);\n    final ListenableFuture<TaskStatus> future2 = runTask(task2);\n\n    \r\n    insertData();\n\n    \r\n    Assert.assertEquals(TaskState.SUCCESS, future1.get().getStatusCode());\n    Assert.assertEquals(TaskState.SUCCESS, future2.get().getStatusCode());\n\n    \r\n    Assert.assertEquals(3, task1.getRunner().getRowIngestionMeters().getProcessed());\n    Assert.assertEquals(0, task1.getRunner().getRowIngestionMeters().getUnparseable());\n    Assert.assertEquals(0, task1.getRunner().getRowIngestionMeters().getThrownAway());\n    Assert.assertEquals(1, task2.getRunner().getRowIngestionMeters().getProcessed());\n    Assert.assertEquals(0, task2.getRunner().getRowIngestionMeters().getUnparseable());\n    Assert.assertEquals(0, task2.getRunner().getRowIngestionMeters().getThrownAway());\n\n    \r\n    SegmentDescriptor desc1 = SD(task1, \"2010/P1D\", 0);\n    SegmentDescriptor desc2 = SD(task1, \"2011/P1D\", 0);\n    SegmentDescriptor desc3 = SD(task2, \"2012/P1D\", 0);\n    Assert.assertEquals(ImmutableSet.of(desc1, desc2, desc3), publishedDescriptors());\n    Assert.assertEquals(\n        new KafkaDataSourceMetadata(new SeekableStreamPartitions<>(topic, ImmutableMap.of(0, 5L, 1, 1L))),\n        metadataStorageCoordinator.getDataSourceMetadata(DATA_SCHEMA.getDataSource())\n    );\n\n    \r\n    Assert.assertEquals(ImmutableList.of(\"c\"), readSegmentColumn(\"dim1\", desc1));\n    Assert.assertEquals(ImmutableList.of(\"d\", \"e\"), readSegmentColumn(\"dim1\", desc2));\n    Assert.assertEquals(ImmutableList.of(\"g\"), readSegmentColumn(\"dim1\", desc3));\n  }\n","date":"2019-02-19 03:50:08","endLine":1739,"groupId":"4897","id":9,"instanceNumber":1,"isCurCommit":0,"methodName":"testRunTwoTasksTwoPartitions","params":"()","path":"/mnt/clonedata/CloneManagementServer/ManagementServer/consistResult/result-druid-10-0.7/blobInfo/CC_OUT/blobs/3d/6308cb0a96b01947db7e17b51cab010dd61129.src","preCode":"  public void testRunTwoTasksTwoPartitions() throws Exception\n  {\n    final KafkaIndexTask task1 = createTask(\n        null,\n        new KafkaIndexTaskIOConfig(\n            0,\n            \"sequence0\",\n            new SeekableStreamPartitions<>(topic, ImmutableMap.of(0, 2L)),\n            new SeekableStreamPartitions<>(topic, ImmutableMap.of(0, 5L)),\n            kafkaServer.consumerProperties(),\n            KafkaSupervisorIOConfig.DEFAULT_POLL_TIMEOUT_MILLIS,\n            true,\n            null,\n            null,\n            false\n        )\n    );\n    final KafkaIndexTask task2 = createTask(\n        null,\n        new KafkaIndexTaskIOConfig(\n            1,\n            \"sequence1\",\n            new SeekableStreamPartitions<>(topic, ImmutableMap.of(1, 0L)),\n            new SeekableStreamPartitions<>(topic, ImmutableMap.of(1, 1L)),\n            kafkaServer.consumerProperties(),\n            KafkaSupervisorIOConfig.DEFAULT_POLL_TIMEOUT_MILLIS,\n            true,\n            null,\n            null,\n            false\n        )\n    );\n\n    final ListenableFuture<TaskStatus> future1 = runTask(task1);\n    final ListenableFuture<TaskStatus> future2 = runTask(task2);\n\n    \r\n    try (final KafkaProducer<byte[], byte[]> kafkaProducer = kafkaServer.newProducer()) {\n      for (ProducerRecord<byte[], byte[]> record : records) {\n        kafkaProducer.send(record).get();\n      }\n    }\n\n    \r\n    Assert.assertEquals(TaskState.SUCCESS, future1.get().getStatusCode());\n    Assert.assertEquals(TaskState.SUCCESS, future2.get().getStatusCode());\n\n    \r\n    Assert.assertEquals(3, task1.getRunner().getRowIngestionMeters().getProcessed());\n    Assert.assertEquals(0, task1.getRunner().getRowIngestionMeters().getUnparseable());\n    Assert.assertEquals(0, task1.getRunner().getRowIngestionMeters().getThrownAway());\n    Assert.assertEquals(1, task2.getRunner().getRowIngestionMeters().getProcessed());\n    Assert.assertEquals(0, task2.getRunner().getRowIngestionMeters().getUnparseable());\n    Assert.assertEquals(0, task2.getRunner().getRowIngestionMeters().getThrownAway());\n\n    \r\n    SegmentDescriptor desc1 = SD(task1, \"2010/P1D\", 0);\n    SegmentDescriptor desc2 = SD(task1, \"2011/P1D\", 0);\n    SegmentDescriptor desc3 = SD(task2, \"2012/P1D\", 0);\n    Assert.assertEquals(ImmutableSet.of(desc1, desc2, desc3), publishedDescriptors());\n    Assert.assertEquals(\n        new KafkaDataSourceMetadata(new SeekableStreamPartitions<>(topic, ImmutableMap.of(0, 5L, 1, 1L))),\n        metadataStorageCoordinator.getDataSourceMetadata(DATA_SCHEMA.getDataSource())\n    );\n\n    \r\n    Assert.assertEquals(ImmutableList.of(\"c\"), readSegmentColumn(\"dim1\", desc1));\n    Assert.assertEquals(ImmutableList.of(\"d\", \"e\"), readSegmentColumn(\"dim1\", desc2));\n    Assert.assertEquals(ImmutableList.of(\"g\"), readSegmentColumn(\"dim1\", desc3));\n  }\n","realPath":"extensions-core/kafka-indexing-service/src/test/java/org/apache/druid/indexing/kafka/KafkaIndexTaskTest.java","repoName":"druid","snippetEndLine":0,"snippetStartLine":0,"startLine":1676,"status":"M"},{"authorDate":"2019-02-19 03:50:08","commitOrder":5,"curCode":"  public void testRestore() throws Exception\n  {\n    final KafkaIndexTask task1 = createTask(\n        null,\n        new KafkaIndexTaskIOConfig(\n            0,\n            \"sequence0\",\n            new SeekableStreamPartitions<>(topic, ImmutableMap.of(0, 2L)),\n            new SeekableStreamPartitions<>(topic, ImmutableMap.of(0, 6L)),\n            kafkaServer.consumerProperties(),\n            KafkaSupervisorIOConfig.DEFAULT_POLL_TIMEOUT_MILLIS,\n            true,\n            null,\n            null\n        )\n    );\n\n    final ListenableFuture<TaskStatus> future1 = runTask(task1);\n\n    \r\n    try (final KafkaProducer<byte[], byte[]> kafkaProducer = kafkaServer.newProducer()) {\n      kafkaProducer.initTransactions();\n      kafkaProducer.beginTransaction();\n      for (ProducerRecord<byte[], byte[]> record : Iterables.limit(records, 4)) {\n        kafkaProducer.send(record).get();\n      }\n      kafkaProducer.commitTransaction();\n    }\n\n    while (countEvents(task1) != 2) {\n      Thread.sleep(25);\n    }\n\n    Assert.assertEquals(2, countEvents(task1));\n\n    \r\n    task1.stopGracefully(toolboxFactory.build(task1).getConfig());\n    unlockAppenderatorBasePersistDirForTask(task1);\n\n    Assert.assertEquals(TaskState.SUCCESS, future1.get().getStatusCode());\n\n    \r\n    final KafkaIndexTask task2 = createTask(\n        task1.getId(),\n        new KafkaIndexTaskIOConfig(\n            0,\n            \"sequence0\",\n            new SeekableStreamPartitions<>(topic, ImmutableMap.of(0, 2L)),\n            new SeekableStreamPartitions<>(topic, ImmutableMap.of(0, 6L)),\n            kafkaServer.consumerProperties(),\n            KafkaSupervisorIOConfig.DEFAULT_POLL_TIMEOUT_MILLIS,\n            true,\n            null,\n            null\n        )\n    );\n\n    final ListenableFuture<TaskStatus> future2 = runTask(task2);\n\n    \r\n    try (final KafkaProducer<byte[], byte[]> kafkaProducer = kafkaServer.newProducer()) {\n      kafkaProducer.initTransactions();\n      kafkaProducer.beginTransaction();\n      for (ProducerRecord<byte[], byte[]> record : Iterables.skip(records, 4)) {\n        kafkaProducer.send(record).get();\n      }\n      kafkaProducer.commitTransaction();\n    }\n\n    \r\n\n    Assert.assertEquals(TaskState.SUCCESS, future2.get().getStatusCode());\n\n    \r\n    Assert.assertEquals(2, task1.getRunner().getRowIngestionMeters().getProcessed());\n    Assert.assertEquals(0, task1.getRunner().getRowIngestionMeters().getUnparseable());\n    Assert.assertEquals(0, task1.getRunner().getRowIngestionMeters().getThrownAway());\n    Assert.assertEquals(1, task2.getRunner().getRowIngestionMeters().getProcessed());\n    Assert.assertEquals(0, task2.getRunner().getRowIngestionMeters().getUnparseable());\n    Assert.assertEquals(0, task2.getRunner().getRowIngestionMeters().getThrownAway());\n\n    \r\n    SegmentDescriptor desc1 = SD(task1, \"2010/P1D\", 0);\n    SegmentDescriptor desc2 = SD(task1, \"2011/P1D\", 0);\n    Assert.assertEquals(ImmutableSet.of(desc1, desc2), publishedDescriptors());\n    Assert.assertEquals(\n        new KafkaDataSourceMetadata(new SeekableStreamPartitions<>(topic, ImmutableMap.of(0, 6L))),\n        metadataStorageCoordinator.getDataSourceMetadata(DATA_SCHEMA.getDataSource())\n    );\n\n    \r\n    Assert.assertEquals(ImmutableList.of(\"c\"), readSegmentColumn(\"dim1\", desc1));\n    Assert.assertEquals(ImmutableList.of(\"d\", \"e\"), readSegmentColumn(\"dim1\", desc2));\n  }\n","date":"2019-02-19 03:50:08","endLine":1835,"groupId":"2275","id":10,"instanceNumber":2,"isCurCommit":0,"methodName":"testRestore","params":"()","path":"/mnt/clonedata/CloneManagementServer/ManagementServer/consistResult/result-druid-10-0.7/blobInfo/CC_OUT/blobs/3d/6308cb0a96b01947db7e17b51cab010dd61129.src","preCode":"  public void testRestore() throws Exception\n  {\n    final KafkaIndexTask task1 = createTask(\n        null,\n        new KafkaIndexTaskIOConfig(\n            0,\n            \"sequence0\",\n            new SeekableStreamPartitions<>(topic, ImmutableMap.of(0, 2L)),\n            new SeekableStreamPartitions<>(topic, ImmutableMap.of(0, 5L)),\n            kafkaServer.consumerProperties(),\n            KafkaSupervisorIOConfig.DEFAULT_POLL_TIMEOUT_MILLIS,\n            true,\n            null,\n            null,\n            false\n        )\n    );\n\n    final ListenableFuture<TaskStatus> future1 = runTask(task1);\n\n    \r\n    try (final KafkaProducer<byte[], byte[]> kafkaProducer = kafkaServer.newProducer()) {\n      for (ProducerRecord<byte[], byte[]> record : Iterables.limit(records, 4)) {\n        kafkaProducer.send(record).get();\n      }\n    }\n\n    while (countEvents(task1) != 2) {\n      Thread.sleep(25);\n    }\n\n    Assert.assertEquals(2, countEvents(task1));\n\n    \r\n    task1.stopGracefully(toolboxFactory.build(task1).getConfig());\n    unlockAppenderatorBasePersistDirForTask(task1);\n\n    Assert.assertEquals(TaskState.SUCCESS, future1.get().getStatusCode());\n\n    \r\n    final KafkaIndexTask task2 = createTask(\n        task1.getId(),\n        new KafkaIndexTaskIOConfig(\n            0,\n            \"sequence0\",\n            new SeekableStreamPartitions<>(topic, ImmutableMap.of(0, 2L)),\n            new SeekableStreamPartitions<>(topic, ImmutableMap.of(0, 5L)),\n            kafkaServer.consumerProperties(),\n            KafkaSupervisorIOConfig.DEFAULT_POLL_TIMEOUT_MILLIS,\n            true,\n            null,\n            null,\n            false\n        )\n    );\n\n    final ListenableFuture<TaskStatus> future2 = runTask(task2);\n\n    \r\n    try (final KafkaProducer<byte[], byte[]> kafkaProducer = kafkaServer.newProducer()) {\n      for (ProducerRecord<byte[], byte[]> record : Iterables.skip(records, 4)) {\n        kafkaProducer.send(record).get();\n      }\n    }\n\n    \r\n    Assert.assertEquals(TaskState.SUCCESS, future2.get().getStatusCode());\n\n    \r\n    Assert.assertEquals(2, task1.getRunner().getRowIngestionMeters().getProcessed());\n    Assert.assertEquals(0, task1.getRunner().getRowIngestionMeters().getUnparseable());\n    Assert.assertEquals(0, task1.getRunner().getRowIngestionMeters().getThrownAway());\n    Assert.assertEquals(1, task2.getRunner().getRowIngestionMeters().getProcessed());\n    Assert.assertEquals(0, task2.getRunner().getRowIngestionMeters().getUnparseable());\n    Assert.assertEquals(0, task2.getRunner().getRowIngestionMeters().getThrownAway());\n\n    \r\n    SegmentDescriptor desc1 = SD(task1, \"2010/P1D\", 0);\n    SegmentDescriptor desc2 = SD(task1, \"2011/P1D\", 0);\n    Assert.assertEquals(ImmutableSet.of(desc1, desc2), publishedDescriptors());\n    Assert.assertEquals(\n        new KafkaDataSourceMetadata(new SeekableStreamPartitions<>(topic, ImmutableMap.of(0, 5L))),\n        metadataStorageCoordinator.getDataSourceMetadata(DATA_SCHEMA.getDataSource())\n    );\n\n    \r\n    Assert.assertEquals(ImmutableList.of(\"c\"), readSegmentColumn(\"dim1\", desc1));\n    Assert.assertEquals(ImmutableList.of(\"d\", \"e\"), readSegmentColumn(\"dim1\", desc2));\n  }\n","realPath":"extensions-core/kafka-indexing-service/src/test/java/org/apache/druid/indexing/kafka/KafkaIndexTaskTest.java","repoName":"druid","snippetEndLine":0,"snippetStartLine":0,"startLine":1742,"status":"M"}],"commitId":"80a2ef7be46c4fc807ac2a74507b8ba8b6a44049","commitMessage":"@@@Support kafka transactional topics (#5404) (#6496)\n\n* Support kafka transactional topics\n\n* update kafka to version 2.0.0\n* Remove the skipOffsetGaps option since it's not used anymore\n* Adjust kafka consumer to use transactional semantics\n* Update tests\n\n* Remove unused import from test\n\n* Fix compilation\n\n* Invoke transaction api to fix a unit test\n\n* temporary modification of travis.yml for debugging\n\n* another attempt to get travis tasklogs\n\n* update kafka to 2.0.1 at all places\n\n* Remove druid-kafka-eight dependency from integration-tests.  remove the kafka firehose test and deprecate kafka-eight classes\n\n* Add deprecated in docs for kafka-eight and kafka-simple extensions\n\n* Remove skipOffsetGaps and code changes for transaction support\n\n* Fix indentation\n\n* remove skipOffsetGaps from kinesis\n\n* Add transaction api to KafkaRecordSupplierTest\n\n* Fix indent\n\n* Fix test\n\n* update kafka version to 2.1.0\n","date":"2019-02-19 03:50:08","modifiedFileCount":"25","status":"M","submitter":"Surekha"},{"authorTime":"2019-02-24 12:10:31","codes":[{"authorDate":"2019-02-24 12:10:31","commitOrder":6,"curCode":"  public void testRunTwoTasksTwoPartitions() throws Exception\n  {\n    final KafkaIndexTask task1 = createTask(\n        null,\n        new KafkaIndexTaskIOConfig(\n            0,\n            \"sequence0\",\n            new SeekableStreamPartitions<>(topic, ImmutableMap.of(0, 2L)),\n            new SeekableStreamPartitions<>(topic, ImmutableMap.of(0, 5L)),\n            kafkaServer.consumerProperties(),\n            KafkaSupervisorIOConfig.DEFAULT_POLL_TIMEOUT_MILLIS,\n            true,\n            null,\n            null\n        )\n    );\n    final KafkaIndexTask task2 = createTask(\n        null,\n        new KafkaIndexTaskIOConfig(\n            1,\n            \"sequence1\",\n            new SeekableStreamPartitions<>(topic, ImmutableMap.of(1, 0L)),\n            new SeekableStreamPartitions<>(topic, ImmutableMap.of(1, 1L)),\n            kafkaServer.consumerProperties(),\n            KafkaSupervisorIOConfig.DEFAULT_POLL_TIMEOUT_MILLIS,\n            true,\n            null,\n            null\n        )\n    );\n\n    final ListenableFuture<TaskStatus> future1 = runTask(task1);\n    final ListenableFuture<TaskStatus> future2 = runTask(task2);\n\n    \r\n    insertData();\n\n    \r\n    Assert.assertEquals(TaskState.SUCCESS, future1.get().getStatusCode());\n    Assert.assertEquals(TaskState.SUCCESS, future2.get().getStatusCode());\n\n    \r\n    Assert.assertEquals(3, task1.getRunner().getRowIngestionMeters().getProcessed());\n    Assert.assertEquals(0, task1.getRunner().getRowIngestionMeters().getUnparseable());\n    Assert.assertEquals(0, task1.getRunner().getRowIngestionMeters().getThrownAway());\n    Assert.assertEquals(1, task2.getRunner().getRowIngestionMeters().getProcessed());\n    Assert.assertEquals(0, task2.getRunner().getRowIngestionMeters().getUnparseable());\n    Assert.assertEquals(0, task2.getRunner().getRowIngestionMeters().getThrownAway());\n\n    \r\n    SegmentDescriptor desc1 = sd(task1, \"2010/P1D\", 0);\n    SegmentDescriptor desc2 = sd(task1, \"2011/P1D\", 0);\n    SegmentDescriptor desc3 = sd(task2, \"2012/P1D\", 0);\n    Assert.assertEquals(ImmutableSet.of(desc1, desc2, desc3), publishedDescriptors());\n    Assert.assertEquals(\n        new KafkaDataSourceMetadata(new SeekableStreamPartitions<>(topic, ImmutableMap.of(0, 5L, 1, 1L))),\n        metadataStorageCoordinator.getDataSourceMetadata(DATA_SCHEMA.getDataSource())\n    );\n\n    \r\n    Assert.assertEquals(ImmutableList.of(\"c\"), readSegmentColumn(\"dim1\", desc1));\n    Assert.assertEquals(ImmutableList.of(\"d\", \"e\"), readSegmentColumn(\"dim1\", desc2));\n    Assert.assertEquals(ImmutableList.of(\"g\"), readSegmentColumn(\"dim1\", desc3));\n  }\n","date":"2019-02-24 12:10:31","endLine":1744,"groupId":"4897","id":11,"instanceNumber":1,"isCurCommit":0,"methodName":"testRunTwoTasksTwoPartitions","params":"()","path":"/mnt/clonedata/CloneManagementServer/ManagementServer/consistResult/result-druid-10-0.7/blobInfo/CC_OUT/blobs/b1/9bf8ad3716e1bda5912843b5946146e1e703d6.src","preCode":"  public void testRunTwoTasksTwoPartitions() throws Exception\n  {\n    final KafkaIndexTask task1 = createTask(\n        null,\n        new KafkaIndexTaskIOConfig(\n            0,\n            \"sequence0\",\n            new SeekableStreamPartitions<>(topic, ImmutableMap.of(0, 2L)),\n            new SeekableStreamPartitions<>(topic, ImmutableMap.of(0, 5L)),\n            kafkaServer.consumerProperties(),\n            KafkaSupervisorIOConfig.DEFAULT_POLL_TIMEOUT_MILLIS,\n            true,\n            null,\n            null\n        )\n    );\n    final KafkaIndexTask task2 = createTask(\n        null,\n        new KafkaIndexTaskIOConfig(\n            1,\n            \"sequence1\",\n            new SeekableStreamPartitions<>(topic, ImmutableMap.of(1, 0L)),\n            new SeekableStreamPartitions<>(topic, ImmutableMap.of(1, 1L)),\n            kafkaServer.consumerProperties(),\n            KafkaSupervisorIOConfig.DEFAULT_POLL_TIMEOUT_MILLIS,\n            true,\n            null,\n            null\n        )\n    );\n\n    final ListenableFuture<TaskStatus> future1 = runTask(task1);\n    final ListenableFuture<TaskStatus> future2 = runTask(task2);\n\n    \r\n    insertData();\n\n    \r\n    Assert.assertEquals(TaskState.SUCCESS, future1.get().getStatusCode());\n    Assert.assertEquals(TaskState.SUCCESS, future2.get().getStatusCode());\n\n    \r\n    Assert.assertEquals(3, task1.getRunner().getRowIngestionMeters().getProcessed());\n    Assert.assertEquals(0, task1.getRunner().getRowIngestionMeters().getUnparseable());\n    Assert.assertEquals(0, task1.getRunner().getRowIngestionMeters().getThrownAway());\n    Assert.assertEquals(1, task2.getRunner().getRowIngestionMeters().getProcessed());\n    Assert.assertEquals(0, task2.getRunner().getRowIngestionMeters().getUnparseable());\n    Assert.assertEquals(0, task2.getRunner().getRowIngestionMeters().getThrownAway());\n\n    \r\n    SegmentDescriptor desc1 = SD(task1, \"2010/P1D\", 0);\n    SegmentDescriptor desc2 = SD(task1, \"2011/P1D\", 0);\n    SegmentDescriptor desc3 = SD(task2, \"2012/P1D\", 0);\n    Assert.assertEquals(ImmutableSet.of(desc1, desc2, desc3), publishedDescriptors());\n    Assert.assertEquals(\n        new KafkaDataSourceMetadata(new SeekableStreamPartitions<>(topic, ImmutableMap.of(0, 5L, 1, 1L))),\n        metadataStorageCoordinator.getDataSourceMetadata(DATA_SCHEMA.getDataSource())\n    );\n\n    \r\n    Assert.assertEquals(ImmutableList.of(\"c\"), readSegmentColumn(\"dim1\", desc1));\n    Assert.assertEquals(ImmutableList.of(\"d\", \"e\"), readSegmentColumn(\"dim1\", desc2));\n    Assert.assertEquals(ImmutableList.of(\"g\"), readSegmentColumn(\"dim1\", desc3));\n  }\n","realPath":"extensions-core/kafka-indexing-service/src/test/java/org/apache/druid/indexing/kafka/KafkaIndexTaskTest.java","repoName":"druid","snippetEndLine":0,"snippetStartLine":0,"startLine":1681,"status":"M"},{"authorDate":"2019-02-24 12:10:31","commitOrder":6,"curCode":"  public void testRestore() throws Exception\n  {\n    final KafkaIndexTask task1 = createTask(\n        null,\n        new KafkaIndexTaskIOConfig(\n            0,\n            \"sequence0\",\n            new SeekableStreamPartitions<>(topic, ImmutableMap.of(0, 2L)),\n            new SeekableStreamPartitions<>(topic, ImmutableMap.of(0, 6L)),\n            kafkaServer.consumerProperties(),\n            KafkaSupervisorIOConfig.DEFAULT_POLL_TIMEOUT_MILLIS,\n            true,\n            null,\n            null\n        )\n    );\n\n    final ListenableFuture<TaskStatus> future1 = runTask(task1);\n\n    \r\n    try (final KafkaProducer<byte[], byte[]> kafkaProducer = kafkaServer.newProducer()) {\n      kafkaProducer.initTransactions();\n      kafkaProducer.beginTransaction();\n      for (ProducerRecord<byte[], byte[]> record : Iterables.limit(records, 4)) {\n        kafkaProducer.send(record).get();\n      }\n      kafkaProducer.commitTransaction();\n    }\n\n    while (countEvents(task1) != 2) {\n      Thread.sleep(25);\n    }\n\n    Assert.assertEquals(2, countEvents(task1));\n\n    \r\n    task1.stopGracefully(toolboxFactory.build(task1).getConfig());\n    unlockAppenderatorBasePersistDirForTask(task1);\n\n    Assert.assertEquals(TaskState.SUCCESS, future1.get().getStatusCode());\n\n    \r\n    final KafkaIndexTask task2 = createTask(\n        task1.getId(),\n        new KafkaIndexTaskIOConfig(\n            0,\n            \"sequence0\",\n            new SeekableStreamPartitions<>(topic, ImmutableMap.of(0, 2L)),\n            new SeekableStreamPartitions<>(topic, ImmutableMap.of(0, 6L)),\n            kafkaServer.consumerProperties(),\n            KafkaSupervisorIOConfig.DEFAULT_POLL_TIMEOUT_MILLIS,\n            true,\n            null,\n            null\n        )\n    );\n\n    final ListenableFuture<TaskStatus> future2 = runTask(task2);\n\n    \r\n    try (final KafkaProducer<byte[], byte[]> kafkaProducer = kafkaServer.newProducer()) {\n      kafkaProducer.initTransactions();\n      kafkaProducer.beginTransaction();\n      for (ProducerRecord<byte[], byte[]> record : Iterables.skip(records, 4)) {\n        kafkaProducer.send(record).get();\n      }\n      kafkaProducer.commitTransaction();\n    }\n\n    \r\n\n    Assert.assertEquals(TaskState.SUCCESS, future2.get().getStatusCode());\n\n    \r\n    Assert.assertEquals(2, task1.getRunner().getRowIngestionMeters().getProcessed());\n    Assert.assertEquals(0, task1.getRunner().getRowIngestionMeters().getUnparseable());\n    Assert.assertEquals(0, task1.getRunner().getRowIngestionMeters().getThrownAway());\n    Assert.assertEquals(1, task2.getRunner().getRowIngestionMeters().getProcessed());\n    Assert.assertEquals(0, task2.getRunner().getRowIngestionMeters().getUnparseable());\n    Assert.assertEquals(0, task2.getRunner().getRowIngestionMeters().getThrownAway());\n\n    \r\n    SegmentDescriptor desc1 = sd(task1, \"2010/P1D\", 0);\n    SegmentDescriptor desc2 = sd(task1, \"2011/P1D\", 0);\n    Assert.assertEquals(ImmutableSet.of(desc1, desc2), publishedDescriptors());\n    Assert.assertEquals(\n        new KafkaDataSourceMetadata(new SeekableStreamPartitions<>(topic, ImmutableMap.of(0, 6L))),\n        metadataStorageCoordinator.getDataSourceMetadata(DATA_SCHEMA.getDataSource())\n    );\n\n    \r\n    Assert.assertEquals(ImmutableList.of(\"c\"), readSegmentColumn(\"dim1\", desc1));\n    Assert.assertEquals(ImmutableList.of(\"d\", \"e\"), readSegmentColumn(\"dim1\", desc2));\n  }\n","date":"2019-02-24 12:10:31","endLine":1840,"groupId":"2275","id":12,"instanceNumber":2,"isCurCommit":0,"methodName":"testRestore","params":"()","path":"/mnt/clonedata/CloneManagementServer/ManagementServer/consistResult/result-druid-10-0.7/blobInfo/CC_OUT/blobs/b1/9bf8ad3716e1bda5912843b5946146e1e703d6.src","preCode":"  public void testRestore() throws Exception\n  {\n    final KafkaIndexTask task1 = createTask(\n        null,\n        new KafkaIndexTaskIOConfig(\n            0,\n            \"sequence0\",\n            new SeekableStreamPartitions<>(topic, ImmutableMap.of(0, 2L)),\n            new SeekableStreamPartitions<>(topic, ImmutableMap.of(0, 6L)),\n            kafkaServer.consumerProperties(),\n            KafkaSupervisorIOConfig.DEFAULT_POLL_TIMEOUT_MILLIS,\n            true,\n            null,\n            null\n        )\n    );\n\n    final ListenableFuture<TaskStatus> future1 = runTask(task1);\n\n    \r\n    try (final KafkaProducer<byte[], byte[]> kafkaProducer = kafkaServer.newProducer()) {\n      kafkaProducer.initTransactions();\n      kafkaProducer.beginTransaction();\n      for (ProducerRecord<byte[], byte[]> record : Iterables.limit(records, 4)) {\n        kafkaProducer.send(record).get();\n      }\n      kafkaProducer.commitTransaction();\n    }\n\n    while (countEvents(task1) != 2) {\n      Thread.sleep(25);\n    }\n\n    Assert.assertEquals(2, countEvents(task1));\n\n    \r\n    task1.stopGracefully(toolboxFactory.build(task1).getConfig());\n    unlockAppenderatorBasePersistDirForTask(task1);\n\n    Assert.assertEquals(TaskState.SUCCESS, future1.get().getStatusCode());\n\n    \r\n    final KafkaIndexTask task2 = createTask(\n        task1.getId(),\n        new KafkaIndexTaskIOConfig(\n            0,\n            \"sequence0\",\n            new SeekableStreamPartitions<>(topic, ImmutableMap.of(0, 2L)),\n            new SeekableStreamPartitions<>(topic, ImmutableMap.of(0, 6L)),\n            kafkaServer.consumerProperties(),\n            KafkaSupervisorIOConfig.DEFAULT_POLL_TIMEOUT_MILLIS,\n            true,\n            null,\n            null\n        )\n    );\n\n    final ListenableFuture<TaskStatus> future2 = runTask(task2);\n\n    \r\n    try (final KafkaProducer<byte[], byte[]> kafkaProducer = kafkaServer.newProducer()) {\n      kafkaProducer.initTransactions();\n      kafkaProducer.beginTransaction();\n      for (ProducerRecord<byte[], byte[]> record : Iterables.skip(records, 4)) {\n        kafkaProducer.send(record).get();\n      }\n      kafkaProducer.commitTransaction();\n    }\n\n    \r\n\n    Assert.assertEquals(TaskState.SUCCESS, future2.get().getStatusCode());\n\n    \r\n    Assert.assertEquals(2, task1.getRunner().getRowIngestionMeters().getProcessed());\n    Assert.assertEquals(0, task1.getRunner().getRowIngestionMeters().getUnparseable());\n    Assert.assertEquals(0, task1.getRunner().getRowIngestionMeters().getThrownAway());\n    Assert.assertEquals(1, task2.getRunner().getRowIngestionMeters().getProcessed());\n    Assert.assertEquals(0, task2.getRunner().getRowIngestionMeters().getUnparseable());\n    Assert.assertEquals(0, task2.getRunner().getRowIngestionMeters().getThrownAway());\n\n    \r\n    SegmentDescriptor desc1 = SD(task1, \"2010/P1D\", 0);\n    SegmentDescriptor desc2 = SD(task1, \"2011/P1D\", 0);\n    Assert.assertEquals(ImmutableSet.of(desc1, desc2), publishedDescriptors());\n    Assert.assertEquals(\n        new KafkaDataSourceMetadata(new SeekableStreamPartitions<>(topic, ImmutableMap.of(0, 6L))),\n        metadataStorageCoordinator.getDataSourceMetadata(DATA_SCHEMA.getDataSource())\n    );\n\n    \r\n    Assert.assertEquals(ImmutableList.of(\"c\"), readSegmentColumn(\"dim1\", desc1));\n    Assert.assertEquals(ImmutableList.of(\"d\", \"e\"), readSegmentColumn(\"dim1\", desc2));\n  }\n","realPath":"extensions-core/kafka-indexing-service/src/test/java/org/apache/druid/indexing/kafka/KafkaIndexTaskTest.java","repoName":"druid","snippetEndLine":0,"snippetStartLine":0,"startLine":1747,"status":"M"}],"commitId":"8b803cbc22b15799fd0526de8d6d0eea155ad733","commitMessage":"@@@Added checkstyle for \"Methods starting with Capital Letters\"  (#7118)\n\n* Added checkstyle for \"Methods starting with Capital Letters\" and changed the method names violating this.\n\n* Un-abbreviate the method names in the calcite tests\n\n* Fixed checkstyle errors\n\n* Changed asserts position in the code\n","date":"2019-02-24 12:10:31","modifiedFileCount":"24","status":"M","submitter":"Himanshu Pandey"},{"authorTime":"2019-03-22 04:12:22","codes":[{"authorDate":"2019-03-22 04:12:22","commitOrder":7,"curCode":"  public void testRunTwoTasksTwoPartitions() throws Exception\n  {\n    final KafkaIndexTask task1 = createTask(\n        null,\n        new KafkaIndexTaskIOConfig(\n            0,\n            \"sequence0\",\n            new SeekableStreamStartSequenceNumbers<>(topic, ImmutableMap.of(0, 2L), ImmutableSet.of()),\n            new SeekableStreamEndSequenceNumbers<>(topic, ImmutableMap.of(0, 5L)),\n            kafkaServer.consumerProperties(),\n            KafkaSupervisorIOConfig.DEFAULT_POLL_TIMEOUT_MILLIS,\n            true,\n            null,\n            null\n        )\n    );\n    final KafkaIndexTask task2 = createTask(\n        null,\n        new KafkaIndexTaskIOConfig(\n            1,\n            \"sequence1\",\n            new SeekableStreamStartSequenceNumbers<>(topic, ImmutableMap.of(1, 0L), ImmutableSet.of()),\n            new SeekableStreamEndSequenceNumbers<>(topic, ImmutableMap.of(1, 1L)),\n            kafkaServer.consumerProperties(),\n            KafkaSupervisorIOConfig.DEFAULT_POLL_TIMEOUT_MILLIS,\n            true,\n            null,\n            null\n        )\n    );\n\n    final ListenableFuture<TaskStatus> future1 = runTask(task1);\n    final ListenableFuture<TaskStatus> future2 = runTask(task2);\n\n    \r\n    insertData();\n\n    \r\n    Assert.assertEquals(TaskState.SUCCESS, future1.get().getStatusCode());\n    Assert.assertEquals(TaskState.SUCCESS, future2.get().getStatusCode());\n\n    \r\n    Assert.assertEquals(3, task1.getRunner().getRowIngestionMeters().getProcessed());\n    Assert.assertEquals(0, task1.getRunner().getRowIngestionMeters().getUnparseable());\n    Assert.assertEquals(0, task1.getRunner().getRowIngestionMeters().getThrownAway());\n    Assert.assertEquals(1, task2.getRunner().getRowIngestionMeters().getProcessed());\n    Assert.assertEquals(0, task2.getRunner().getRowIngestionMeters().getUnparseable());\n    Assert.assertEquals(0, task2.getRunner().getRowIngestionMeters().getThrownAway());\n\n    \r\n    SegmentDescriptor desc1 = sd(task1, \"2010/P1D\", 0);\n    SegmentDescriptor desc2 = sd(task1, \"2011/P1D\", 0);\n    SegmentDescriptor desc3 = sd(task2, \"2012/P1D\", 0);\n    Assert.assertEquals(ImmutableSet.of(desc1, desc2, desc3), publishedDescriptors());\n    Assert.assertEquals(\n        new KafkaDataSourceMetadata(\n            new SeekableStreamEndSequenceNumbers<>(topic, ImmutableMap.of(0, 5L, 1, 1L))\n        ),\n        metadataStorageCoordinator.getDataSourceMetadata(DATA_SCHEMA.getDataSource())\n    );\n\n    \r\n    Assert.assertEquals(ImmutableList.of(\"c\"), readSegmentColumn(\"dim1\", desc1));\n    Assert.assertEquals(ImmutableList.of(\"d\", \"e\"), readSegmentColumn(\"dim1\", desc2));\n    Assert.assertEquals(ImmutableList.of(\"g\"), readSegmentColumn(\"dim1\", desc3));\n  }\n","date":"2019-03-22 04:12:22","endLine":1762,"groupId":"4102","id":13,"instanceNumber":1,"isCurCommit":0,"methodName":"testRunTwoTasksTwoPartitions","params":"()","path":"/mnt/clonedata/CloneManagementServer/ManagementServer/consistResult/result-druid-10-0.7/blobInfo/CC_OUT/blobs/eb/ef3fc4596a43b59efefcaf094105a8ae34ab5f.src","preCode":"  public void testRunTwoTasksTwoPartitions() throws Exception\n  {\n    final KafkaIndexTask task1 = createTask(\n        null,\n        new KafkaIndexTaskIOConfig(\n            0,\n            \"sequence0\",\n            new SeekableStreamPartitions<>(topic, ImmutableMap.of(0, 2L)),\n            new SeekableStreamPartitions<>(topic, ImmutableMap.of(0, 5L)),\n            kafkaServer.consumerProperties(),\n            KafkaSupervisorIOConfig.DEFAULT_POLL_TIMEOUT_MILLIS,\n            true,\n            null,\n            null\n        )\n    );\n    final KafkaIndexTask task2 = createTask(\n        null,\n        new KafkaIndexTaskIOConfig(\n            1,\n            \"sequence1\",\n            new SeekableStreamPartitions<>(topic, ImmutableMap.of(1, 0L)),\n            new SeekableStreamPartitions<>(topic, ImmutableMap.of(1, 1L)),\n            kafkaServer.consumerProperties(),\n            KafkaSupervisorIOConfig.DEFAULT_POLL_TIMEOUT_MILLIS,\n            true,\n            null,\n            null\n        )\n    );\n\n    final ListenableFuture<TaskStatus> future1 = runTask(task1);\n    final ListenableFuture<TaskStatus> future2 = runTask(task2);\n\n    \r\n    insertData();\n\n    \r\n    Assert.assertEquals(TaskState.SUCCESS, future1.get().getStatusCode());\n    Assert.assertEquals(TaskState.SUCCESS, future2.get().getStatusCode());\n\n    \r\n    Assert.assertEquals(3, task1.getRunner().getRowIngestionMeters().getProcessed());\n    Assert.assertEquals(0, task1.getRunner().getRowIngestionMeters().getUnparseable());\n    Assert.assertEquals(0, task1.getRunner().getRowIngestionMeters().getThrownAway());\n    Assert.assertEquals(1, task2.getRunner().getRowIngestionMeters().getProcessed());\n    Assert.assertEquals(0, task2.getRunner().getRowIngestionMeters().getUnparseable());\n    Assert.assertEquals(0, task2.getRunner().getRowIngestionMeters().getThrownAway());\n\n    \r\n    SegmentDescriptor desc1 = sd(task1, \"2010/P1D\", 0);\n    SegmentDescriptor desc2 = sd(task1, \"2011/P1D\", 0);\n    SegmentDescriptor desc3 = sd(task2, \"2012/P1D\", 0);\n    Assert.assertEquals(ImmutableSet.of(desc1, desc2, desc3), publishedDescriptors());\n    Assert.assertEquals(\n        new KafkaDataSourceMetadata(new SeekableStreamPartitions<>(topic, ImmutableMap.of(0, 5L, 1, 1L))),\n        metadataStorageCoordinator.getDataSourceMetadata(DATA_SCHEMA.getDataSource())\n    );\n\n    \r\n    Assert.assertEquals(ImmutableList.of(\"c\"), readSegmentColumn(\"dim1\", desc1));\n    Assert.assertEquals(ImmutableList.of(\"d\", \"e\"), readSegmentColumn(\"dim1\", desc2));\n    Assert.assertEquals(ImmutableList.of(\"g\"), readSegmentColumn(\"dim1\", desc3));\n  }\n","realPath":"extensions-core/kafka-indexing-service/src/test/java/org/apache/druid/indexing/kafka/KafkaIndexTaskTest.java","repoName":"druid","snippetEndLine":0,"snippetStartLine":0,"startLine":1697,"status":"M"},{"authorDate":"2019-03-22 04:12:22","commitOrder":7,"curCode":"  public void testRestore() throws Exception\n  {\n    final KafkaIndexTask task1 = createTask(\n        null,\n        new KafkaIndexTaskIOConfig(\n            0,\n            \"sequence0\",\n            new SeekableStreamStartSequenceNumbers<>(topic, ImmutableMap.of(0, 2L), ImmutableSet.of()),\n            new SeekableStreamEndSequenceNumbers<>(topic, ImmutableMap.of(0, 6L)),\n            kafkaServer.consumerProperties(),\n            KafkaSupervisorIOConfig.DEFAULT_POLL_TIMEOUT_MILLIS,\n            true,\n            null,\n            null\n        )\n    );\n\n    final ListenableFuture<TaskStatus> future1 = runTask(task1);\n\n    \r\n    try (final KafkaProducer<byte[], byte[]> kafkaProducer = kafkaServer.newProducer()) {\n      kafkaProducer.initTransactions();\n      kafkaProducer.beginTransaction();\n      for (ProducerRecord<byte[], byte[]> record : Iterables.limit(records, 4)) {\n        kafkaProducer.send(record).get();\n      }\n      kafkaProducer.commitTransaction();\n    }\n\n    while (countEvents(task1) != 2) {\n      Thread.sleep(25);\n    }\n\n    Assert.assertEquals(2, countEvents(task1));\n\n    \r\n    task1.stopGracefully(toolboxFactory.build(task1).getConfig());\n    unlockAppenderatorBasePersistDirForTask(task1);\n\n    Assert.assertEquals(TaskState.SUCCESS, future1.get().getStatusCode());\n\n    \r\n    final KafkaIndexTask task2 = createTask(\n        task1.getId(),\n        new KafkaIndexTaskIOConfig(\n            0,\n            \"sequence0\",\n            new SeekableStreamStartSequenceNumbers<>(topic, ImmutableMap.of(0, 2L), ImmutableSet.of()),\n            new SeekableStreamEndSequenceNumbers<>(topic, ImmutableMap.of(0, 6L)),\n            kafkaServer.consumerProperties(),\n            KafkaSupervisorIOConfig.DEFAULT_POLL_TIMEOUT_MILLIS,\n            true,\n            null,\n            null\n        )\n    );\n\n    final ListenableFuture<TaskStatus> future2 = runTask(task2);\n\n    \r\n    try (final KafkaProducer<byte[], byte[]> kafkaProducer = kafkaServer.newProducer()) {\n      kafkaProducer.initTransactions();\n      kafkaProducer.beginTransaction();\n      for (ProducerRecord<byte[], byte[]> record : Iterables.skip(records, 4)) {\n        kafkaProducer.send(record).get();\n      }\n      kafkaProducer.commitTransaction();\n    }\n\n    \r\n\n    Assert.assertEquals(TaskState.SUCCESS, future2.get().getStatusCode());\n\n    \r\n    Assert.assertEquals(2, task1.getRunner().getRowIngestionMeters().getProcessed());\n    Assert.assertEquals(0, task1.getRunner().getRowIngestionMeters().getUnparseable());\n    Assert.assertEquals(0, task1.getRunner().getRowIngestionMeters().getThrownAway());\n    Assert.assertEquals(1, task2.getRunner().getRowIngestionMeters().getProcessed());\n    Assert.assertEquals(0, task2.getRunner().getRowIngestionMeters().getUnparseable());\n    Assert.assertEquals(0, task2.getRunner().getRowIngestionMeters().getThrownAway());\n\n    \r\n    SegmentDescriptor desc1 = sd(task1, \"2010/P1D\", 0);\n    SegmentDescriptor desc2 = sd(task1, \"2011/P1D\", 0);\n    Assert.assertEquals(ImmutableSet.of(desc1, desc2), publishedDescriptors());\n    Assert.assertEquals(\n        new KafkaDataSourceMetadata(new SeekableStreamEndSequenceNumbers<>(topic, ImmutableMap.of(0, 6L))),\n        metadataStorageCoordinator.getDataSourceMetadata(DATA_SCHEMA.getDataSource())\n    );\n\n    \r\n    Assert.assertEquals(ImmutableList.of(\"c\"), readSegmentColumn(\"dim1\", desc1));\n    Assert.assertEquals(ImmutableList.of(\"d\", \"e\"), readSegmentColumn(\"dim1\", desc2));\n  }\n","date":"2019-03-22 04:12:22","endLine":1858,"groupId":"2275","id":14,"instanceNumber":2,"isCurCommit":0,"methodName":"testRestore","params":"()","path":"/mnt/clonedata/CloneManagementServer/ManagementServer/consistResult/result-druid-10-0.7/blobInfo/CC_OUT/blobs/eb/ef3fc4596a43b59efefcaf094105a8ae34ab5f.src","preCode":"  public void testRestore() throws Exception\n  {\n    final KafkaIndexTask task1 = createTask(\n        null,\n        new KafkaIndexTaskIOConfig(\n            0,\n            \"sequence0\",\n            new SeekableStreamPartitions<>(topic, ImmutableMap.of(0, 2L)),\n            new SeekableStreamPartitions<>(topic, ImmutableMap.of(0, 6L)),\n            kafkaServer.consumerProperties(),\n            KafkaSupervisorIOConfig.DEFAULT_POLL_TIMEOUT_MILLIS,\n            true,\n            null,\n            null\n        )\n    );\n\n    final ListenableFuture<TaskStatus> future1 = runTask(task1);\n\n    \r\n    try (final KafkaProducer<byte[], byte[]> kafkaProducer = kafkaServer.newProducer()) {\n      kafkaProducer.initTransactions();\n      kafkaProducer.beginTransaction();\n      for (ProducerRecord<byte[], byte[]> record : Iterables.limit(records, 4)) {\n        kafkaProducer.send(record).get();\n      }\n      kafkaProducer.commitTransaction();\n    }\n\n    while (countEvents(task1) != 2) {\n      Thread.sleep(25);\n    }\n\n    Assert.assertEquals(2, countEvents(task1));\n\n    \r\n    task1.stopGracefully(toolboxFactory.build(task1).getConfig());\n    unlockAppenderatorBasePersistDirForTask(task1);\n\n    Assert.assertEquals(TaskState.SUCCESS, future1.get().getStatusCode());\n\n    \r\n    final KafkaIndexTask task2 = createTask(\n        task1.getId(),\n        new KafkaIndexTaskIOConfig(\n            0,\n            \"sequence0\",\n            new SeekableStreamPartitions<>(topic, ImmutableMap.of(0, 2L)),\n            new SeekableStreamPartitions<>(topic, ImmutableMap.of(0, 6L)),\n            kafkaServer.consumerProperties(),\n            KafkaSupervisorIOConfig.DEFAULT_POLL_TIMEOUT_MILLIS,\n            true,\n            null,\n            null\n        )\n    );\n\n    final ListenableFuture<TaskStatus> future2 = runTask(task2);\n\n    \r\n    try (final KafkaProducer<byte[], byte[]> kafkaProducer = kafkaServer.newProducer()) {\n      kafkaProducer.initTransactions();\n      kafkaProducer.beginTransaction();\n      for (ProducerRecord<byte[], byte[]> record : Iterables.skip(records, 4)) {\n        kafkaProducer.send(record).get();\n      }\n      kafkaProducer.commitTransaction();\n    }\n\n    \r\n\n    Assert.assertEquals(TaskState.SUCCESS, future2.get().getStatusCode());\n\n    \r\n    Assert.assertEquals(2, task1.getRunner().getRowIngestionMeters().getProcessed());\n    Assert.assertEquals(0, task1.getRunner().getRowIngestionMeters().getUnparseable());\n    Assert.assertEquals(0, task1.getRunner().getRowIngestionMeters().getThrownAway());\n    Assert.assertEquals(1, task2.getRunner().getRowIngestionMeters().getProcessed());\n    Assert.assertEquals(0, task2.getRunner().getRowIngestionMeters().getUnparseable());\n    Assert.assertEquals(0, task2.getRunner().getRowIngestionMeters().getThrownAway());\n\n    \r\n    SegmentDescriptor desc1 = sd(task1, \"2010/P1D\", 0);\n    SegmentDescriptor desc2 = sd(task1, \"2011/P1D\", 0);\n    Assert.assertEquals(ImmutableSet.of(desc1, desc2), publishedDescriptors());\n    Assert.assertEquals(\n        new KafkaDataSourceMetadata(new SeekableStreamPartitions<>(topic, ImmutableMap.of(0, 6L))),\n        metadataStorageCoordinator.getDataSourceMetadata(DATA_SCHEMA.getDataSource())\n    );\n\n    \r\n    Assert.assertEquals(ImmutableList.of(\"c\"), readSegmentColumn(\"dim1\", desc1));\n    Assert.assertEquals(ImmutableList.of(\"d\", \"e\"), readSegmentColumn(\"dim1\", desc2));\n  }\n","realPath":"extensions-core/kafka-indexing-service/src/test/java/org/apache/druid/indexing/kafka/KafkaIndexTaskTest.java","repoName":"druid","snippetEndLine":0,"snippetStartLine":0,"startLine":1765,"status":"M"}],"commitId":"0c5dcf5586e33607849e397209f3eb0b10661f1e","commitMessage":"@@@Fix exclusivity for start offset in kinesis indexing service & check exclusivity properly in IndexerSQLMetadataStorageCoordinator (#7291)\n\n* Fix exclusivity for start offset in kinesis indexing service\n\n* some adjustment\n\n* Fix SeekableStreamDataSourceMetadata\n\n* Add missing javadocs\n\n* Add missing comments and unit test\n\n* fix SeekableStreamStartSequenceNumbers.plus and add comments\n\n* remove extra exclusivePartitions in KafkaIOConfig and fix downgrade issue\n\n* Add javadocs\n\n* fix compilation\n\n* fix test\n\n* remove unused variable\n","date":"2019-03-22 04:12:22","modifiedFileCount":"30","status":"M","submitter":"Jihoon Son"},{"authorTime":"2019-07-25 08:35:46","codes":[{"authorDate":"2019-07-25 08:35:46","commitOrder":8,"curCode":"  public void testRunTwoTasksTwoPartitions() throws Exception\n  {\n    final KafkaIndexTask task1 = createTask(\n        null,\n        new KafkaIndexTaskIOConfig(\n            0,\n            \"sequence0\",\n            new SeekableStreamStartSequenceNumbers<>(topic, ImmutableMap.of(0, 2L), ImmutableSet.of()),\n            new SeekableStreamEndSequenceNumbers<>(topic, ImmutableMap.of(0, 5L)),\n            kafkaServer.consumerProperties(),\n            KafkaSupervisorIOConfig.DEFAULT_POLL_TIMEOUT_MILLIS,\n            true,\n            null,\n            null\n        )\n    );\n    final KafkaIndexTask task2 = createTask(\n        null,\n        new KafkaIndexTaskIOConfig(\n            1,\n            \"sequence1\",\n            new SeekableStreamStartSequenceNumbers<>(topic, ImmutableMap.of(1, 0L), ImmutableSet.of()),\n            new SeekableStreamEndSequenceNumbers<>(topic, ImmutableMap.of(1, 1L)),\n            kafkaServer.consumerProperties(),\n            KafkaSupervisorIOConfig.DEFAULT_POLL_TIMEOUT_MILLIS,\n            true,\n            null,\n            null\n        )\n    );\n\n    final ListenableFuture<TaskStatus> future1 = runTask(task1);\n    final ListenableFuture<TaskStatus> future2 = runTask(task2);\n\n    \r\n    insertData();\n\n    \r\n    Assert.assertEquals(TaskState.SUCCESS, future1.get().getStatusCode());\n    Assert.assertEquals(TaskState.SUCCESS, future2.get().getStatusCode());\n\n    \r\n    Assert.assertEquals(3, task1.getRunner().getRowIngestionMeters().getProcessed());\n    Assert.assertEquals(0, task1.getRunner().getRowIngestionMeters().getUnparseable());\n    Assert.assertEquals(0, task1.getRunner().getRowIngestionMeters().getThrownAway());\n    Assert.assertEquals(1, task2.getRunner().getRowIngestionMeters().getProcessed());\n    Assert.assertEquals(0, task2.getRunner().getRowIngestionMeters().getUnparseable());\n    Assert.assertEquals(0, task2.getRunner().getRowIngestionMeters().getThrownAway());\n\n    \r\n    SegmentDescriptor desc1 = sd(\"2010/P1D\", 0);\n    SegmentDescriptor desc2 = sd(\"2011/P1D\", 0);\n    SegmentDescriptor desc3 = sd(\"2012/P1D\", 0);\n    assertEqualsExceptVersion(ImmutableList.of(desc1, desc2, desc3), publishedDescriptors());\n    Assert.assertEquals(\n        new KafkaDataSourceMetadata(\n            new SeekableStreamEndSequenceNumbers<>(topic, ImmutableMap.of(0, 5L, 1, 1L))\n        ),\n        metadataStorageCoordinator.getDataSourceMetadata(DATA_SCHEMA.getDataSource())\n    );\n\n    \r\n    final List<SegmentDescriptor> publishedDescriptors = publishedDescriptors();\n    Assert.assertEquals(ImmutableList.of(\"c\"), readSegmentColumn(\"dim1\", publishedDescriptors.get(0)));\n    Assert.assertEquals(ImmutableList.of(\"d\", \"e\"), readSegmentColumn(\"dim1\", publishedDescriptors.get(1)));\n    Assert.assertEquals(ImmutableList.of(\"g\"), readSegmentColumn(\"dim1\", publishedDescriptors.get(2)));\n  }\n","date":"2019-07-25 08:35:46","endLine":1765,"groupId":"4102","id":15,"instanceNumber":1,"isCurCommit":0,"methodName":"testRunTwoTasksTwoPartitions","params":"()","path":"/mnt/clonedata/CloneManagementServer/ManagementServer/consistResult/result-druid-10-0.7/blobInfo/CC_OUT/blobs/87/f522eae18ccef1c23a5fd9ae035dbbe3410dfc.src","preCode":"  public void testRunTwoTasksTwoPartitions() throws Exception\n  {\n    final KafkaIndexTask task1 = createTask(\n        null,\n        new KafkaIndexTaskIOConfig(\n            0,\n            \"sequence0\",\n            new SeekableStreamStartSequenceNumbers<>(topic, ImmutableMap.of(0, 2L), ImmutableSet.of()),\n            new SeekableStreamEndSequenceNumbers<>(topic, ImmutableMap.of(0, 5L)),\n            kafkaServer.consumerProperties(),\n            KafkaSupervisorIOConfig.DEFAULT_POLL_TIMEOUT_MILLIS,\n            true,\n            null,\n            null\n        )\n    );\n    final KafkaIndexTask task2 = createTask(\n        null,\n        new KafkaIndexTaskIOConfig(\n            1,\n            \"sequence1\",\n            new SeekableStreamStartSequenceNumbers<>(topic, ImmutableMap.of(1, 0L), ImmutableSet.of()),\n            new SeekableStreamEndSequenceNumbers<>(topic, ImmutableMap.of(1, 1L)),\n            kafkaServer.consumerProperties(),\n            KafkaSupervisorIOConfig.DEFAULT_POLL_TIMEOUT_MILLIS,\n            true,\n            null,\n            null\n        )\n    );\n\n    final ListenableFuture<TaskStatus> future1 = runTask(task1);\n    final ListenableFuture<TaskStatus> future2 = runTask(task2);\n\n    \r\n    insertData();\n\n    \r\n    Assert.assertEquals(TaskState.SUCCESS, future1.get().getStatusCode());\n    Assert.assertEquals(TaskState.SUCCESS, future2.get().getStatusCode());\n\n    \r\n    Assert.assertEquals(3, task1.getRunner().getRowIngestionMeters().getProcessed());\n    Assert.assertEquals(0, task1.getRunner().getRowIngestionMeters().getUnparseable());\n    Assert.assertEquals(0, task1.getRunner().getRowIngestionMeters().getThrownAway());\n    Assert.assertEquals(1, task2.getRunner().getRowIngestionMeters().getProcessed());\n    Assert.assertEquals(0, task2.getRunner().getRowIngestionMeters().getUnparseable());\n    Assert.assertEquals(0, task2.getRunner().getRowIngestionMeters().getThrownAway());\n\n    \r\n    SegmentDescriptor desc1 = sd(task1, \"2010/P1D\", 0);\n    SegmentDescriptor desc2 = sd(task1, \"2011/P1D\", 0);\n    SegmentDescriptor desc3 = sd(task2, \"2012/P1D\", 0);\n    Assert.assertEquals(ImmutableSet.of(desc1, desc2, desc3), publishedDescriptors());\n    Assert.assertEquals(\n        new KafkaDataSourceMetadata(\n            new SeekableStreamEndSequenceNumbers<>(topic, ImmutableMap.of(0, 5L, 1, 1L))\n        ),\n        metadataStorageCoordinator.getDataSourceMetadata(DATA_SCHEMA.getDataSource())\n    );\n\n    \r\n    Assert.assertEquals(ImmutableList.of(\"c\"), readSegmentColumn(\"dim1\", desc1));\n    Assert.assertEquals(ImmutableList.of(\"d\", \"e\"), readSegmentColumn(\"dim1\", desc2));\n    Assert.assertEquals(ImmutableList.of(\"g\"), readSegmentColumn(\"dim1\", desc3));\n  }\n","realPath":"extensions-core/kafka-indexing-service/src/test/java/org/apache/druid/indexing/kafka/KafkaIndexTaskTest.java","repoName":"druid","snippetEndLine":0,"snippetStartLine":0,"startLine":1699,"status":"M"},{"authorDate":"2019-07-25 08:35:46","commitOrder":8,"curCode":"  public void testRestore() throws Exception\n  {\n    final KafkaIndexTask task1 = createTask(\n        null,\n        new KafkaIndexTaskIOConfig(\n            0,\n            \"sequence0\",\n            new SeekableStreamStartSequenceNumbers<>(topic, ImmutableMap.of(0, 2L), ImmutableSet.of()),\n            new SeekableStreamEndSequenceNumbers<>(topic, ImmutableMap.of(0, 6L)),\n            kafkaServer.consumerProperties(),\n            KafkaSupervisorIOConfig.DEFAULT_POLL_TIMEOUT_MILLIS,\n            true,\n            null,\n            null\n        )\n    );\n\n    final ListenableFuture<TaskStatus> future1 = runTask(task1);\n\n    \r\n    try (final KafkaProducer<byte[], byte[]> kafkaProducer = kafkaServer.newProducer()) {\n      kafkaProducer.initTransactions();\n      kafkaProducer.beginTransaction();\n      for (ProducerRecord<byte[], byte[]> record : Iterables.limit(records, 4)) {\n        kafkaProducer.send(record).get();\n      }\n      kafkaProducer.commitTransaction();\n    }\n\n    while (countEvents(task1) != 2) {\n      Thread.sleep(25);\n    }\n\n    Assert.assertEquals(2, countEvents(task1));\n\n    \r\n    task1.stopGracefully(toolboxFactory.build(task1).getConfig());\n    unlockAppenderatorBasePersistDirForTask(task1);\n\n    Assert.assertEquals(TaskState.SUCCESS, future1.get().getStatusCode());\n\n    \r\n    final KafkaIndexTask task2 = createTask(\n        task1.getId(),\n        new KafkaIndexTaskIOConfig(\n            0,\n            \"sequence0\",\n            new SeekableStreamStartSequenceNumbers<>(topic, ImmutableMap.of(0, 2L), ImmutableSet.of()),\n            new SeekableStreamEndSequenceNumbers<>(topic, ImmutableMap.of(0, 6L)),\n            kafkaServer.consumerProperties(),\n            KafkaSupervisorIOConfig.DEFAULT_POLL_TIMEOUT_MILLIS,\n            true,\n            null,\n            null\n        )\n    );\n\n    final ListenableFuture<TaskStatus> future2 = runTask(task2);\n\n    \r\n    try (final KafkaProducer<byte[], byte[]> kafkaProducer = kafkaServer.newProducer()) {\n      kafkaProducer.initTransactions();\n      kafkaProducer.beginTransaction();\n      for (ProducerRecord<byte[], byte[]> record : Iterables.skip(records, 4)) {\n        kafkaProducer.send(record).get();\n      }\n      kafkaProducer.commitTransaction();\n    }\n\n    \r\n\n    Assert.assertEquals(TaskState.SUCCESS, future2.get().getStatusCode());\n\n    \r\n    Assert.assertEquals(2, task1.getRunner().getRowIngestionMeters().getProcessed());\n    Assert.assertEquals(0, task1.getRunner().getRowIngestionMeters().getUnparseable());\n    Assert.assertEquals(0, task1.getRunner().getRowIngestionMeters().getThrownAway());\n    Assert.assertEquals(1, task2.getRunner().getRowIngestionMeters().getProcessed());\n    Assert.assertEquals(0, task2.getRunner().getRowIngestionMeters().getUnparseable());\n    Assert.assertEquals(0, task2.getRunner().getRowIngestionMeters().getThrownAway());\n\n    \r\n    SegmentDescriptor desc1 = sd(\"2010/P1D\", 0);\n    SegmentDescriptor desc2 = sd(\"2011/P1D\", 0);\n    assertEqualsExceptVersion(ImmutableList.of(desc1, desc2), publishedDescriptors());\n    Assert.assertEquals(\n        new KafkaDataSourceMetadata(new SeekableStreamEndSequenceNumbers<>(topic, ImmutableMap.of(0, 6L))),\n        metadataStorageCoordinator.getDataSourceMetadata(DATA_SCHEMA.getDataSource())\n    );\n\n    \r\n    final List<SegmentDescriptor> publishedDescriptors = publishedDescriptors();\n    Assert.assertEquals(ImmutableList.of(\"c\"), readSegmentColumn(\"dim1\", publishedDescriptors.get(0)));\n    Assert.assertEquals(ImmutableList.of(\"d\", \"e\"), readSegmentColumn(\"dim1\", publishedDescriptors.get(1)));\n  }\n","date":"2019-07-25 08:35:46","endLine":1862,"groupId":"2275","id":16,"instanceNumber":2,"isCurCommit":0,"methodName":"testRestore","params":"()","path":"/mnt/clonedata/CloneManagementServer/ManagementServer/consistResult/result-druid-10-0.7/blobInfo/CC_OUT/blobs/87/f522eae18ccef1c23a5fd9ae035dbbe3410dfc.src","preCode":"  public void testRestore() throws Exception\n  {\n    final KafkaIndexTask task1 = createTask(\n        null,\n        new KafkaIndexTaskIOConfig(\n            0,\n            \"sequence0\",\n            new SeekableStreamStartSequenceNumbers<>(topic, ImmutableMap.of(0, 2L), ImmutableSet.of()),\n            new SeekableStreamEndSequenceNumbers<>(topic, ImmutableMap.of(0, 6L)),\n            kafkaServer.consumerProperties(),\n            KafkaSupervisorIOConfig.DEFAULT_POLL_TIMEOUT_MILLIS,\n            true,\n            null,\n            null\n        )\n    );\n\n    final ListenableFuture<TaskStatus> future1 = runTask(task1);\n\n    \r\n    try (final KafkaProducer<byte[], byte[]> kafkaProducer = kafkaServer.newProducer()) {\n      kafkaProducer.initTransactions();\n      kafkaProducer.beginTransaction();\n      for (ProducerRecord<byte[], byte[]> record : Iterables.limit(records, 4)) {\n        kafkaProducer.send(record).get();\n      }\n      kafkaProducer.commitTransaction();\n    }\n\n    while (countEvents(task1) != 2) {\n      Thread.sleep(25);\n    }\n\n    Assert.assertEquals(2, countEvents(task1));\n\n    \r\n    task1.stopGracefully(toolboxFactory.build(task1).getConfig());\n    unlockAppenderatorBasePersistDirForTask(task1);\n\n    Assert.assertEquals(TaskState.SUCCESS, future1.get().getStatusCode());\n\n    \r\n    final KafkaIndexTask task2 = createTask(\n        task1.getId(),\n        new KafkaIndexTaskIOConfig(\n            0,\n            \"sequence0\",\n            new SeekableStreamStartSequenceNumbers<>(topic, ImmutableMap.of(0, 2L), ImmutableSet.of()),\n            new SeekableStreamEndSequenceNumbers<>(topic, ImmutableMap.of(0, 6L)),\n            kafkaServer.consumerProperties(),\n            KafkaSupervisorIOConfig.DEFAULT_POLL_TIMEOUT_MILLIS,\n            true,\n            null,\n            null\n        )\n    );\n\n    final ListenableFuture<TaskStatus> future2 = runTask(task2);\n\n    \r\n    try (final KafkaProducer<byte[], byte[]> kafkaProducer = kafkaServer.newProducer()) {\n      kafkaProducer.initTransactions();\n      kafkaProducer.beginTransaction();\n      for (ProducerRecord<byte[], byte[]> record : Iterables.skip(records, 4)) {\n        kafkaProducer.send(record).get();\n      }\n      kafkaProducer.commitTransaction();\n    }\n\n    \r\n\n    Assert.assertEquals(TaskState.SUCCESS, future2.get().getStatusCode());\n\n    \r\n    Assert.assertEquals(2, task1.getRunner().getRowIngestionMeters().getProcessed());\n    Assert.assertEquals(0, task1.getRunner().getRowIngestionMeters().getUnparseable());\n    Assert.assertEquals(0, task1.getRunner().getRowIngestionMeters().getThrownAway());\n    Assert.assertEquals(1, task2.getRunner().getRowIngestionMeters().getProcessed());\n    Assert.assertEquals(0, task2.getRunner().getRowIngestionMeters().getUnparseable());\n    Assert.assertEquals(0, task2.getRunner().getRowIngestionMeters().getThrownAway());\n\n    \r\n    SegmentDescriptor desc1 = sd(task1, \"2010/P1D\", 0);\n    SegmentDescriptor desc2 = sd(task1, \"2011/P1D\", 0);\n    Assert.assertEquals(ImmutableSet.of(desc1, desc2), publishedDescriptors());\n    Assert.assertEquals(\n        new KafkaDataSourceMetadata(new SeekableStreamEndSequenceNumbers<>(topic, ImmutableMap.of(0, 6L))),\n        metadataStorageCoordinator.getDataSourceMetadata(DATA_SCHEMA.getDataSource())\n    );\n\n    \r\n    Assert.assertEquals(ImmutableList.of(\"c\"), readSegmentColumn(\"dim1\", desc1));\n    Assert.assertEquals(ImmutableList.of(\"d\", \"e\"), readSegmentColumn(\"dim1\", desc2));\n  }\n","realPath":"extensions-core/kafka-indexing-service/src/test/java/org/apache/druid/indexing/kafka/KafkaIndexTaskTest.java","repoName":"druid","snippetEndLine":0,"snippetStartLine":0,"startLine":1768,"status":"M"}],"commitId":"db149462073d59e7563f0d3834e69d44a2bb4011","commitMessage":"@@@Add support minor compaction with segment locking (#7547)\n\n* Segment locking\n\n* Allow both timeChunk and segment lock in the same gruop\n\n* fix it test\n\n* Fix adding same chunk to atomicUpdateGroup\n\n* resolving todos\n\n* Fix segments to lock\n\n* fix segments to lock\n\n* fix kill task\n\n* resolving todos\n\n* resolving todos\n\n* fix teamcity\n\n* remove unused class\n\n* fix single map\n\n* resolving todos\n\n* fix build\n\n* fix SQLMetadataSegmentManager\n\n* fix findInputSegments\n\n* adding more tests\n\n* fixing task lock checks\n\n* add SegmentTransactionalOverwriteAction\n\n* changing publisher\n\n* fixing something\n\n* fix for perfect rollup\n\n* fix test\n\n* adjust package-lock.json\n\n* fix test\n\n* fix style\n\n* adding javadocs\n\n* remove unused classes\n\n* add more javadocs\n\n* unused import\n\n* fix test\n\n* fix test\n\n* Support forceTimeChunk context and force timeChunk lock for parallel index task if intervals are missing\n\n* fix travis\n\n* fix travis\n\n* unused import\n\n* spotbug\n\n* revert getMaxVersion\n\n* address comments\n\n* fix tc\n\n* add missing error handling\n\n* fix backward compatibility\n\n* unused import\n\n* Fix perf of versionedIntervalTimeline\n\n* fix timeline\n\n* fix tc\n\n* remove remaining todos\n\n* add comment for parallel index\n\n* fix javadoc and typos\n\n* typo\n\n* address comments\n","date":"2019-07-25 08:35:46","modifiedFileCount":"130","status":"M","submitter":"Jihoon Son"},{"authorTime":"2019-11-07 03:07:04","codes":[{"authorDate":"2019-11-07 03:07:04","commitOrder":9,"curCode":"  public void testRunTwoTasksTwoPartitions() throws Exception\n  {\n    final KafkaIndexTask task1 = createTask(\n        null,\n        new KafkaIndexTaskIOConfig(\n            0,\n            \"sequence0\",\n            new SeekableStreamStartSequenceNumbers<>(topic, ImmutableMap.of(0, 2L), ImmutableSet.of()),\n            new SeekableStreamEndSequenceNumbers<>(topic, ImmutableMap.of(0, 5L)),\n            kafkaServer.consumerProperties(),\n            KafkaSupervisorIOConfig.DEFAULT_POLL_TIMEOUT_MILLIS,\n            true,\n            null,\n            null\n        )\n    );\n    final KafkaIndexTask task2 = createTask(\n        null,\n        new KafkaIndexTaskIOConfig(\n            1,\n            \"sequence1\",\n            new SeekableStreamStartSequenceNumbers<>(topic, ImmutableMap.of(1, 0L), ImmutableSet.of()),\n            new SeekableStreamEndSequenceNumbers<>(topic, ImmutableMap.of(1, 1L)),\n            kafkaServer.consumerProperties(),\n            KafkaSupervisorIOConfig.DEFAULT_POLL_TIMEOUT_MILLIS,\n            true,\n            null,\n            null\n        )\n    );\n\n    final ListenableFuture<TaskStatus> future1 = runTask(task1);\n    final ListenableFuture<TaskStatus> future2 = runTask(task2);\n\n    \r\n    insertData();\n\n    \r\n    Assert.assertEquals(TaskState.SUCCESS, future1.get().getStatusCode());\n    Assert.assertEquals(TaskState.SUCCESS, future2.get().getStatusCode());\n\n    \r\n    Assert.assertEquals(3, task1.getRunner().getRowIngestionMeters().getProcessed());\n    Assert.assertEquals(0, task1.getRunner().getRowIngestionMeters().getUnparseable());\n    Assert.assertEquals(0, task1.getRunner().getRowIngestionMeters().getThrownAway());\n    Assert.assertEquals(1, task2.getRunner().getRowIngestionMeters().getProcessed());\n    Assert.assertEquals(0, task2.getRunner().getRowIngestionMeters().getUnparseable());\n    Assert.assertEquals(0, task2.getRunner().getRowIngestionMeters().getThrownAway());\n\n    \r\n    assertEqualsExceptVersion(\n        ImmutableList.of(\n            sdd(\"2010/P1D\", 0, ImmutableList.of(\"c\")),\n            sdd(\"2011/P1D\", 0, ImmutableList.of(\"d\", \"e\")),\n            sdd(\"2012/P1D\", 0, ImmutableList.of(\"g\"))\n        ),\n        publishedDescriptors()\n    );\n    Assert.assertEquals(\n        new KafkaDataSourceMetadata(\n            new SeekableStreamEndSequenceNumbers<>(topic, ImmutableMap.of(0, 5L, 1, 1L))\n        ),\n        metadataStorageCoordinator.getDataSourceMetadata(DATA_SCHEMA.getDataSource())\n    );\n  }\n","date":"2019-11-07 03:07:04","endLine":1673,"groupId":"4102","id":17,"instanceNumber":1,"isCurCommit":0,"methodName":"testRunTwoTasksTwoPartitions","params":"()","path":"/mnt/clonedata/CloneManagementServer/ManagementServer/consistResult/result-druid-10-0.7/blobInfo/CC_OUT/blobs/c8/35bb95038567e2b9b3112cb98971f6d2418766.src","preCode":"  public void testRunTwoTasksTwoPartitions() throws Exception\n  {\n    final KafkaIndexTask task1 = createTask(\n        null,\n        new KafkaIndexTaskIOConfig(\n            0,\n            \"sequence0\",\n            new SeekableStreamStartSequenceNumbers<>(topic, ImmutableMap.of(0, 2L), ImmutableSet.of()),\n            new SeekableStreamEndSequenceNumbers<>(topic, ImmutableMap.of(0, 5L)),\n            kafkaServer.consumerProperties(),\n            KafkaSupervisorIOConfig.DEFAULT_POLL_TIMEOUT_MILLIS,\n            true,\n            null,\n            null\n        )\n    );\n    final KafkaIndexTask task2 = createTask(\n        null,\n        new KafkaIndexTaskIOConfig(\n            1,\n            \"sequence1\",\n            new SeekableStreamStartSequenceNumbers<>(topic, ImmutableMap.of(1, 0L), ImmutableSet.of()),\n            new SeekableStreamEndSequenceNumbers<>(topic, ImmutableMap.of(1, 1L)),\n            kafkaServer.consumerProperties(),\n            KafkaSupervisorIOConfig.DEFAULT_POLL_TIMEOUT_MILLIS,\n            true,\n            null,\n            null\n        )\n    );\n\n    final ListenableFuture<TaskStatus> future1 = runTask(task1);\n    final ListenableFuture<TaskStatus> future2 = runTask(task2);\n\n    \r\n    insertData();\n\n    \r\n    Assert.assertEquals(TaskState.SUCCESS, future1.get().getStatusCode());\n    Assert.assertEquals(TaskState.SUCCESS, future2.get().getStatusCode());\n\n    \r\n    Assert.assertEquals(3, task1.getRunner().getRowIngestionMeters().getProcessed());\n    Assert.assertEquals(0, task1.getRunner().getRowIngestionMeters().getUnparseable());\n    Assert.assertEquals(0, task1.getRunner().getRowIngestionMeters().getThrownAway());\n    Assert.assertEquals(1, task2.getRunner().getRowIngestionMeters().getProcessed());\n    Assert.assertEquals(0, task2.getRunner().getRowIngestionMeters().getUnparseable());\n    Assert.assertEquals(0, task2.getRunner().getRowIngestionMeters().getThrownAway());\n\n    \r\n    SegmentDescriptor desc1 = sd(\"2010/P1D\", 0);\n    SegmentDescriptor desc2 = sd(\"2011/P1D\", 0);\n    SegmentDescriptor desc3 = sd(\"2012/P1D\", 0);\n    assertEqualsExceptVersion(ImmutableList.of(desc1, desc2, desc3), publishedDescriptors());\n    Assert.assertEquals(\n        new KafkaDataSourceMetadata(\n            new SeekableStreamEndSequenceNumbers<>(topic, ImmutableMap.of(0, 5L, 1, 1L))\n        ),\n        metadataStorageCoordinator.getDataSourceMetadata(DATA_SCHEMA.getDataSource())\n    );\n\n    \r\n    final List<SegmentDescriptor> publishedDescriptors = publishedDescriptors();\n    Assert.assertEquals(ImmutableList.of(\"c\"), readSegmentColumn(\"dim1\", publishedDescriptors.get(0)));\n    Assert.assertEquals(ImmutableList.of(\"d\", \"e\"), readSegmentColumn(\"dim1\", publishedDescriptors.get(1)));\n    Assert.assertEquals(ImmutableList.of(\"g\"), readSegmentColumn(\"dim1\", publishedDescriptors.get(2)));\n  }\n","realPath":"extensions-core/kafka-indexing-service/src/test/java/org/apache/druid/indexing/kafka/KafkaIndexTaskTest.java","repoName":"druid","snippetEndLine":0,"snippetStartLine":0,"startLine":1609,"status":"M"},{"authorDate":"2019-11-07 03:07:04","commitOrder":9,"curCode":"  public void testRestore() throws Exception\n  {\n    final KafkaIndexTask task1 = createTask(\n        null,\n        new KafkaIndexTaskIOConfig(\n            0,\n            \"sequence0\",\n            new SeekableStreamStartSequenceNumbers<>(topic, ImmutableMap.of(0, 2L), ImmutableSet.of()),\n            new SeekableStreamEndSequenceNumbers<>(topic, ImmutableMap.of(0, 6L)),\n            kafkaServer.consumerProperties(),\n            KafkaSupervisorIOConfig.DEFAULT_POLL_TIMEOUT_MILLIS,\n            true,\n            null,\n            null\n        )\n    );\n\n    final ListenableFuture<TaskStatus> future1 = runTask(task1);\n\n    \r\n    try (final KafkaProducer<byte[], byte[]> kafkaProducer = kafkaServer.newProducer()) {\n      kafkaProducer.initTransactions();\n      kafkaProducer.beginTransaction();\n      for (ProducerRecord<byte[], byte[]> record : Iterables.limit(records, 4)) {\n        kafkaProducer.send(record).get();\n      }\n      kafkaProducer.commitTransaction();\n    }\n\n    while (countEvents(task1) != 2) {\n      Thread.sleep(25);\n    }\n\n    Assert.assertEquals(2, countEvents(task1));\n\n    \r\n    task1.stopGracefully(toolboxFactory.build(task1).getConfig());\n    unlockAppenderatorBasePersistDirForTask(task1);\n\n    Assert.assertEquals(TaskState.SUCCESS, future1.get().getStatusCode());\n\n    \r\n    final KafkaIndexTask task2 = createTask(\n        task1.getId(),\n        new KafkaIndexTaskIOConfig(\n            0,\n            \"sequence0\",\n            new SeekableStreamStartSequenceNumbers<>(topic, ImmutableMap.of(0, 2L), ImmutableSet.of()),\n            new SeekableStreamEndSequenceNumbers<>(topic, ImmutableMap.of(0, 6L)),\n            kafkaServer.consumerProperties(),\n            KafkaSupervisorIOConfig.DEFAULT_POLL_TIMEOUT_MILLIS,\n            true,\n            null,\n            null\n        )\n    );\n\n    final ListenableFuture<TaskStatus> future2 = runTask(task2);\n\n    \r\n    try (final KafkaProducer<byte[], byte[]> kafkaProducer = kafkaServer.newProducer()) {\n      kafkaProducer.initTransactions();\n      kafkaProducer.beginTransaction();\n      for (ProducerRecord<byte[], byte[]> record : Iterables.skip(records, 4)) {\n        kafkaProducer.send(record).get();\n      }\n      kafkaProducer.commitTransaction();\n    }\n\n    \r\n\n    Assert.assertEquals(TaskState.SUCCESS, future2.get().getStatusCode());\n\n    \r\n    Assert.assertEquals(2, task1.getRunner().getRowIngestionMeters().getProcessed());\n    Assert.assertEquals(0, task1.getRunner().getRowIngestionMeters().getUnparseable());\n    Assert.assertEquals(0, task1.getRunner().getRowIngestionMeters().getThrownAway());\n    Assert.assertEquals(1, task2.getRunner().getRowIngestionMeters().getProcessed());\n    Assert.assertEquals(0, task2.getRunner().getRowIngestionMeters().getUnparseable());\n    Assert.assertEquals(0, task2.getRunner().getRowIngestionMeters().getThrownAway());\n\n    \r\n    assertEqualsExceptVersion(\n        ImmutableList.of(\n            sdd(\"2010/P1D\", 0, ImmutableList.of(\"c\")),\n            sdd(\"2011/P1D\", 0, ImmutableList.of(\"d\", \"e\"))\n        ),\n        publishedDescriptors()\n    );\n    Assert.assertEquals(\n        new KafkaDataSourceMetadata(new SeekableStreamEndSequenceNumbers<>(topic, ImmutableMap.of(0, 6L))),\n        metadataStorageCoordinator.getDataSourceMetadata(DATA_SCHEMA.getDataSource())\n    );\n  }\n","date":"2019-11-07 03:07:04","endLine":1769,"groupId":"2275","id":18,"instanceNumber":2,"isCurCommit":0,"methodName":"testRestore","params":"()","path":"/mnt/clonedata/CloneManagementServer/ManagementServer/consistResult/result-druid-10-0.7/blobInfo/CC_OUT/blobs/c8/35bb95038567e2b9b3112cb98971f6d2418766.src","preCode":"  public void testRestore() throws Exception\n  {\n    final KafkaIndexTask task1 = createTask(\n        null,\n        new KafkaIndexTaskIOConfig(\n            0,\n            \"sequence0\",\n            new SeekableStreamStartSequenceNumbers<>(topic, ImmutableMap.of(0, 2L), ImmutableSet.of()),\n            new SeekableStreamEndSequenceNumbers<>(topic, ImmutableMap.of(0, 6L)),\n            kafkaServer.consumerProperties(),\n            KafkaSupervisorIOConfig.DEFAULT_POLL_TIMEOUT_MILLIS,\n            true,\n            null,\n            null\n        )\n    );\n\n    final ListenableFuture<TaskStatus> future1 = runTask(task1);\n\n    \r\n    try (final KafkaProducer<byte[], byte[]> kafkaProducer = kafkaServer.newProducer()) {\n      kafkaProducer.initTransactions();\n      kafkaProducer.beginTransaction();\n      for (ProducerRecord<byte[], byte[]> record : Iterables.limit(records, 4)) {\n        kafkaProducer.send(record).get();\n      }\n      kafkaProducer.commitTransaction();\n    }\n\n    while (countEvents(task1) != 2) {\n      Thread.sleep(25);\n    }\n\n    Assert.assertEquals(2, countEvents(task1));\n\n    \r\n    task1.stopGracefully(toolboxFactory.build(task1).getConfig());\n    unlockAppenderatorBasePersistDirForTask(task1);\n\n    Assert.assertEquals(TaskState.SUCCESS, future1.get().getStatusCode());\n\n    \r\n    final KafkaIndexTask task2 = createTask(\n        task1.getId(),\n        new KafkaIndexTaskIOConfig(\n            0,\n            \"sequence0\",\n            new SeekableStreamStartSequenceNumbers<>(topic, ImmutableMap.of(0, 2L), ImmutableSet.of()),\n            new SeekableStreamEndSequenceNumbers<>(topic, ImmutableMap.of(0, 6L)),\n            kafkaServer.consumerProperties(),\n            KafkaSupervisorIOConfig.DEFAULT_POLL_TIMEOUT_MILLIS,\n            true,\n            null,\n            null\n        )\n    );\n\n    final ListenableFuture<TaskStatus> future2 = runTask(task2);\n\n    \r\n    try (final KafkaProducer<byte[], byte[]> kafkaProducer = kafkaServer.newProducer()) {\n      kafkaProducer.initTransactions();\n      kafkaProducer.beginTransaction();\n      for (ProducerRecord<byte[], byte[]> record : Iterables.skip(records, 4)) {\n        kafkaProducer.send(record).get();\n      }\n      kafkaProducer.commitTransaction();\n    }\n\n    \r\n\n    Assert.assertEquals(TaskState.SUCCESS, future2.get().getStatusCode());\n\n    \r\n    Assert.assertEquals(2, task1.getRunner().getRowIngestionMeters().getProcessed());\n    Assert.assertEquals(0, task1.getRunner().getRowIngestionMeters().getUnparseable());\n    Assert.assertEquals(0, task1.getRunner().getRowIngestionMeters().getThrownAway());\n    Assert.assertEquals(1, task2.getRunner().getRowIngestionMeters().getProcessed());\n    Assert.assertEquals(0, task2.getRunner().getRowIngestionMeters().getUnparseable());\n    Assert.assertEquals(0, task2.getRunner().getRowIngestionMeters().getThrownAway());\n\n    \r\n    SegmentDescriptor desc1 = sd(\"2010/P1D\", 0);\n    SegmentDescriptor desc2 = sd(\"2011/P1D\", 0);\n    assertEqualsExceptVersion(ImmutableList.of(desc1, desc2), publishedDescriptors());\n    Assert.assertEquals(\n        new KafkaDataSourceMetadata(new SeekableStreamEndSequenceNumbers<>(topic, ImmutableMap.of(0, 6L))),\n        metadataStorageCoordinator.getDataSourceMetadata(DATA_SCHEMA.getDataSource())\n    );\n\n    \r\n    final List<SegmentDescriptor> publishedDescriptors = publishedDescriptors();\n    Assert.assertEquals(ImmutableList.of(\"c\"), readSegmentColumn(\"dim1\", publishedDescriptors.get(0)));\n    Assert.assertEquals(ImmutableList.of(\"d\", \"e\"), readSegmentColumn(\"dim1\", publishedDescriptors.get(1)));\n  }\n","realPath":"extensions-core/kafka-indexing-service/src/test/java/org/apache/druid/indexing/kafka/KafkaIndexTaskTest.java","repoName":"druid","snippetEndLine":0,"snippetStartLine":0,"startLine":1676,"status":"M"}],"commitId":"5c0fc0a13ab4d259b430bf50b322f631504c4529","commitMessage":"@@@Fix ambiguity about IndexerSQLMetadataStorageCoordinator.getUsedSegmentsForInterval() returning only non-overshadowed or all used segments (#8564)\n\n* IndexerSQLMetadataStorageCoordinator.getTimelineForIntervalsWithHandle() don't fetch abutting intervals; simplify getUsedSegmentsForIntervals()\n\n* Add VersionedIntervalTimeline.findNonOvershadowedObjectsInInterval() method; Propagate the decision about whether only visible segmetns or visible and overshadowed segments should be returned from IndexerMetadataStorageCoordinator's methods to the user logic; Rename SegmentListUsedAction to RetrieveUsedSegmentsAction.  SegmetnListUnusedAction to RetrieveUnusedSegmentsAction.  and UsedSegmentLister to UsedSegmentsRetriever\n\n* Fix tests\n\n* More fixes\n\n* Add javadoc notes about returning Collection instead of Set. Add JacksonUtils.readValue() to reduce boilerplate code\n\n* Fix KinesisIndexTaskTest.  factor out common parts from KinesisIndexTaskTest and KafkaIndexTaskTest into SeekableStreamIndexTaskTestBase\n\n* More test fixes\n\n* More test fixes\n\n* Add a comment to VersionedIntervalTimelineTestBase\n\n* Fix tests\n\n* Set DataSegment.size(0) in more tests\n\n* Specify DataSegment.size(0) in more places in tests\n\n* Fix more tests\n\n* Fix DruidSchemaTest\n\n* Set DataSegment's size in more tests and benchmarks\n\n* Fix HdfsDataSegmentPusherTest\n\n* Doc changes addressing comments\n\n* Extended doc for visibility\n\n* Typo\n\n* Typo 2\n\n* Address comment\n","date":"2019-11-07 03:07:04","modifiedFileCount":"88","status":"M","submitter":"Roman Leventov"},{"authorTime":"2019-11-21 06:51:25","codes":[{"authorDate":"2019-11-21 06:51:25","commitOrder":10,"curCode":"  public void testRunTwoTasksTwoPartitions() throws Exception\n  {\n    final KafkaIndexTask task1 = createTask(\n        null,\n        new KafkaIndexTaskIOConfig(\n            0,\n            \"sequence0\",\n            new SeekableStreamStartSequenceNumbers<>(topic, ImmutableMap.of(0, 2L), ImmutableSet.of()),\n            new SeekableStreamEndSequenceNumbers<>(topic, ImmutableMap.of(0, 5L)),\n            kafkaServer.consumerProperties(),\n            KafkaSupervisorIOConfig.DEFAULT_POLL_TIMEOUT_MILLIS,\n            true,\n            null,\n            null,\n            INPUT_FORMAT\n        )\n    );\n    final KafkaIndexTask task2 = createTask(\n        null,\n        new KafkaIndexTaskIOConfig(\n            1,\n            \"sequence1\",\n            new SeekableStreamStartSequenceNumbers<>(topic, ImmutableMap.of(1, 0L), ImmutableSet.of()),\n            new SeekableStreamEndSequenceNumbers<>(topic, ImmutableMap.of(1, 1L)),\n            kafkaServer.consumerProperties(),\n            KafkaSupervisorIOConfig.DEFAULT_POLL_TIMEOUT_MILLIS,\n            true,\n            null,\n            null,\n            INPUT_FORMAT\n        )\n    );\n\n    final ListenableFuture<TaskStatus> future1 = runTask(task1);\n    final ListenableFuture<TaskStatus> future2 = runTask(task2);\n\n    \r\n    insertData();\n\n    \r\n    Assert.assertEquals(TaskState.SUCCESS, future1.get().getStatusCode());\n    Assert.assertEquals(TaskState.SUCCESS, future2.get().getStatusCode());\n\n    \r\n    Assert.assertEquals(3, task1.getRunner().getRowIngestionMeters().getProcessed());\n    Assert.assertEquals(0, task1.getRunner().getRowIngestionMeters().getUnparseable());\n    Assert.assertEquals(0, task1.getRunner().getRowIngestionMeters().getThrownAway());\n    Assert.assertEquals(1, task2.getRunner().getRowIngestionMeters().getProcessed());\n    Assert.assertEquals(0, task2.getRunner().getRowIngestionMeters().getUnparseable());\n    Assert.assertEquals(0, task2.getRunner().getRowIngestionMeters().getThrownAway());\n\n    \r\n    assertEqualsExceptVersion(\n        ImmutableList.of(\n            sdd(\"2010/P1D\", 0, ImmutableList.of(\"c\")),\n            sdd(\"2011/P1D\", 0, ImmutableList.of(\"d\", \"e\")),\n            sdd(\"2012/P1D\", 0, ImmutableList.of(\"g\"))\n        ),\n        publishedDescriptors()\n    );\n    Assert.assertEquals(\n        new KafkaDataSourceMetadata(\n            new SeekableStreamEndSequenceNumbers<>(topic, ImmutableMap.of(0, 5L, 1, 1L))\n        ),\n        metadataStorageCoordinator.getDataSourceMetadata(NEW_DATA_SCHEMA.getDataSource())\n    );\n  }\n","date":"2019-11-21 06:51:25","endLine":1746,"groupId":"6366","id":19,"instanceNumber":1,"isCurCommit":0,"methodName":"testRunTwoTasksTwoPartitions","params":"()","path":"/mnt/clonedata/CloneManagementServer/ManagementServer/consistResult/result-druid-10-0.7/blobInfo/CC_OUT/blobs/e8/bde11469c6f5c09cd43b1c5d562e5b78e0f593.src","preCode":"  public void testRunTwoTasksTwoPartitions() throws Exception\n  {\n    final KafkaIndexTask task1 = createTask(\n        null,\n        new KafkaIndexTaskIOConfig(\n            0,\n            \"sequence0\",\n            new SeekableStreamStartSequenceNumbers<>(topic, ImmutableMap.of(0, 2L), ImmutableSet.of()),\n            new SeekableStreamEndSequenceNumbers<>(topic, ImmutableMap.of(0, 5L)),\n            kafkaServer.consumerProperties(),\n            KafkaSupervisorIOConfig.DEFAULT_POLL_TIMEOUT_MILLIS,\n            true,\n            null,\n            null\n        )\n    );\n    final KafkaIndexTask task2 = createTask(\n        null,\n        new KafkaIndexTaskIOConfig(\n            1,\n            \"sequence1\",\n            new SeekableStreamStartSequenceNumbers<>(topic, ImmutableMap.of(1, 0L), ImmutableSet.of()),\n            new SeekableStreamEndSequenceNumbers<>(topic, ImmutableMap.of(1, 1L)),\n            kafkaServer.consumerProperties(),\n            KafkaSupervisorIOConfig.DEFAULT_POLL_TIMEOUT_MILLIS,\n            true,\n            null,\n            null\n        )\n    );\n\n    final ListenableFuture<TaskStatus> future1 = runTask(task1);\n    final ListenableFuture<TaskStatus> future2 = runTask(task2);\n\n    \r\n    insertData();\n\n    \r\n    Assert.assertEquals(TaskState.SUCCESS, future1.get().getStatusCode());\n    Assert.assertEquals(TaskState.SUCCESS, future2.get().getStatusCode());\n\n    \r\n    Assert.assertEquals(3, task1.getRunner().getRowIngestionMeters().getProcessed());\n    Assert.assertEquals(0, task1.getRunner().getRowIngestionMeters().getUnparseable());\n    Assert.assertEquals(0, task1.getRunner().getRowIngestionMeters().getThrownAway());\n    Assert.assertEquals(1, task2.getRunner().getRowIngestionMeters().getProcessed());\n    Assert.assertEquals(0, task2.getRunner().getRowIngestionMeters().getUnparseable());\n    Assert.assertEquals(0, task2.getRunner().getRowIngestionMeters().getThrownAway());\n\n    \r\n    assertEqualsExceptVersion(\n        ImmutableList.of(\n            sdd(\"2010/P1D\", 0, ImmutableList.of(\"c\")),\n            sdd(\"2011/P1D\", 0, ImmutableList.of(\"d\", \"e\")),\n            sdd(\"2012/P1D\", 0, ImmutableList.of(\"g\"))\n        ),\n        publishedDescriptors()\n    );\n    Assert.assertEquals(\n        new KafkaDataSourceMetadata(\n            new SeekableStreamEndSequenceNumbers<>(topic, ImmutableMap.of(0, 5L, 1, 1L))\n        ),\n        metadataStorageCoordinator.getDataSourceMetadata(DATA_SCHEMA.getDataSource())\n    );\n  }\n","realPath":"extensions-core/kafka-indexing-service/src/test/java/org/apache/druid/indexing/kafka/KafkaIndexTaskTest.java","repoName":"druid","snippetEndLine":0,"snippetStartLine":0,"startLine":1680,"status":"M"},{"authorDate":"2019-11-21 06:51:25","commitOrder":10,"curCode":"  public void testRestore() throws Exception\n  {\n    final KafkaIndexTask task1 = createTask(\n        null,\n        new KafkaIndexTaskIOConfig(\n            0,\n            \"sequence0\",\n            new SeekableStreamStartSequenceNumbers<>(topic, ImmutableMap.of(0, 2L), ImmutableSet.of()),\n            new SeekableStreamEndSequenceNumbers<>(topic, ImmutableMap.of(0, 6L)),\n            kafkaServer.consumerProperties(),\n            KafkaSupervisorIOConfig.DEFAULT_POLL_TIMEOUT_MILLIS,\n            true,\n            null,\n            null,\n            INPUT_FORMAT\n        )\n    );\n\n    final ListenableFuture<TaskStatus> future1 = runTask(task1);\n\n    \r\n    try (final KafkaProducer<byte[], byte[]> kafkaProducer = kafkaServer.newProducer()) {\n      kafkaProducer.initTransactions();\n      kafkaProducer.beginTransaction();\n      for (ProducerRecord<byte[], byte[]> record : Iterables.limit(records, 4)) {\n        kafkaProducer.send(record).get();\n      }\n      kafkaProducer.commitTransaction();\n    }\n\n    while (countEvents(task1) != 2) {\n      Thread.sleep(25);\n    }\n\n    Assert.assertEquals(2, countEvents(task1));\n\n    \r\n    task1.stopGracefully(toolboxFactory.build(task1).getConfig());\n    unlockAppenderatorBasePersistDirForTask(task1);\n\n    Assert.assertEquals(TaskState.SUCCESS, future1.get().getStatusCode());\n\n    \r\n    final KafkaIndexTask task2 = createTask(\n        task1.getId(),\n        new KafkaIndexTaskIOConfig(\n            0,\n            \"sequence0\",\n            new SeekableStreamStartSequenceNumbers<>(topic, ImmutableMap.of(0, 2L), ImmutableSet.of()),\n            new SeekableStreamEndSequenceNumbers<>(topic, ImmutableMap.of(0, 6L)),\n            kafkaServer.consumerProperties(),\n            KafkaSupervisorIOConfig.DEFAULT_POLL_TIMEOUT_MILLIS,\n            true,\n            null,\n            null,\n            INPUT_FORMAT\n        )\n    );\n\n    final ListenableFuture<TaskStatus> future2 = runTask(task2);\n\n    \r\n    try (final KafkaProducer<byte[], byte[]> kafkaProducer = kafkaServer.newProducer()) {\n      kafkaProducer.initTransactions();\n      kafkaProducer.beginTransaction();\n      for (ProducerRecord<byte[], byte[]> record : Iterables.skip(records, 4)) {\n        kafkaProducer.send(record).get();\n      }\n      kafkaProducer.commitTransaction();\n    }\n\n    \r\n\n    Assert.assertEquals(TaskState.SUCCESS, future2.get().getStatusCode());\n\n    \r\n    Assert.assertEquals(2, task1.getRunner().getRowIngestionMeters().getProcessed());\n    Assert.assertEquals(0, task1.getRunner().getRowIngestionMeters().getUnparseable());\n    Assert.assertEquals(0, task1.getRunner().getRowIngestionMeters().getThrownAway());\n    Assert.assertEquals(1, task2.getRunner().getRowIngestionMeters().getProcessed());\n    Assert.assertEquals(0, task2.getRunner().getRowIngestionMeters().getUnparseable());\n    Assert.assertEquals(0, task2.getRunner().getRowIngestionMeters().getThrownAway());\n\n    \r\n    assertEqualsExceptVersion(\n        ImmutableList.of(\n            sdd(\"2010/P1D\", 0, ImmutableList.of(\"c\")),\n            sdd(\"2011/P1D\", 0, ImmutableList.of(\"d\", \"e\"))\n        ),\n        publishedDescriptors()\n    );\n    Assert.assertEquals(\n        new KafkaDataSourceMetadata(new SeekableStreamEndSequenceNumbers<>(topic, ImmutableMap.of(0, 6L))),\n        metadataStorageCoordinator.getDataSourceMetadata(NEW_DATA_SCHEMA.getDataSource())\n    );\n  }\n","date":"2019-11-21 06:51:25","endLine":1844,"groupId":"17081","id":20,"instanceNumber":2,"isCurCommit":0,"methodName":"testRestore","params":"()","path":"/mnt/clonedata/CloneManagementServer/ManagementServer/consistResult/result-druid-10-0.7/blobInfo/CC_OUT/blobs/e8/bde11469c6f5c09cd43b1c5d562e5b78e0f593.src","preCode":"  public void testRestore() throws Exception\n  {\n    final KafkaIndexTask task1 = createTask(\n        null,\n        new KafkaIndexTaskIOConfig(\n            0,\n            \"sequence0\",\n            new SeekableStreamStartSequenceNumbers<>(topic, ImmutableMap.of(0, 2L), ImmutableSet.of()),\n            new SeekableStreamEndSequenceNumbers<>(topic, ImmutableMap.of(0, 6L)),\n            kafkaServer.consumerProperties(),\n            KafkaSupervisorIOConfig.DEFAULT_POLL_TIMEOUT_MILLIS,\n            true,\n            null,\n            null\n        )\n    );\n\n    final ListenableFuture<TaskStatus> future1 = runTask(task1);\n\n    \r\n    try (final KafkaProducer<byte[], byte[]> kafkaProducer = kafkaServer.newProducer()) {\n      kafkaProducer.initTransactions();\n      kafkaProducer.beginTransaction();\n      for (ProducerRecord<byte[], byte[]> record : Iterables.limit(records, 4)) {\n        kafkaProducer.send(record).get();\n      }\n      kafkaProducer.commitTransaction();\n    }\n\n    while (countEvents(task1) != 2) {\n      Thread.sleep(25);\n    }\n\n    Assert.assertEquals(2, countEvents(task1));\n\n    \r\n    task1.stopGracefully(toolboxFactory.build(task1).getConfig());\n    unlockAppenderatorBasePersistDirForTask(task1);\n\n    Assert.assertEquals(TaskState.SUCCESS, future1.get().getStatusCode());\n\n    \r\n    final KafkaIndexTask task2 = createTask(\n        task1.getId(),\n        new KafkaIndexTaskIOConfig(\n            0,\n            \"sequence0\",\n            new SeekableStreamStartSequenceNumbers<>(topic, ImmutableMap.of(0, 2L), ImmutableSet.of()),\n            new SeekableStreamEndSequenceNumbers<>(topic, ImmutableMap.of(0, 6L)),\n            kafkaServer.consumerProperties(),\n            KafkaSupervisorIOConfig.DEFAULT_POLL_TIMEOUT_MILLIS,\n            true,\n            null,\n            null\n        )\n    );\n\n    final ListenableFuture<TaskStatus> future2 = runTask(task2);\n\n    \r\n    try (final KafkaProducer<byte[], byte[]> kafkaProducer = kafkaServer.newProducer()) {\n      kafkaProducer.initTransactions();\n      kafkaProducer.beginTransaction();\n      for (ProducerRecord<byte[], byte[]> record : Iterables.skip(records, 4)) {\n        kafkaProducer.send(record).get();\n      }\n      kafkaProducer.commitTransaction();\n    }\n\n    \r\n\n    Assert.assertEquals(TaskState.SUCCESS, future2.get().getStatusCode());\n\n    \r\n    Assert.assertEquals(2, task1.getRunner().getRowIngestionMeters().getProcessed());\n    Assert.assertEquals(0, task1.getRunner().getRowIngestionMeters().getUnparseable());\n    Assert.assertEquals(0, task1.getRunner().getRowIngestionMeters().getThrownAway());\n    Assert.assertEquals(1, task2.getRunner().getRowIngestionMeters().getProcessed());\n    Assert.assertEquals(0, task2.getRunner().getRowIngestionMeters().getUnparseable());\n    Assert.assertEquals(0, task2.getRunner().getRowIngestionMeters().getThrownAway());\n\n    \r\n    assertEqualsExceptVersion(\n        ImmutableList.of(\n            sdd(\"2010/P1D\", 0, ImmutableList.of(\"c\")),\n            sdd(\"2011/P1D\", 0, ImmutableList.of(\"d\", \"e\"))\n        ),\n        publishedDescriptors()\n    );\n    Assert.assertEquals(\n        new KafkaDataSourceMetadata(new SeekableStreamEndSequenceNumbers<>(topic, ImmutableMap.of(0, 6L))),\n        metadataStorageCoordinator.getDataSourceMetadata(DATA_SCHEMA.getDataSource())\n    );\n  }\n","realPath":"extensions-core/kafka-indexing-service/src/test/java/org/apache/druid/indexing/kafka/KafkaIndexTaskTest.java","repoName":"druid","snippetEndLine":0,"snippetStartLine":0,"startLine":1749,"status":"M"}],"commitId":"ac6d703814ccb5b258c586b63e0bc33d669e0f57","commitMessage":"@@@Support inputFormat and inputSource for sampler (#8901)\n\n* Support inputFormat and inputSource for sampler\n\n* Cleanup javadocs and names\n\n* fix style\n\n* fix timed shutoff input source reader\n\n* fix timed shutoff input source reader again\n\n* tidy up timed shutoff reader\n\n* unused imports\n\n* fix tc\n","date":"2019-11-21 06:51:25","modifiedFileCount":"66","status":"M","submitter":"Jihoon Son"},{"authorTime":"2020-01-28 03:24:29","codes":[{"authorDate":"2020-01-28 03:24:29","commitOrder":11,"curCode":"  public void testRunTwoTasksTwoPartitions() throws Exception\n  {\n    final KafkaIndexTask task1 = createTask(\n        null,\n        new KafkaIndexTaskIOConfig(\n            0,\n            \"sequence0\",\n            new SeekableStreamStartSequenceNumbers<>(topic, ImmutableMap.of(0, 2L), ImmutableSet.of()),\n            new SeekableStreamEndSequenceNumbers<>(topic, ImmutableMap.of(0, 5L)),\n            kafkaServer.consumerProperties(),\n            KafkaSupervisorIOConfig.DEFAULT_POLL_TIMEOUT_MILLIS,\n            true,\n            null,\n            null,\n            INPUT_FORMAT\n        )\n    );\n    final KafkaIndexTask task2 = createTask(\n        null,\n        new KafkaIndexTaskIOConfig(\n            1,\n            \"sequence1\",\n            new SeekableStreamStartSequenceNumbers<>(topic, ImmutableMap.of(1, 0L), ImmutableSet.of()),\n            new SeekableStreamEndSequenceNumbers<>(topic, ImmutableMap.of(1, 1L)),\n            kafkaServer.consumerProperties(),\n            KafkaSupervisorIOConfig.DEFAULT_POLL_TIMEOUT_MILLIS,\n            true,\n            null,\n            null,\n            INPUT_FORMAT\n        )\n    );\n\n    final ListenableFuture<TaskStatus> future1 = runTask(task1);\n    final ListenableFuture<TaskStatus> future2 = runTask(task2);\n\n    \r\n    insertData();\n\n    \r\n    Assert.assertEquals(TaskState.SUCCESS, future1.get().getStatusCode());\n    Assert.assertEquals(TaskState.SUCCESS, future2.get().getStatusCode());\n\n    \r\n    Assert.assertEquals(3, task1.getRunner().getRowIngestionMeters().getProcessed());\n    Assert.assertEquals(0, task1.getRunner().getRowIngestionMeters().getUnparseable());\n    Assert.assertEquals(0, task1.getRunner().getRowIngestionMeters().getThrownAway());\n    Assert.assertEquals(1, task2.getRunner().getRowIngestionMeters().getProcessed());\n    Assert.assertEquals(0, task2.getRunner().getRowIngestionMeters().getUnparseable());\n    Assert.assertEquals(0, task2.getRunner().getRowIngestionMeters().getThrownAway());\n\n    \r\n    assertEqualsExceptVersion(\n        ImmutableList.of(\n            sdd(\"2010/P1D\", 0, ImmutableList.of(\"c\")),\n            sdd(\"2011/P1D\", 0, ImmutableList.of(\"d\", \"e\")),\n            sdd(\"2012/P1D\", 0, ImmutableList.of(\"g\"))\n        ),\n        publishedDescriptors()\n    );\n    Assert.assertEquals(\n        new KafkaDataSourceMetadata(\n            new SeekableStreamEndSequenceNumbers<>(topic, ImmutableMap.of(0, 5L, 1, 1L))\n        ),\n        newDataSchemaMetadata()\n    );\n  }\n","date":"2020-01-28 03:24:29","endLine":1749,"groupId":"102528","id":21,"instanceNumber":1,"isCurCommit":0,"methodName":"testRunTwoTasksTwoPartitions","params":"()","path":"/mnt/clonedata/CloneManagementServer/ManagementServer/consistResult/result-druid-10-0.7/blobInfo/CC_OUT/blobs/d4/468bd39e032386fe654402d0006ecea542747e.src","preCode":"  public void testRunTwoTasksTwoPartitions() throws Exception\n  {\n    final KafkaIndexTask task1 = createTask(\n        null,\n        new KafkaIndexTaskIOConfig(\n            0,\n            \"sequence0\",\n            new SeekableStreamStartSequenceNumbers<>(topic, ImmutableMap.of(0, 2L), ImmutableSet.of()),\n            new SeekableStreamEndSequenceNumbers<>(topic, ImmutableMap.of(0, 5L)),\n            kafkaServer.consumerProperties(),\n            KafkaSupervisorIOConfig.DEFAULT_POLL_TIMEOUT_MILLIS,\n            true,\n            null,\n            null,\n            INPUT_FORMAT\n        )\n    );\n    final KafkaIndexTask task2 = createTask(\n        null,\n        new KafkaIndexTaskIOConfig(\n            1,\n            \"sequence1\",\n            new SeekableStreamStartSequenceNumbers<>(topic, ImmutableMap.of(1, 0L), ImmutableSet.of()),\n            new SeekableStreamEndSequenceNumbers<>(topic, ImmutableMap.of(1, 1L)),\n            kafkaServer.consumerProperties(),\n            KafkaSupervisorIOConfig.DEFAULT_POLL_TIMEOUT_MILLIS,\n            true,\n            null,\n            null,\n            INPUT_FORMAT\n        )\n    );\n\n    final ListenableFuture<TaskStatus> future1 = runTask(task1);\n    final ListenableFuture<TaskStatus> future2 = runTask(task2);\n\n    \r\n    insertData();\n\n    \r\n    Assert.assertEquals(TaskState.SUCCESS, future1.get().getStatusCode());\n    Assert.assertEquals(TaskState.SUCCESS, future2.get().getStatusCode());\n\n    \r\n    Assert.assertEquals(3, task1.getRunner().getRowIngestionMeters().getProcessed());\n    Assert.assertEquals(0, task1.getRunner().getRowIngestionMeters().getUnparseable());\n    Assert.assertEquals(0, task1.getRunner().getRowIngestionMeters().getThrownAway());\n    Assert.assertEquals(1, task2.getRunner().getRowIngestionMeters().getProcessed());\n    Assert.assertEquals(0, task2.getRunner().getRowIngestionMeters().getUnparseable());\n    Assert.assertEquals(0, task2.getRunner().getRowIngestionMeters().getThrownAway());\n\n    \r\n    assertEqualsExceptVersion(\n        ImmutableList.of(\n            sdd(\"2010/P1D\", 0, ImmutableList.of(\"c\")),\n            sdd(\"2011/P1D\", 0, ImmutableList.of(\"d\", \"e\")),\n            sdd(\"2012/P1D\", 0, ImmutableList.of(\"g\"))\n        ),\n        publishedDescriptors()\n    );\n    Assert.assertEquals(\n        new KafkaDataSourceMetadata(\n            new SeekableStreamEndSequenceNumbers<>(topic, ImmutableMap.of(0, 5L, 1, 1L))\n        ),\n        metadataStorageCoordinator.getDataSourceMetadata(NEW_DATA_SCHEMA.getDataSource())\n    );\n  }\n","realPath":"extensions-core/kafka-indexing-service/src/test/java/org/apache/druid/indexing/kafka/KafkaIndexTaskTest.java","repoName":"druid","snippetEndLine":0,"snippetStartLine":0,"startLine":1683,"status":"M"},{"authorDate":"2020-01-28 03:24:29","commitOrder":11,"curCode":"  public void testRestore() throws Exception\n  {\n    final KafkaIndexTask task1 = createTask(\n        null,\n        new KafkaIndexTaskIOConfig(\n            0,\n            \"sequence0\",\n            new SeekableStreamStartSequenceNumbers<>(topic, ImmutableMap.of(0, 2L), ImmutableSet.of()),\n            new SeekableStreamEndSequenceNumbers<>(topic, ImmutableMap.of(0, 6L)),\n            kafkaServer.consumerProperties(),\n            KafkaSupervisorIOConfig.DEFAULT_POLL_TIMEOUT_MILLIS,\n            true,\n            null,\n            null,\n            INPUT_FORMAT\n        )\n    );\n\n    final ListenableFuture<TaskStatus> future1 = runTask(task1);\n\n    \r\n    try (final KafkaProducer<byte[], byte[]> kafkaProducer = kafkaServer.newProducer()) {\n      kafkaProducer.initTransactions();\n      kafkaProducer.beginTransaction();\n      for (ProducerRecord<byte[], byte[]> record : Iterables.limit(records, 4)) {\n        kafkaProducer.send(record).get();\n      }\n      kafkaProducer.commitTransaction();\n    }\n\n    while (countEvents(task1) != 2) {\n      Thread.sleep(25);\n    }\n\n    Assert.assertEquals(2, countEvents(task1));\n\n    \r\n    task1.stopGracefully(toolboxFactory.build(task1).getConfig());\n    unlockAppenderatorBasePersistDirForTask(task1);\n\n    Assert.assertEquals(TaskState.SUCCESS, future1.get().getStatusCode());\n\n    \r\n    final KafkaIndexTask task2 = createTask(\n        task1.getId(),\n        new KafkaIndexTaskIOConfig(\n            0,\n            \"sequence0\",\n            new SeekableStreamStartSequenceNumbers<>(topic, ImmutableMap.of(0, 2L), ImmutableSet.of()),\n            new SeekableStreamEndSequenceNumbers<>(topic, ImmutableMap.of(0, 6L)),\n            kafkaServer.consumerProperties(),\n            KafkaSupervisorIOConfig.DEFAULT_POLL_TIMEOUT_MILLIS,\n            true,\n            null,\n            null,\n            INPUT_FORMAT\n        )\n    );\n\n    final ListenableFuture<TaskStatus> future2 = runTask(task2);\n\n    \r\n    try (final KafkaProducer<byte[], byte[]> kafkaProducer = kafkaServer.newProducer()) {\n      kafkaProducer.initTransactions();\n      kafkaProducer.beginTransaction();\n      for (ProducerRecord<byte[], byte[]> record : Iterables.skip(records, 4)) {\n        kafkaProducer.send(record).get();\n      }\n      kafkaProducer.commitTransaction();\n    }\n\n    \r\n\n    Assert.assertEquals(TaskState.SUCCESS, future2.get().getStatusCode());\n\n    \r\n    Assert.assertEquals(2, task1.getRunner().getRowIngestionMeters().getProcessed());\n    Assert.assertEquals(0, task1.getRunner().getRowIngestionMeters().getUnparseable());\n    Assert.assertEquals(0, task1.getRunner().getRowIngestionMeters().getThrownAway());\n    Assert.assertEquals(1, task2.getRunner().getRowIngestionMeters().getProcessed());\n    Assert.assertEquals(0, task2.getRunner().getRowIngestionMeters().getUnparseable());\n    Assert.assertEquals(0, task2.getRunner().getRowIngestionMeters().getThrownAway());\n\n    \r\n    assertEqualsExceptVersion(\n        ImmutableList.of(\n            sdd(\"2010/P1D\", 0, ImmutableList.of(\"c\")),\n            sdd(\"2011/P1D\", 0, ImmutableList.of(\"d\", \"e\"))\n        ),\n        publishedDescriptors()\n    );\n    Assert.assertEquals(\n        new KafkaDataSourceMetadata(new SeekableStreamEndSequenceNumbers<>(topic, ImmutableMap.of(0, 6L))),\n        newDataSchemaMetadata()\n    );\n  }\n","date":"2020-01-28 03:24:29","endLine":1847,"groupId":"102528","id":22,"instanceNumber":2,"isCurCommit":0,"methodName":"testRestore","params":"()","path":"/mnt/clonedata/CloneManagementServer/ManagementServer/consistResult/result-druid-10-0.7/blobInfo/CC_OUT/blobs/d4/468bd39e032386fe654402d0006ecea542747e.src","preCode":"  public void testRestore() throws Exception\n  {\n    final KafkaIndexTask task1 = createTask(\n        null,\n        new KafkaIndexTaskIOConfig(\n            0,\n            \"sequence0\",\n            new SeekableStreamStartSequenceNumbers<>(topic, ImmutableMap.of(0, 2L), ImmutableSet.of()),\n            new SeekableStreamEndSequenceNumbers<>(topic, ImmutableMap.of(0, 6L)),\n            kafkaServer.consumerProperties(),\n            KafkaSupervisorIOConfig.DEFAULT_POLL_TIMEOUT_MILLIS,\n            true,\n            null,\n            null,\n            INPUT_FORMAT\n        )\n    );\n\n    final ListenableFuture<TaskStatus> future1 = runTask(task1);\n\n    \r\n    try (final KafkaProducer<byte[], byte[]> kafkaProducer = kafkaServer.newProducer()) {\n      kafkaProducer.initTransactions();\n      kafkaProducer.beginTransaction();\n      for (ProducerRecord<byte[], byte[]> record : Iterables.limit(records, 4)) {\n        kafkaProducer.send(record).get();\n      }\n      kafkaProducer.commitTransaction();\n    }\n\n    while (countEvents(task1) != 2) {\n      Thread.sleep(25);\n    }\n\n    Assert.assertEquals(2, countEvents(task1));\n\n    \r\n    task1.stopGracefully(toolboxFactory.build(task1).getConfig());\n    unlockAppenderatorBasePersistDirForTask(task1);\n\n    Assert.assertEquals(TaskState.SUCCESS, future1.get().getStatusCode());\n\n    \r\n    final KafkaIndexTask task2 = createTask(\n        task1.getId(),\n        new KafkaIndexTaskIOConfig(\n            0,\n            \"sequence0\",\n            new SeekableStreamStartSequenceNumbers<>(topic, ImmutableMap.of(0, 2L), ImmutableSet.of()),\n            new SeekableStreamEndSequenceNumbers<>(topic, ImmutableMap.of(0, 6L)),\n            kafkaServer.consumerProperties(),\n            KafkaSupervisorIOConfig.DEFAULT_POLL_TIMEOUT_MILLIS,\n            true,\n            null,\n            null,\n            INPUT_FORMAT\n        )\n    );\n\n    final ListenableFuture<TaskStatus> future2 = runTask(task2);\n\n    \r\n    try (final KafkaProducer<byte[], byte[]> kafkaProducer = kafkaServer.newProducer()) {\n      kafkaProducer.initTransactions();\n      kafkaProducer.beginTransaction();\n      for (ProducerRecord<byte[], byte[]> record : Iterables.skip(records, 4)) {\n        kafkaProducer.send(record).get();\n      }\n      kafkaProducer.commitTransaction();\n    }\n\n    \r\n\n    Assert.assertEquals(TaskState.SUCCESS, future2.get().getStatusCode());\n\n    \r\n    Assert.assertEquals(2, task1.getRunner().getRowIngestionMeters().getProcessed());\n    Assert.assertEquals(0, task1.getRunner().getRowIngestionMeters().getUnparseable());\n    Assert.assertEquals(0, task1.getRunner().getRowIngestionMeters().getThrownAway());\n    Assert.assertEquals(1, task2.getRunner().getRowIngestionMeters().getProcessed());\n    Assert.assertEquals(0, task2.getRunner().getRowIngestionMeters().getUnparseable());\n    Assert.assertEquals(0, task2.getRunner().getRowIngestionMeters().getThrownAway());\n\n    \r\n    assertEqualsExceptVersion(\n        ImmutableList.of(\n            sdd(\"2010/P1D\", 0, ImmutableList.of(\"c\")),\n            sdd(\"2011/P1D\", 0, ImmutableList.of(\"d\", \"e\"))\n        ),\n        publishedDescriptors()\n    );\n    Assert.assertEquals(\n        new KafkaDataSourceMetadata(new SeekableStreamEndSequenceNumbers<>(topic, ImmutableMap.of(0, 6L))),\n        metadataStorageCoordinator.getDataSourceMetadata(NEW_DATA_SCHEMA.getDataSource())\n    );\n  }\n","realPath":"extensions-core/kafka-indexing-service/src/test/java/org/apache/druid/indexing/kafka/KafkaIndexTaskTest.java","repoName":"druid","snippetEndLine":0,"snippetStartLine":0,"startLine":1752,"status":"M"}],"commitId":"b9186f8f9ff2ff52aceda42bc5f24ffd47a7d17e","commitMessage":"@@@Reconcile terminology and method naming to 'used/unused segments'; Rename MetadataSegmentManager to MetadataSegmentsManager (#7306)\n\n* Reconcile terminology and method naming to 'used/unused segments'; Don't use terms 'enable/disable data source'; Rename MetadataSegmentManager to MetadataSegments; Make REST API methods which mark segments as used/unused to return server error instead of an empty response in case of error\n\n* Fix brace\n\n* Import order\n\n* Rename withKillDataSourceWhitelist to withSpecificDataSourcesToKill\n\n* Fix tests\n\n* Fix tests by adding proper methods without interval parameters to IndexerMetadataStorageCoordinator instead of hacking with Intervals.ETERNITY\n\n* More aligned names of DruidCoordinatorHelpers.  rename several CoordinatorDynamicConfig parameters\n\n* Rename ClientCompactTaskQuery to ClientCompactionTaskQuery for consistency with CompactionTask; ClientCompactQueryTuningConfig to ClientCompactionTaskQueryTuningConfig\n\n* More variable and method renames\n\n* Rename MetadataSegments to SegmentsMetadata\n\n* Javadoc update\n\n* Simplify SegmentsMetadata.getUnusedSegmentIntervals().  more javadocs\n\n* Update Javadoc of VersionedIntervalTimeline.iterateAllObjects()\n\n* Reorder imports\n\n* Rename SegmentsMetadata.tryMark... methods to mark... and make them to return boolean and the numbers of segments changed and relay exceptions to callers\n\n* Complete merge\n\n* Add CollectionUtils.newTreeSet(); Refactor DruidCoordinatorRuntimeParams creation in tests\n\n* Remove MetadataSegmentManager\n\n* Rename millisLagSinceCoordinatorBecomesLeaderBeforeCanMarkAsUnusedOvershadowedSegments to leadingTimeMillisBeforeCanMarkAsUnusedOvershadowedSegments\n\n* Fix tests.  refactor DruidCluster creation in tests into DruidClusterBuilder\n\n* Fix inspections\n\n* Fix SQLMetadataSegmentManagerEmptyTest and rename it to SqlSegmentsMetadataEmptyTest\n\n* Rename SegmentsAndMetadata to SegmentsAndCommitMetadata to reduce the similarity with SegmentsMetadata; Rename some methods\n\n* Rename DruidCoordinatorHelper to CoordinatorDuty.  refactor DruidCoordinator\n\n* Unused import\n\n* Optimize imports\n\n* Rename IndexerSQLMetadataStorageCoordinator.getDataSourceMetadata() to retrieveDataSourceMetadata()\n\n* Unused import\n\n* Update terminology in datasource-view.tsx\n\n* Fix label in datasource-view.spec.tsx.snap\n\n* Fix lint errors in datasource-view.tsx\n\n* Doc improvements\n\n* Another attempt to please TSLint\n\n* Another attempt to please TSLint\n\n* Style fixes\n\n* Fix IndexerSQLMetadataStorageCoordinator.createUsedSegmentsSqlQueryForIntervals() (wrong merge)\n\n* Try to fix docs build issue\n\n* Javadoc and spelling fixes\n\n* Rename SegmentsMetadata to SegmentsMetadataManager.  address other comments\n\n* Address more comments\n","date":"2020-01-28 03:24:29","modifiedFileCount":"127","status":"M","submitter":"Roman Leventov"}]
