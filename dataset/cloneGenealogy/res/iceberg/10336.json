[{"authorTime":"2020-03-07 03:38:06","codes":[{"authorDate":"2020-03-07 03:38:06","commitOrder":1,"curCode":"  public void testAllEntriesTable() throws Exception {\n    TableIdentifier tableIdentifier = TableIdentifier.of(\"db\", \"entries_test\");\n    Table table = createTable(tableIdentifier, SCHEMA, PartitionSpec.unpartitioned());\n    Table entriesTable = loadTable(tableIdentifier, \"all_entries\");\n\n    Dataset<Row> df1 = spark.createDataFrame(Lists.newArrayList(new SimpleRecord(1, \"a\")), SimpleRecord.class);\n    Dataset<Row> df2 = spark.createDataFrame(Lists.newArrayList(new SimpleRecord(1, \"b\")), SimpleRecord.class);\n\n    df1.select(\"id\", \"data\").write()\n        .format(\"iceberg\")\n        .mode(\"append\")\n        .save(loadLocation(tableIdentifier));\n\n    \r\n    table.newDelete().deleteFromRowFilter(Expressions.equal(\"id\", 1)).commit();\n\n    \r\n    df2.select(\"id\", \"data\").write()\n        .format(\"iceberg\")\n        .mode(\"append\")\n        .save(loadLocation(tableIdentifier));\n\n    \r\n    table.refresh();\n\n    List<Row> actual = spark.read()\n        .format(\"iceberg\")\n        .load(loadLocation(tableIdentifier, \"all_entries\"))\n        .orderBy(\"snapshot_id\")\n        .collectAsList();\n\n    List<GenericData.Record> expected = Lists.newArrayList();\n    for (ManifestFile manifest : Iterables.concat(Iterables.transform(table.snapshots(), Snapshot::manifests))) {\n      InputFile in = table.io().newInputFile(manifest.path());\n      try (CloseableIterable<GenericData.Record> rows = Avro.read(in).project(entriesTable.schema()).build()) {\n        for (GenericData.Record record : rows) {\n          expected.add(record);\n        }\n      }\n    }\n\n    expected.sort(Comparator.comparing(o -> (Long) o.get(\"snapshot_id\")));\n\n    Assert.assertEquals(\"Entries table should have 3 rows\", 3, expected.size());\n    Assert.assertEquals(\"Actual results should have 3 rows\", 3, actual.size());\n    for (int i = 0; i < expected.size(); i += 1) {\n      TestHelpers.assertEqualsSafe(entriesTable.schema().asStruct(), expected.get(i), actual.get(i));\n    }\n  }\n","date":"2020-03-07 03:38:06","endLine":179,"groupId":"3984","id":1,"instanceNumber":1,"isCurCommit":0,"methodName":"testAllEntriesTable","params":"()","path":"/mnt/clonedata/CloneManagementServer/ManagementServer/consistResult/result-iceberg-10-0.7/blobInfo/CC_OUT/blobs/fc/27ea887f93e75a914230753ad06d7939fc2fd5.src","preCode":"  public void testAllEntriesTable() throws Exception {\n    TableIdentifier tableIdentifier = TableIdentifier.of(\"db\", \"entries_test\");\n    Table table = createTable(tableIdentifier, SCHEMA, PartitionSpec.unpartitioned());\n    Table entriesTable = loadTable(tableIdentifier, \"all_entries\");\n\n    Dataset<Row> df1 = spark.createDataFrame(Lists.newArrayList(new SimpleRecord(1, \"a\")), SimpleRecord.class);\n    Dataset<Row> df2 = spark.createDataFrame(Lists.newArrayList(new SimpleRecord(1, \"b\")), SimpleRecord.class);\n\n    df1.select(\"id\", \"data\").write()\n        .format(\"iceberg\")\n        .mode(\"append\")\n        .save(loadLocation(tableIdentifier));\n\n    \r\n    table.newDelete().deleteFromRowFilter(Expressions.equal(\"id\", 1)).commit();\n\n    \r\n    df2.select(\"id\", \"data\").write()\n        .format(\"iceberg\")\n        .mode(\"append\")\n        .save(loadLocation(tableIdentifier));\n\n    \r\n    table.refresh();\n\n    List<Row> actual = spark.read()\n        .format(\"iceberg\")\n        .load(loadLocation(tableIdentifier, \"all_entries\"))\n        .orderBy(\"snapshot_id\")\n        .collectAsList();\n\n    List<GenericData.Record> expected = Lists.newArrayList();\n    for (ManifestFile manifest : Iterables.concat(Iterables.transform(table.snapshots(), Snapshot::manifests))) {\n      InputFile in = table.io().newInputFile(manifest.path());\n      try (CloseableIterable<GenericData.Record> rows = Avro.read(in).project(entriesTable.schema()).build()) {\n        for (GenericData.Record record : rows) {\n          expected.add(record);\n        }\n      }\n    }\n\n    expected.sort(Comparator.comparing(o -> (Long) o.get(\"snapshot_id\")));\n\n    Assert.assertEquals(\"Entries table should have 3 rows\", 3, expected.size());\n    Assert.assertEquals(\"Actual results should have 3 rows\", 3, actual.size());\n    for (int i = 0; i < expected.size(); i += 1) {\n      TestHelpers.assertEqualsSafe(entriesTable.schema().asStruct(), expected.get(i), actual.get(i));\n    }\n  }\n","realPath":"spark/src/test/java/org/apache/iceberg/spark/source/TestIcebergSourceTablesBase.java","repoName":"iceberg","snippetEndLine":0,"snippetStartLine":0,"startLine":131,"status":"B"},{"authorDate":"2020-03-07 03:38:06","commitOrder":1,"curCode":"  public void testAllDataFilesTable() throws Exception {\n    TableIdentifier tableIdentifier = TableIdentifier.of(\"db\", \"files_test\");\n    Table table = createTable(tableIdentifier, SCHEMA, PartitionSpec.builderFor(SCHEMA).identity(\"id\").build());\n    Table entriesTable = loadTable(tableIdentifier, \"entries\");\n    Table filesTable = loadTable(tableIdentifier, \"all_data_files\");\n\n    Dataset<Row> df1 = spark.createDataFrame(Lists.newArrayList(new SimpleRecord(1, \"a\")), SimpleRecord.class);\n    Dataset<Row> df2 = spark.createDataFrame(Lists.newArrayList(new SimpleRecord(2, \"b\")), SimpleRecord.class);\n\n    df1.select(\"id\", \"data\").write()\n        .format(\"iceberg\")\n        .mode(\"append\")\n        .save(loadLocation(tableIdentifier));\n\n    \r\n    table.newDelete().deleteFromRowFilter(Expressions.equal(\"id\", 1)).commit();\n\n    \r\n    df2.select(\"id\", \"data\").write()\n        .format(\"iceberg\")\n        .mode(\"append\")\n        .save(loadLocation(tableIdentifier));\n\n    \r\n    table.refresh();\n\n    List<Row> actual = spark.read()\n        .format(\"iceberg\")\n        .load(loadLocation(tableIdentifier, \"all_data_files\"))\n        .orderBy(\"file_path\")\n        .collectAsList();\n    actual.sort(Comparator.comparing(o -> o.getString(0)));\n\n    List<GenericData.Record> expected = Lists.newArrayList();\n    for (ManifestFile manifest : Iterables.concat(Iterables.transform(table.snapshots(), Snapshot::manifests))) {\n      InputFile in = table.io().newInputFile(manifest.path());\n      try (CloseableIterable<GenericData.Record> rows = Avro.read(in).project(entriesTable.schema()).build()) {\n        for (GenericData.Record record : rows) {\n          if ((Integer) record.get(\"status\") < 2 ) {\n            expected.add((GenericData.Record) record.get(\"data_file\"));\n          }\n        }\n      }\n    }\n\n    expected.sort(Comparator.comparing(o -> o.get(\"file_path\").toString()));\n\n    Assert.assertEquals(\"Files table should have two rows\", 2, expected.size());\n    Assert.assertEquals(\"Actual results should have two rows\", 2, actual.size());\n    for (int i = 0; i < expected.size(); i += 1) {\n      TestHelpers.assertEqualsSafe(filesTable.schema().asStruct(), expected.get(i), actual.get(i));\n    }\n  }\n","date":"2020-03-07 03:38:06","endLine":382,"groupId":"3989","id":2,"instanceNumber":2,"isCurCommit":0,"methodName":"testAllDataFilesTable","params":"()","path":"/mnt/clonedata/CloneManagementServer/ManagementServer/consistResult/result-iceberg-10-0.7/blobInfo/CC_OUT/blobs/fc/27ea887f93e75a914230753ad06d7939fc2fd5.src","preCode":"  public void testAllDataFilesTable() throws Exception {\n    TableIdentifier tableIdentifier = TableIdentifier.of(\"db\", \"files_test\");\n    Table table = createTable(tableIdentifier, SCHEMA, PartitionSpec.builderFor(SCHEMA).identity(\"id\").build());\n    Table entriesTable = loadTable(tableIdentifier, \"entries\");\n    Table filesTable = loadTable(tableIdentifier, \"all_data_files\");\n\n    Dataset<Row> df1 = spark.createDataFrame(Lists.newArrayList(new SimpleRecord(1, \"a\")), SimpleRecord.class);\n    Dataset<Row> df2 = spark.createDataFrame(Lists.newArrayList(new SimpleRecord(2, \"b\")), SimpleRecord.class);\n\n    df1.select(\"id\", \"data\").write()\n        .format(\"iceberg\")\n        .mode(\"append\")\n        .save(loadLocation(tableIdentifier));\n\n    \r\n    table.newDelete().deleteFromRowFilter(Expressions.equal(\"id\", 1)).commit();\n\n    \r\n    df2.select(\"id\", \"data\").write()\n        .format(\"iceberg\")\n        .mode(\"append\")\n        .save(loadLocation(tableIdentifier));\n\n    \r\n    table.refresh();\n\n    List<Row> actual = spark.read()\n        .format(\"iceberg\")\n        .load(loadLocation(tableIdentifier, \"all_data_files\"))\n        .orderBy(\"file_path\")\n        .collectAsList();\n    actual.sort(Comparator.comparing(o -> o.getString(0)));\n\n    List<GenericData.Record> expected = Lists.newArrayList();\n    for (ManifestFile manifest : Iterables.concat(Iterables.transform(table.snapshots(), Snapshot::manifests))) {\n      InputFile in = table.io().newInputFile(manifest.path());\n      try (CloseableIterable<GenericData.Record> rows = Avro.read(in).project(entriesTable.schema()).build()) {\n        for (GenericData.Record record : rows) {\n          if ((Integer) record.get(\"status\") < 2 ) {\n            expected.add((GenericData.Record) record.get(\"data_file\"));\n          }\n        }\n      }\n    }\n\n    expected.sort(Comparator.comparing(o -> o.get(\"file_path\").toString()));\n\n    Assert.assertEquals(\"Files table should have two rows\", 2, expected.size());\n    Assert.assertEquals(\"Actual results should have two rows\", 2, actual.size());\n    for (int i = 0; i < expected.size(); i += 1) {\n      TestHelpers.assertEqualsSafe(filesTable.schema().asStruct(), expected.get(i), actual.get(i));\n    }\n  }\n","realPath":"spark/src/test/java/org/apache/iceberg/spark/source/TestIcebergSourceTablesBase.java","repoName":"iceberg","snippetEndLine":0,"snippetStartLine":0,"startLine":330,"status":"B"}],"commitId":"548112517032897f02f80cc564ecc5b1f82918de","commitMessage":"@@@Spark: Update table tests to use common test cases (#827)\n\nCo-authored-by: Lammott <jlammott@usa483e72e3960.am.sony.com>","date":"2020-03-07 03:38:06","modifiedFileCount":"2","status":"B","submitter":"Jacob Lammott"},{"authorTime":"2020-05-24 03:10:42","codes":[{"authorDate":"2020-05-24 03:10:42","commitOrder":2,"curCode":"  public void testAllEntriesTable() throws Exception {\n    TableIdentifier tableIdentifier = TableIdentifier.of(\"db\", \"entries_test\");\n    Table table = createTable(tableIdentifier, SCHEMA, PartitionSpec.unpartitioned());\n    Table entriesTable = loadTable(tableIdentifier, \"all_entries\");\n\n    Dataset<Row> df1 = spark.createDataFrame(Lists.newArrayList(new SimpleRecord(1, \"a\")), SimpleRecord.class);\n    Dataset<Row> df2 = spark.createDataFrame(Lists.newArrayList(new SimpleRecord(1, \"b\")), SimpleRecord.class);\n\n    df1.select(\"id\", \"data\").write()\n        .format(\"iceberg\")\n        .mode(\"append\")\n        .save(loadLocation(tableIdentifier));\n\n    \r\n    table.newDelete().deleteFromRowFilter(Expressions.equal(\"id\", 1)).commit();\n\n    \r\n    df2.select(\"id\", \"data\").write()\n        .format(\"iceberg\")\n        .mode(\"append\")\n        .save(loadLocation(tableIdentifier));\n\n    \r\n    table.refresh();\n\n    List<Row> actual = spark.read()\n        .format(\"iceberg\")\n        .load(loadLocation(tableIdentifier, \"all_entries\"))\n        .orderBy(\"snapshot_id\")\n        .collectAsList();\n\n    List<GenericData.Record> expected = Lists.newArrayList();\n    for (ManifestFile manifest : Iterables.concat(Iterables.transform(table.snapshots(), Snapshot::manifests))) {\n      InputFile in = table.io().newInputFile(manifest.path());\n      try (CloseableIterable<GenericData.Record> rows = Avro.read(in).project(entriesTable.schema()).build()) {\n        \r\n        rows.forEach(row -> {\n          row.put(2, 0L);\n          GenericData.Record file = (GenericData.Record) row.get(\"data_file\");\n          file.put(0, FileContent.DATA.id());\n          expected.add(row);\n        });\n      }\n    }\n\n    expected.sort(Comparator.comparing(o -> (Long) o.get(\"snapshot_id\")));\n\n    Assert.assertEquals(\"Entries table should have 3 rows\", 3, expected.size());\n    Assert.assertEquals(\"Actual results should have 3 rows\", 3, actual.size());\n    for (int i = 0; i < expected.size(); i += 1) {\n      TestHelpers.assertEqualsSafe(entriesTable.schema().asStruct(), expected.get(i), actual.get(i));\n    }\n  }\n","date":"2020-05-24 03:10:42","endLine":194,"groupId":"3984","id":3,"instanceNumber":1,"isCurCommit":0,"methodName":"testAllEntriesTable","params":"()","path":"/mnt/clonedata/CloneManagementServer/ManagementServer/consistResult/result-iceberg-10-0.7/blobInfo/CC_OUT/blobs/a0/206113de23d9910ae9c2a310650285c4f49912.src","preCode":"  public void testAllEntriesTable() throws Exception {\n    TableIdentifier tableIdentifier = TableIdentifier.of(\"db\", \"entries_test\");\n    Table table = createTable(tableIdentifier, SCHEMA, PartitionSpec.unpartitioned());\n    Table entriesTable = loadTable(tableIdentifier, \"all_entries\");\n\n    Dataset<Row> df1 = spark.createDataFrame(Lists.newArrayList(new SimpleRecord(1, \"a\")), SimpleRecord.class);\n    Dataset<Row> df2 = spark.createDataFrame(Lists.newArrayList(new SimpleRecord(1, \"b\")), SimpleRecord.class);\n\n    df1.select(\"id\", \"data\").write()\n        .format(\"iceberg\")\n        .mode(\"append\")\n        .save(loadLocation(tableIdentifier));\n\n    \r\n    table.newDelete().deleteFromRowFilter(Expressions.equal(\"id\", 1)).commit();\n\n    \r\n    df2.select(\"id\", \"data\").write()\n        .format(\"iceberg\")\n        .mode(\"append\")\n        .save(loadLocation(tableIdentifier));\n\n    \r\n    table.refresh();\n\n    List<Row> actual = spark.read()\n        .format(\"iceberg\")\n        .load(loadLocation(tableIdentifier, \"all_entries\"))\n        .orderBy(\"snapshot_id\")\n        .collectAsList();\n\n    List<GenericData.Record> expected = Lists.newArrayList();\n    for (ManifestFile manifest : Iterables.concat(Iterables.transform(table.snapshots(), Snapshot::manifests))) {\n      InputFile in = table.io().newInputFile(manifest.path());\n      try (CloseableIterable<GenericData.Record> rows = Avro.read(in).project(entriesTable.schema()).build()) {\n        for (GenericData.Record record : rows) {\n          expected.add(record);\n        }\n      }\n    }\n\n    expected.sort(Comparator.comparing(o -> (Long) o.get(\"snapshot_id\")));\n\n    Assert.assertEquals(\"Entries table should have 3 rows\", 3, expected.size());\n    Assert.assertEquals(\"Actual results should have 3 rows\", 3, actual.size());\n    for (int i = 0; i < expected.size(); i += 1) {\n      TestHelpers.assertEqualsSafe(entriesTable.schema().asStruct(), expected.get(i), actual.get(i));\n    }\n  }\n","realPath":"spark/src/test/java/org/apache/iceberg/spark/source/TestIcebergSourceTablesBase.java","repoName":"iceberg","snippetEndLine":0,"snippetStartLine":0,"startLine":142,"status":"M"},{"authorDate":"2020-05-24 03:10:42","commitOrder":2,"curCode":"  public void testAllDataFilesTable() throws Exception {\n    TableIdentifier tableIdentifier = TableIdentifier.of(\"db\", \"files_test\");\n    Table table = createTable(tableIdentifier, SCHEMA, PartitionSpec.builderFor(SCHEMA).identity(\"id\").build());\n    Table entriesTable = loadTable(tableIdentifier, \"entries\");\n    Table filesTable = loadTable(tableIdentifier, \"all_data_files\");\n\n    Dataset<Row> df1 = spark.createDataFrame(Lists.newArrayList(new SimpleRecord(1, \"a\")), SimpleRecord.class);\n    Dataset<Row> df2 = spark.createDataFrame(Lists.newArrayList(new SimpleRecord(2, \"b\")), SimpleRecord.class);\n\n    df1.select(\"id\", \"data\").write()\n        .format(\"iceberg\")\n        .mode(\"append\")\n        .save(loadLocation(tableIdentifier));\n\n    \r\n    table.newDelete().deleteFromRowFilter(Expressions.equal(\"id\", 1)).commit();\n\n    \r\n    df2.select(\"id\", \"data\").write()\n        .format(\"iceberg\")\n        .mode(\"append\")\n        .save(loadLocation(tableIdentifier));\n\n    \r\n    table.refresh();\n\n    List<Row> actual = spark.read()\n        .format(\"iceberg\")\n        .load(loadLocation(tableIdentifier, \"all_data_files\"))\n        .orderBy(\"file_path\")\n        .collectAsList();\n    actual.sort(Comparator.comparing(o -> o.getString(1)));\n\n    List<GenericData.Record> expected = Lists.newArrayList();\n    for (ManifestFile manifest : Iterables.concat(Iterables.transform(table.snapshots(), Snapshot::manifests))) {\n      InputFile in = table.io().newInputFile(manifest.path());\n      try (CloseableIterable<GenericData.Record> rows = Avro.read(in).project(entriesTable.schema()).build()) {\n        for (GenericData.Record record : rows) {\n          if ((Integer) record.get(\"status\") < 2 ) {\n            GenericData.Record file = (GenericData.Record) record.get(\"data_file\");\n            file.put(0, FileContent.DATA.id());\n            expected.add(file);\n          }\n        }\n      }\n    }\n\n    expected.sort(Comparator.comparing(o -> o.get(\"file_path\").toString()));\n\n    Assert.assertEquals(\"Files table should have two rows\", 2, expected.size());\n    Assert.assertEquals(\"Actual results should have two rows\", 2, actual.size());\n    for (int i = 0; i < expected.size(); i += 1) {\n      TestHelpers.assertEqualsSafe(filesTable.schema().asStruct(), expected.get(i), actual.get(i));\n    }\n  }\n","date":"2020-05-24 03:10:42","endLine":496,"groupId":"3989","id":4,"instanceNumber":2,"isCurCommit":0,"methodName":"testAllDataFilesTable","params":"()","path":"/mnt/clonedata/CloneManagementServer/ManagementServer/consistResult/result-iceberg-10-0.7/blobInfo/CC_OUT/blobs/a0/206113de23d9910ae9c2a310650285c4f49912.src","preCode":"  public void testAllDataFilesTable() throws Exception {\n    TableIdentifier tableIdentifier = TableIdentifier.of(\"db\", \"files_test\");\n    Table table = createTable(tableIdentifier, SCHEMA, PartitionSpec.builderFor(SCHEMA).identity(\"id\").build());\n    Table entriesTable = loadTable(tableIdentifier, \"entries\");\n    Table filesTable = loadTable(tableIdentifier, \"all_data_files\");\n\n    Dataset<Row> df1 = spark.createDataFrame(Lists.newArrayList(new SimpleRecord(1, \"a\")), SimpleRecord.class);\n    Dataset<Row> df2 = spark.createDataFrame(Lists.newArrayList(new SimpleRecord(2, \"b\")), SimpleRecord.class);\n\n    df1.select(\"id\", \"data\").write()\n        .format(\"iceberg\")\n        .mode(\"append\")\n        .save(loadLocation(tableIdentifier));\n\n    \r\n    table.newDelete().deleteFromRowFilter(Expressions.equal(\"id\", 1)).commit();\n\n    \r\n    df2.select(\"id\", \"data\").write()\n        .format(\"iceberg\")\n        .mode(\"append\")\n        .save(loadLocation(tableIdentifier));\n\n    \r\n    table.refresh();\n\n    List<Row> actual = spark.read()\n        .format(\"iceberg\")\n        .load(loadLocation(tableIdentifier, \"all_data_files\"))\n        .orderBy(\"file_path\")\n        .collectAsList();\n    actual.sort(Comparator.comparing(o -> o.getString(0)));\n\n    List<GenericData.Record> expected = Lists.newArrayList();\n    for (ManifestFile manifest : Iterables.concat(Iterables.transform(table.snapshots(), Snapshot::manifests))) {\n      InputFile in = table.io().newInputFile(manifest.path());\n      try (CloseableIterable<GenericData.Record> rows = Avro.read(in).project(entriesTable.schema()).build()) {\n        for (GenericData.Record record : rows) {\n          if ((Integer) record.get(\"status\") < 2 ) {\n            expected.add((GenericData.Record) record.get(\"data_file\"));\n          }\n        }\n      }\n    }\n\n    expected.sort(Comparator.comparing(o -> o.get(\"file_path\").toString()));\n\n    Assert.assertEquals(\"Files table should have two rows\", 2, expected.size());\n    Assert.assertEquals(\"Actual results should have two rows\", 2, actual.size());\n    for (int i = 0; i < expected.size(); i += 1) {\n      TestHelpers.assertEqualsSafe(filesTable.schema().asStruct(), expected.get(i), actual.get(i));\n    }\n  }\n","realPath":"spark/src/test/java/org/apache/iceberg/spark/source/TestIcebergSourceTablesBase.java","repoName":"iceberg","snippetEndLine":0,"snippetStartLine":0,"startLine":442,"status":"M"}],"commitId":"01d1462756db20a14a9ac67166e5bf56966861b4","commitMessage":"@@@Add content types to DataFile and ManifestFile (#1030)\n\n","date":"2020-05-24 03:10:42","modifiedFileCount":"14","status":"M","submitter":"Ryan Blue"},{"authorTime":"2020-06-03 03:43:03","codes":[{"authorDate":"2020-06-03 03:43:03","commitOrder":3,"curCode":"  public void testAllEntriesTable() throws Exception {\n    TableIdentifier tableIdentifier = TableIdentifier.of(\"db\", \"entries_test\");\n    Table table = createTable(tableIdentifier, SCHEMA, PartitionSpec.unpartitioned());\n    Table entriesTable = loadTable(tableIdentifier, \"all_entries\");\n\n    Dataset<Row> df1 = spark.createDataFrame(Lists.newArrayList(new SimpleRecord(1, \"a\")), SimpleRecord.class);\n    Dataset<Row> df2 = spark.createDataFrame(Lists.newArrayList(new SimpleRecord(1, \"b\")), SimpleRecord.class);\n\n    df1.select(\"id\", \"data\").write()\n        .format(\"iceberg\")\n        .mode(\"append\")\n        .save(loadLocation(tableIdentifier));\n\n    \r\n    table.newDelete().deleteFromRowFilter(Expressions.equal(\"id\", 1)).commit();\n\n    \r\n    df2.select(\"id\", \"data\").write()\n        .format(\"iceberg\")\n        .mode(\"append\")\n        .save(loadLocation(tableIdentifier));\n\n    \r\n    table.refresh();\n\n    List<Row> actual = spark.read()\n        .format(\"iceberg\")\n        .load(loadLocation(tableIdentifier, \"all_entries\"))\n        .orderBy(\"snapshot_id\")\n        .collectAsList();\n\n    List<GenericData.Record> expected = Lists.newArrayList();\n    for (ManifestFile manifest : Iterables.concat(Iterables.transform(table.snapshots(), Snapshot::allManifests))) {\n      InputFile in = table.io().newInputFile(manifest.path());\n      try (CloseableIterable<GenericData.Record> rows = Avro.read(in).project(entriesTable.schema()).build()) {\n        \r\n        rows.forEach(row -> {\n          row.put(2, 0L);\n          GenericData.Record file = (GenericData.Record) row.get(\"data_file\");\n          file.put(0, FileContent.DATA.id());\n          expected.add(row);\n        });\n      }\n    }\n\n    expected.sort(Comparator.comparing(o -> (Long) o.get(\"snapshot_id\")));\n\n    Assert.assertEquals(\"Entries table should have 3 rows\", 3, expected.size());\n    Assert.assertEquals(\"Actual results should have 3 rows\", 3, actual.size());\n    for (int i = 0; i < expected.size(); i += 1) {\n      TestHelpers.assertEqualsSafe(entriesTable.schema().asStruct(), expected.get(i), actual.get(i));\n    }\n  }\n","date":"2020-06-03 03:43:03","endLine":194,"groupId":"10336","id":5,"instanceNumber":1,"isCurCommit":0,"methodName":"testAllEntriesTable","params":"()","path":"/mnt/clonedata/CloneManagementServer/ManagementServer/consistResult/result-iceberg-10-0.7/blobInfo/CC_OUT/blobs/e1/7836c12443771cd658898d4f5583b5612e8d99.src","preCode":"  public void testAllEntriesTable() throws Exception {\n    TableIdentifier tableIdentifier = TableIdentifier.of(\"db\", \"entries_test\");\n    Table table = createTable(tableIdentifier, SCHEMA, PartitionSpec.unpartitioned());\n    Table entriesTable = loadTable(tableIdentifier, \"all_entries\");\n\n    Dataset<Row> df1 = spark.createDataFrame(Lists.newArrayList(new SimpleRecord(1, \"a\")), SimpleRecord.class);\n    Dataset<Row> df2 = spark.createDataFrame(Lists.newArrayList(new SimpleRecord(1, \"b\")), SimpleRecord.class);\n\n    df1.select(\"id\", \"data\").write()\n        .format(\"iceberg\")\n        .mode(\"append\")\n        .save(loadLocation(tableIdentifier));\n\n    \r\n    table.newDelete().deleteFromRowFilter(Expressions.equal(\"id\", 1)).commit();\n\n    \r\n    df2.select(\"id\", \"data\").write()\n        .format(\"iceberg\")\n        .mode(\"append\")\n        .save(loadLocation(tableIdentifier));\n\n    \r\n    table.refresh();\n\n    List<Row> actual = spark.read()\n        .format(\"iceberg\")\n        .load(loadLocation(tableIdentifier, \"all_entries\"))\n        .orderBy(\"snapshot_id\")\n        .collectAsList();\n\n    List<GenericData.Record> expected = Lists.newArrayList();\n    for (ManifestFile manifest : Iterables.concat(Iterables.transform(table.snapshots(), Snapshot::manifests))) {\n      InputFile in = table.io().newInputFile(manifest.path());\n      try (CloseableIterable<GenericData.Record> rows = Avro.read(in).project(entriesTable.schema()).build()) {\n        \r\n        rows.forEach(row -> {\n          row.put(2, 0L);\n          GenericData.Record file = (GenericData.Record) row.get(\"data_file\");\n          file.put(0, FileContent.DATA.id());\n          expected.add(row);\n        });\n      }\n    }\n\n    expected.sort(Comparator.comparing(o -> (Long) o.get(\"snapshot_id\")));\n\n    Assert.assertEquals(\"Entries table should have 3 rows\", 3, expected.size());\n    Assert.assertEquals(\"Actual results should have 3 rows\", 3, actual.size());\n    for (int i = 0; i < expected.size(); i += 1) {\n      TestHelpers.assertEqualsSafe(entriesTable.schema().asStruct(), expected.get(i), actual.get(i));\n    }\n  }\n","realPath":"spark/src/test/java/org/apache/iceberg/spark/source/TestIcebergSourceTablesBase.java","repoName":"iceberg","snippetEndLine":0,"snippetStartLine":0,"startLine":142,"status":"M"},{"authorDate":"2020-06-03 03:43:03","commitOrder":3,"curCode":"  public void testAllDataFilesTable() throws Exception {\n    TableIdentifier tableIdentifier = TableIdentifier.of(\"db\", \"files_test\");\n    Table table = createTable(tableIdentifier, SCHEMA, PartitionSpec.builderFor(SCHEMA).identity(\"id\").build());\n    Table entriesTable = loadTable(tableIdentifier, \"entries\");\n    Table filesTable = loadTable(tableIdentifier, \"all_data_files\");\n\n    Dataset<Row> df1 = spark.createDataFrame(Lists.newArrayList(new SimpleRecord(1, \"a\")), SimpleRecord.class);\n    Dataset<Row> df2 = spark.createDataFrame(Lists.newArrayList(new SimpleRecord(2, \"b\")), SimpleRecord.class);\n\n    df1.select(\"id\", \"data\").write()\n        .format(\"iceberg\")\n        .mode(\"append\")\n        .save(loadLocation(tableIdentifier));\n\n    \r\n    table.newDelete().deleteFromRowFilter(Expressions.equal(\"id\", 1)).commit();\n\n    \r\n    df2.select(\"id\", \"data\").write()\n        .format(\"iceberg\")\n        .mode(\"append\")\n        .save(loadLocation(tableIdentifier));\n\n    \r\n    table.refresh();\n\n    List<Row> actual = spark.read()\n        .format(\"iceberg\")\n        .load(loadLocation(tableIdentifier, \"all_data_files\"))\n        .orderBy(\"file_path\")\n        .collectAsList();\n    actual.sort(Comparator.comparing(o -> o.getString(1)));\n\n    List<GenericData.Record> expected = Lists.newArrayList();\n    for (ManifestFile manifest : Iterables.concat(Iterables.transform(table.snapshots(), Snapshot::dataManifests))) {\n      InputFile in = table.io().newInputFile(manifest.path());\n      try (CloseableIterable<GenericData.Record> rows = Avro.read(in).project(entriesTable.schema()).build()) {\n        for (GenericData.Record record : rows) {\n          if ((Integer) record.get(\"status\") < 2 ) {\n            GenericData.Record file = (GenericData.Record) record.get(\"data_file\");\n            file.put(0, FileContent.DATA.id());\n            expected.add(file);\n          }\n        }\n      }\n    }\n\n    expected.sort(Comparator.comparing(o -> o.get(\"file_path\").toString()));\n\n    Assert.assertEquals(\"Files table should have two rows\", 2, expected.size());\n    Assert.assertEquals(\"Actual results should have two rows\", 2, actual.size());\n    for (int i = 0; i < expected.size(); i += 1) {\n      TestHelpers.assertEqualsSafe(filesTable.schema().asStruct(), expected.get(i), actual.get(i));\n    }\n  }\n","date":"2020-06-03 03:43:03","endLine":496,"groupId":"10336","id":6,"instanceNumber":2,"isCurCommit":0,"methodName":"testAllDataFilesTable","params":"()","path":"/mnt/clonedata/CloneManagementServer/ManagementServer/consistResult/result-iceberg-10-0.7/blobInfo/CC_OUT/blobs/e1/7836c12443771cd658898d4f5583b5612e8d99.src","preCode":"  public void testAllDataFilesTable() throws Exception {\n    TableIdentifier tableIdentifier = TableIdentifier.of(\"db\", \"files_test\");\n    Table table = createTable(tableIdentifier, SCHEMA, PartitionSpec.builderFor(SCHEMA).identity(\"id\").build());\n    Table entriesTable = loadTable(tableIdentifier, \"entries\");\n    Table filesTable = loadTable(tableIdentifier, \"all_data_files\");\n\n    Dataset<Row> df1 = spark.createDataFrame(Lists.newArrayList(new SimpleRecord(1, \"a\")), SimpleRecord.class);\n    Dataset<Row> df2 = spark.createDataFrame(Lists.newArrayList(new SimpleRecord(2, \"b\")), SimpleRecord.class);\n\n    df1.select(\"id\", \"data\").write()\n        .format(\"iceberg\")\n        .mode(\"append\")\n        .save(loadLocation(tableIdentifier));\n\n    \r\n    table.newDelete().deleteFromRowFilter(Expressions.equal(\"id\", 1)).commit();\n\n    \r\n    df2.select(\"id\", \"data\").write()\n        .format(\"iceberg\")\n        .mode(\"append\")\n        .save(loadLocation(tableIdentifier));\n\n    \r\n    table.refresh();\n\n    List<Row> actual = spark.read()\n        .format(\"iceberg\")\n        .load(loadLocation(tableIdentifier, \"all_data_files\"))\n        .orderBy(\"file_path\")\n        .collectAsList();\n    actual.sort(Comparator.comparing(o -> o.getString(1)));\n\n    List<GenericData.Record> expected = Lists.newArrayList();\n    for (ManifestFile manifest : Iterables.concat(Iterables.transform(table.snapshots(), Snapshot::manifests))) {\n      InputFile in = table.io().newInputFile(manifest.path());\n      try (CloseableIterable<GenericData.Record> rows = Avro.read(in).project(entriesTable.schema()).build()) {\n        for (GenericData.Record record : rows) {\n          if ((Integer) record.get(\"status\") < 2 ) {\n            GenericData.Record file = (GenericData.Record) record.get(\"data_file\");\n            file.put(0, FileContent.DATA.id());\n            expected.add(file);\n          }\n        }\n      }\n    }\n\n    expected.sort(Comparator.comparing(o -> o.get(\"file_path\").toString()));\n\n    Assert.assertEquals(\"Files table should have two rows\", 2, expected.size());\n    Assert.assertEquals(\"Actual results should have two rows\", 2, actual.size());\n    for (int i = 0; i < expected.size(); i += 1) {\n      TestHelpers.assertEqualsSafe(filesTable.schema().asStruct(), expected.get(i), actual.get(i));\n    }\n  }\n","realPath":"spark/src/test/java/org/apache/iceberg/spark/source/TestIcebergSourceTablesBase.java","repoName":"iceberg","snippetEndLine":0,"snippetStartLine":0,"startLine":442,"status":"M"}],"commitId":"482f420141486f5b7b02bd62dc14cdb94b9a9cca","commitMessage":"@@@Split Snapshot.manifests into dataManifests and deleteManifests (#1080)\n\nThis replaces all calls to Snapshot.manifests with calls to one of 3 new methods:\n\n* `Snapshot.allManifests` returns both delete and data manifests\n* `Snapshot.deleteManifests` returns only delete manifests\n* `Snapshot.dataManifests` returns only data manifests\n\nExisting references mostly use either `allManifests` or `dataManifests`.  depending on the context. For example.  tests with assertions for the number of manifests use `allManifests` because the test cases should validate there are no new delete manifests.  but other tests that validate rewritten manifests are deleted use `dataManifests` because only data manifests are rewritten and deleted.\n\nThis tries to make minimal changes that preserve the current behavior. Operations are not updated to support delete manifests (rewrite still only rewrites data manifests).  but will carry through the list of delete manifests correctly.","date":"2020-06-03 03:43:03","modifiedFileCount":"46","status":"M","submitter":"Ryan Blue"}]
