[{"authorTime":"2020-05-26 01:15:35","codes":[{"authorDate":"2019-12-28 05:07:53","commitOrder":5,"curCode":"  public static void createInputFile() throws IOException {\n    if (PARQUET_FILE.exists()) {\n      Assert.assertTrue(PARQUET_FILE.delete());\n    }\n\n    \r\n    org.apache.avro.Schema structSchema = AvroSchemaUtil.convert(_structFieldType);\n\n    OutputFile outFile = Files.localOutput(PARQUET_FILE);\n    try (FileAppender<Record> appender = Parquet.write(outFile)\n        .schema(FILE_SCHEMA)\n        .build()) {\n      GenericRecordBuilder builder = new GenericRecordBuilder(convert(FILE_SCHEMA, \"table\"));\n      \r\n      for (int copy = 0; copy < 20; copy += 1) {\n        \r\n        for (int i = 0; i < INT_MAX_VALUE - INT_MIN_VALUE + 1; i += 1) {\n          builder.set(\"_id\", INT_MIN_VALUE + i); \r\n          builder.set(\"_no_stats\", TOO_LONG_FOR_STATS); \r\n          builder.set(\"_required\", \"req\"); \r\n          builder.set(\"_all_nulls\", null); \r\n          builder.set(\"_some_nulls\", (i % 10 == 0) ? null : \"some\"); \r\n          builder.set(\"_no_nulls\", \"\"); \r\n          builder.set(\"_non_dict\", UUID.randomUUID().toString()); \r\n\n          Record structNotNull = new Record(structSchema);\n          structNotNull.put(\"_int_field\", INT_MIN_VALUE + i);\n          builder.set(\"_struct_not_null\", structNotNull); \r\n\n          appender.add(builder.build());\n        }\n      }\n    }\n\n    InputFile inFile = Files.localInput(PARQUET_FILE);\n\n    ParquetFileReader reader = ParquetFileReader.open(ParquetIO.file(inFile));\n\n    Assert.assertEquals(\"Should create only one row group\", 1, reader.getRowGroups().size());\n    rowGroupMetadata = reader.getRowGroups().get(0);\n    parquetSchema = reader.getFileMetaData().getSchema();\n    dictionaryStore = reader.getNextDictionaryReader();\n\n    PARQUET_FILE.deleteOnExit();\n  }\n","date":"2019-12-28 05:07:53","endLine":163,"groupId":"628","id":1,"instanceNumber":1,"isCurCommit":0,"methodName":"createInputFile","params":"()","path":"/mnt/clonedata/CloneManagementServer/ManagementServer/consistResult/result-iceberg-10-0.7/blobInfo/CC_OUT/blobs/f6/ab6438afdd384189a76a151d96761ed5b717a3.src","preCode":"  public static void createInputFile() throws IOException {\n    if (PARQUET_FILE.exists()) {\n      Assert.assertTrue(PARQUET_FILE.delete());\n    }\n\n    \r\n    org.apache.avro.Schema structSchema = AvroSchemaUtil.convert(_structFieldType);\n\n    OutputFile outFile = Files.localOutput(PARQUET_FILE);\n    try (FileAppender<Record> appender = Parquet.write(outFile)\n        .schema(FILE_SCHEMA)\n        .build()) {\n      GenericRecordBuilder builder = new GenericRecordBuilder(convert(FILE_SCHEMA, \"table\"));\n      \r\n      for (int copy = 0; copy < 20; copy += 1) {\n        \r\n        for (int i = 0; i < INT_MAX_VALUE - INT_MIN_VALUE + 1; i += 1) {\n          builder.set(\"_id\", INT_MIN_VALUE + i); \r\n          builder.set(\"_no_stats\", TOO_LONG_FOR_STATS); \r\n          builder.set(\"_required\", \"req\"); \r\n          builder.set(\"_all_nulls\", null); \r\n          builder.set(\"_some_nulls\", (i % 10 == 0) ? null : \"some\"); \r\n          builder.set(\"_no_nulls\", \"\"); \r\n          builder.set(\"_non_dict\", UUID.randomUUID().toString()); \r\n\n          Record structNotNull = new Record(structSchema);\n          structNotNull.put(\"_int_field\", INT_MIN_VALUE + i);\n          builder.set(\"_struct_not_null\", structNotNull); \r\n\n          appender.add(builder.build());\n        }\n      }\n    }\n\n    InputFile inFile = Files.localInput(PARQUET_FILE);\n\n    ParquetFileReader reader = ParquetFileReader.open(ParquetIO.file(inFile));\n\n    Assert.assertEquals(\"Should create only one row group\", 1, reader.getRowGroups().size());\n    rowGroupMetadata = reader.getRowGroups().get(0);\n    parquetSchema = reader.getFileMetaData().getSchema();\n    dictionaryStore = reader.getNextDictionaryReader();\n\n    PARQUET_FILE.deleteOnExit();\n  }\n","realPath":"parquet/src/test/java/org/apache/iceberg/parquet/TestDictionaryRowGroupFilter.java","repoName":"iceberg","snippetEndLine":0,"snippetStartLine":0,"startLine":119,"status":"NB"},{"authorDate":"2020-05-26 01:15:35","commitOrder":5,"curCode":"  public void createParquetInputFile(List<Record> records) throws IOException {\n    if (PARQUET_FILE.exists()) {\n      Assert.assertTrue(PARQUET_FILE.delete());\n    }\n\n    OutputFile outFile = Files.localOutput(PARQUET_FILE);\n    try (FileAppender<Record> appender = Parquet.write(outFile)\n        .schema(FILE_SCHEMA)\n        .createWriterFunc(GenericParquetWriter::buildWriter)\n        .build()) {\n      appender.addAll(records);\n    }\n\n    InputFile inFile = Files.localInput(PARQUET_FILE);\n    try (ParquetFileReader reader = ParquetFileReader.open(parquetInputFile(inFile))) {\n      Assert.assertEquals(\"Should create only one row group\", 1, reader.getRowGroups().size());\n      rowGroupMetadata = reader.getRowGroups().get(0);\n      parquetSchema = reader.getFileMetaData().getSchema();\n    }\n\n    PARQUET_FILE.deleteOnExit();\n  }\n","date":"2020-05-26 01:15:35","endLine":218,"groupId":"3302","id":2,"instanceNumber":2,"isCurCommit":0,"methodName":"createParquetInputFile","params":"(List<Record>records)","path":"/mnt/clonedata/CloneManagementServer/ManagementServer/consistResult/result-iceberg-10-0.7/blobInfo/CC_OUT/blobs/86/47a9c7dee02b6ce46ecee97a74de7421cd16f4.src","preCode":"  public void createParquetInputFile(List<Record> records) throws IOException {\n    if (PARQUET_FILE.exists()) {\n      Assert.assertTrue(PARQUET_FILE.delete());\n    }\n\n    OutputFile outFile = Files.localOutput(PARQUET_FILE);\n    try (FileAppender<Record> appender = Parquet.write(outFile)\n        .schema(FILE_SCHEMA)\n        .createWriterFunc(GenericParquetWriter::buildWriter)\n        .build()) {\n      appender.addAll(records);\n    }\n\n    InputFile inFile = Files.localInput(PARQUET_FILE);\n    try (ParquetFileReader reader = ParquetFileReader.open(parquetInputFile(inFile))) {\n      Assert.assertEquals(\"Should create only one row group\", 1, reader.getRowGroups().size());\n      rowGroupMetadata = reader.getRowGroups().get(0);\n      parquetSchema = reader.getFileMetaData().getSchema();\n    }\n\n    PARQUET_FILE.deleteOnExit();\n  }\n","realPath":"data/src/test/java/org/apache/iceberg/data/TestMetricsRowGroupFilterTypes.java","repoName":"iceberg","snippetEndLine":0,"snippetStartLine":0,"startLine":197,"status":"B"}],"commitId":"e8f7379ffe253623e9dd27a1ada7d4421af8b937","commitMessage":"@@@ORC: Push down Iceberg filters (#973)\n\n","date":"2020-05-26 01:15:35","modifiedFileCount":"6","status":"M","submitter":"Shardul Mahadik"},{"authorTime":"2020-05-26 01:15:35","codes":[{"authorDate":"2020-10-09 23:46:12","commitOrder":6,"curCode":"  public void createInputFile() throws IOException {\n    File parquetFile = temp.newFile();\n    Assert.assertTrue(parquetFile.delete());\n\n    \r\n    org.apache.avro.Schema structSchema = AvroSchemaUtil.convert(_structFieldType);\n\n    OutputFile outFile = Files.localOutput(parquetFile);\n    try (FileAppender<Record> appender = Parquet.write(outFile)\n        .schema(FILE_SCHEMA)\n        .build()) {\n      GenericRecordBuilder builder = new GenericRecordBuilder(convert(FILE_SCHEMA, \"table\"));\n      \r\n      for (int copy = 0; copy < 20; copy += 1) {\n        \r\n        for (int i = 0; i < INT_MAX_VALUE - INT_MIN_VALUE + 1; i += 1) {\n          builder.set(\"_id\", INT_MIN_VALUE + i); \r\n          builder.set(\"_no_stats\", TOO_LONG_FOR_STATS); \r\n          builder.set(\"_required\", \"req\"); \r\n          builder.set(\"_all_nulls\", null); \r\n          builder.set(\"_some_nulls\", (i % 10 == 0) ? null : \"some\"); \r\n          builder.set(\"_no_nulls\", \"\"); \r\n          builder.set(\"_non_dict\", UUID.randomUUID().toString()); \r\n\n          Record structNotNull = new Record(structSchema);\n          structNotNull.put(\"_int_field\", INT_MIN_VALUE + i);\n          builder.set(\"_struct_not_null\", structNotNull); \r\n\n          appender.add(builder.build());\n        }\n      }\n    }\n\n    InputFile inFile = Files.localInput(parquetFile);\n\n    ParquetFileReader reader = ParquetFileReader.open(ParquetIO.file(inFile));\n\n    Assert.assertEquals(\"Should create only one row group\", 1, reader.getRowGroups().size());\n    rowGroupMetadata = reader.getRowGroups().get(0);\n    parquetSchema = reader.getFileMetaData().getSchema();\n    dictionaryStore = reader.getNextDictionaryReader();\n  }\n","date":"2020-10-09 23:46:12","endLine":164,"groupId":"628","id":3,"instanceNumber":1,"isCurCommit":0,"methodName":"createInputFile","params":"()","path":"/mnt/clonedata/CloneManagementServer/ManagementServer/consistResult/result-iceberg-10-0.7/blobInfo/CC_OUT/blobs/89/4a8e46949f01328e6567d0554b5331e0f2c893.src","preCode":"  public static void createInputFile() throws IOException {\n    if (PARQUET_FILE.exists()) {\n      Assert.assertTrue(PARQUET_FILE.delete());\n    }\n\n    \r\n    org.apache.avro.Schema structSchema = AvroSchemaUtil.convert(_structFieldType);\n\n    OutputFile outFile = Files.localOutput(PARQUET_FILE);\n    try (FileAppender<Record> appender = Parquet.write(outFile)\n        .schema(FILE_SCHEMA)\n        .build()) {\n      GenericRecordBuilder builder = new GenericRecordBuilder(convert(FILE_SCHEMA, \"table\"));\n      \r\n      for (int copy = 0; copy < 20; copy += 1) {\n        \r\n        for (int i = 0; i < INT_MAX_VALUE - INT_MIN_VALUE + 1; i += 1) {\n          builder.set(\"_id\", INT_MIN_VALUE + i); \r\n          builder.set(\"_no_stats\", TOO_LONG_FOR_STATS); \r\n          builder.set(\"_required\", \"req\"); \r\n          builder.set(\"_all_nulls\", null); \r\n          builder.set(\"_some_nulls\", (i % 10 == 0) ? null : \"some\"); \r\n          builder.set(\"_no_nulls\", \"\"); \r\n          builder.set(\"_non_dict\", UUID.randomUUID().toString()); \r\n\n          Record structNotNull = new Record(structSchema);\n          structNotNull.put(\"_int_field\", INT_MIN_VALUE + i);\n          builder.set(\"_struct_not_null\", structNotNull); \r\n\n          appender.add(builder.build());\n        }\n      }\n    }\n\n    InputFile inFile = Files.localInput(PARQUET_FILE);\n\n    ParquetFileReader reader = ParquetFileReader.open(ParquetIO.file(inFile));\n\n    Assert.assertEquals(\"Should create only one row group\", 1, reader.getRowGroups().size());\n    rowGroupMetadata = reader.getRowGroups().get(0);\n    parquetSchema = reader.getFileMetaData().getSchema();\n    dictionaryStore = reader.getNextDictionaryReader();\n\n    PARQUET_FILE.deleteOnExit();\n  }\n","realPath":"parquet/src/test/java/org/apache/iceberg/parquet/TestDictionaryRowGroupFilter.java","repoName":"iceberg","snippetEndLine":0,"snippetStartLine":0,"startLine":123,"status":"M"},{"authorDate":"2020-05-26 01:15:35","commitOrder":6,"curCode":"  public void createParquetInputFile(List<Record> records) throws IOException {\n    if (PARQUET_FILE.exists()) {\n      Assert.assertTrue(PARQUET_FILE.delete());\n    }\n\n    OutputFile outFile = Files.localOutput(PARQUET_FILE);\n    try (FileAppender<Record> appender = Parquet.write(outFile)\n        .schema(FILE_SCHEMA)\n        .createWriterFunc(GenericParquetWriter::buildWriter)\n        .build()) {\n      appender.addAll(records);\n    }\n\n    InputFile inFile = Files.localInput(PARQUET_FILE);\n    try (ParquetFileReader reader = ParquetFileReader.open(parquetInputFile(inFile))) {\n      Assert.assertEquals(\"Should create only one row group\", 1, reader.getRowGroups().size());\n      rowGroupMetadata = reader.getRowGroups().get(0);\n      parquetSchema = reader.getFileMetaData().getSchema();\n    }\n\n    PARQUET_FILE.deleteOnExit();\n  }\n","date":"2020-05-26 01:15:35","endLine":218,"groupId":"3302","id":4,"instanceNumber":2,"isCurCommit":0,"methodName":"createParquetInputFile","params":"(List<Record>records)","path":"/mnt/clonedata/CloneManagementServer/ManagementServer/consistResult/result-iceberg-10-0.7/blobInfo/CC_OUT/blobs/86/47a9c7dee02b6ce46ecee97a74de7421cd16f4.src","preCode":"  public void createParquetInputFile(List<Record> records) throws IOException {\n    if (PARQUET_FILE.exists()) {\n      Assert.assertTrue(PARQUET_FILE.delete());\n    }\n\n    OutputFile outFile = Files.localOutput(PARQUET_FILE);\n    try (FileAppender<Record> appender = Parquet.write(outFile)\n        .schema(FILE_SCHEMA)\n        .createWriterFunc(GenericParquetWriter::buildWriter)\n        .build()) {\n      appender.addAll(records);\n    }\n\n    InputFile inFile = Files.localInput(PARQUET_FILE);\n    try (ParquetFileReader reader = ParquetFileReader.open(parquetInputFile(inFile))) {\n      Assert.assertEquals(\"Should create only one row group\", 1, reader.getRowGroups().size());\n      rowGroupMetadata = reader.getRowGroups().get(0);\n      parquetSchema = reader.getFileMetaData().getSchema();\n    }\n\n    PARQUET_FILE.deleteOnExit();\n  }\n","realPath":"data/src/test/java/org/apache/iceberg/data/TestMetricsRowGroupFilterTypes.java","repoName":"iceberg","snippetEndLine":0,"snippetStartLine":0,"startLine":197,"status":"N"}],"commitId":"29c245471b9b9244e944953a2d6e556a347573ad","commitMessage":"@@@Parquet: Remove hard-coded file paths from tests (#1562)\n\n* Remove hard-coded file paths from tests.\n\n* Fix checkstyle in tests.","date":"2020-10-09 23:46:12","modifiedFileCount":"2","status":"M","submitter":"Ryan Blue"},{"authorTime":"2020-05-26 01:15:35","codes":[{"authorDate":"2020-12-06 09:14:27","commitOrder":7,"curCode":"  public void createInputFile() throws IOException {\n    File parquetFile = temp.newFile();\n    Assert.assertTrue(parquetFile.delete());\n\n    \r\n    org.apache.avro.Schema structSchema = AvroSchemaUtil.convert(_structFieldType);\n\n    OutputFile outFile = Files.localOutput(parquetFile);\n    try (FileAppender<Record> appender = Parquet.write(outFile)\n        .schema(FILE_SCHEMA)\n        .build()) {\n      GenericRecordBuilder builder = new GenericRecordBuilder(convert(FILE_SCHEMA, \"table\"));\n      \r\n      for (int copy = 0; copy < 20; copy += 1) {\n        \r\n        for (int i = 0; i < INT_MAX_VALUE - INT_MIN_VALUE + 1; i += 1) {\n          builder.set(\"_id\", INT_MIN_VALUE + i); \r\n          builder.set(\"_no_stats\", TOO_LONG_FOR_STATS); \r\n          builder.set(\"_required\", \"req\"); \r\n          builder.set(\"_all_nulls\", null); \r\n          builder.set(\"_some_nulls\", (i % 10 == 0) ? null : \"some\"); \r\n          builder.set(\"_no_nulls\", \"\"); \r\n          builder.set(\"_non_dict\", UUID.randomUUID().toString()); \r\n          builder.set(\"_all_nans\", Double.NaN); \r\n          builder.set(\"_some_nans\", (i % 10 == 0) ? Float.NaN : 2F); \r\n          builder.set(\"_no_nans\", 3D); \r\n\n          Record structNotNull = new Record(structSchema);\n          structNotNull.put(\"_int_field\", INT_MIN_VALUE + i);\n          builder.set(\"_struct_not_null\", structNotNull); \r\n\n          appender.add(builder.build());\n        }\n      }\n    }\n\n    InputFile inFile = Files.localInput(parquetFile);\n\n    ParquetFileReader reader = ParquetFileReader.open(ParquetIO.file(inFile));\n\n    Assert.assertEquals(\"Should create only one row group\", 1, reader.getRowGroups().size());\n    rowGroupMetadata = reader.getRowGroups().get(0);\n    parquetSchema = reader.getFileMetaData().getSchema();\n    dictionaryStore = reader.getNextDictionaryReader();\n  }\n","date":"2020-12-06 09:14:27","endLine":177,"groupId":"4850","id":5,"instanceNumber":1,"isCurCommit":0,"methodName":"createInputFile","params":"()","path":"/mnt/clonedata/CloneManagementServer/ManagementServer/consistResult/result-iceberg-10-0.7/blobInfo/CC_OUT/blobs/a5/e7a353093d56114c6bdc621a4398bec032df37.src","preCode":"  public void createInputFile() throws IOException {\n    File parquetFile = temp.newFile();\n    Assert.assertTrue(parquetFile.delete());\n\n    \r\n    org.apache.avro.Schema structSchema = AvroSchemaUtil.convert(_structFieldType);\n\n    OutputFile outFile = Files.localOutput(parquetFile);\n    try (FileAppender<Record> appender = Parquet.write(outFile)\n        .schema(FILE_SCHEMA)\n        .build()) {\n      GenericRecordBuilder builder = new GenericRecordBuilder(convert(FILE_SCHEMA, \"table\"));\n      \r\n      for (int copy = 0; copy < 20; copy += 1) {\n        \r\n        for (int i = 0; i < INT_MAX_VALUE - INT_MIN_VALUE + 1; i += 1) {\n          builder.set(\"_id\", INT_MIN_VALUE + i); \r\n          builder.set(\"_no_stats\", TOO_LONG_FOR_STATS); \r\n          builder.set(\"_required\", \"req\"); \r\n          builder.set(\"_all_nulls\", null); \r\n          builder.set(\"_some_nulls\", (i % 10 == 0) ? null : \"some\"); \r\n          builder.set(\"_no_nulls\", \"\"); \r\n          builder.set(\"_non_dict\", UUID.randomUUID().toString()); \r\n\n          Record structNotNull = new Record(structSchema);\n          structNotNull.put(\"_int_field\", INT_MIN_VALUE + i);\n          builder.set(\"_struct_not_null\", structNotNull); \r\n\n          appender.add(builder.build());\n        }\n      }\n    }\n\n    InputFile inFile = Files.localInput(parquetFile);\n\n    ParquetFileReader reader = ParquetFileReader.open(ParquetIO.file(inFile));\n\n    Assert.assertEquals(\"Should create only one row group\", 1, reader.getRowGroups().size());\n    rowGroupMetadata = reader.getRowGroups().get(0);\n    parquetSchema = reader.getFileMetaData().getSchema();\n    dictionaryStore = reader.getNextDictionaryReader();\n  }\n","realPath":"parquet/src/test/java/org/apache/iceberg/parquet/TestDictionaryRowGroupFilter.java","repoName":"iceberg","snippetEndLine":0,"snippetStartLine":0,"startLine":133,"status":"M"},{"authorDate":"2020-05-26 01:15:35","commitOrder":7,"curCode":"  public void createParquetInputFile(List<Record> records) throws IOException {\n    if (PARQUET_FILE.exists()) {\n      Assert.assertTrue(PARQUET_FILE.delete());\n    }\n\n    OutputFile outFile = Files.localOutput(PARQUET_FILE);\n    try (FileAppender<Record> appender = Parquet.write(outFile)\n        .schema(FILE_SCHEMA)\n        .createWriterFunc(GenericParquetWriter::buildWriter)\n        .build()) {\n      appender.addAll(records);\n    }\n\n    InputFile inFile = Files.localInput(PARQUET_FILE);\n    try (ParquetFileReader reader = ParquetFileReader.open(parquetInputFile(inFile))) {\n      Assert.assertEquals(\"Should create only one row group\", 1, reader.getRowGroups().size());\n      rowGroupMetadata = reader.getRowGroups().get(0);\n      parquetSchema = reader.getFileMetaData().getSchema();\n    }\n\n    PARQUET_FILE.deleteOnExit();\n  }\n","date":"2020-05-26 01:15:35","endLine":218,"groupId":"3302","id":6,"instanceNumber":2,"isCurCommit":0,"methodName":"createParquetInputFile","params":"(List<Record>records)","path":"/mnt/clonedata/CloneManagementServer/ManagementServer/consistResult/result-iceberg-10-0.7/blobInfo/CC_OUT/blobs/86/47a9c7dee02b6ce46ecee97a74de7421cd16f4.src","preCode":"  public void createParquetInputFile(List<Record> records) throws IOException {\n    if (PARQUET_FILE.exists()) {\n      Assert.assertTrue(PARQUET_FILE.delete());\n    }\n\n    OutputFile outFile = Files.localOutput(PARQUET_FILE);\n    try (FileAppender<Record> appender = Parquet.write(outFile)\n        .schema(FILE_SCHEMA)\n        .createWriterFunc(GenericParquetWriter::buildWriter)\n        .build()) {\n      appender.addAll(records);\n    }\n\n    InputFile inFile = Files.localInput(PARQUET_FILE);\n    try (ParquetFileReader reader = ParquetFileReader.open(parquetInputFile(inFile))) {\n      Assert.assertEquals(\"Should create only one row group\", 1, reader.getRowGroups().size());\n      rowGroupMetadata = reader.getRowGroups().get(0);\n      parquetSchema = reader.getFileMetaData().getSchema();\n    }\n\n    PARQUET_FILE.deleteOnExit();\n  }\n","realPath":"data/src/test/java/org/apache/iceberg/data/TestMetricsRowGroupFilterTypes.java","repoName":"iceberg","snippetEndLine":0,"snippetStartLine":0,"startLine":197,"status":"N"}],"commitId":"fab4a5f2db140fdb132205e78934a145e646758b","commitMessage":"@@@API: add isNaN and notNaN predicates (#1747)\n\n","date":"2020-12-06 09:14:27","modifiedFileCount":"27","status":"M","submitter":"yyanyy"},{"authorTime":"2020-05-26 01:15:35","codes":[{"authorDate":"2021-05-21 03:30:58","commitOrder":8,"curCode":"  public void createInputFile() throws IOException {\n    File parquetFile = temp.newFile();\n    Assert.assertTrue(parquetFile.delete());\n\n    \r\n    org.apache.avro.Schema structSchema = AvroSchemaUtil.convert(_structFieldType);\n\n    OutputFile outFile = Files.localOutput(parquetFile);\n    try (FileAppender<Record> appender = Parquet.write(outFile)\n        .schema(FILE_SCHEMA)\n        .withWriterVersion(writerVersion)\n        .build()) {\n      GenericRecordBuilder builder = new GenericRecordBuilder(convert(FILE_SCHEMA, \"table\"));\n      \r\n      for (int copy = 0; copy < 20; copy += 1) {\n        \r\n        for (int i = 0; i < INT_MAX_VALUE - INT_MIN_VALUE + 1; i += 1) {\n          builder.set(\"_id\", INT_MIN_VALUE + i); \r\n          builder.set(\"_no_stats\", TOO_LONG_FOR_STATS); \r\n          builder.set(\"_required\", \"req\"); \r\n          builder.set(\"_all_nulls\", null); \r\n          builder.set(\"_some_nulls\", (i % 10 == 0) ? null : \"some\"); \r\n          builder.set(\"_no_nulls\", \"\"); \r\n          builder.set(\"_non_dict\", UUID.randomUUID().toString()); \r\n          builder.set(\"_all_nans\", Double.NaN); \r\n          builder.set(\"_some_nans\", (i % 10 == 0) ? Float.NaN : 2F); \r\n          builder.set(\"_no_nans\", 3D); \r\n\n          \r\n          builder.set(\"_decimal_fixed\", DECIMAL_MIN_VALUE.add(DECIMAL_STEP.multiply(new BigDecimal(i))));\n\n          Record structNotNull = new Record(structSchema);\n          structNotNull.put(\"_int_field\", INT_MIN_VALUE + i);\n          builder.set(\"_struct_not_null\", structNotNull); \r\n\n          appender.add(builder.build());\n        }\n      }\n    }\n\n    InputFile inFile = Files.localInput(parquetFile);\n\n    ParquetFileReader reader = ParquetFileReader.open(ParquetIO.file(inFile));\n\n    Assert.assertEquals(\"Should create only one row group\", 1, reader.getRowGroups().size());\n    rowGroupMetadata = reader.getRowGroups().get(0);\n    parquetSchema = reader.getFileMetaData().getSchema();\n    dictionaryStore = reader.getNextDictionaryReader();\n  }\n","date":"2021-05-21 03:30:58","endLine":211,"groupId":"10582","id":7,"instanceNumber":1,"isCurCommit":1,"methodName":"createInputFile","params":"()","path":"/mnt/clonedata/CloneManagementServer/ManagementServer/consistResult/result-iceberg-10-0.7/blobInfo/CC_OUT/blobs/a7/685a643077fce12940e73daad166a0a6ffa823.src","preCode":"  public void createInputFile() throws IOException {\n    File parquetFile = temp.newFile();\n    Assert.assertTrue(parquetFile.delete());\n\n    \r\n    org.apache.avro.Schema structSchema = AvroSchemaUtil.convert(_structFieldType);\n\n    OutputFile outFile = Files.localOutput(parquetFile);\n    try (FileAppender<Record> appender = Parquet.write(outFile)\n        .schema(FILE_SCHEMA)\n        .build()) {\n      GenericRecordBuilder builder = new GenericRecordBuilder(convert(FILE_SCHEMA, \"table\"));\n      \r\n      for (int copy = 0; copy < 20; copy += 1) {\n        \r\n        for (int i = 0; i < INT_MAX_VALUE - INT_MIN_VALUE + 1; i += 1) {\n          builder.set(\"_id\", INT_MIN_VALUE + i); \r\n          builder.set(\"_no_stats\", TOO_LONG_FOR_STATS); \r\n          builder.set(\"_required\", \"req\"); \r\n          builder.set(\"_all_nulls\", null); \r\n          builder.set(\"_some_nulls\", (i % 10 == 0) ? null : \"some\"); \r\n          builder.set(\"_no_nulls\", \"\"); \r\n          builder.set(\"_non_dict\", UUID.randomUUID().toString()); \r\n          builder.set(\"_all_nans\", Double.NaN); \r\n          builder.set(\"_some_nans\", (i % 10 == 0) ? Float.NaN : 2F); \r\n          builder.set(\"_no_nans\", 3D); \r\n\n          Record structNotNull = new Record(structSchema);\n          structNotNull.put(\"_int_field\", INT_MIN_VALUE + i);\n          builder.set(\"_struct_not_null\", structNotNull); \r\n\n          appender.add(builder.build());\n        }\n      }\n    }\n\n    InputFile inFile = Files.localInput(parquetFile);\n\n    ParquetFileReader reader = ParquetFileReader.open(ParquetIO.file(inFile));\n\n    Assert.assertEquals(\"Should create only one row group\", 1, reader.getRowGroups().size());\n    rowGroupMetadata = reader.getRowGroups().get(0);\n    parquetSchema = reader.getFileMetaData().getSchema();\n    dictionaryStore = reader.getNextDictionaryReader();\n  }\n","realPath":"parquet/src/test/java/org/apache/iceberg/parquet/TestDictionaryRowGroupFilter.java","repoName":"iceberg","snippetEndLine":0,"snippetStartLine":0,"startLine":163,"status":"M"},{"authorDate":"2020-05-26 01:15:35","commitOrder":8,"curCode":"  public void createParquetInputFile(List<Record> records) throws IOException {\n    if (PARQUET_FILE.exists()) {\n      Assert.assertTrue(PARQUET_FILE.delete());\n    }\n\n    OutputFile outFile = Files.localOutput(PARQUET_FILE);\n    try (FileAppender<Record> appender = Parquet.write(outFile)\n        .schema(FILE_SCHEMA)\n        .createWriterFunc(GenericParquetWriter::buildWriter)\n        .build()) {\n      appender.addAll(records);\n    }\n\n    InputFile inFile = Files.localInput(PARQUET_FILE);\n    try (ParquetFileReader reader = ParquetFileReader.open(parquetInputFile(inFile))) {\n      Assert.assertEquals(\"Should create only one row group\", 1, reader.getRowGroups().size());\n      rowGroupMetadata = reader.getRowGroups().get(0);\n      parquetSchema = reader.getFileMetaData().getSchema();\n    }\n\n    PARQUET_FILE.deleteOnExit();\n  }\n","date":"2020-05-26 01:15:35","endLine":218,"groupId":"10582","id":8,"instanceNumber":2,"isCurCommit":0,"methodName":"createParquetInputFile","params":"(List<Record>records)","path":"/mnt/clonedata/CloneManagementServer/ManagementServer/consistResult/result-iceberg-10-0.7/blobInfo/CC_OUT/blobs/86/47a9c7dee02b6ce46ecee97a74de7421cd16f4.src","preCode":"  public void createParquetInputFile(List<Record> records) throws IOException {\n    if (PARQUET_FILE.exists()) {\n      Assert.assertTrue(PARQUET_FILE.delete());\n    }\n\n    OutputFile outFile = Files.localOutput(PARQUET_FILE);\n    try (FileAppender<Record> appender = Parquet.write(outFile)\n        .schema(FILE_SCHEMA)\n        .createWriterFunc(GenericParquetWriter::buildWriter)\n        .build()) {\n      appender.addAll(records);\n    }\n\n    InputFile inFile = Files.localInput(PARQUET_FILE);\n    try (ParquetFileReader reader = ParquetFileReader.open(parquetInputFile(inFile))) {\n      Assert.assertEquals(\"Should create only one row group\", 1, reader.getRowGroups().size());\n      rowGroupMetadata = reader.getRowGroups().get(0);\n      parquetSchema = reader.getFileMetaData().getSchema();\n    }\n\n    PARQUET_FILE.deleteOnExit();\n  }\n","realPath":"data/src/test/java/org/apache/iceberg/data/TestMetricsRowGroupFilterTypes.java","repoName":"iceberg","snippetEndLine":0,"snippetStartLine":0,"startLine":197,"status":"N"}],"commitId":"0e4f0f0bf5644faa372b96abd7df25fd8930ddc1","commitMessage":"@@@Parquet: Fix filtering with dictionary-encoded fixed (#2551)\n\n","date":"2021-05-21 03:30:58","modifiedFileCount":"3","status":"M","submitter":"Gabor Szadovszky"}]
