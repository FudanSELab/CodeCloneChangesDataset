[{"authorTime":"2020-12-04 18:09:09","codes":[{"authorDate":"2020-09-18 01:14:41","commitOrder":5,"curCode":"  public static void startMetastoreAndSpark() {\n    SparkTestBase.metastore = new TestHiveMetastore();\n    metastore.start();\n    SparkTestBase.hiveConf = metastore.hiveConf();\n\n    SparkTestBase.spark = SparkSession.builder()\n        .master(\"local[2]\")\n        .config(SQLConf.PARTITION_OVERWRITE_MODE().key(), \"dynamic\")\n        .config(\"spark.hadoop.\" + METASTOREURIS.varname, hiveConf.get(METASTOREURIS.varname))\n        .enableHiveSupport()\n        .getOrCreate();\n\n    SparkTestBase.catalog = new HiveCatalog(spark.sessionState().newHadoopConf());\n\n    try {\n      catalog.createNamespace(Namespace.of(\"default\"));\n    } catch (AlreadyExistsException ignored) {\n      \r\n    }\n  }\n","date":"2020-09-18 01:14:41","endLine":70,"groupId":"1612","id":1,"instanceNumber":1,"isCurCommit":0,"methodName":"startMetastoreAndSpark","params":"()","path":"/mnt/clonedata/CloneManagementServer/ManagementServer/consistResult/result-iceberg-10-0.7/blobInfo/CC_OUT/blobs/30/21288d7c942c19364349987d6f1befd1b2fd9c.src","preCode":"  public static void startMetastoreAndSpark() {\n    SparkTestBase.metastore = new TestHiveMetastore();\n    metastore.start();\n    SparkTestBase.hiveConf = metastore.hiveConf();\n\n    SparkTestBase.spark = SparkSession.builder()\n        .master(\"local[2]\")\n        .config(SQLConf.PARTITION_OVERWRITE_MODE().key(), \"dynamic\")\n        .config(\"spark.hadoop.\" + METASTOREURIS.varname, hiveConf.get(METASTOREURIS.varname))\n        .enableHiveSupport()\n        .getOrCreate();\n\n    SparkTestBase.catalog = new HiveCatalog(spark.sessionState().newHadoopConf());\n\n    try {\n      catalog.createNamespace(Namespace.of(\"default\"));\n    } catch (AlreadyExistsException ignored) {\n      \r\n    }\n  }\n","realPath":"spark/src/test/java/org/apache/iceberg/spark/SparkTestBase.java","repoName":"iceberg","snippetEndLine":0,"snippetStartLine":0,"startLine":51,"status":"NB"},{"authorDate":"2020-12-04 18:09:09","commitOrder":5,"curCode":"  public static void startMetastoreAndSpark() {\n    SparkTestBase.metastore = new TestHiveMetastore();\n    metastore.start();\n    SparkTestBase.hiveConf = metastore.hiveConf();\n\n    SparkTestBase.spark = SparkSession.builder()\n        .master(\"local[2]\")\n        .config(\"spark.testing\", \"true\")\n        .config(SQLConf.PARTITION_OVERWRITE_MODE().key(), \"dynamic\")\n        .config(\"spark.sql.extensions\", IcebergSparkSessionExtensions.class.getName())\n        .config(\"spark.hadoop.\" + METASTOREURIS.varname, hiveConf.get(METASTOREURIS.varname))\n        .enableHiveSupport()\n        .getOrCreate();\n\n    SparkTestBase.catalog = new HiveCatalog(spark.sessionState().newHadoopConf());\n  }\n","date":"2020-12-04 18:09:09","endLine":55,"groupId":"3520","id":2,"instanceNumber":2,"isCurCommit":0,"methodName":"startMetastoreAndSpark","params":"()","path":"/mnt/clonedata/CloneManagementServer/ManagementServer/consistResult/result-iceberg-10-0.7/blobInfo/CC_OUT/blobs/34/2f95127906d73f8b5c4fdf4600cb418c02f498.src","preCode":"  public static void startMetastoreAndSpark() {\n    SparkTestBase.metastore = new TestHiveMetastore();\n    metastore.start();\n    SparkTestBase.hiveConf = metastore.hiveConf();\n\n    SparkTestBase.spark = SparkSession.builder()\n        .master(\"local[2]\")\n        .config(\"spark.testing\", \"true\")\n        .config(SQLConf.PARTITION_OVERWRITE_MODE().key(), \"dynamic\")\n        .config(\"spark.sql.extensions\", IcebergSparkSessionExtensions.class.getName())\n        .config(\"spark.hadoop.\" + METASTOREURIS.varname, hiveConf.get(METASTOREURIS.varname))\n        .enableHiveSupport()\n        .getOrCreate();\n\n    SparkTestBase.catalog = new HiveCatalog(spark.sessionState().newHadoopConf());\n  }\n","realPath":"spark3-extensions/src/test/java/org/apache/iceberg/spark/extensions/SparkExtensionsTestBase.java","repoName":"iceberg","snippetEndLine":0,"snippetStartLine":0,"startLine":40,"status":"MB"}],"commitId":"bfddabbf876660ab4369495c5c321e4cfd19faac","commitMessage":"@@@Spark: Add RemoveOrphanFilesProcedure (#1869)\n\nFixes #1599.\n\nLead-authored-by: Kun Liu <liukun@apache.org>\nCo-authored-by: Anton Okolnychyi <aokolnychyi@apple.com>","date":"2020-12-04 18:09:09","modifiedFileCount":"2","status":"M","submitter":"Anton Okolnychyi"},{"authorTime":"2020-12-10 21:40:06","codes":[{"authorDate":"2020-09-18 01:14:41","commitOrder":6,"curCode":"  public static void startMetastoreAndSpark() {\n    SparkTestBase.metastore = new TestHiveMetastore();\n    metastore.start();\n    SparkTestBase.hiveConf = metastore.hiveConf();\n\n    SparkTestBase.spark = SparkSession.builder()\n        .master(\"local[2]\")\n        .config(SQLConf.PARTITION_OVERWRITE_MODE().key(), \"dynamic\")\n        .config(\"spark.hadoop.\" + METASTOREURIS.varname, hiveConf.get(METASTOREURIS.varname))\n        .enableHiveSupport()\n        .getOrCreate();\n\n    SparkTestBase.catalog = new HiveCatalog(spark.sessionState().newHadoopConf());\n\n    try {\n      catalog.createNamespace(Namespace.of(\"default\"));\n    } catch (AlreadyExistsException ignored) {\n      \r\n    }\n  }\n","date":"2020-09-18 01:14:41","endLine":70,"groupId":"1612","id":3,"instanceNumber":1,"isCurCommit":0,"methodName":"startMetastoreAndSpark","params":"()","path":"/mnt/clonedata/CloneManagementServer/ManagementServer/consistResult/result-iceberg-10-0.7/blobInfo/CC_OUT/blobs/30/21288d7c942c19364349987d6f1befd1b2fd9c.src","preCode":"  public static void startMetastoreAndSpark() {\n    SparkTestBase.metastore = new TestHiveMetastore();\n    metastore.start();\n    SparkTestBase.hiveConf = metastore.hiveConf();\n\n    SparkTestBase.spark = SparkSession.builder()\n        .master(\"local[2]\")\n        .config(SQLConf.PARTITION_OVERWRITE_MODE().key(), \"dynamic\")\n        .config(\"spark.hadoop.\" + METASTOREURIS.varname, hiveConf.get(METASTOREURIS.varname))\n        .enableHiveSupport()\n        .getOrCreate();\n\n    SparkTestBase.catalog = new HiveCatalog(spark.sessionState().newHadoopConf());\n\n    try {\n      catalog.createNamespace(Namespace.of(\"default\"));\n    } catch (AlreadyExistsException ignored) {\n      \r\n    }\n  }\n","realPath":"spark/src/test/java/org/apache/iceberg/spark/SparkTestBase.java","repoName":"iceberg","snippetEndLine":0,"snippetStartLine":0,"startLine":51,"status":"N"},{"authorDate":"2020-12-10 21:40:06","commitOrder":6,"curCode":"  public static void startMetastoreAndSpark() {\n    SparkTestBase.metastore = new TestHiveMetastore();\n    metastore.start();\n    SparkTestBase.hiveConf = metastore.hiveConf();\n\n    SparkTestBase.spark = SparkSession.builder()\n        .master(\"local[2]\")\n        .config(\"spark.testing\", \"true\")\n        .config(SQLConf.PARTITION_OVERWRITE_MODE().key(), \"dynamic\")\n        .config(\"spark.sql.extensions\", IcebergSparkSessionExtensions.class.getName())\n        .config(\"spark.hadoop.\" + METASTOREURIS.varname, hiveConf.get(METASTOREURIS.varname))\n        .config(\"spark.sql.shuffle.partitions\", \"4\")\n        .enableHiveSupport()\n        .getOrCreate();\n\n    SparkTestBase.catalog = new HiveCatalog(spark.sessionState().newHadoopConf());\n  }\n","date":"2020-12-10 21:40:06","endLine":56,"groupId":"3520","id":4,"instanceNumber":2,"isCurCommit":0,"methodName":"startMetastoreAndSpark","params":"()","path":"/mnt/clonedata/CloneManagementServer/ManagementServer/consistResult/result-iceberg-10-0.7/blobInfo/CC_OUT/blobs/83/c104c2281debce35815c3004d5baf568513b09.src","preCode":"  public static void startMetastoreAndSpark() {\n    SparkTestBase.metastore = new TestHiveMetastore();\n    metastore.start();\n    SparkTestBase.hiveConf = metastore.hiveConf();\n\n    SparkTestBase.spark = SparkSession.builder()\n        .master(\"local[2]\")\n        .config(\"spark.testing\", \"true\")\n        .config(SQLConf.PARTITION_OVERWRITE_MODE().key(), \"dynamic\")\n        .config(\"spark.sql.extensions\", IcebergSparkSessionExtensions.class.getName())\n        .config(\"spark.hadoop.\" + METASTOREURIS.varname, hiveConf.get(METASTOREURIS.varname))\n        .enableHiveSupport()\n        .getOrCreate();\n\n    SparkTestBase.catalog = new HiveCatalog(spark.sessionState().newHadoopConf());\n  }\n","realPath":"spark3-extensions/src/test/java/org/apache/iceberg/spark/extensions/SparkExtensionsTestBase.java","repoName":"iceberg","snippetEndLine":0,"snippetStartLine":0,"startLine":40,"status":"M"}],"commitId":"38a48a52dd6f4affd26c173aafedab8d3959ad5c","commitMessage":"@@@Spark: Speed up tests for extensions (#1903)\n\n","date":"2020-12-10 21:40:06","modifiedFileCount":"1","status":"M","submitter":"Anton Okolnychyi"},{"authorTime":"2021-03-12 13:53:17","codes":[{"authorDate":"2021-03-12 13:53:17","commitOrder":7,"curCode":"  public static void startMetastoreAndSpark() {\n    SparkTestBase.metastore = new TestHiveMetastore();\n    metastore.start();\n    SparkTestBase.hiveConf = metastore.hiveConf();\n\n    SparkTestBase.spark = SparkSession.builder()\n        .master(\"local[2]\")\n        .config(SQLConf.PARTITION_OVERWRITE_MODE().key(), \"dynamic\")\n        .config(\"spark.hadoop.\" + METASTOREURIS.varname, hiveConf.get(METASTOREURIS.varname))\n        .enableHiveSupport()\n        .getOrCreate();\n\n    SparkTestBase.catalog = (HiveCatalog)\n        CatalogUtil.loadCatalog(HiveCatalog.class.getName(), \"hive\", ImmutableMap.of(), hiveConf);\n\n    try {\n      catalog.createNamespace(Namespace.of(\"default\"));\n    } catch (AlreadyExistsException ignored) {\n      \r\n    }\n  }\n","date":"2021-03-12 13:53:17","endLine":75,"groupId":"1612","id":5,"instanceNumber":1,"isCurCommit":0,"methodName":"startMetastoreAndSpark","params":"()","path":"/mnt/clonedata/CloneManagementServer/ManagementServer/consistResult/result-iceberg-10-0.7/blobInfo/CC_OUT/blobs/7b/5643ba893267f4051ff54e26ba8813562ec69e.src","preCode":"  public static void startMetastoreAndSpark() {\n    SparkTestBase.metastore = new TestHiveMetastore();\n    metastore.start();\n    SparkTestBase.hiveConf = metastore.hiveConf();\n\n    SparkTestBase.spark = SparkSession.builder()\n        .master(\"local[2]\")\n        .config(SQLConf.PARTITION_OVERWRITE_MODE().key(), \"dynamic\")\n        .config(\"spark.hadoop.\" + METASTOREURIS.varname, hiveConf.get(METASTOREURIS.varname))\n        .enableHiveSupport()\n        .getOrCreate();\n\n    SparkTestBase.catalog = new HiveCatalog(spark.sessionState().newHadoopConf());\n\n    try {\n      catalog.createNamespace(Namespace.of(\"default\"));\n    } catch (AlreadyExistsException ignored) {\n      \r\n    }\n  }\n","realPath":"spark/src/test/java/org/apache/iceberg/spark/SparkTestBase.java","repoName":"iceberg","snippetEndLine":0,"snippetStartLine":0,"startLine":55,"status":"M"},{"authorDate":"2021-03-12 13:53:17","commitOrder":7,"curCode":"  public static void startMetastoreAndSpark() {\n    SparkTestBase.metastore = new TestHiveMetastore();\n    metastore.start();\n    SparkTestBase.hiveConf = metastore.hiveConf();\n\n    SparkTestBase.spark = SparkSession.builder()\n        .master(\"local[2]\")\n        .config(\"spark.testing\", \"true\")\n        .config(SQLConf.PARTITION_OVERWRITE_MODE().key(), \"dynamic\")\n        .config(\"spark.sql.extensions\", IcebergSparkSessionExtensions.class.getName())\n        .config(\"spark.hadoop.\" + METASTOREURIS.varname, hiveConf.get(METASTOREURIS.varname))\n        .config(\"spark.sql.shuffle.partitions\", \"4\")\n        .enableHiveSupport()\n        .getOrCreate();\n\n    SparkTestBase.catalog = (HiveCatalog)\n        CatalogUtil.loadCatalog(HiveCatalog.class.getName(), \"hive\", ImmutableMap.of(), hiveConf);\n  }\n","date":"2021-03-12 13:53:17","endLine":59,"groupId":"3520","id":6,"instanceNumber":2,"isCurCommit":0,"methodName":"startMetastoreAndSpark","params":"()","path":"/mnt/clonedata/CloneManagementServer/ManagementServer/consistResult/result-iceberg-10-0.7/blobInfo/CC_OUT/blobs/d5/6c1dd60c08b4afc09d89ac194561b891755289.src","preCode":"  public static void startMetastoreAndSpark() {\n    SparkTestBase.metastore = new TestHiveMetastore();\n    metastore.start();\n    SparkTestBase.hiveConf = metastore.hiveConf();\n\n    SparkTestBase.spark = SparkSession.builder()\n        .master(\"local[2]\")\n        .config(\"spark.testing\", \"true\")\n        .config(SQLConf.PARTITION_OVERWRITE_MODE().key(), \"dynamic\")\n        .config(\"spark.sql.extensions\", IcebergSparkSessionExtensions.class.getName())\n        .config(\"spark.hadoop.\" + METASTOREURIS.varname, hiveConf.get(METASTOREURIS.varname))\n        .config(\"spark.sql.shuffle.partitions\", \"4\")\n        .enableHiveSupport()\n        .getOrCreate();\n\n    SparkTestBase.catalog = new HiveCatalog(spark.sessionState().newHadoopConf());\n  }\n","realPath":"spark3-extensions/src/test/java/org/apache/iceberg/spark/extensions/SparkExtensionsTestBase.java","repoName":"iceberg","snippetEndLine":0,"snippetStartLine":0,"startLine":42,"status":"M"}],"commitId":"c8b74c19b831ecbbf33f78094b5c8098c46eddec","commitMessage":"@@@Hive: Refactor HiveCatalog and HiveClientPool constructors (#2203)\n\n","date":"2021-03-12 13:53:17","modifiedFileCount":"11","status":"M","submitter":"Ryan Murray"},{"authorTime":"2021-09-10 06:01:28","codes":[{"authorDate":"2021-03-12 13:53:17","commitOrder":8,"curCode":"  public static void startMetastoreAndSpark() {\n    SparkTestBase.metastore = new TestHiveMetastore();\n    metastore.start();\n    SparkTestBase.hiveConf = metastore.hiveConf();\n\n    SparkTestBase.spark = SparkSession.builder()\n        .master(\"local[2]\")\n        .config(SQLConf.PARTITION_OVERWRITE_MODE().key(), \"dynamic\")\n        .config(\"spark.hadoop.\" + METASTOREURIS.varname, hiveConf.get(METASTOREURIS.varname))\n        .enableHiveSupport()\n        .getOrCreate();\n\n    SparkTestBase.catalog = (HiveCatalog)\n        CatalogUtil.loadCatalog(HiveCatalog.class.getName(), \"hive\", ImmutableMap.of(), hiveConf);\n\n    try {\n      catalog.createNamespace(Namespace.of(\"default\"));\n    } catch (AlreadyExistsException ignored) {\n      \r\n    }\n  }\n","date":"2021-03-12 13:53:17","endLine":75,"groupId":"10362","id":7,"instanceNumber":1,"isCurCommit":0,"methodName":"startMetastoreAndSpark","params":"()","path":"/mnt/clonedata/CloneManagementServer/ManagementServer/consistResult/result-iceberg-10-0.7/blobInfo/CC_OUT/blobs/7b/5643ba893267f4051ff54e26ba8813562ec69e.src","preCode":"  public static void startMetastoreAndSpark() {\n    SparkTestBase.metastore = new TestHiveMetastore();\n    metastore.start();\n    SparkTestBase.hiveConf = metastore.hiveConf();\n\n    SparkTestBase.spark = SparkSession.builder()\n        .master(\"local[2]\")\n        .config(SQLConf.PARTITION_OVERWRITE_MODE().key(), \"dynamic\")\n        .config(\"spark.hadoop.\" + METASTOREURIS.varname, hiveConf.get(METASTOREURIS.varname))\n        .enableHiveSupport()\n        .getOrCreate();\n\n    SparkTestBase.catalog = (HiveCatalog)\n        CatalogUtil.loadCatalog(HiveCatalog.class.getName(), \"hive\", ImmutableMap.of(), hiveConf);\n\n    try {\n      catalog.createNamespace(Namespace.of(\"default\"));\n    } catch (AlreadyExistsException ignored) {\n      \r\n    }\n  }\n","realPath":"spark/src/test/java/org/apache/iceberg/spark/SparkTestBase.java","repoName":"iceberg","snippetEndLine":0,"snippetStartLine":0,"startLine":55,"status":"N"},{"authorDate":"2021-09-10 06:01:28","commitOrder":8,"curCode":"  public static void startMetastoreAndSpark() {\n    SparkTestBase.metastore = new TestHiveMetastore();\n    metastore.start();\n    SparkTestBase.hiveConf = metastore.hiveConf();\n\n    SparkTestBase.spark = SparkSession.builder()\n        .master(\"local[2]\")\n        .config(\"spark.testing\", \"true\")\n        .config(SQLConf.PARTITION_OVERWRITE_MODE().key(), \"dynamic\")\n        .config(\"spark.sql.extensions\", IcebergSparkSessionExtensions.class.getName())\n        .config(\"spark.hadoop.\" + METASTOREURIS.varname, hiveConf.get(METASTOREURIS.varname))\n        .config(\"spark.sql.shuffle.partitions\", \"4\")\n        .config(\"spark.sql.hive.metastorePartitionPruningFallbackOnException\", \"true\")\n        .enableHiveSupport()\n        .getOrCreate();\n\n    SparkTestBase.catalog = (HiveCatalog)\n        CatalogUtil.loadCatalog(HiveCatalog.class.getName(), \"hive\", ImmutableMap.of(), hiveConf);\n  }\n","date":"2021-09-10 06:01:28","endLine":60,"groupId":"10362","id":8,"instanceNumber":2,"isCurCommit":1,"methodName":"startMetastoreAndSpark","params":"()","path":"/mnt/clonedata/CloneManagementServer/ManagementServer/consistResult/result-iceberg-10-0.7/blobInfo/CC_OUT/blobs/36/ca608ccd3bc4a6e8eaf377c9125348d9d71770.src","preCode":"  public static void startMetastoreAndSpark() {\n    SparkTestBase.metastore = new TestHiveMetastore();\n    metastore.start();\n    SparkTestBase.hiveConf = metastore.hiveConf();\n\n    SparkTestBase.spark = SparkSession.builder()\n        .master(\"local[2]\")\n        .config(\"spark.testing\", \"true\")\n        .config(SQLConf.PARTITION_OVERWRITE_MODE().key(), \"dynamic\")\n        .config(\"spark.sql.extensions\", IcebergSparkSessionExtensions.class.getName())\n        .config(\"spark.hadoop.\" + METASTOREURIS.varname, hiveConf.get(METASTOREURIS.varname))\n        .config(\"spark.sql.shuffle.partitions\", \"4\")\n        .enableHiveSupport()\n        .getOrCreate();\n\n    SparkTestBase.catalog = (HiveCatalog)\n        CatalogUtil.loadCatalog(HiveCatalog.class.getName(), \"hive\", ImmutableMap.of(), hiveConf);\n  }\n","realPath":"spark3-extensions/src/test/java/org/apache/iceberg/spark/extensions/SparkExtensionsTestBase.java","repoName":"iceberg","snippetEndLine":0,"snippetStartLine":0,"startLine":42,"status":"M"}],"commitId":"af4bde667ff2ed9c2a32a313ed00f51de580dfc1","commitMessage":"@@@Spark: Add config needed in tests after SPARK-36128 (#3090)\n\n","date":"2021-09-10 06:01:28","modifiedFileCount":"1","status":"M","submitter":"Kyle Bendickson"}]
