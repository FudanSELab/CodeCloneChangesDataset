[{"authorTime":"2017-09-18 14:15:08","codes":[{"authorDate":"2017-09-18 14:15:08","commitOrder":1,"curCode":"  public void testBytesSkippedAfterRead() throws IOException {\n    for (int i = 0; i < 1024; i++) {\n      assertEquals(randomBytes[i], (byte) inputStream.read());\n    }\n    assertEquals(1024, inputStream.skip(1024));\n    for (int i = 2048; i < randomBytes.length; i++) {\n      assertEquals(randomBytes[i], (byte) inputStream.read());\n    }\n  }\n","date":"2017-09-18 14:15:08","endLine":92,"groupId":"1316","id":1,"instanceNumber":1,"isCurCommit":0,"methodName":"testBytesSkippedAfterRead","params":"()","path":"/mnt/clonedata/CloneManagementServer/ManagementServer/consistResult/result-spark-10-0.7/blobInfo/CC_OUT/blobs/34/40e1aea2f469e818872ab4de07f66c0601a001.src","preCode":"  public void testBytesSkippedAfterRead() throws IOException {\n    for (int i = 0; i < 1024; i++) {\n      assertEquals(randomBytes[i], (byte) inputStream.read());\n    }\n    assertEquals(1024, inputStream.skip(1024));\n    for (int i = 2048; i < randomBytes.length; i++) {\n      assertEquals(randomBytes[i], (byte) inputStream.read());\n    }\n  }\n","realPath":"core/src/test/java/org/apache/spark/io/GenericFileInputStreamSuite.java","repoName":"spark","snippetEndLine":0,"snippetStartLine":0,"startLine":84,"status":"B"},{"authorDate":"2017-09-18 14:15:08","commitOrder":1,"curCode":"  public void testNegativeBytesSkippedAfterRead() throws IOException {\n    for (int i = 0; i < 1024; i++) {\n      assertEquals(randomBytes[i], (byte) inputStream.read());\n    }\n    \r\n    assertEquals(0, inputStream.skip(-1));\n    assertEquals(0, inputStream.skip(-1024));\n    assertEquals(0, inputStream.skip(Long.MIN_VALUE));\n    assertEquals(1024, inputStream.skip(1024));\n    for (int i = 2048; i < randomBytes.length; i++) {\n      assertEquals(randomBytes[i], (byte) inputStream.read());\n    }\n  }\n","date":"2017-09-18 14:15:08","endLine":107,"groupId":"1316","id":2,"instanceNumber":2,"isCurCommit":0,"methodName":"testNegativeBytesSkippedAfterRead","params":"()","path":"/mnt/clonedata/CloneManagementServer/ManagementServer/consistResult/result-spark-10-0.7/blobInfo/CC_OUT/blobs/34/40e1aea2f469e818872ab4de07f66c0601a001.src","preCode":"  public void testNegativeBytesSkippedAfterRead() throws IOException {\n    for (int i = 0; i < 1024; i++) {\n      assertEquals(randomBytes[i], (byte) inputStream.read());\n    }\n    \r\n    assertEquals(0, inputStream.skip(-1));\n    assertEquals(0, inputStream.skip(-1024));\n    assertEquals(0, inputStream.skip(Long.MIN_VALUE));\n    assertEquals(1024, inputStream.skip(1024));\n    for (int i = 2048; i < randomBytes.length; i++) {\n      assertEquals(randomBytes[i], (byte) inputStream.read());\n    }\n  }\n","realPath":"core/src/test/java/org/apache/spark/io/GenericFileInputStreamSuite.java","repoName":"spark","snippetEndLine":0,"snippetStartLine":0,"startLine":95,"status":"B"}],"commitId":"1e978b17d63d7ba20368057aa4e65f5ef6e87369","commitMessage":"@@@[SPARK-21113][CORE] Read ahead input stream to amortize disk IO cost ?\n\nProfiling some of our big jobs.  we see that around 30% of the time is being spent in reading the spill files from disk. In order to amortize the disk IO cost.  the idea is to implement a read ahead input stream which asynchronously reads ahead from the underlying input stream when specified amount of data has been read from the current buffer. It does it by maintaining two buffer - active buffer and read ahead buffer. The active buffer contains data which should be returned when a read() call is issued. The read-ahead buffer is used to asynchronously read from the underlying input stream and once the active buffer is exhausted.  we flip the two buffers so that we can start reading from the read ahead buffer without being blocked in disk I/O.\n\n## How was this patch tested?\n\nTested by running a job on the cluster and could see up to 8% CPU improvement.\n\nAuthor: Sital Kedia <skedia@fb.com>\nAuthor: Shixiong Zhu <zsxwing@gmail.com>\nAuthor: Sital Kedia <sitalkedia@users.noreply.github.com>\n\nCloses #18317 from sitalkedia/read_ahead_buffer.\n","date":"2017-09-18 14:15:08","modifiedFileCount":"1","status":"B","submitter":"Sital Kedia"},{"authorTime":"2018-02-15 17:09:06","codes":[{"authorDate":"2018-02-15 17:09:06","commitOrder":2,"curCode":"  public void testBytesSkippedAfterRead() throws IOException {\n    for (InputStream inputStream: inputStreams) {\n      for (int i = 0; i < 1024; i++) {\n        assertEquals(randomBytes[i], (byte) inputStream.read());\n      }\n      assertEquals(1024, inputStream.skip(1024));\n      for (int i = 2048; i < randomBytes.length; i++) {\n        assertEquals(randomBytes[i], (byte) inputStream.read());\n      }\n    }\n  }\n","date":"2018-02-15 17:09:06","endLine":100,"groupId":"10568","id":3,"instanceNumber":1,"isCurCommit":0,"methodName":"testBytesSkippedAfterRead","params":"()","path":"/mnt/clonedata/CloneManagementServer/ManagementServer/consistResult/result-spark-10-0.7/blobInfo/CC_OUT/blobs/22/db3592ecc961a2093a509a697222ceecef05f4.src","preCode":"  public void testBytesSkippedAfterRead() throws IOException {\n    for (int i = 0; i < 1024; i++) {\n      assertEquals(randomBytes[i], (byte) inputStream.read());\n    }\n    assertEquals(1024, inputStream.skip(1024));\n    for (int i = 2048; i < randomBytes.length; i++) {\n      assertEquals(randomBytes[i], (byte) inputStream.read());\n    }\n  }\n","realPath":"core/src/test/java/org/apache/spark/io/GenericFileInputStreamSuite.java","repoName":"spark","snippetEndLine":0,"snippetStartLine":0,"startLine":90,"status":"M"},{"authorDate":"2018-02-15 17:09:06","commitOrder":2,"curCode":"  public void testNegativeBytesSkippedAfterRead() throws IOException {\n    for (InputStream inputStream: inputStreams) {\n      for (int i = 0; i < 1024; i++) {\n        assertEquals(randomBytes[i], (byte) inputStream.read());\n      }\n      \r\n      assertEquals(0, inputStream.skip(-1));\n      assertEquals(0, inputStream.skip(-1024));\n      assertEquals(0, inputStream.skip(Long.MIN_VALUE));\n      assertEquals(1024, inputStream.skip(1024));\n      for (int i = 2048; i < randomBytes.length; i++) {\n        assertEquals(randomBytes[i], (byte) inputStream.read());\n      }\n    }\n  }\n","date":"2018-02-15 17:09:06","endLine":117,"groupId":"10568","id":4,"instanceNumber":2,"isCurCommit":0,"methodName":"testNegativeBytesSkippedAfterRead","params":"()","path":"/mnt/clonedata/CloneManagementServer/ManagementServer/consistResult/result-spark-10-0.7/blobInfo/CC_OUT/blobs/22/db3592ecc961a2093a509a697222ceecef05f4.src","preCode":"  public void testNegativeBytesSkippedAfterRead() throws IOException {\n    for (int i = 0; i < 1024; i++) {\n      assertEquals(randomBytes[i], (byte) inputStream.read());\n    }\n    \r\n    assertEquals(0, inputStream.skip(-1));\n    assertEquals(0, inputStream.skip(-1024));\n    assertEquals(0, inputStream.skip(Long.MIN_VALUE));\n    assertEquals(1024, inputStream.skip(1024));\n    for (int i = 2048; i < randomBytes.length; i++) {\n      assertEquals(randomBytes[i], (byte) inputStream.read());\n    }\n  }\n","realPath":"core/src/test/java/org/apache/spark/io/GenericFileInputStreamSuite.java","repoName":"spark","snippetEndLine":0,"snippetStartLine":0,"startLine":103,"status":"M"}],"commitId":"7539ae59d6c354c95c50528abe9ddff6972e960f","commitMessage":"@@@[SPARK-23366] Improve hot reading path in ReadAheadInputStream\n\n## What changes were proposed in this pull request?\n\n`ReadAheadInputStream` was introduced in https://github.com/apache/spark/pull/18317/ to optimize reading spill files from disk.\nHowever.  from the profiles it seems that the hot path of reading small amounts of data (like readInt) is inefficient - it involves taking locks.  and multiple checks.\n\nOptimize locking: Lock is not needed when simply accessing the active buffer. Only lock when needing to swap buffers or trigger async reading.  or get information about the async state.\n\nOptimize short-path single byte reads.  that are used e.g. by Java library DataInputStream.readInt.\n\nThe asyncReader used to call \"read\" only once on the underlying stream.  that never filled the underlying buffer when it was wrapping an LZ4BlockInputStream. If the buffer was returned unfilled.  that would trigger the async reader to be triggered to fill the read ahead buffer on each call.  because the reader would see that the active buffer is below the refill threshold all the time.\n\nHowever.  filling the full buffer all the time could introduce increased latency.  so also add an `AtomicBoolean` flag for the async reader to return earlier if there is a reader waiting for data.\n\nRemove `readAheadThresholdInBytes` and instead immediately trigger async read when switching the buffers. It allows to simplify code paths.  especially the hot one that then only has to check if there is available data in the active buffer.  without worrying if it needs to retrigger async read. It seems to have positive effect on perf.\n\n## How was this patch tested?\n\nIt was noticed as a regression in some workloads after upgrading to Spark 2.3.?\n\nIt was particularly visible on TPCDS Q95 running on instances with fast disk (i3 AWS instances).\nRunning with profiling:\n*?Spark 2.2 - 5.2-5.3 minutes 9.5% in LZ4BlockInputStream.read\n*?Spark 2.3 - 6.4-6.6 minutes 31.1% in ReadAheadInputStream.read\n*?Spark 2.3 + fix?- 5.3-5.4 minutes 13.3% in ReadAheadInputStream.read - very slightly slower.  practically within noise.\n\nWe didn't see other regressions.  and many workloads in general seem to be faster with Spark 2.3 (not investigated if thanks to async readed.  or unrelated).\n\nAuthor: Juliusz Sompolski <julek@databricks.com>\n\nCloses #20555 from juliuszsompolski/SPARK-23366.\n","date":"2018-02-15 17:09:06","modifiedFileCount":"5","status":"M","submitter":"Juliusz Sompolski"}]
