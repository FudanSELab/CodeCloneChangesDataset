[{"authorTime":"2020-05-04 14:30:10","codes":[{"authorDate":"2018-05-09 12:27:32","commitOrder":2,"curCode":"  public final void readLongs(int total, WritableColumnVector c, int rowId) {\n    int requiredBytes = total * 8;\n    ByteBuffer buffer = getBuffer(requiredBytes);\n\n    if (buffer.hasArray()) {\n      int offset = buffer.arrayOffset() + buffer.position();\n      c.putLongsLittleEndian(rowId, total, buffer.array(), offset);\n    } else {\n      for (int i = 0; i < total; i += 1) {\n        c.putLong(rowId + i, buffer.getLong());\n      }\n    }\n  }\n","date":"2018-05-09 12:27:32","endLine":98,"groupId":"1448","id":1,"instanceNumber":1,"isCurCommit":0,"methodName":"readLongs","params":"(inttotal@WritableColumnVectorc@introwId)","path":"/mnt/clonedata/CloneManagementServer/ManagementServer/consistResult/result-spark-10-0.7/blobInfo/CC_OUT/blobs/aa/cefacfc1c1ab41bafb376a6cf2207e8f6a15b9.src","preCode":"  public final void readLongs(int total, WritableColumnVector c, int rowId) {\n    int requiredBytes = total * 8;\n    ByteBuffer buffer = getBuffer(requiredBytes);\n\n    if (buffer.hasArray()) {\n      int offset = buffer.arrayOffset() + buffer.position();\n      c.putLongsLittleEndian(rowId, total, buffer.array(), offset);\n    } else {\n      for (int i = 0; i < total; i += 1) {\n        c.putLong(rowId + i, buffer.getLong());\n      }\n    }\n  }\n","realPath":"sql/core/src/main/java/org/apache/spark/sql/execution/datasources/parquet/VectorizedPlainValuesReader.java","repoName":"spark","snippetEndLine":0,"snippetStartLine":0,"startLine":86,"status":"NB"},{"authorDate":"2020-05-04 14:30:10","commitOrder":2,"curCode":"  public final void readLongsWithRebase(int total, WritableColumnVector c, int rowId) {\n    int requiredBytes = total * 8;\n    ByteBuffer buffer = getBuffer(requiredBytes);\n    boolean rebase = false;\n    for (int i = 0; i < total; i += 1) {\n      rebase |= buffer.getLong(buffer.position() + i * 8) < RebaseDateTime.lastSwitchJulianTs();\n    }\n    if (rebase) {\n      for (int i = 0; i < total; i += 1) {\n        c.putLong(rowId + i, RebaseDateTime.rebaseJulianToGregorianMicros(buffer.getLong()));\n      }\n    } else {\n      if (buffer.hasArray()) {\n        int offset = buffer.arrayOffset() + buffer.position();\n        c.putLongsLittleEndian(rowId, total, buffer.array(), offset);\n      } else {\n        for (int i = 0; i < total; i += 1) {\n          c.putLong(rowId + i, buffer.getLong());\n        }\n      }\n    }\n  }\n","date":"2020-05-04 14:30:10","endLine":152,"groupId":"1448","id":2,"instanceNumber":2,"isCurCommit":0,"methodName":"readLongsWithRebase","params":"(inttotal@WritableColumnVectorc@introwId)","path":"/mnt/clonedata/CloneManagementServer/ManagementServer/consistResult/result-spark-10-0.7/blobInfo/CC_OUT/blobs/2e/d2e11b60c038fb7e6eff1932d3de64204dc9b5.src","preCode":"  public final void readLongsWithRebase(int total, WritableColumnVector c, int rowId) {\n    int requiredBytes = total * 8;\n    ByteBuffer buffer = getBuffer(requiredBytes);\n    boolean rebase = false;\n    for (int i = 0; i < total; i += 1) {\n      rebase |= buffer.getLong(buffer.position() + i * 8) < RebaseDateTime.lastSwitchJulianTs();\n    }\n    if (rebase) {\n      for (int i = 0; i < total; i += 1) {\n        c.putLong(rowId + i, RebaseDateTime.rebaseJulianToGregorianMicros(buffer.getLong()));\n      }\n    } else {\n      if (buffer.hasArray()) {\n        int offset = buffer.arrayOffset() + buffer.position();\n        c.putLongsLittleEndian(rowId, total, buffer.array(), offset);\n      } else {\n        for (int i = 0; i < total; i += 1) {\n          c.putLong(rowId + i, buffer.getLong());\n        }\n      }\n    }\n  }\n","realPath":"sql/core/src/main/java/org/apache/spark/sql/execution/datasources/parquet/VectorizedPlainValuesReader.java","repoName":"spark","snippetEndLine":0,"snippetStartLine":0,"startLine":131,"status":"B"}],"commitId":"f72220b8ab256e8e6532205a4ce51d50b69c26e9","commitMessage":"@@@[SPARK-31606][SQL] Reduce the perf regression of vectorized parquet reader caused by datetime rebase\n\n\n What changes were proposed in this pull request?\n\nPush the rebase logic to the lower level of the parquet vectorized reader.  to make the final code more vectorization-friendly.\n\n\n Why are the changes needed?\n\nParquet vectorized reader is carefully implemented.  to make it more likely to be vectorized by the JVM. However.  the newly added datetime rebase degrade the performance a lot.  as it breaks vectorization.  even if the datetime values don't need to rebase (this is very likely as dates before 1582 is rare).\n\n\n Does this PR introduce any user-facing change?\n\nno\n\n\n How was this patch tested?\n\nRun part of the `DateTimeRebaseBenchmark` locally. The results:\nbefore this patch\n```\n[info] Load dates from parquet:                  Best Time(ms)   Avg Time(ms)   Stdev(ms)    Rate(M/s)   Per Row(ns)   Relative\n[info] ------------------------------------------------------------------------------------------------------------------------\n[info] after 1582.  vec on.  rebase off                     2677           2838         142         37.4          26.8       1.0X\n[info] after 1582.  vec on.  rebase on                      3828           4331         805         26.1          38.3       0.7X\n[info] before 1582.  vec on.  rebase off                    2903           2926          34         34.4          29.0       0.9X\n[info] before 1582.  vec on.  rebase on                     4163           4197          38         24.0          41.6       0.6X\n\n[info] Load timestamps from parquet:             Best Time(ms)   Avg Time(ms)   Stdev(ms)    Rate(M/s)   Per Row(ns)   Relative\n[info] ------------------------------------------------------------------------------------------------------------------------\n[info] after 1900.  vec on.  rebase off                     3537           3627         104         28.3          35.4       1.0X\n[info] after 1900.  vec on.  rebase on                      6891           7010         105         14.5          68.9       0.5X\n[info] before 1900.  vec on.  rebase off                    3692           3770          72         27.1          36.9       1.0X\n[info] before 1900.  vec on.  rebase on                     7588           7610          30         13.2          75.9       0.5X\n```\n\nAfter this patch\n```\n[info] Load dates from parquet:                  Best Time(ms)   Avg Time(ms)   Stdev(ms)    Rate(M/s)   Per Row(ns)   Relative\n[info] ------------------------------------------------------------------------------------------------------------------------\n[info] after 1582.  vec on.  rebase off                     2758           2944         197         36.3          27.6       1.0X\n[info] after 1582.  vec on.  rebase on                      2908           2966          51         34.4          29.1       0.9X\n[info] before 1582.  vec on.  rebase off                    2840           2878          37         35.2          28.4       1.0X\n[info] before 1582.  vec on.  rebase on                     3407           3433          24         29.4          34.1       0.8X\n\n[info] Load timestamps from parquet:             Best Time(ms)   Avg Time(ms)   Stdev(ms)    Rate(M/s)   Per Row(ns)   Relative\n[info] ------------------------------------------------------------------------------------------------------------------------\n[info] after 1900.  vec on.  rebase off                     3861           4003         139         25.9          38.6       1.0X\n[info] after 1900.  vec on.  rebase on                      4194           4283          77         23.8          41.9       0.9X\n[info] before 1900.  vec on.  rebase off                    3849           3937          79         26.0          38.5       1.0X\n[info] before 1900.  vec on.  rebase on                     7512           7546          55         13.3          75.1       0.5X\n```\n\nDate type is 30% faster if the values don't need to rebase.  20% faster if need to rebase.\nTimestamp type is 60% faster if the values don't need to rebase.  no difference if need to rebase.\n\nCloses #28406 from cloud-fan/perf.\n\nLead-authored-by: Wenchen Fan <wenchen@databricks.com>\nCo-authored-by: Maxim Gekk <max.gekk@gmail.com>\nSigned-off-by: HyukjinKwon <gurwls223@apache.org>\n","date":"2020-05-04 14:30:10","modifiedFileCount":"4","status":"M","submitter":"Wenchen Fan"},{"authorTime":"2020-05-14 11:32:40","codes":[{"authorDate":"2018-05-09 12:27:32","commitOrder":3,"curCode":"  public final void readLongs(int total, WritableColumnVector c, int rowId) {\n    int requiredBytes = total * 8;\n    ByteBuffer buffer = getBuffer(requiredBytes);\n\n    if (buffer.hasArray()) {\n      int offset = buffer.arrayOffset() + buffer.position();\n      c.putLongsLittleEndian(rowId, total, buffer.array(), offset);\n    } else {\n      for (int i = 0; i < total; i += 1) {\n        c.putLong(rowId + i, buffer.getLong());\n      }\n    }\n  }\n","date":"2018-05-09 12:27:32","endLine":98,"groupId":"10432","id":3,"instanceNumber":1,"isCurCommit":0,"methodName":"readLongs","params":"(inttotal@WritableColumnVectorc@introwId)","path":"/mnt/clonedata/CloneManagementServer/ManagementServer/consistResult/result-spark-10-0.7/blobInfo/CC_OUT/blobs/aa/cefacfc1c1ab41bafb376a6cf2207e8f6a15b9.src","preCode":"  public final void readLongs(int total, WritableColumnVector c, int rowId) {\n    int requiredBytes = total * 8;\n    ByteBuffer buffer = getBuffer(requiredBytes);\n\n    if (buffer.hasArray()) {\n      int offset = buffer.arrayOffset() + buffer.position();\n      c.putLongsLittleEndian(rowId, total, buffer.array(), offset);\n    } else {\n      for (int i = 0; i < total; i += 1) {\n        c.putLong(rowId + i, buffer.getLong());\n      }\n    }\n  }\n","realPath":"sql/core/src/main/java/org/apache/spark/sql/execution/datasources/parquet/VectorizedPlainValuesReader.java","repoName":"spark","snippetEndLine":0,"snippetStartLine":0,"startLine":86,"status":"N"},{"authorDate":"2020-05-14 11:32:40","commitOrder":3,"curCode":"  public final void readLongsWithRebase(\n      int total, WritableColumnVector c, int rowId, boolean failIfRebase) {\n    int requiredBytes = total * 8;\n    ByteBuffer buffer = getBuffer(requiredBytes);\n    boolean rebase = false;\n    for (int i = 0; i < total; i += 1) {\n      rebase |= buffer.getLong(buffer.position() + i * 8) < RebaseDateTime.lastSwitchJulianTs();\n    }\n    if (rebase) {\n      if (failIfRebase) {\n        throw DataSourceUtils.newRebaseExceptionInRead(\"Parquet\");\n      } else {\n        for (int i = 0; i < total; i += 1) {\n          c.putLong(rowId + i, RebaseDateTime.rebaseJulianToGregorianMicros(buffer.getLong()));\n        }\n      }\n    } else {\n      if (buffer.hasArray()) {\n        int offset = buffer.arrayOffset() + buffer.position();\n        c.putLongsLittleEndian(rowId, total, buffer.array(), offset);\n      } else {\n        for (int i = 0; i < total; i += 1) {\n          c.putLong(rowId + i, buffer.getLong());\n        }\n      }\n    }\n  }\n","date":"2020-05-14 11:32:40","endLine":163,"groupId":"10432","id":4,"instanceNumber":2,"isCurCommit":0,"methodName":"readLongsWithRebase","params":"(inttotal@WritableColumnVectorc@introwId@booleanfailIfRebase)","path":"/mnt/clonedata/CloneManagementServer/ManagementServer/consistResult/result-spark-10-0.7/blobInfo/CC_OUT/blobs/ed/dbf39178e9a829582c8216947d14a3b17babd7.src","preCode":"  public final void readLongsWithRebase(int total, WritableColumnVector c, int rowId) {\n    int requiredBytes = total * 8;\n    ByteBuffer buffer = getBuffer(requiredBytes);\n    boolean rebase = false;\n    for (int i = 0; i < total; i += 1) {\n      rebase |= buffer.getLong(buffer.position() + i * 8) < RebaseDateTime.lastSwitchJulianTs();\n    }\n    if (rebase) {\n      for (int i = 0; i < total; i += 1) {\n        c.putLong(rowId + i, RebaseDateTime.rebaseJulianToGregorianMicros(buffer.getLong()));\n      }\n    } else {\n      if (buffer.hasArray()) {\n        int offset = buffer.arrayOffset() + buffer.position();\n        c.putLongsLittleEndian(rowId, total, buffer.array(), offset);\n      } else {\n        for (int i = 0; i < total; i += 1) {\n          c.putLong(rowId + i, buffer.getLong());\n        }\n      }\n    }\n  }\n","realPath":"sql/core/src/main/java/org/apache/spark/sql/execution/datasources/parquet/VectorizedPlainValuesReader.java","repoName":"spark","snippetEndLine":0,"snippetStartLine":0,"startLine":137,"status":"M"}],"commitId":"fd2d55c9919ece5463377bc6f45f2cdb8bf90515","commitMessage":"@@@[SPARK-31405][SQL] Fail by default when reading/writing legacy datetime values from/to Parquet/Avro files\n\n\n What changes were proposed in this pull request?\n\nWhen reading/writing datetime values that before the rebase switch day.  from/to Avro/Parquet files.  fail by default and ask users to set a config to explicitly do rebase or not.\n\n\n Why are the changes needed?\n\nRebase or not rebase have different behaviors and we should let users decide it explicitly. In most cases.  users won't hit this exception as it only affects ancient datetime values.\n\n\n Does this PR introduce _any_ user-facing change?\n\nYes.  now users will see an error when reading/writing dates before 1582-10-15 or timestamps before 1900-01-01 from/to Parquet/Avro files.  with an error message to ask setting a config.\n\n\n How was this patch tested?\n\nupdated tests\n\nCloses #28477 from cloud-fan/rebase.\n\nAuthored-by: Wenchen Fan <wenchen@databricks.com>\nSigned-off-by: HyukjinKwon <gurwls223@apache.org>\n","date":"2020-05-14 11:32:40","modifiedFileCount":"5","status":"M","submitter":"Wenchen Fan"}]
