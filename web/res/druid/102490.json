[{"authorTime":"2018-09-14 05:42:18","codes":[{"authorDate":"2018-08-31 00:56:26","commitOrder":2,"curCode":"  public void testStopGracefully() throws Exception\n  {\n    final TaskLocation location1 = new TaskLocation(\"testHost\", 1234, -1);\n    final TaskLocation location2 = new TaskLocation(\"testHost2\", 145, -1);\n    final DateTime startTime = DateTimes.nowUtc();\n\n    supervisor = getSupervisor(2, 1, true, \"PT1H\", null, null, false);\n    addSomeEvents(1);\n\n    Task id1 = createKafkaIndexTask(\n        \"id1\",\n        DATASOURCE,\n        0,\n        new KafkaPartitions(\"topic\", ImmutableMap.of(0, 0L, 1, 0L, 2, 0L)),\n        new KafkaPartitions(\"topic\", ImmutableMap.of(0, Long.MAX_VALUE, 1, Long.MAX_VALUE, 2, Long.MAX_VALUE)),\n        null,\n        null\n    );\n\n    Task id2 = createKafkaIndexTask(\n        \"id2\",\n        DATASOURCE,\n        0,\n        new KafkaPartitions(\"topic\", ImmutableMap.of(0, 10L, 1, 20L, 2, 30L)),\n        new KafkaPartitions(\"topic\", ImmutableMap.of(0, Long.MAX_VALUE, 1, Long.MAX_VALUE, 2, Long.MAX_VALUE)),\n        null,\n        null\n    );\n\n    Task id3 = createKafkaIndexTask(\n        \"id3\",\n        DATASOURCE,\n        0,\n        new KafkaPartitions(\"topic\", ImmutableMap.of(0, 10L, 1, 20L, 2, 30L)),\n        new KafkaPartitions(\"topic\", ImmutableMap.of(0, Long.MAX_VALUE, 1, Long.MAX_VALUE, 2, Long.MAX_VALUE)),\n        null,\n        null\n    );\n\n    Collection workItems = new ArrayList<>();\n    workItems.add(new TestTaskRunnerWorkItem(id1, null, location1));\n    workItems.add(new TestTaskRunnerWorkItem(id2, null, location2));\n\n    expect(taskMaster.getTaskQueue()).andReturn(Optional.of(taskQueue)).anyTimes();\n    expect(taskMaster.getTaskRunner()).andReturn(Optional.of(taskRunner)).anyTimes();\n    expect(taskRunner.getRunningTasks()).andReturn(workItems).anyTimes();\n    expect(taskStorage.getActiveTasks()).andReturn(ImmutableList.of(id1, id2, id3)).anyTimes();\n    expect(taskStorage.getStatus(\"id1\")).andReturn(Optional.of(TaskStatus.running(\"id1\"))).anyTimes();\n    expect(taskStorage.getStatus(\"id2\")).andReturn(Optional.of(TaskStatus.running(\"id2\"))).anyTimes();\n    expect(taskStorage.getStatus(\"id3\")).andReturn(Optional.of(TaskStatus.running(\"id3\"))).anyTimes();\n    expect(taskStorage.getTask(\"id1\")).andReturn(Optional.of(id1)).anyTimes();\n    expect(taskStorage.getTask(\"id2\")).andReturn(Optional.of(id2)).anyTimes();\n    expect(taskStorage.getTask(\"id3\")).andReturn(Optional.of(id3)).anyTimes();\n    expect(indexerMetadataStorageCoordinator.getDataSourceMetadata(DATASOURCE)).andReturn(\n        new KafkaDataSourceMetadata(\n            null\n        )\n    ).anyTimes();\n    expect(taskClient.getStatusAsync(\"id1\")).andReturn(Futures.immediateFuture(KafkaIndexTask.Status.PUBLISHING));\n    expect(taskClient.getStatusAsync(\"id2\")).andReturn(Futures.immediateFuture(KafkaIndexTask.Status.READING));\n    expect(taskClient.getStatusAsync(\"id3\")).andReturn(Futures.immediateFuture(KafkaIndexTask.Status.READING));\n    expect(taskClient.getStartTimeAsync(\"id2\")).andReturn(Futures.immediateFuture(startTime));\n    expect(taskClient.getStartTimeAsync(\"id3\")).andReturn(Futures.immediateFuture(startTime));\n    expect(taskClient.getEndOffsets(\"id1\")).andReturn(ImmutableMap.of(0, 10L, 1, 20L, 2, 30L));\n\n    \r\n    TreeMap<Integer, Map<Integer, Long>> checkpoints = new TreeMap<>();\n    checkpoints.put(0, ImmutableMap.of(0, 10L, 1, 20L, 2, 30L));\n    expect(taskClient.getCheckpointsAsync(EasyMock.contains(\"id2\"), anyBoolean()))\n        .andReturn(Futures.immediateFuture(checkpoints))\n        .times(1);\n    expect(taskClient.getCheckpointsAsync(EasyMock.contains(\"id3\"), anyBoolean()))\n        .andReturn(Futures.immediateFuture(checkpoints))\n        .times(1);\n\n    taskRunner.registerListener(anyObject(TaskRunnerListener.class), anyObject(Executor.class));\n    replayAll();\n\n    supervisor.start();\n    supervisor.runInternal();\n    verifyAll();\n\n    reset(taskRunner, taskClient, taskQueue);\n    expect(taskRunner.getRunningTasks()).andReturn(workItems).anyTimes();\n    expect(taskClient.pauseAsync(\"id2\"))\n        .andReturn(Futures.immediateFuture((Map<Integer, Long>) ImmutableMap.of(0, 15L, 1, 25L, 2, 30L)));\n    expect(taskClient.setEndOffsetsAsync(\"id2\", ImmutableMap.of(0, 15L, 1, 25L, 2, 30L), true))\n        .andReturn(Futures.immediateFuture(true));\n    taskQueue.shutdown(\"id3\");\n    expectLastCall().times(2);\n\n    replay(taskRunner, taskClient, taskQueue);\n\n    supervisor.gracefulShutdownInternal();\n    verifyAll();\n  }\n","date":"2018-08-31 00:56:26","endLine":1739,"groupId":"5276","id":1,"instanceNumber":1,"isCurCommit":0,"methodName":"testStopGracefully","params":"()","path":"/mnt/clonedata/CloneManagementServer/ManagementServer/consistResult/result-druid-10-0.7/blobInfo/CC_OUT/blobs/5d/773db661d0c8e87703ae229b128551316998e5.src","preCode":"  public void testStopGracefully() throws Exception\n  {\n    final TaskLocation location1 = new TaskLocation(\"testHost\", 1234, -1);\n    final TaskLocation location2 = new TaskLocation(\"testHost2\", 145, -1);\n    final DateTime startTime = DateTimes.nowUtc();\n\n    supervisor = getSupervisor(2, 1, true, \"PT1H\", null, null, false);\n    addSomeEvents(1);\n\n    Task id1 = createKafkaIndexTask(\n        \"id1\",\n        DATASOURCE,\n        0,\n        new KafkaPartitions(\"topic\", ImmutableMap.of(0, 0L, 1, 0L, 2, 0L)),\n        new KafkaPartitions(\"topic\", ImmutableMap.of(0, Long.MAX_VALUE, 1, Long.MAX_VALUE, 2, Long.MAX_VALUE)),\n        null,\n        null\n    );\n\n    Task id2 = createKafkaIndexTask(\n        \"id2\",\n        DATASOURCE,\n        0,\n        new KafkaPartitions(\"topic\", ImmutableMap.of(0, 10L, 1, 20L, 2, 30L)),\n        new KafkaPartitions(\"topic\", ImmutableMap.of(0, Long.MAX_VALUE, 1, Long.MAX_VALUE, 2, Long.MAX_VALUE)),\n        null,\n        null\n    );\n\n    Task id3 = createKafkaIndexTask(\n        \"id3\",\n        DATASOURCE,\n        0,\n        new KafkaPartitions(\"topic\", ImmutableMap.of(0, 10L, 1, 20L, 2, 30L)),\n        new KafkaPartitions(\"topic\", ImmutableMap.of(0, Long.MAX_VALUE, 1, Long.MAX_VALUE, 2, Long.MAX_VALUE)),\n        null,\n        null\n    );\n\n    Collection workItems = new ArrayList<>();\n    workItems.add(new TestTaskRunnerWorkItem(id1, null, location1));\n    workItems.add(new TestTaskRunnerWorkItem(id2, null, location2));\n\n    expect(taskMaster.getTaskQueue()).andReturn(Optional.of(taskQueue)).anyTimes();\n    expect(taskMaster.getTaskRunner()).andReturn(Optional.of(taskRunner)).anyTimes();\n    expect(taskRunner.getRunningTasks()).andReturn(workItems).anyTimes();\n    expect(taskStorage.getActiveTasks()).andReturn(ImmutableList.of(id1, id2, id3)).anyTimes();\n    expect(taskStorage.getStatus(\"id1\")).andReturn(Optional.of(TaskStatus.running(\"id1\"))).anyTimes();\n    expect(taskStorage.getStatus(\"id2\")).andReturn(Optional.of(TaskStatus.running(\"id2\"))).anyTimes();\n    expect(taskStorage.getStatus(\"id3\")).andReturn(Optional.of(TaskStatus.running(\"id3\"))).anyTimes();\n    expect(taskStorage.getTask(\"id1\")).andReturn(Optional.of(id1)).anyTimes();\n    expect(taskStorage.getTask(\"id2\")).andReturn(Optional.of(id2)).anyTimes();\n    expect(taskStorage.getTask(\"id3\")).andReturn(Optional.of(id3)).anyTimes();\n    expect(indexerMetadataStorageCoordinator.getDataSourceMetadata(DATASOURCE)).andReturn(\n        new KafkaDataSourceMetadata(\n            null\n        )\n    ).anyTimes();\n    expect(taskClient.getStatusAsync(\"id1\")).andReturn(Futures.immediateFuture(KafkaIndexTask.Status.PUBLISHING));\n    expect(taskClient.getStatusAsync(\"id2\")).andReturn(Futures.immediateFuture(KafkaIndexTask.Status.READING));\n    expect(taskClient.getStatusAsync(\"id3\")).andReturn(Futures.immediateFuture(KafkaIndexTask.Status.READING));\n    expect(taskClient.getStartTimeAsync(\"id2\")).andReturn(Futures.immediateFuture(startTime));\n    expect(taskClient.getStartTimeAsync(\"id3\")).andReturn(Futures.immediateFuture(startTime));\n    expect(taskClient.getEndOffsets(\"id1\")).andReturn(ImmutableMap.of(0, 10L, 1, 20L, 2, 30L));\n\n    \r\n    TreeMap<Integer, Map<Integer, Long>> checkpoints = new TreeMap<>();\n    checkpoints.put(0, ImmutableMap.of(0, 10L, 1, 20L, 2, 30L));\n    expect(taskClient.getCheckpointsAsync(EasyMock.contains(\"id2\"), anyBoolean()))\n        .andReturn(Futures.immediateFuture(checkpoints))\n        .times(1);\n    expect(taskClient.getCheckpointsAsync(EasyMock.contains(\"id3\"), anyBoolean()))\n        .andReturn(Futures.immediateFuture(checkpoints))\n        .times(1);\n\n    taskRunner.registerListener(anyObject(TaskRunnerListener.class), anyObject(Executor.class));\n    replayAll();\n\n    supervisor.start();\n    supervisor.runInternal();\n    verifyAll();\n\n    reset(taskRunner, taskClient, taskQueue);\n    expect(taskRunner.getRunningTasks()).andReturn(workItems).anyTimes();\n    expect(taskClient.pauseAsync(\"id2\"))\n        .andReturn(Futures.immediateFuture((Map<Integer, Long>) ImmutableMap.of(0, 15L, 1, 25L, 2, 30L)));\n    expect(taskClient.setEndOffsetsAsync(\"id2\", ImmutableMap.of(0, 15L, 1, 25L, 2, 30L), true))\n        .andReturn(Futures.immediateFuture(true));\n    taskQueue.shutdown(\"id3\");\n    expectLastCall().times(2);\n\n    replay(taskRunner, taskClient, taskQueue);\n\n    supervisor.gracefulShutdownInternal();\n    verifyAll();\n  }\n","realPath":"extensions-core/kafka-indexing-service/src/test/java/org/apache/druid/indexing/kafka/supervisor/KafkaSupervisorTest.java","repoName":"druid","snippetEndLine":0,"snippetStartLine":0,"startLine":1644,"status":"NB"},{"authorDate":"2018-09-14 05:42:18","commitOrder":2,"curCode":"  public void testSuspendedRunningTasks() throws Exception\n  {\n    \r\n\n    final TaskLocation location1 = new TaskLocation(\"testHost\", 1234, -1);\n    final TaskLocation location2 = new TaskLocation(\"testHost2\", 145, -1);\n    final DateTime startTime = DateTimes.nowUtc();\n\n    supervisor = getSupervisor(2, 1, true, \"PT1H\", null, null, false, true);\n    addSomeEvents(1);\n\n    Task id1 = createKafkaIndexTask(\n        \"id1\",\n        DATASOURCE,\n        0,\n        new KafkaPartitions(\"topic\", ImmutableMap.of(0, 0L, 1, 0L, 2, 0L)),\n        new KafkaPartitions(\"topic\", ImmutableMap.of(0, Long.MAX_VALUE, 1, Long.MAX_VALUE, 2, Long.MAX_VALUE)),\n        null,\n        null\n    );\n\n    Task id2 = createKafkaIndexTask(\n        \"id2\",\n        DATASOURCE,\n        0,\n        new KafkaPartitions(\"topic\", ImmutableMap.of(0, 10L, 1, 20L, 2, 30L)),\n        new KafkaPartitions(\"topic\", ImmutableMap.of(0, Long.MAX_VALUE, 1, Long.MAX_VALUE, 2, Long.MAX_VALUE)),\n        null,\n        null\n    );\n\n    Task id3 = createKafkaIndexTask(\n        \"id3\",\n        DATASOURCE,\n        0,\n        new KafkaPartitions(\"topic\", ImmutableMap.of(0, 10L, 1, 20L, 2, 30L)),\n        new KafkaPartitions(\"topic\", ImmutableMap.of(0, Long.MAX_VALUE, 1, Long.MAX_VALUE, 2, Long.MAX_VALUE)),\n        null,\n        null\n    );\n\n    Collection workItems = new ArrayList<>();\n    workItems.add(new TestTaskRunnerWorkItem(id1, null, location1));\n    workItems.add(new TestTaskRunnerWorkItem(id2, null, location2));\n\n    expect(taskMaster.getTaskQueue()).andReturn(Optional.of(taskQueue)).anyTimes();\n    expect(taskMaster.getTaskRunner()).andReturn(Optional.of(taskRunner)).anyTimes();\n    expect(taskRunner.getRunningTasks()).andReturn(workItems).anyTimes();\n    expect(taskStorage.getActiveTasks()).andReturn(ImmutableList.of(id1, id2, id3)).anyTimes();\n    expect(taskStorage.getStatus(\"id1\")).andReturn(Optional.of(TaskStatus.running(\"id1\"))).anyTimes();\n    expect(taskStorage.getStatus(\"id2\")).andReturn(Optional.of(TaskStatus.running(\"id2\"))).anyTimes();\n    expect(taskStorage.getStatus(\"id3\")).andReturn(Optional.of(TaskStatus.running(\"id3\"))).anyTimes();\n    expect(taskStorage.getTask(\"id1\")).andReturn(Optional.of(id1)).anyTimes();\n    expect(taskStorage.getTask(\"id2\")).andReturn(Optional.of(id2)).anyTimes();\n    expect(taskStorage.getTask(\"id3\")).andReturn(Optional.of(id3)).anyTimes();\n    expect(indexerMetadataStorageCoordinator.getDataSourceMetadata(DATASOURCE)).andReturn(\n        new KafkaDataSourceMetadata(\n            null\n        )\n    ).anyTimes();\n    expect(taskClient.getStatusAsync(\"id1\")).andReturn(Futures.immediateFuture(KafkaIndexTask.Status.PUBLISHING));\n    expect(taskClient.getStatusAsync(\"id2\")).andReturn(Futures.immediateFuture(KafkaIndexTask.Status.READING));\n    expect(taskClient.getStatusAsync(\"id3\")).andReturn(Futures.immediateFuture(KafkaIndexTask.Status.READING));\n    expect(taskClient.getStartTimeAsync(\"id2\")).andReturn(Futures.immediateFuture(startTime));\n    expect(taskClient.getStartTimeAsync(\"id3\")).andReturn(Futures.immediateFuture(startTime));\n    expect(taskClient.getEndOffsets(\"id1\")).andReturn(ImmutableMap.of(0, 10L, 1, 20L, 2, 30L));\n\n    \r\n    TreeMap<Integer, Map<Integer, Long>> checkpoints = new TreeMap<>();\n    checkpoints.put(0, ImmutableMap.of(0, 10L, 1, 20L, 2, 30L));\n    expect(taskClient.getCheckpointsAsync(EasyMock.contains(\"id2\"), anyBoolean()))\n        .andReturn(Futures.immediateFuture(checkpoints))\n        .times(1);\n    expect(taskClient.getCheckpointsAsync(EasyMock.contains(\"id3\"), anyBoolean()))\n        .andReturn(Futures.immediateFuture(checkpoints))\n        .times(1);\n\n    taskRunner.registerListener(anyObject(TaskRunnerListener.class), anyObject(Executor.class));\n\n    expect(taskClient.pauseAsync(\"id2\"))\n        .andReturn(Futures.immediateFuture(ImmutableMap.of(0, 15L, 1, 25L, 2, 30L)));\n    expect(taskClient.setEndOffsetsAsync(\"id2\", ImmutableMap.of(0, 15L, 1, 25L, 2, 30L), true))\n        .andReturn(Futures.immediateFuture(true));\n    taskQueue.shutdown(\"id3\");\n    expectLastCall().times(2);\n\n    replayAll();\n    supervisor.start();\n    supervisor.runInternal();\n    verifyAll();\n  }\n","date":"2018-09-14 05:42:18","endLine":2415,"groupId":"5276","id":2,"instanceNumber":2,"isCurCommit":0,"methodName":"testSuspendedRunningTasks","params":"()","path":"/mnt/clonedata/CloneManagementServer/ManagementServer/consistResult/result-druid-10-0.7/blobInfo/CC_OUT/blobs/97/8de2f6886f266967316122d333a4c4ceee74f2.src","preCode":"  public void testSuspendedRunningTasks() throws Exception\n  {\n    \r\n\n    final TaskLocation location1 = new TaskLocation(\"testHost\", 1234, -1);\n    final TaskLocation location2 = new TaskLocation(\"testHost2\", 145, -1);\n    final DateTime startTime = DateTimes.nowUtc();\n\n    supervisor = getSupervisor(2, 1, true, \"PT1H\", null, null, false, true);\n    addSomeEvents(1);\n\n    Task id1 = createKafkaIndexTask(\n        \"id1\",\n        DATASOURCE,\n        0,\n        new KafkaPartitions(\"topic\", ImmutableMap.of(0, 0L, 1, 0L, 2, 0L)),\n        new KafkaPartitions(\"topic\", ImmutableMap.of(0, Long.MAX_VALUE, 1, Long.MAX_VALUE, 2, Long.MAX_VALUE)),\n        null,\n        null\n    );\n\n    Task id2 = createKafkaIndexTask(\n        \"id2\",\n        DATASOURCE,\n        0,\n        new KafkaPartitions(\"topic\", ImmutableMap.of(0, 10L, 1, 20L, 2, 30L)),\n        new KafkaPartitions(\"topic\", ImmutableMap.of(0, Long.MAX_VALUE, 1, Long.MAX_VALUE, 2, Long.MAX_VALUE)),\n        null,\n        null\n    );\n\n    Task id3 = createKafkaIndexTask(\n        \"id3\",\n        DATASOURCE,\n        0,\n        new KafkaPartitions(\"topic\", ImmutableMap.of(0, 10L, 1, 20L, 2, 30L)),\n        new KafkaPartitions(\"topic\", ImmutableMap.of(0, Long.MAX_VALUE, 1, Long.MAX_VALUE, 2, Long.MAX_VALUE)),\n        null,\n        null\n    );\n\n    Collection workItems = new ArrayList<>();\n    workItems.add(new TestTaskRunnerWorkItem(id1, null, location1));\n    workItems.add(new TestTaskRunnerWorkItem(id2, null, location2));\n\n    expect(taskMaster.getTaskQueue()).andReturn(Optional.of(taskQueue)).anyTimes();\n    expect(taskMaster.getTaskRunner()).andReturn(Optional.of(taskRunner)).anyTimes();\n    expect(taskRunner.getRunningTasks()).andReturn(workItems).anyTimes();\n    expect(taskStorage.getActiveTasks()).andReturn(ImmutableList.of(id1, id2, id3)).anyTimes();\n    expect(taskStorage.getStatus(\"id1\")).andReturn(Optional.of(TaskStatus.running(\"id1\"))).anyTimes();\n    expect(taskStorage.getStatus(\"id2\")).andReturn(Optional.of(TaskStatus.running(\"id2\"))).anyTimes();\n    expect(taskStorage.getStatus(\"id3\")).andReturn(Optional.of(TaskStatus.running(\"id3\"))).anyTimes();\n    expect(taskStorage.getTask(\"id1\")).andReturn(Optional.of(id1)).anyTimes();\n    expect(taskStorage.getTask(\"id2\")).andReturn(Optional.of(id2)).anyTimes();\n    expect(taskStorage.getTask(\"id3\")).andReturn(Optional.of(id3)).anyTimes();\n    expect(indexerMetadataStorageCoordinator.getDataSourceMetadata(DATASOURCE)).andReturn(\n        new KafkaDataSourceMetadata(\n            null\n        )\n    ).anyTimes();\n    expect(taskClient.getStatusAsync(\"id1\")).andReturn(Futures.immediateFuture(KafkaIndexTask.Status.PUBLISHING));\n    expect(taskClient.getStatusAsync(\"id2\")).andReturn(Futures.immediateFuture(KafkaIndexTask.Status.READING));\n    expect(taskClient.getStatusAsync(\"id3\")).andReturn(Futures.immediateFuture(KafkaIndexTask.Status.READING));\n    expect(taskClient.getStartTimeAsync(\"id2\")).andReturn(Futures.immediateFuture(startTime));\n    expect(taskClient.getStartTimeAsync(\"id3\")).andReturn(Futures.immediateFuture(startTime));\n    expect(taskClient.getEndOffsets(\"id1\")).andReturn(ImmutableMap.of(0, 10L, 1, 20L, 2, 30L));\n\n    \r\n    TreeMap<Integer, Map<Integer, Long>> checkpoints = new TreeMap<>();\n    checkpoints.put(0, ImmutableMap.of(0, 10L, 1, 20L, 2, 30L));\n    expect(taskClient.getCheckpointsAsync(EasyMock.contains(\"id2\"), anyBoolean()))\n        .andReturn(Futures.immediateFuture(checkpoints))\n        .times(1);\n    expect(taskClient.getCheckpointsAsync(EasyMock.contains(\"id3\"), anyBoolean()))\n        .andReturn(Futures.immediateFuture(checkpoints))\n        .times(1);\n\n    taskRunner.registerListener(anyObject(TaskRunnerListener.class), anyObject(Executor.class));\n\n    expect(taskClient.pauseAsync(\"id2\"))\n        .andReturn(Futures.immediateFuture(ImmutableMap.of(0, 15L, 1, 25L, 2, 30L)));\n    expect(taskClient.setEndOffsetsAsync(\"id2\", ImmutableMap.of(0, 15L, 1, 25L, 2, 30L), true))\n        .andReturn(Futures.immediateFuture(true));\n    taskQueue.shutdown(\"id3\");\n    expectLastCall().times(2);\n\n    replayAll();\n    supervisor.start();\n    supervisor.runInternal();\n    verifyAll();\n  }\n","realPath":"extensions-core/kafka-indexing-service/src/test/java/org/apache/druid/indexing/kafka/supervisor/KafkaSupervisorTest.java","repoName":"druid","snippetEndLine":0,"snippetStartLine":0,"startLine":2325,"status":"B"}],"commitId":"91a37c692dd8db34d6e4323cd61dba38a00145b1","commitMessage":"@@@'suspend' and 'resume' support for supervisors (kafka indexing service.  materialized views) (#6234)\n\n* 'suspend' and 'resume' support for kafka indexing service\nchanges:\n* introduces `SuspendableSupervisorSpec` interface to describe supervisors which support suspend/resume functionality controlled through the `SupervisorManager`.  which will gracefully shutdown the supervisor and it's tasks.  update it's `SupervisorSpec` with either a suspended or running state.  and update with the toggled spec. Spec updates are provided by `SuspendableSupervisorSpec.createSuspendedSpec` and `SuspendableSupervisorSpec.createRunningSpec` respectively.\n* `KafkaSupervisorSpec` extends `SuspendableSupervisorSpec` and now supports suspend/resume functionality. The difference in behavior between 'running' and 'suspended' state is whether the supervisor will attempt to ensure that indexing tasks are or are not running respectively. Behavior is identical otherwise.\n* `SupervisorResource` now provides `/druid/indexer/v1/supervisor/{id}/suspend` and `/druid/indexer/v1/supervisor/{id}/resume` which are used to suspend/resume suspendable supervisors\n* Deprecated `/druid/indexer/v1/supervisor/{id}/shutdown` and moved it's functionality to `/druid/indexer/v1/supervisor/{id}/terminate` since 'shutdown' is ambiguous verbage for something that effectively stops a supervisor forever\n* Added ability to get all supervisor specs from `/druid/indexer/v1/supervisor` by supplying the 'full' query parameter `/druid/indexer/v1/supervisor?full` which will return a list of json objects of the form `{\"id\":<id>.  \"spec\":<SupervisorSpec>}`\n* Updated overlord console ui to enable suspend/resume.  and changed 'shutdown' to 'terminate'\n\n* move overlord console status to own column in supervisor table so does not look like garbage\n\n* spacing\n\n* padding\n\n* other kind of spacing\n\n* fix rebase fail\n\n* fix more better\n\n* all supervisors now suspendable.  updated materialized view supervisor to support suspend.  more tests\n\n* fix log\n","date":"2018-09-14 05:42:18","modifiedFileCount":"16","status":"M","submitter":"Clint Wylie"},{"authorTime":"2018-10-04 10:08:20","codes":[{"authorDate":"2018-08-31 00:56:26","commitOrder":3,"curCode":"  public void testStopGracefully() throws Exception\n  {\n    final TaskLocation location1 = new TaskLocation(\"testHost\", 1234, -1);\n    final TaskLocation location2 = new TaskLocation(\"testHost2\", 145, -1);\n    final DateTime startTime = DateTimes.nowUtc();\n\n    supervisor = getSupervisor(2, 1, true, \"PT1H\", null, null, false);\n    addSomeEvents(1);\n\n    Task id1 = createKafkaIndexTask(\n        \"id1\",\n        DATASOURCE,\n        0,\n        new KafkaPartitions(\"topic\", ImmutableMap.of(0, 0L, 1, 0L, 2, 0L)),\n        new KafkaPartitions(\"topic\", ImmutableMap.of(0, Long.MAX_VALUE, 1, Long.MAX_VALUE, 2, Long.MAX_VALUE)),\n        null,\n        null\n    );\n\n    Task id2 = createKafkaIndexTask(\n        \"id2\",\n        DATASOURCE,\n        0,\n        new KafkaPartitions(\"topic\", ImmutableMap.of(0, 10L, 1, 20L, 2, 30L)),\n        new KafkaPartitions(\"topic\", ImmutableMap.of(0, Long.MAX_VALUE, 1, Long.MAX_VALUE, 2, Long.MAX_VALUE)),\n        null,\n        null\n    );\n\n    Task id3 = createKafkaIndexTask(\n        \"id3\",\n        DATASOURCE,\n        0,\n        new KafkaPartitions(\"topic\", ImmutableMap.of(0, 10L, 1, 20L, 2, 30L)),\n        new KafkaPartitions(\"topic\", ImmutableMap.of(0, Long.MAX_VALUE, 1, Long.MAX_VALUE, 2, Long.MAX_VALUE)),\n        null,\n        null\n    );\n\n    Collection workItems = new ArrayList<>();\n    workItems.add(new TestTaskRunnerWorkItem(id1, null, location1));\n    workItems.add(new TestTaskRunnerWorkItem(id2, null, location2));\n\n    expect(taskMaster.getTaskQueue()).andReturn(Optional.of(taskQueue)).anyTimes();\n    expect(taskMaster.getTaskRunner()).andReturn(Optional.of(taskRunner)).anyTimes();\n    expect(taskRunner.getRunningTasks()).andReturn(workItems).anyTimes();\n    expect(taskStorage.getActiveTasks()).andReturn(ImmutableList.of(id1, id2, id3)).anyTimes();\n    expect(taskStorage.getStatus(\"id1\")).andReturn(Optional.of(TaskStatus.running(\"id1\"))).anyTimes();\n    expect(taskStorage.getStatus(\"id2\")).andReturn(Optional.of(TaskStatus.running(\"id2\"))).anyTimes();\n    expect(taskStorage.getStatus(\"id3\")).andReturn(Optional.of(TaskStatus.running(\"id3\"))).anyTimes();\n    expect(taskStorage.getTask(\"id1\")).andReturn(Optional.of(id1)).anyTimes();\n    expect(taskStorage.getTask(\"id2\")).andReturn(Optional.of(id2)).anyTimes();\n    expect(taskStorage.getTask(\"id3\")).andReturn(Optional.of(id3)).anyTimes();\n    expect(indexerMetadataStorageCoordinator.getDataSourceMetadata(DATASOURCE)).andReturn(\n        new KafkaDataSourceMetadata(\n            null\n        )\n    ).anyTimes();\n    expect(taskClient.getStatusAsync(\"id1\")).andReturn(Futures.immediateFuture(KafkaIndexTask.Status.PUBLISHING));\n    expect(taskClient.getStatusAsync(\"id2\")).andReturn(Futures.immediateFuture(KafkaIndexTask.Status.READING));\n    expect(taskClient.getStatusAsync(\"id3\")).andReturn(Futures.immediateFuture(KafkaIndexTask.Status.READING));\n    expect(taskClient.getStartTimeAsync(\"id2\")).andReturn(Futures.immediateFuture(startTime));\n    expect(taskClient.getStartTimeAsync(\"id3\")).andReturn(Futures.immediateFuture(startTime));\n    expect(taskClient.getEndOffsets(\"id1\")).andReturn(ImmutableMap.of(0, 10L, 1, 20L, 2, 30L));\n\n    \r\n    TreeMap<Integer, Map<Integer, Long>> checkpoints = new TreeMap<>();\n    checkpoints.put(0, ImmutableMap.of(0, 10L, 1, 20L, 2, 30L));\n    expect(taskClient.getCheckpointsAsync(EasyMock.contains(\"id2\"), anyBoolean()))\n        .andReturn(Futures.immediateFuture(checkpoints))\n        .times(1);\n    expect(taskClient.getCheckpointsAsync(EasyMock.contains(\"id3\"), anyBoolean()))\n        .andReturn(Futures.immediateFuture(checkpoints))\n        .times(1);\n\n    taskRunner.registerListener(anyObject(TaskRunnerListener.class), anyObject(Executor.class));\n    replayAll();\n\n    supervisor.start();\n    supervisor.runInternal();\n    verifyAll();\n\n    reset(taskRunner, taskClient, taskQueue);\n    expect(taskRunner.getRunningTasks()).andReturn(workItems).anyTimes();\n    expect(taskClient.pauseAsync(\"id2\"))\n        .andReturn(Futures.immediateFuture((Map<Integer, Long>) ImmutableMap.of(0, 15L, 1, 25L, 2, 30L)));\n    expect(taskClient.setEndOffsetsAsync(\"id2\", ImmutableMap.of(0, 15L, 1, 25L, 2, 30L), true))\n        .andReturn(Futures.immediateFuture(true));\n    taskQueue.shutdown(\"id3\");\n    expectLastCall().times(2);\n\n    replay(taskRunner, taskClient, taskQueue);\n\n    supervisor.gracefulShutdownInternal();\n    verifyAll();\n  }\n","date":"2018-08-31 00:56:26","endLine":1739,"groupId":"5276","id":3,"instanceNumber":1,"isCurCommit":0,"methodName":"testStopGracefully","params":"()","path":"/mnt/clonedata/CloneManagementServer/ManagementServer/consistResult/result-druid-10-0.7/blobInfo/CC_OUT/blobs/5d/773db661d0c8e87703ae229b128551316998e5.src","preCode":"  public void testStopGracefully() throws Exception\n  {\n    final TaskLocation location1 = new TaskLocation(\"testHost\", 1234, -1);\n    final TaskLocation location2 = new TaskLocation(\"testHost2\", 145, -1);\n    final DateTime startTime = DateTimes.nowUtc();\n\n    supervisor = getSupervisor(2, 1, true, \"PT1H\", null, null, false);\n    addSomeEvents(1);\n\n    Task id1 = createKafkaIndexTask(\n        \"id1\",\n        DATASOURCE,\n        0,\n        new KafkaPartitions(\"topic\", ImmutableMap.of(0, 0L, 1, 0L, 2, 0L)),\n        new KafkaPartitions(\"topic\", ImmutableMap.of(0, Long.MAX_VALUE, 1, Long.MAX_VALUE, 2, Long.MAX_VALUE)),\n        null,\n        null\n    );\n\n    Task id2 = createKafkaIndexTask(\n        \"id2\",\n        DATASOURCE,\n        0,\n        new KafkaPartitions(\"topic\", ImmutableMap.of(0, 10L, 1, 20L, 2, 30L)),\n        new KafkaPartitions(\"topic\", ImmutableMap.of(0, Long.MAX_VALUE, 1, Long.MAX_VALUE, 2, Long.MAX_VALUE)),\n        null,\n        null\n    );\n\n    Task id3 = createKafkaIndexTask(\n        \"id3\",\n        DATASOURCE,\n        0,\n        new KafkaPartitions(\"topic\", ImmutableMap.of(0, 10L, 1, 20L, 2, 30L)),\n        new KafkaPartitions(\"topic\", ImmutableMap.of(0, Long.MAX_VALUE, 1, Long.MAX_VALUE, 2, Long.MAX_VALUE)),\n        null,\n        null\n    );\n\n    Collection workItems = new ArrayList<>();\n    workItems.add(new TestTaskRunnerWorkItem(id1, null, location1));\n    workItems.add(new TestTaskRunnerWorkItem(id2, null, location2));\n\n    expect(taskMaster.getTaskQueue()).andReturn(Optional.of(taskQueue)).anyTimes();\n    expect(taskMaster.getTaskRunner()).andReturn(Optional.of(taskRunner)).anyTimes();\n    expect(taskRunner.getRunningTasks()).andReturn(workItems).anyTimes();\n    expect(taskStorage.getActiveTasks()).andReturn(ImmutableList.of(id1, id2, id3)).anyTimes();\n    expect(taskStorage.getStatus(\"id1\")).andReturn(Optional.of(TaskStatus.running(\"id1\"))).anyTimes();\n    expect(taskStorage.getStatus(\"id2\")).andReturn(Optional.of(TaskStatus.running(\"id2\"))).anyTimes();\n    expect(taskStorage.getStatus(\"id3\")).andReturn(Optional.of(TaskStatus.running(\"id3\"))).anyTimes();\n    expect(taskStorage.getTask(\"id1\")).andReturn(Optional.of(id1)).anyTimes();\n    expect(taskStorage.getTask(\"id2\")).andReturn(Optional.of(id2)).anyTimes();\n    expect(taskStorage.getTask(\"id3\")).andReturn(Optional.of(id3)).anyTimes();\n    expect(indexerMetadataStorageCoordinator.getDataSourceMetadata(DATASOURCE)).andReturn(\n        new KafkaDataSourceMetadata(\n            null\n        )\n    ).anyTimes();\n    expect(taskClient.getStatusAsync(\"id1\")).andReturn(Futures.immediateFuture(KafkaIndexTask.Status.PUBLISHING));\n    expect(taskClient.getStatusAsync(\"id2\")).andReturn(Futures.immediateFuture(KafkaIndexTask.Status.READING));\n    expect(taskClient.getStatusAsync(\"id3\")).andReturn(Futures.immediateFuture(KafkaIndexTask.Status.READING));\n    expect(taskClient.getStartTimeAsync(\"id2\")).andReturn(Futures.immediateFuture(startTime));\n    expect(taskClient.getStartTimeAsync(\"id3\")).andReturn(Futures.immediateFuture(startTime));\n    expect(taskClient.getEndOffsets(\"id1\")).andReturn(ImmutableMap.of(0, 10L, 1, 20L, 2, 30L));\n\n    \r\n    TreeMap<Integer, Map<Integer, Long>> checkpoints = new TreeMap<>();\n    checkpoints.put(0, ImmutableMap.of(0, 10L, 1, 20L, 2, 30L));\n    expect(taskClient.getCheckpointsAsync(EasyMock.contains(\"id2\"), anyBoolean()))\n        .andReturn(Futures.immediateFuture(checkpoints))\n        .times(1);\n    expect(taskClient.getCheckpointsAsync(EasyMock.contains(\"id3\"), anyBoolean()))\n        .andReturn(Futures.immediateFuture(checkpoints))\n        .times(1);\n\n    taskRunner.registerListener(anyObject(TaskRunnerListener.class), anyObject(Executor.class));\n    replayAll();\n\n    supervisor.start();\n    supervisor.runInternal();\n    verifyAll();\n\n    reset(taskRunner, taskClient, taskQueue);\n    expect(taskRunner.getRunningTasks()).andReturn(workItems).anyTimes();\n    expect(taskClient.pauseAsync(\"id2\"))\n        .andReturn(Futures.immediateFuture((Map<Integer, Long>) ImmutableMap.of(0, 15L, 1, 25L, 2, 30L)));\n    expect(taskClient.setEndOffsetsAsync(\"id2\", ImmutableMap.of(0, 15L, 1, 25L, 2, 30L), true))\n        .andReturn(Futures.immediateFuture(true));\n    taskQueue.shutdown(\"id3\");\n    expectLastCall().times(2);\n\n    replay(taskRunner, taskClient, taskQueue);\n\n    supervisor.gracefulShutdownInternal();\n    verifyAll();\n  }\n","realPath":"extensions-core/kafka-indexing-service/src/test/java/org/apache/druid/indexing/kafka/supervisor/KafkaSupervisorTest.java","repoName":"druid","snippetEndLine":0,"snippetStartLine":0,"startLine":1644,"status":"N"},{"authorDate":"2018-10-04 10:08:20","commitOrder":3,"curCode":"  public void testSuspendedRunningTasks() throws Exception\n  {\n    \r\n\n    final TaskLocation location1 = new TaskLocation(\"testHost\", 1234, -1);\n    final TaskLocation location2 = new TaskLocation(\"testHost2\", 145, -1);\n    final DateTime startTime = DateTimes.nowUtc();\n\n    supervisor = getSupervisor(2, 1, true, \"PT1H\", null, null, false, true, kafkaHost);\n    addSomeEvents(1);\n\n    Task id1 = createKafkaIndexTask(\n        \"id1\",\n        DATASOURCE,\n        0,\n        new KafkaPartitions(\"topic\", ImmutableMap.of(0, 0L, 1, 0L, 2, 0L)),\n        new KafkaPartitions(\"topic\", ImmutableMap.of(0, Long.MAX_VALUE, 1, Long.MAX_VALUE, 2, Long.MAX_VALUE)),\n        null,\n        null\n    );\n\n    Task id2 = createKafkaIndexTask(\n        \"id2\",\n        DATASOURCE,\n        0,\n        new KafkaPartitions(\"topic\", ImmutableMap.of(0, 10L, 1, 20L, 2, 30L)),\n        new KafkaPartitions(\"topic\", ImmutableMap.of(0, Long.MAX_VALUE, 1, Long.MAX_VALUE, 2, Long.MAX_VALUE)),\n        null,\n        null\n    );\n\n    Task id3 = createKafkaIndexTask(\n        \"id3\",\n        DATASOURCE,\n        0,\n        new KafkaPartitions(\"topic\", ImmutableMap.of(0, 10L, 1, 20L, 2, 30L)),\n        new KafkaPartitions(\"topic\", ImmutableMap.of(0, Long.MAX_VALUE, 1, Long.MAX_VALUE, 2, Long.MAX_VALUE)),\n        null,\n        null\n    );\n\n    Collection workItems = new ArrayList<>();\n    workItems.add(new TestTaskRunnerWorkItem(id1, null, location1));\n    workItems.add(new TestTaskRunnerWorkItem(id2, null, location2));\n\n    expect(taskMaster.getTaskQueue()).andReturn(Optional.of(taskQueue)).anyTimes();\n    expect(taskMaster.getTaskRunner()).andReturn(Optional.of(taskRunner)).anyTimes();\n    expect(taskRunner.getRunningTasks()).andReturn(workItems).anyTimes();\n    expect(taskStorage.getActiveTasks()).andReturn(ImmutableList.of(id1, id2, id3)).anyTimes();\n    expect(taskStorage.getStatus(\"id1\")).andReturn(Optional.of(TaskStatus.running(\"id1\"))).anyTimes();\n    expect(taskStorage.getStatus(\"id2\")).andReturn(Optional.of(TaskStatus.running(\"id2\"))).anyTimes();\n    expect(taskStorage.getStatus(\"id3\")).andReturn(Optional.of(TaskStatus.running(\"id3\"))).anyTimes();\n    expect(taskStorage.getTask(\"id1\")).andReturn(Optional.of(id1)).anyTimes();\n    expect(taskStorage.getTask(\"id2\")).andReturn(Optional.of(id2)).anyTimes();\n    expect(taskStorage.getTask(\"id3\")).andReturn(Optional.of(id3)).anyTimes();\n    expect(indexerMetadataStorageCoordinator.getDataSourceMetadata(DATASOURCE)).andReturn(\n        new KafkaDataSourceMetadata(\n            null\n        )\n    ).anyTimes();\n    expect(taskClient.getStatusAsync(\"id1\")).andReturn(Futures.immediateFuture(KafkaIndexTask.Status.PUBLISHING));\n    expect(taskClient.getStatusAsync(\"id2\")).andReturn(Futures.immediateFuture(KafkaIndexTask.Status.READING));\n    expect(taskClient.getStatusAsync(\"id3\")).andReturn(Futures.immediateFuture(KafkaIndexTask.Status.READING));\n    expect(taskClient.getStartTimeAsync(\"id2\")).andReturn(Futures.immediateFuture(startTime));\n    expect(taskClient.getStartTimeAsync(\"id3\")).andReturn(Futures.immediateFuture(startTime));\n    expect(taskClient.getEndOffsets(\"id1\")).andReturn(ImmutableMap.of(0, 10L, 1, 20L, 2, 30L));\n\n    \r\n    TreeMap<Integer, Map<Integer, Long>> checkpoints = new TreeMap<>();\n    checkpoints.put(0, ImmutableMap.of(0, 10L, 1, 20L, 2, 30L));\n    expect(taskClient.getCheckpointsAsync(EasyMock.contains(\"id2\"), anyBoolean()))\n        .andReturn(Futures.immediateFuture(checkpoints))\n        .times(1);\n    expect(taskClient.getCheckpointsAsync(EasyMock.contains(\"id3\"), anyBoolean()))\n        .andReturn(Futures.immediateFuture(checkpoints))\n        .times(1);\n\n    taskRunner.registerListener(anyObject(TaskRunnerListener.class), anyObject(Executor.class));\n\n    expect(taskClient.pauseAsync(\"id2\"))\n        .andReturn(Futures.immediateFuture(ImmutableMap.of(0, 15L, 1, 25L, 2, 30L)));\n    expect(taskClient.setEndOffsetsAsync(\"id2\", ImmutableMap.of(0, 15L, 1, 25L, 2, 30L), true))\n        .andReturn(Futures.immediateFuture(true));\n    taskQueue.shutdown(\"id3\");\n    expectLastCall().times(2);\n\n    replayAll();\n    supervisor.start();\n    supervisor.runInternal();\n    verifyAll();\n  }\n","date":"2018-10-04 10:08:20","endLine":2416,"groupId":"5276","id":4,"instanceNumber":2,"isCurCommit":0,"methodName":"testSuspendedRunningTasks","params":"()","path":"/mnt/clonedata/CloneManagementServer/ManagementServer/consistResult/result-druid-10-0.7/blobInfo/CC_OUT/blobs/d5/b048a239c6a2fa79c36e7b2dea6584a1db1668.src","preCode":"  public void testSuspendedRunningTasks() throws Exception\n  {\n    \r\n\n    final TaskLocation location1 = new TaskLocation(\"testHost\", 1234, -1);\n    final TaskLocation location2 = new TaskLocation(\"testHost2\", 145, -1);\n    final DateTime startTime = DateTimes.nowUtc();\n\n    supervisor = getSupervisor(2, 1, true, \"PT1H\", null, null, false, true);\n    addSomeEvents(1);\n\n    Task id1 = createKafkaIndexTask(\n        \"id1\",\n        DATASOURCE,\n        0,\n        new KafkaPartitions(\"topic\", ImmutableMap.of(0, 0L, 1, 0L, 2, 0L)),\n        new KafkaPartitions(\"topic\", ImmutableMap.of(0, Long.MAX_VALUE, 1, Long.MAX_VALUE, 2, Long.MAX_VALUE)),\n        null,\n        null\n    );\n\n    Task id2 = createKafkaIndexTask(\n        \"id2\",\n        DATASOURCE,\n        0,\n        new KafkaPartitions(\"topic\", ImmutableMap.of(0, 10L, 1, 20L, 2, 30L)),\n        new KafkaPartitions(\"topic\", ImmutableMap.of(0, Long.MAX_VALUE, 1, Long.MAX_VALUE, 2, Long.MAX_VALUE)),\n        null,\n        null\n    );\n\n    Task id3 = createKafkaIndexTask(\n        \"id3\",\n        DATASOURCE,\n        0,\n        new KafkaPartitions(\"topic\", ImmutableMap.of(0, 10L, 1, 20L, 2, 30L)),\n        new KafkaPartitions(\"topic\", ImmutableMap.of(0, Long.MAX_VALUE, 1, Long.MAX_VALUE, 2, Long.MAX_VALUE)),\n        null,\n        null\n    );\n\n    Collection workItems = new ArrayList<>();\n    workItems.add(new TestTaskRunnerWorkItem(id1, null, location1));\n    workItems.add(new TestTaskRunnerWorkItem(id2, null, location2));\n\n    expect(taskMaster.getTaskQueue()).andReturn(Optional.of(taskQueue)).anyTimes();\n    expect(taskMaster.getTaskRunner()).andReturn(Optional.of(taskRunner)).anyTimes();\n    expect(taskRunner.getRunningTasks()).andReturn(workItems).anyTimes();\n    expect(taskStorage.getActiveTasks()).andReturn(ImmutableList.of(id1, id2, id3)).anyTimes();\n    expect(taskStorage.getStatus(\"id1\")).andReturn(Optional.of(TaskStatus.running(\"id1\"))).anyTimes();\n    expect(taskStorage.getStatus(\"id2\")).andReturn(Optional.of(TaskStatus.running(\"id2\"))).anyTimes();\n    expect(taskStorage.getStatus(\"id3\")).andReturn(Optional.of(TaskStatus.running(\"id3\"))).anyTimes();\n    expect(taskStorage.getTask(\"id1\")).andReturn(Optional.of(id1)).anyTimes();\n    expect(taskStorage.getTask(\"id2\")).andReturn(Optional.of(id2)).anyTimes();\n    expect(taskStorage.getTask(\"id3\")).andReturn(Optional.of(id3)).anyTimes();\n    expect(indexerMetadataStorageCoordinator.getDataSourceMetadata(DATASOURCE)).andReturn(\n        new KafkaDataSourceMetadata(\n            null\n        )\n    ).anyTimes();\n    expect(taskClient.getStatusAsync(\"id1\")).andReturn(Futures.immediateFuture(KafkaIndexTask.Status.PUBLISHING));\n    expect(taskClient.getStatusAsync(\"id2\")).andReturn(Futures.immediateFuture(KafkaIndexTask.Status.READING));\n    expect(taskClient.getStatusAsync(\"id3\")).andReturn(Futures.immediateFuture(KafkaIndexTask.Status.READING));\n    expect(taskClient.getStartTimeAsync(\"id2\")).andReturn(Futures.immediateFuture(startTime));\n    expect(taskClient.getStartTimeAsync(\"id3\")).andReturn(Futures.immediateFuture(startTime));\n    expect(taskClient.getEndOffsets(\"id1\")).andReturn(ImmutableMap.of(0, 10L, 1, 20L, 2, 30L));\n\n    \r\n    TreeMap<Integer, Map<Integer, Long>> checkpoints = new TreeMap<>();\n    checkpoints.put(0, ImmutableMap.of(0, 10L, 1, 20L, 2, 30L));\n    expect(taskClient.getCheckpointsAsync(EasyMock.contains(\"id2\"), anyBoolean()))\n        .andReturn(Futures.immediateFuture(checkpoints))\n        .times(1);\n    expect(taskClient.getCheckpointsAsync(EasyMock.contains(\"id3\"), anyBoolean()))\n        .andReturn(Futures.immediateFuture(checkpoints))\n        .times(1);\n\n    taskRunner.registerListener(anyObject(TaskRunnerListener.class), anyObject(Executor.class));\n\n    expect(taskClient.pauseAsync(\"id2\"))\n        .andReturn(Futures.immediateFuture(ImmutableMap.of(0, 15L, 1, 25L, 2, 30L)));\n    expect(taskClient.setEndOffsetsAsync(\"id2\", ImmutableMap.of(0, 15L, 1, 25L, 2, 30L), true))\n        .andReturn(Futures.immediateFuture(true));\n    taskQueue.shutdown(\"id3\");\n    expectLastCall().times(2);\n\n    replayAll();\n    supervisor.start();\n    supervisor.runInternal();\n    verifyAll();\n  }\n","realPath":"extensions-core/kafka-indexing-service/src/test/java/org/apache/druid/indexing/kafka/supervisor/KafkaSupervisorTest.java","repoName":"druid","snippetEndLine":0,"snippetStartLine":0,"startLine":2326,"status":"M"}],"commitId":"c7ac8785a11bfb4d50237abefac93a8553216174","commitMessage":"@@@Prevent failed KafkaConsumer creation from blocking overlord startup (#6383)\n\n* Prevent failed KafkaConsumer creation from blocking overlord startup\n\n* PR comments\n\n* Fix random task ID length\n\n* Adjust test timer\n\n* Use Integer.SIZE\n","date":"2018-10-04 10:08:20","modifiedFileCount":"3","status":"M","submitter":"Jonathan Wei"},{"authorTime":"2018-11-16 10:01:56","codes":[{"authorDate":"2018-11-16 10:01:56","commitOrder":4,"curCode":"  public void testStopGracefully() throws Exception\n  {\n    final TaskLocation location1 = new TaskLocation(\"testHost\", 1234, -1);\n    final TaskLocation location2 = new TaskLocation(\"testHost2\", 145, -1);\n    final DateTime startTime = DateTimes.nowUtc();\n\n    supervisor = getSupervisor(2, 1, true, \"PT1H\", null, null, false);\n    addSomeEvents(1);\n\n    Task id1 = createKafkaIndexTask(\n        \"id1\",\n        DATASOURCE,\n        0,\n        new KafkaPartitions(\"topic\", ImmutableMap.of(0, 0L, 1, 0L, 2, 0L)),\n        new KafkaPartitions(\"topic\", ImmutableMap.of(0, Long.MAX_VALUE, 1, Long.MAX_VALUE, 2, Long.MAX_VALUE)),\n        null,\n        null\n    );\n\n    Task id2 = createKafkaIndexTask(\n        \"id2\",\n        DATASOURCE,\n        0,\n        new KafkaPartitions(\"topic\", ImmutableMap.of(0, 10L, 1, 20L, 2, 30L)),\n        new KafkaPartitions(\"topic\", ImmutableMap.of(0, Long.MAX_VALUE, 1, Long.MAX_VALUE, 2, Long.MAX_VALUE)),\n        null,\n        null\n    );\n\n    Task id3 = createKafkaIndexTask(\n        \"id3\",\n        DATASOURCE,\n        0,\n        new KafkaPartitions(\"topic\", ImmutableMap.of(0, 10L, 1, 20L, 2, 30L)),\n        new KafkaPartitions(\"topic\", ImmutableMap.of(0, Long.MAX_VALUE, 1, Long.MAX_VALUE, 2, Long.MAX_VALUE)),\n        null,\n        null\n    );\n\n    Collection workItems = new ArrayList<>();\n    workItems.add(new TestTaskRunnerWorkItem(id1, null, location1));\n    workItems.add(new TestTaskRunnerWorkItem(id2, null, location2));\n\n    expect(taskMaster.getTaskQueue()).andReturn(Optional.of(taskQueue)).anyTimes();\n    expect(taskMaster.getTaskRunner()).andReturn(Optional.of(taskRunner)).anyTimes();\n    expect(taskRunner.getRunningTasks()).andReturn(workItems).anyTimes();\n    expect(taskStorage.getActiveTasks()).andReturn(ImmutableList.of(id1, id2, id3)).anyTimes();\n    expect(taskStorage.getStatus(\"id1\")).andReturn(Optional.of(TaskStatus.running(\"id1\"))).anyTimes();\n    expect(taskStorage.getStatus(\"id2\")).andReturn(Optional.of(TaskStatus.running(\"id2\"))).anyTimes();\n    expect(taskStorage.getStatus(\"id3\")).andReturn(Optional.of(TaskStatus.running(\"id3\"))).anyTimes();\n    expect(taskStorage.getTask(\"id1\")).andReturn(Optional.of(id1)).anyTimes();\n    expect(taskStorage.getTask(\"id2\")).andReturn(Optional.of(id2)).anyTimes();\n    expect(taskStorage.getTask(\"id3\")).andReturn(Optional.of(id3)).anyTimes();\n    expect(indexerMetadataStorageCoordinator.getDataSourceMetadata(DATASOURCE)).andReturn(\n        new KafkaDataSourceMetadata(\n            null\n        )\n    ).anyTimes();\n    expect(taskClient.getStatusAsync(\"id1\")).andReturn(Futures.immediateFuture(KafkaIndexTask.Status.PUBLISHING));\n    expect(taskClient.getStatusAsync(\"id2\")).andReturn(Futures.immediateFuture(KafkaIndexTask.Status.READING));\n    expect(taskClient.getStatusAsync(\"id3\")).andReturn(Futures.immediateFuture(KafkaIndexTask.Status.READING));\n    expect(taskClient.getStartTimeAsync(\"id2\")).andReturn(Futures.immediateFuture(startTime));\n    expect(taskClient.getStartTimeAsync(\"id3\")).andReturn(Futures.immediateFuture(startTime));\n    expect(taskClient.getEndOffsets(\"id1\")).andReturn(ImmutableMap.of(0, 10L, 1, 20L, 2, 30L));\n\n    \r\n    TreeMap<Integer, Map<Integer, Long>> checkpoints = new TreeMap<>();\n    checkpoints.put(0, ImmutableMap.of(0, 10L, 1, 20L, 2, 30L));\n    expect(taskClient.getCheckpointsAsync(EasyMock.contains(\"id2\"), anyBoolean()))\n        .andReturn(Futures.immediateFuture(checkpoints))\n        .times(1);\n    expect(taskClient.getCheckpointsAsync(EasyMock.contains(\"id3\"), anyBoolean()))\n        .andReturn(Futures.immediateFuture(checkpoints))\n        .times(1);\n\n    taskRunner.registerListener(anyObject(TaskRunnerListener.class), anyObject(Executor.class));\n    replayAll();\n\n    supervisor.start();\n    supervisor.runInternal();\n    verifyAll();\n\n    reset(taskRunner, taskClient, taskQueue);\n    expect(taskRunner.getRunningTasks()).andReturn(workItems).anyTimes();\n    expect(taskClient.pauseAsync(\"id2\"))\n        .andReturn(Futures.immediateFuture((Map<Integer, Long>) ImmutableMap.of(0, 15L, 1, 25L, 2, 30L)));\n    expect(taskClient.setEndOffsetsAsync(\"id2\", ImmutableMap.of(0, 15L, 1, 25L, 2, 30L), true))\n        .andReturn(Futures.immediateFuture(true));\n    taskQueue.shutdown(\"id3\", \"Killing task for graceful shutdown\");\n    expectLastCall().times(1);\n    taskQueue.shutdown(\"id3\", \"Killing task [%s] which hasn't been assigned to a worker\", \"id3\");\n    expectLastCall().times(1);\n\n    replay(taskRunner, taskClient, taskQueue);\n\n    supervisor.gracefulShutdownInternal();\n    verifyAll();\n  }\n","date":"2018-11-16 10:01:56","endLine":1769,"groupId":"5276","id":5,"instanceNumber":1,"isCurCommit":0,"methodName":"testStopGracefully","params":"()","path":"/mnt/clonedata/CloneManagementServer/ManagementServer/consistResult/result-druid-10-0.7/blobInfo/CC_OUT/blobs/9b/4a5a6cf4afd687a410813d77e5a040693c65ce.src","preCode":"  public void testStopGracefully() throws Exception\n  {\n    final TaskLocation location1 = new TaskLocation(\"testHost\", 1234, -1);\n    final TaskLocation location2 = new TaskLocation(\"testHost2\", 145, -1);\n    final DateTime startTime = DateTimes.nowUtc();\n\n    supervisor = getSupervisor(2, 1, true, \"PT1H\", null, null, false);\n    addSomeEvents(1);\n\n    Task id1 = createKafkaIndexTask(\n        \"id1\",\n        DATASOURCE,\n        0,\n        new KafkaPartitions(\"topic\", ImmutableMap.of(0, 0L, 1, 0L, 2, 0L)),\n        new KafkaPartitions(\"topic\", ImmutableMap.of(0, Long.MAX_VALUE, 1, Long.MAX_VALUE, 2, Long.MAX_VALUE)),\n        null,\n        null\n    );\n\n    Task id2 = createKafkaIndexTask(\n        \"id2\",\n        DATASOURCE,\n        0,\n        new KafkaPartitions(\"topic\", ImmutableMap.of(0, 10L, 1, 20L, 2, 30L)),\n        new KafkaPartitions(\"topic\", ImmutableMap.of(0, Long.MAX_VALUE, 1, Long.MAX_VALUE, 2, Long.MAX_VALUE)),\n        null,\n        null\n    );\n\n    Task id3 = createKafkaIndexTask(\n        \"id3\",\n        DATASOURCE,\n        0,\n        new KafkaPartitions(\"topic\", ImmutableMap.of(0, 10L, 1, 20L, 2, 30L)),\n        new KafkaPartitions(\"topic\", ImmutableMap.of(0, Long.MAX_VALUE, 1, Long.MAX_VALUE, 2, Long.MAX_VALUE)),\n        null,\n        null\n    );\n\n    Collection workItems = new ArrayList<>();\n    workItems.add(new TestTaskRunnerWorkItem(id1, null, location1));\n    workItems.add(new TestTaskRunnerWorkItem(id2, null, location2));\n\n    expect(taskMaster.getTaskQueue()).andReturn(Optional.of(taskQueue)).anyTimes();\n    expect(taskMaster.getTaskRunner()).andReturn(Optional.of(taskRunner)).anyTimes();\n    expect(taskRunner.getRunningTasks()).andReturn(workItems).anyTimes();\n    expect(taskStorage.getActiveTasks()).andReturn(ImmutableList.of(id1, id2, id3)).anyTimes();\n    expect(taskStorage.getStatus(\"id1\")).andReturn(Optional.of(TaskStatus.running(\"id1\"))).anyTimes();\n    expect(taskStorage.getStatus(\"id2\")).andReturn(Optional.of(TaskStatus.running(\"id2\"))).anyTimes();\n    expect(taskStorage.getStatus(\"id3\")).andReturn(Optional.of(TaskStatus.running(\"id3\"))).anyTimes();\n    expect(taskStorage.getTask(\"id1\")).andReturn(Optional.of(id1)).anyTimes();\n    expect(taskStorage.getTask(\"id2\")).andReturn(Optional.of(id2)).anyTimes();\n    expect(taskStorage.getTask(\"id3\")).andReturn(Optional.of(id3)).anyTimes();\n    expect(indexerMetadataStorageCoordinator.getDataSourceMetadata(DATASOURCE)).andReturn(\n        new KafkaDataSourceMetadata(\n            null\n        )\n    ).anyTimes();\n    expect(taskClient.getStatusAsync(\"id1\")).andReturn(Futures.immediateFuture(KafkaIndexTask.Status.PUBLISHING));\n    expect(taskClient.getStatusAsync(\"id2\")).andReturn(Futures.immediateFuture(KafkaIndexTask.Status.READING));\n    expect(taskClient.getStatusAsync(\"id3\")).andReturn(Futures.immediateFuture(KafkaIndexTask.Status.READING));\n    expect(taskClient.getStartTimeAsync(\"id2\")).andReturn(Futures.immediateFuture(startTime));\n    expect(taskClient.getStartTimeAsync(\"id3\")).andReturn(Futures.immediateFuture(startTime));\n    expect(taskClient.getEndOffsets(\"id1\")).andReturn(ImmutableMap.of(0, 10L, 1, 20L, 2, 30L));\n\n    \r\n    TreeMap<Integer, Map<Integer, Long>> checkpoints = new TreeMap<>();\n    checkpoints.put(0, ImmutableMap.of(0, 10L, 1, 20L, 2, 30L));\n    expect(taskClient.getCheckpointsAsync(EasyMock.contains(\"id2\"), anyBoolean()))\n        .andReturn(Futures.immediateFuture(checkpoints))\n        .times(1);\n    expect(taskClient.getCheckpointsAsync(EasyMock.contains(\"id3\"), anyBoolean()))\n        .andReturn(Futures.immediateFuture(checkpoints))\n        .times(1);\n\n    taskRunner.registerListener(anyObject(TaskRunnerListener.class), anyObject(Executor.class));\n    replayAll();\n\n    supervisor.start();\n    supervisor.runInternal();\n    verifyAll();\n\n    reset(taskRunner, taskClient, taskQueue);\n    expect(taskRunner.getRunningTasks()).andReturn(workItems).anyTimes();\n    expect(taskClient.pauseAsync(\"id2\"))\n        .andReturn(Futures.immediateFuture((Map<Integer, Long>) ImmutableMap.of(0, 15L, 1, 25L, 2, 30L)));\n    expect(taskClient.setEndOffsetsAsync(\"id2\", ImmutableMap.of(0, 15L, 1, 25L, 2, 30L), true))\n        .andReturn(Futures.immediateFuture(true));\n    taskQueue.shutdown(\"id3\");\n    expectLastCall().times(2);\n\n    replay(taskRunner, taskClient, taskQueue);\n\n    supervisor.gracefulShutdownInternal();\n    verifyAll();\n  }\n","realPath":"extensions-core/kafka-indexing-service/src/test/java/org/apache/druid/indexing/kafka/supervisor/KafkaSupervisorTest.java","repoName":"druid","snippetEndLine":0,"snippetStartLine":0,"startLine":1672,"status":"M"},{"authorDate":"2018-11-16 10:01:56","commitOrder":4,"curCode":"  public void testSuspendedRunningTasks() throws Exception\n  {\n    \r\n\n    final TaskLocation location1 = new TaskLocation(\"testHost\", 1234, -1);\n    final TaskLocation location2 = new TaskLocation(\"testHost2\", 145, -1);\n    final DateTime startTime = DateTimes.nowUtc();\n\n    supervisor = getSupervisor(2, 1, true, \"PT1H\", null, null, false, true, kafkaHost);\n    addSomeEvents(1);\n\n    Task id1 = createKafkaIndexTask(\n        \"id1\",\n        DATASOURCE,\n        0,\n        new KafkaPartitions(\"topic\", ImmutableMap.of(0, 0L, 1, 0L, 2, 0L)),\n        new KafkaPartitions(\"topic\", ImmutableMap.of(0, Long.MAX_VALUE, 1, Long.MAX_VALUE, 2, Long.MAX_VALUE)),\n        null,\n        null\n    );\n\n    Task id2 = createKafkaIndexTask(\n        \"id2\",\n        DATASOURCE,\n        0,\n        new KafkaPartitions(\"topic\", ImmutableMap.of(0, 10L, 1, 20L, 2, 30L)),\n        new KafkaPartitions(\"topic\", ImmutableMap.of(0, Long.MAX_VALUE, 1, Long.MAX_VALUE, 2, Long.MAX_VALUE)),\n        null,\n        null\n    );\n\n    Task id3 = createKafkaIndexTask(\n        \"id3\",\n        DATASOURCE,\n        0,\n        new KafkaPartitions(\"topic\", ImmutableMap.of(0, 10L, 1, 20L, 2, 30L)),\n        new KafkaPartitions(\"topic\", ImmutableMap.of(0, Long.MAX_VALUE, 1, Long.MAX_VALUE, 2, Long.MAX_VALUE)),\n        null,\n        null\n    );\n\n    Collection workItems = new ArrayList<>();\n    workItems.add(new TestTaskRunnerWorkItem(id1, null, location1));\n    workItems.add(new TestTaskRunnerWorkItem(id2, null, location2));\n\n    expect(taskMaster.getTaskQueue()).andReturn(Optional.of(taskQueue)).anyTimes();\n    expect(taskMaster.getTaskRunner()).andReturn(Optional.of(taskRunner)).anyTimes();\n    expect(taskRunner.getRunningTasks()).andReturn(workItems).anyTimes();\n    expect(taskStorage.getActiveTasks()).andReturn(ImmutableList.of(id1, id2, id3)).anyTimes();\n    expect(taskStorage.getStatus(\"id1\")).andReturn(Optional.of(TaskStatus.running(\"id1\"))).anyTimes();\n    expect(taskStorage.getStatus(\"id2\")).andReturn(Optional.of(TaskStatus.running(\"id2\"))).anyTimes();\n    expect(taskStorage.getStatus(\"id3\")).andReturn(Optional.of(TaskStatus.running(\"id3\"))).anyTimes();\n    expect(taskStorage.getTask(\"id1\")).andReturn(Optional.of(id1)).anyTimes();\n    expect(taskStorage.getTask(\"id2\")).andReturn(Optional.of(id2)).anyTimes();\n    expect(taskStorage.getTask(\"id3\")).andReturn(Optional.of(id3)).anyTimes();\n    expect(indexerMetadataStorageCoordinator.getDataSourceMetadata(DATASOURCE)).andReturn(\n        new KafkaDataSourceMetadata(\n            null\n        )\n    ).anyTimes();\n    expect(taskClient.getStatusAsync(\"id1\")).andReturn(Futures.immediateFuture(KafkaIndexTask.Status.PUBLISHING));\n    expect(taskClient.getStatusAsync(\"id2\")).andReturn(Futures.immediateFuture(KafkaIndexTask.Status.READING));\n    expect(taskClient.getStatusAsync(\"id3\")).andReturn(Futures.immediateFuture(KafkaIndexTask.Status.READING));\n    expect(taskClient.getStartTimeAsync(\"id2\")).andReturn(Futures.immediateFuture(startTime));\n    expect(taskClient.getStartTimeAsync(\"id3\")).andReturn(Futures.immediateFuture(startTime));\n    expect(taskClient.getEndOffsets(\"id1\")).andReturn(ImmutableMap.of(0, 10L, 1, 20L, 2, 30L));\n\n    \r\n    TreeMap<Integer, Map<Integer, Long>> checkpoints = new TreeMap<>();\n    checkpoints.put(0, ImmutableMap.of(0, 10L, 1, 20L, 2, 30L));\n    expect(taskClient.getCheckpointsAsync(EasyMock.contains(\"id2\"), anyBoolean()))\n        .andReturn(Futures.immediateFuture(checkpoints))\n        .times(1);\n    expect(taskClient.getCheckpointsAsync(EasyMock.contains(\"id3\"), anyBoolean()))\n        .andReturn(Futures.immediateFuture(checkpoints))\n        .times(1);\n\n    taskRunner.registerListener(anyObject(TaskRunnerListener.class), anyObject(Executor.class));\n\n    expect(taskClient.pauseAsync(\"id2\"))\n        .andReturn(Futures.immediateFuture(ImmutableMap.of(0, 15L, 1, 25L, 2, 30L)));\n    expect(taskClient.setEndOffsetsAsync(\"id2\", ImmutableMap.of(0, 15L, 1, 25L, 2, 30L), true))\n        .andReturn(Futures.immediateFuture(true));\n    taskQueue.shutdown(\"id3\", \"Killing task for graceful shutdown\");\n    expectLastCall().times(1);\n    taskQueue.shutdown(\"id3\", \"Killing task [%s] which hasn't been assigned to a worker\", \"id3\");\n    expectLastCall().times(1);\n\n    replayAll();\n    supervisor.start();\n    supervisor.runInternal();\n    verifyAll();\n  }\n","date":"2018-11-16 10:01:56","endLine":2446,"groupId":"5276","id":6,"instanceNumber":2,"isCurCommit":0,"methodName":"testSuspendedRunningTasks","params":"()","path":"/mnt/clonedata/CloneManagementServer/ManagementServer/consistResult/result-druid-10-0.7/blobInfo/CC_OUT/blobs/9b/4a5a6cf4afd687a410813d77e5a040693c65ce.src","preCode":"  public void testSuspendedRunningTasks() throws Exception\n  {\n    \r\n\n    final TaskLocation location1 = new TaskLocation(\"testHost\", 1234, -1);\n    final TaskLocation location2 = new TaskLocation(\"testHost2\", 145, -1);\n    final DateTime startTime = DateTimes.nowUtc();\n\n    supervisor = getSupervisor(2, 1, true, \"PT1H\", null, null, false, true, kafkaHost);\n    addSomeEvents(1);\n\n    Task id1 = createKafkaIndexTask(\n        \"id1\",\n        DATASOURCE,\n        0,\n        new KafkaPartitions(\"topic\", ImmutableMap.of(0, 0L, 1, 0L, 2, 0L)),\n        new KafkaPartitions(\"topic\", ImmutableMap.of(0, Long.MAX_VALUE, 1, Long.MAX_VALUE, 2, Long.MAX_VALUE)),\n        null,\n        null\n    );\n\n    Task id2 = createKafkaIndexTask(\n        \"id2\",\n        DATASOURCE,\n        0,\n        new KafkaPartitions(\"topic\", ImmutableMap.of(0, 10L, 1, 20L, 2, 30L)),\n        new KafkaPartitions(\"topic\", ImmutableMap.of(0, Long.MAX_VALUE, 1, Long.MAX_VALUE, 2, Long.MAX_VALUE)),\n        null,\n        null\n    );\n\n    Task id3 = createKafkaIndexTask(\n        \"id3\",\n        DATASOURCE,\n        0,\n        new KafkaPartitions(\"topic\", ImmutableMap.of(0, 10L, 1, 20L, 2, 30L)),\n        new KafkaPartitions(\"topic\", ImmutableMap.of(0, Long.MAX_VALUE, 1, Long.MAX_VALUE, 2, Long.MAX_VALUE)),\n        null,\n        null\n    );\n\n    Collection workItems = new ArrayList<>();\n    workItems.add(new TestTaskRunnerWorkItem(id1, null, location1));\n    workItems.add(new TestTaskRunnerWorkItem(id2, null, location2));\n\n    expect(taskMaster.getTaskQueue()).andReturn(Optional.of(taskQueue)).anyTimes();\n    expect(taskMaster.getTaskRunner()).andReturn(Optional.of(taskRunner)).anyTimes();\n    expect(taskRunner.getRunningTasks()).andReturn(workItems).anyTimes();\n    expect(taskStorage.getActiveTasks()).andReturn(ImmutableList.of(id1, id2, id3)).anyTimes();\n    expect(taskStorage.getStatus(\"id1\")).andReturn(Optional.of(TaskStatus.running(\"id1\"))).anyTimes();\n    expect(taskStorage.getStatus(\"id2\")).andReturn(Optional.of(TaskStatus.running(\"id2\"))).anyTimes();\n    expect(taskStorage.getStatus(\"id3\")).andReturn(Optional.of(TaskStatus.running(\"id3\"))).anyTimes();\n    expect(taskStorage.getTask(\"id1\")).andReturn(Optional.of(id1)).anyTimes();\n    expect(taskStorage.getTask(\"id2\")).andReturn(Optional.of(id2)).anyTimes();\n    expect(taskStorage.getTask(\"id3\")).andReturn(Optional.of(id3)).anyTimes();\n    expect(indexerMetadataStorageCoordinator.getDataSourceMetadata(DATASOURCE)).andReturn(\n        new KafkaDataSourceMetadata(\n            null\n        )\n    ).anyTimes();\n    expect(taskClient.getStatusAsync(\"id1\")).andReturn(Futures.immediateFuture(KafkaIndexTask.Status.PUBLISHING));\n    expect(taskClient.getStatusAsync(\"id2\")).andReturn(Futures.immediateFuture(KafkaIndexTask.Status.READING));\n    expect(taskClient.getStatusAsync(\"id3\")).andReturn(Futures.immediateFuture(KafkaIndexTask.Status.READING));\n    expect(taskClient.getStartTimeAsync(\"id2\")).andReturn(Futures.immediateFuture(startTime));\n    expect(taskClient.getStartTimeAsync(\"id3\")).andReturn(Futures.immediateFuture(startTime));\n    expect(taskClient.getEndOffsets(\"id1\")).andReturn(ImmutableMap.of(0, 10L, 1, 20L, 2, 30L));\n\n    \r\n    TreeMap<Integer, Map<Integer, Long>> checkpoints = new TreeMap<>();\n    checkpoints.put(0, ImmutableMap.of(0, 10L, 1, 20L, 2, 30L));\n    expect(taskClient.getCheckpointsAsync(EasyMock.contains(\"id2\"), anyBoolean()))\n        .andReturn(Futures.immediateFuture(checkpoints))\n        .times(1);\n    expect(taskClient.getCheckpointsAsync(EasyMock.contains(\"id3\"), anyBoolean()))\n        .andReturn(Futures.immediateFuture(checkpoints))\n        .times(1);\n\n    taskRunner.registerListener(anyObject(TaskRunnerListener.class), anyObject(Executor.class));\n\n    expect(taskClient.pauseAsync(\"id2\"))\n        .andReturn(Futures.immediateFuture(ImmutableMap.of(0, 15L, 1, 25L, 2, 30L)));\n    expect(taskClient.setEndOffsetsAsync(\"id2\", ImmutableMap.of(0, 15L, 1, 25L, 2, 30L), true))\n        .andReturn(Futures.immediateFuture(true));\n    taskQueue.shutdown(\"id3\");\n    expectLastCall().times(2);\n\n    replayAll();\n    supervisor.start();\n    supervisor.runInternal();\n    verifyAll();\n  }\n","realPath":"extensions-core/kafka-indexing-service/src/test/java/org/apache/druid/indexing/kafka/supervisor/KafkaSupervisorTest.java","repoName":"druid","snippetEndLine":0,"snippetStartLine":0,"startLine":2354,"status":"M"}],"commitId":"d738ce4d2a4430cf95919e27b0eede171cbdf66c","commitMessage":"@@@Enforce logging when killing a task (#6621)\n\n* Enforce logging when killing a task\n\n* fix test\n\n* address comment\n\n* address comment\n","date":"2018-11-16 10:01:56","modifiedFileCount":"14","status":"M","submitter":"Jihoon Son"},{"authorTime":"2018-12-22 03:49:24","codes":[{"authorDate":"2018-12-22 03:49:24","commitOrder":5,"curCode":"  public void testStopGracefully() throws Exception\n  {\n    final TaskLocation location1 = new TaskLocation(\"testHost\", 1234, -1);\n    final TaskLocation location2 = new TaskLocation(\"testHost2\", 145, -1);\n    final DateTime startTime = DateTimes.nowUtc();\n\n    supervisor = getSupervisor(2, 1, true, \"PT1H\", null, null, false);\n    addSomeEvents(1);\n\n    Task id1 = createKafkaIndexTask(\n        \"id1\",\n        DATASOURCE,\n        0,\n        new SeekableStreamPartitions<>(\"topic\", ImmutableMap.of(0, 0L, 1, 0L, 2, 0L)),\n        new SeekableStreamPartitions<>(\n            \"topic\",\n            ImmutableMap.of(0, Long.MAX_VALUE, 1, Long.MAX_VALUE, 2, Long.MAX_VALUE)\n        ),\n        null,\n        null\n    );\n\n    Task id2 = createKafkaIndexTask(\n        \"id2\",\n        DATASOURCE,\n        0,\n        new SeekableStreamPartitions<>(\"topic\", ImmutableMap.of(0, 10L, 1, 20L, 2, 30L)),\n        new SeekableStreamPartitions<>(\n            \"topic\",\n            ImmutableMap.of(0, Long.MAX_VALUE, 1, Long.MAX_VALUE, 2, Long.MAX_VALUE)\n        ),\n        null,\n        null\n    );\n\n    Task id3 = createKafkaIndexTask(\n        \"id3\",\n        DATASOURCE,\n        0,\n        new SeekableStreamPartitions<>(\"topic\", ImmutableMap.of(0, 10L, 1, 20L, 2, 30L)),\n        new SeekableStreamPartitions<>(\n            \"topic\",\n            ImmutableMap.of(0, Long.MAX_VALUE, 1, Long.MAX_VALUE, 2, Long.MAX_VALUE)\n        ),\n        null,\n        null\n    );\n\n    Collection workItems = new ArrayList<>();\n    workItems.add(new TestTaskRunnerWorkItem(id1, null, location1));\n    workItems.add(new TestTaskRunnerWorkItem(id2, null, location2));\n\n    expect(taskMaster.getTaskQueue()).andReturn(Optional.of(taskQueue)).anyTimes();\n    expect(taskMaster.getTaskRunner()).andReturn(Optional.of(taskRunner)).anyTimes();\n    expect(taskRunner.getRunningTasks()).andReturn(workItems).anyTimes();\n    expect(taskStorage.getActiveTasks()).andReturn(ImmutableList.of(id1, id2, id3)).anyTimes();\n    expect(taskStorage.getStatus(\"id1\")).andReturn(Optional.of(TaskStatus.running(\"id1\"))).anyTimes();\n    expect(taskStorage.getStatus(\"id2\")).andReturn(Optional.of(TaskStatus.running(\"id2\"))).anyTimes();\n    expect(taskStorage.getStatus(\"id3\")).andReturn(Optional.of(TaskStatus.running(\"id3\"))).anyTimes();\n    expect(taskStorage.getTask(\"id1\")).andReturn(Optional.of(id1)).anyTimes();\n    expect(taskStorage.getTask(\"id2\")).andReturn(Optional.of(id2)).anyTimes();\n    expect(taskStorage.getTask(\"id3\")).andReturn(Optional.of(id3)).anyTimes();\n    expect(indexerMetadataStorageCoordinator.getDataSourceMetadata(DATASOURCE)).andReturn(\n        new KafkaDataSourceMetadata(\n            null\n        )\n    ).anyTimes();\n    expect(taskClient.getStatusAsync(\"id1\")).andReturn(Futures.immediateFuture(Status.PUBLISHING));\n    expect(taskClient.getStatusAsync(\"id2\")).andReturn(Futures.immediateFuture(Status.READING));\n    expect(taskClient.getStatusAsync(\"id3\")).andReturn(Futures.immediateFuture(Status.READING));\n    expect(taskClient.getStartTimeAsync(\"id2\")).andReturn(Futures.immediateFuture(startTime));\n    expect(taskClient.getStartTimeAsync(\"id3\")).andReturn(Futures.immediateFuture(startTime));\n    expect(taskClient.getEndOffsets(\"id1\")).andReturn(ImmutableMap.of(0, 10L, 1, 20L, 2, 30L));\n\n    \r\n    TreeMap<Integer, Map<Integer, Long>> checkpoints = new TreeMap<>();\n    checkpoints.put(0, ImmutableMap.of(0, 10L, 1, 20L, 2, 30L));\n    expect(taskClient.getCheckpointsAsync(EasyMock.contains(\"id2\"), EasyMock.anyBoolean()))\n        .andReturn(Futures.immediateFuture(checkpoints))\n        .times(1);\n    expect(taskClient.getCheckpointsAsync(EasyMock.contains(\"id3\"), EasyMock.anyBoolean()))\n        .andReturn(Futures.immediateFuture(checkpoints))\n        .times(1);\n\n    taskRunner.registerListener(anyObject(TaskRunnerListener.class), anyObject(Executor.class));\n    replayAll();\n\n    supervisor.start();\n    supervisor.runInternal();\n    verifyAll();\n\n    EasyMock.reset(taskRunner, taskClient, taskQueue);\n    expect(taskRunner.getRunningTasks()).andReturn(workItems).anyTimes();\n    expect(taskClient.pauseAsync(\"id2\"))\n        .andReturn(Futures.immediateFuture((Map<Integer, Long>) ImmutableMap.of(0, 15L, 1, 25L, 2, 30L)));\n    expect(taskClient.setEndOffsetsAsync(\"id2\", ImmutableMap.of(0, 15L, 1, 25L, 2, 30L), true))\n        .andReturn(Futures.immediateFuture(true));\n    taskQueue.shutdown(\"id3\", \"Killing task for graceful shutdown\");\n    expectLastCall().times(1);\n    taskQueue.shutdown(\"id3\", \"Killing task [%s] which hasn't been assigned to a worker\", \"id3\");\n    expectLastCall().times(1);\n\n    replay(taskRunner, taskClient, taskQueue);\n\n    supervisor.gracefulShutdownInternal();\n    verifyAll();\n  }\n","date":"2018-12-22 03:49:24","endLine":1855,"groupId":"16026","id":7,"instanceNumber":1,"isCurCommit":0,"methodName":"testStopGracefully","params":"()","path":"/mnt/clonedata/CloneManagementServer/ManagementServer/consistResult/result-druid-10-0.7/blobInfo/CC_OUT/blobs/42/7b9d1742dec9346159f08cf020ea57960091eb.src","preCode":"  public void testStopGracefully() throws Exception\n  {\n    final TaskLocation location1 = new TaskLocation(\"testHost\", 1234, -1);\n    final TaskLocation location2 = new TaskLocation(\"testHost2\", 145, -1);\n    final DateTime startTime = DateTimes.nowUtc();\n\n    supervisor = getSupervisor(2, 1, true, \"PT1H\", null, null, false);\n    addSomeEvents(1);\n\n    Task id1 = createKafkaIndexTask(\n        \"id1\",\n        DATASOURCE,\n        0,\n        new KafkaPartitions(\"topic\", ImmutableMap.of(0, 0L, 1, 0L, 2, 0L)),\n        new KafkaPartitions(\"topic\", ImmutableMap.of(0, Long.MAX_VALUE, 1, Long.MAX_VALUE, 2, Long.MAX_VALUE)),\n        null,\n        null\n    );\n\n    Task id2 = createKafkaIndexTask(\n        \"id2\",\n        DATASOURCE,\n        0,\n        new KafkaPartitions(\"topic\", ImmutableMap.of(0, 10L, 1, 20L, 2, 30L)),\n        new KafkaPartitions(\"topic\", ImmutableMap.of(0, Long.MAX_VALUE, 1, Long.MAX_VALUE, 2, Long.MAX_VALUE)),\n        null,\n        null\n    );\n\n    Task id3 = createKafkaIndexTask(\n        \"id3\",\n        DATASOURCE,\n        0,\n        new KafkaPartitions(\"topic\", ImmutableMap.of(0, 10L, 1, 20L, 2, 30L)),\n        new KafkaPartitions(\"topic\", ImmutableMap.of(0, Long.MAX_VALUE, 1, Long.MAX_VALUE, 2, Long.MAX_VALUE)),\n        null,\n        null\n    );\n\n    Collection workItems = new ArrayList<>();\n    workItems.add(new TestTaskRunnerWorkItem(id1, null, location1));\n    workItems.add(new TestTaskRunnerWorkItem(id2, null, location2));\n\n    expect(taskMaster.getTaskQueue()).andReturn(Optional.of(taskQueue)).anyTimes();\n    expect(taskMaster.getTaskRunner()).andReturn(Optional.of(taskRunner)).anyTimes();\n    expect(taskRunner.getRunningTasks()).andReturn(workItems).anyTimes();\n    expect(taskStorage.getActiveTasks()).andReturn(ImmutableList.of(id1, id2, id3)).anyTimes();\n    expect(taskStorage.getStatus(\"id1\")).andReturn(Optional.of(TaskStatus.running(\"id1\"))).anyTimes();\n    expect(taskStorage.getStatus(\"id2\")).andReturn(Optional.of(TaskStatus.running(\"id2\"))).anyTimes();\n    expect(taskStorage.getStatus(\"id3\")).andReturn(Optional.of(TaskStatus.running(\"id3\"))).anyTimes();\n    expect(taskStorage.getTask(\"id1\")).andReturn(Optional.of(id1)).anyTimes();\n    expect(taskStorage.getTask(\"id2\")).andReturn(Optional.of(id2)).anyTimes();\n    expect(taskStorage.getTask(\"id3\")).andReturn(Optional.of(id3)).anyTimes();\n    expect(indexerMetadataStorageCoordinator.getDataSourceMetadata(DATASOURCE)).andReturn(\n        new KafkaDataSourceMetadata(\n            null\n        )\n    ).anyTimes();\n    expect(taskClient.getStatusAsync(\"id1\")).andReturn(Futures.immediateFuture(KafkaIndexTask.Status.PUBLISHING));\n    expect(taskClient.getStatusAsync(\"id2\")).andReturn(Futures.immediateFuture(KafkaIndexTask.Status.READING));\n    expect(taskClient.getStatusAsync(\"id3\")).andReturn(Futures.immediateFuture(KafkaIndexTask.Status.READING));\n    expect(taskClient.getStartTimeAsync(\"id2\")).andReturn(Futures.immediateFuture(startTime));\n    expect(taskClient.getStartTimeAsync(\"id3\")).andReturn(Futures.immediateFuture(startTime));\n    expect(taskClient.getEndOffsets(\"id1\")).andReturn(ImmutableMap.of(0, 10L, 1, 20L, 2, 30L));\n\n    \r\n    TreeMap<Integer, Map<Integer, Long>> checkpoints = new TreeMap<>();\n    checkpoints.put(0, ImmutableMap.of(0, 10L, 1, 20L, 2, 30L));\n    expect(taskClient.getCheckpointsAsync(EasyMock.contains(\"id2\"), anyBoolean()))\n        .andReturn(Futures.immediateFuture(checkpoints))\n        .times(1);\n    expect(taskClient.getCheckpointsAsync(EasyMock.contains(\"id3\"), anyBoolean()))\n        .andReturn(Futures.immediateFuture(checkpoints))\n        .times(1);\n\n    taskRunner.registerListener(anyObject(TaskRunnerListener.class), anyObject(Executor.class));\n    replayAll();\n\n    supervisor.start();\n    supervisor.runInternal();\n    verifyAll();\n\n    reset(taskRunner, taskClient, taskQueue);\n    expect(taskRunner.getRunningTasks()).andReturn(workItems).anyTimes();\n    expect(taskClient.pauseAsync(\"id2\"))\n        .andReturn(Futures.immediateFuture((Map<Integer, Long>) ImmutableMap.of(0, 15L, 1, 25L, 2, 30L)));\n    expect(taskClient.setEndOffsetsAsync(\"id2\", ImmutableMap.of(0, 15L, 1, 25L, 2, 30L), true))\n        .andReturn(Futures.immediateFuture(true));\n    taskQueue.shutdown(\"id3\", \"Killing task for graceful shutdown\");\n    expectLastCall().times(1);\n    taskQueue.shutdown(\"id3\", \"Killing task [%s] which hasn't been assigned to a worker\", \"id3\");\n    expectLastCall().times(1);\n\n    replay(taskRunner, taskClient, taskQueue);\n\n    supervisor.gracefulShutdownInternal();\n    verifyAll();\n  }\n","realPath":"extensions-core/kafka-indexing-service/src/test/java/org/apache/druid/indexing/kafka/supervisor/KafkaSupervisorTest.java","repoName":"druid","snippetEndLine":0,"snippetStartLine":0,"startLine":1749,"status":"M"},{"authorDate":"2018-12-22 03:49:24","commitOrder":5,"curCode":"  public void testSuspendedRunningTasks() throws Exception\n  {\n    \r\n\n    final TaskLocation location1 = new TaskLocation(\"testHost\", 1234, -1);\n    final TaskLocation location2 = new TaskLocation(\"testHost2\", 145, -1);\n    final DateTime startTime = DateTimes.nowUtc();\n\n    supervisor = getSupervisor(2, 1, true, \"PT1H\", null, null, false, true, kafkaHost);\n    addSomeEvents(1);\n\n    Task id1 = createKafkaIndexTask(\n        \"id1\",\n        DATASOURCE,\n        0,\n        new SeekableStreamPartitions<>(\"topic\", ImmutableMap.of(0, 0L, 1, 0L, 2, 0L)),\n        new SeekableStreamPartitions<>(\n            \"topic\",\n            ImmutableMap.of(0, Long.MAX_VALUE, 1, Long.MAX_VALUE, 2, Long.MAX_VALUE)\n        ),\n        null,\n        null\n    );\n\n    Task id2 = createKafkaIndexTask(\n        \"id2\",\n        DATASOURCE,\n        0,\n        new SeekableStreamPartitions<>(\"topic\", ImmutableMap.of(0, 10L, 1, 20L, 2, 30L)),\n        new SeekableStreamPartitions<>(\n            \"topic\",\n            ImmutableMap.of(0, Long.MAX_VALUE, 1, Long.MAX_VALUE, 2, Long.MAX_VALUE)\n        ),\n        null,\n        null\n    );\n\n    Task id3 = createKafkaIndexTask(\n        \"id3\",\n        DATASOURCE,\n        0,\n        new SeekableStreamPartitions<>(\"topic\", ImmutableMap.of(0, 10L, 1, 20L, 2, 30L)),\n        new SeekableStreamPartitions<>(\n            \"topic\",\n            ImmutableMap.of(0, Long.MAX_VALUE, 1, Long.MAX_VALUE, 2, Long.MAX_VALUE)\n        ),\n        null,\n        null\n    );\n\n    Collection workItems = new ArrayList<>();\n    workItems.add(new TestTaskRunnerWorkItem(id1, null, location1));\n    workItems.add(new TestTaskRunnerWorkItem(id2, null, location2));\n\n    expect(taskMaster.getTaskQueue()).andReturn(Optional.of(taskQueue)).anyTimes();\n    expect(taskMaster.getTaskRunner()).andReturn(Optional.of(taskRunner)).anyTimes();\n    expect(taskRunner.getRunningTasks()).andReturn(workItems).anyTimes();\n    expect(taskStorage.getActiveTasks()).andReturn(ImmutableList.of(id1, id2, id3)).anyTimes();\n    expect(taskStorage.getStatus(\"id1\")).andReturn(Optional.of(TaskStatus.running(\"id1\"))).anyTimes();\n    expect(taskStorage.getStatus(\"id2\")).andReturn(Optional.of(TaskStatus.running(\"id2\"))).anyTimes();\n    expect(taskStorage.getStatus(\"id3\")).andReturn(Optional.of(TaskStatus.running(\"id3\"))).anyTimes();\n    expect(taskStorage.getTask(\"id1\")).andReturn(Optional.of(id1)).anyTimes();\n    expect(taskStorage.getTask(\"id2\")).andReturn(Optional.of(id2)).anyTimes();\n    expect(taskStorage.getTask(\"id3\")).andReturn(Optional.of(id3)).anyTimes();\n    expect(indexerMetadataStorageCoordinator.getDataSourceMetadata(DATASOURCE)).andReturn(\n        new KafkaDataSourceMetadata(\n            null\n        )\n    ).anyTimes();\n    expect(taskClient.getStatusAsync(\"id1\"))\n        .andReturn(Futures.immediateFuture(Status.PUBLISHING));\n    expect(taskClient.getStatusAsync(\"id2\"))\n        .andReturn(Futures.immediateFuture(Status.READING));\n    expect(taskClient.getStatusAsync(\"id3\"))\n        .andReturn(Futures.immediateFuture(Status.READING));\n    expect(taskClient.getStartTimeAsync(\"id2\")).andReturn(Futures.immediateFuture(startTime));\n    expect(taskClient.getStartTimeAsync(\"id3\")).andReturn(Futures.immediateFuture(startTime));\n    expect(taskClient.getEndOffsets(\"id1\")).andReturn(ImmutableMap.of(0, 10L, 1, 20L, 2, 30L));\n\n    \r\n    TreeMap<Integer, Map<Integer, Long>> checkpoints = new TreeMap<>();\n    checkpoints.put(0, ImmutableMap.of(0, 10L, 1, 20L, 2, 30L));\n    expect(taskClient.getCheckpointsAsync(EasyMock.contains(\"id2\"), EasyMock.anyBoolean()))\n        .andReturn(Futures.immediateFuture(checkpoints))\n        .times(1);\n    expect(taskClient.getCheckpointsAsync(EasyMock.contains(\"id3\"), EasyMock.anyBoolean()))\n        .andReturn(Futures.immediateFuture(checkpoints))\n        .times(1);\n\n    taskRunner.registerListener(anyObject(TaskRunnerListener.class), anyObject(Executor.class));\n\n    expect(taskClient.pauseAsync(\"id2\"))\n        .andReturn(Futures.immediateFuture(ImmutableMap.of(0, 15L, 1, 25L, 2, 30L)));\n    expect(taskClient.setEndOffsetsAsync(\"id2\", ImmutableMap.of(0, 15L, 1, 25L, 2, 30L), true))\n        .andReturn(Futures.immediateFuture(true));\n    taskQueue.shutdown(\"id3\", \"Killing task for graceful shutdown\");\n    expectLastCall().times(1);\n    taskQueue.shutdown(\"id3\", \"Killing task [%s] which hasn't been assigned to a worker\", \"id3\");\n    expectLastCall().times(1);\n\n    replayAll();\n    supervisor.start();\n    supervisor.runInternal();\n    verifyAll();\n  }\n","date":"2018-12-22 03:49:24","endLine":2570,"groupId":"16026","id":8,"instanceNumber":2,"isCurCommit":0,"methodName":"testSuspendedRunningTasks","params":"()","path":"/mnt/clonedata/CloneManagementServer/ManagementServer/consistResult/result-druid-10-0.7/blobInfo/CC_OUT/blobs/42/7b9d1742dec9346159f08cf020ea57960091eb.src","preCode":"  public void testSuspendedRunningTasks() throws Exception\n  {\n    \r\n\n    final TaskLocation location1 = new TaskLocation(\"testHost\", 1234, -1);\n    final TaskLocation location2 = new TaskLocation(\"testHost2\", 145, -1);\n    final DateTime startTime = DateTimes.nowUtc();\n\n    supervisor = getSupervisor(2, 1, true, \"PT1H\", null, null, false, true, kafkaHost);\n    addSomeEvents(1);\n\n    Task id1 = createKafkaIndexTask(\n        \"id1\",\n        DATASOURCE,\n        0,\n        new KafkaPartitions(\"topic\", ImmutableMap.of(0, 0L, 1, 0L, 2, 0L)),\n        new KafkaPartitions(\"topic\", ImmutableMap.of(0, Long.MAX_VALUE, 1, Long.MAX_VALUE, 2, Long.MAX_VALUE)),\n        null,\n        null\n    );\n\n    Task id2 = createKafkaIndexTask(\n        \"id2\",\n        DATASOURCE,\n        0,\n        new KafkaPartitions(\"topic\", ImmutableMap.of(0, 10L, 1, 20L, 2, 30L)),\n        new KafkaPartitions(\"topic\", ImmutableMap.of(0, Long.MAX_VALUE, 1, Long.MAX_VALUE, 2, Long.MAX_VALUE)),\n        null,\n        null\n    );\n\n    Task id3 = createKafkaIndexTask(\n        \"id3\",\n        DATASOURCE,\n        0,\n        new KafkaPartitions(\"topic\", ImmutableMap.of(0, 10L, 1, 20L, 2, 30L)),\n        new KafkaPartitions(\"topic\", ImmutableMap.of(0, Long.MAX_VALUE, 1, Long.MAX_VALUE, 2, Long.MAX_VALUE)),\n        null,\n        null\n    );\n\n    Collection workItems = new ArrayList<>();\n    workItems.add(new TestTaskRunnerWorkItem(id1, null, location1));\n    workItems.add(new TestTaskRunnerWorkItem(id2, null, location2));\n\n    expect(taskMaster.getTaskQueue()).andReturn(Optional.of(taskQueue)).anyTimes();\n    expect(taskMaster.getTaskRunner()).andReturn(Optional.of(taskRunner)).anyTimes();\n    expect(taskRunner.getRunningTasks()).andReturn(workItems).anyTimes();\n    expect(taskStorage.getActiveTasks()).andReturn(ImmutableList.of(id1, id2, id3)).anyTimes();\n    expect(taskStorage.getStatus(\"id1\")).andReturn(Optional.of(TaskStatus.running(\"id1\"))).anyTimes();\n    expect(taskStorage.getStatus(\"id2\")).andReturn(Optional.of(TaskStatus.running(\"id2\"))).anyTimes();\n    expect(taskStorage.getStatus(\"id3\")).andReturn(Optional.of(TaskStatus.running(\"id3\"))).anyTimes();\n    expect(taskStorage.getTask(\"id1\")).andReturn(Optional.of(id1)).anyTimes();\n    expect(taskStorage.getTask(\"id2\")).andReturn(Optional.of(id2)).anyTimes();\n    expect(taskStorage.getTask(\"id3\")).andReturn(Optional.of(id3)).anyTimes();\n    expect(indexerMetadataStorageCoordinator.getDataSourceMetadata(DATASOURCE)).andReturn(\n        new KafkaDataSourceMetadata(\n            null\n        )\n    ).anyTimes();\n    expect(taskClient.getStatusAsync(\"id1\")).andReturn(Futures.immediateFuture(KafkaIndexTask.Status.PUBLISHING));\n    expect(taskClient.getStatusAsync(\"id2\")).andReturn(Futures.immediateFuture(KafkaIndexTask.Status.READING));\n    expect(taskClient.getStatusAsync(\"id3\")).andReturn(Futures.immediateFuture(KafkaIndexTask.Status.READING));\n    expect(taskClient.getStartTimeAsync(\"id2\")).andReturn(Futures.immediateFuture(startTime));\n    expect(taskClient.getStartTimeAsync(\"id3\")).andReturn(Futures.immediateFuture(startTime));\n    expect(taskClient.getEndOffsets(\"id1\")).andReturn(ImmutableMap.of(0, 10L, 1, 20L, 2, 30L));\n\n    \r\n    TreeMap<Integer, Map<Integer, Long>> checkpoints = new TreeMap<>();\n    checkpoints.put(0, ImmutableMap.of(0, 10L, 1, 20L, 2, 30L));\n    expect(taskClient.getCheckpointsAsync(EasyMock.contains(\"id2\"), anyBoolean()))\n        .andReturn(Futures.immediateFuture(checkpoints))\n        .times(1);\n    expect(taskClient.getCheckpointsAsync(EasyMock.contains(\"id3\"), anyBoolean()))\n        .andReturn(Futures.immediateFuture(checkpoints))\n        .times(1);\n\n    taskRunner.registerListener(anyObject(TaskRunnerListener.class), anyObject(Executor.class));\n\n    expect(taskClient.pauseAsync(\"id2\"))\n        .andReturn(Futures.immediateFuture(ImmutableMap.of(0, 15L, 1, 25L, 2, 30L)));\n    expect(taskClient.setEndOffsetsAsync(\"id2\", ImmutableMap.of(0, 15L, 1, 25L, 2, 30L), true))\n        .andReturn(Futures.immediateFuture(true));\n    taskQueue.shutdown(\"id3\", \"Killing task for graceful shutdown\");\n    expectLastCall().times(1);\n    taskQueue.shutdown(\"id3\", \"Killing task [%s] which hasn't been assigned to a worker\", \"id3\");\n    expectLastCall().times(1);\n\n    replayAll();\n    supervisor.start();\n    supervisor.runInternal();\n    verifyAll();\n  }\n","realPath":"extensions-core/kafka-indexing-service/src/test/java/org/apache/druid/indexing/kafka/supervisor/KafkaSupervisorTest.java","repoName":"druid","snippetEndLine":0,"snippetStartLine":0,"startLine":2466,"status":"M"}],"commitId":"7c7997e8a1183a7bffad731ca94e8b4c381e8665","commitMessage":"@@@Add Kinesis Indexing Service to core Druid (#6431)\n\n* created seekablestream classes\n\n* created seekablestreamsupervisor class\n\n* first attempt to integrate kafa indexing service to use SeekableStream\n\n* seekablestream bug fixes\n\n* kafkarecordsupplier\n\n* integrated kafka indexing service with seekablestream\n\n* implemented resume/suspend and refactored some package names\n\n* moved kinesis indexing service into core druid extensions\n\n* merged some changes from kafka supervisor race condition\n\n* integrated kinesis-indexing-service with seekablestream\n\n* unite tests for kinesis-indexing-service\n\n* various bug fixes for kinesis-indexing-service\n\n* refactored kinesisindexingtask\n\n* finished up more kinesis unit tests\n\n* more bug fixes for kinesis-indexing-service\n\n* finsihed refactoring kinesis unit tests\n\n* removed KinesisParititons and KafkaPartitions to use SeekableStreamPartitions\n\n* kinesis-indexing-service code cleanup and docs\n\n* merge #6291\n\nmerge #6337\n\nmerge #6383\n\n* added more docs and reordered methods\n\n* fixd kinesis tests after merging master and added docs in seekablestream\n\n* fix various things from pr comment\n\n* improve recordsupplier and add unit tests\n\n* migrated to aws-java-sdk-kinesis\n\n* merge changes from master\n\n* fix pom files and forbiddenapi checks\n\n* checkpoint JavaType bug fix\n\n* fix pom and stuff\n\n* disable checkpointing in kinesis\n\n* fix kinesis sequence number null in closed shard\n\n* merge changes from master\n\n* fixes for kinesis tasks\n\n* capitalized <partitionType.  sequenceType>\n\n* removed abstract class loggers\n\n* conform to guava api restrictions\n\n* add docker for travis other modules test\n\n* address comments\n\n* improve RecordSupplier to supply records in batch\n\n* fix strict compile issue\n\n* add test scope for localstack dependency\n\n* kinesis indexing task refactoring\n\n* comments\n\n* github comments\n\n* minor fix\n\n* removed unneeded readme\n\n* fix deserialization bug\n\n* fix various bugs\n\n* KinesisRecordSupplier unable to catch up to earliest position in stream bug fix\n\n* minor changes to kinesis\n\n* implement deaggregate for kinesis\n\n* Merge remote-tracking branch 'upstream/master' into seekablestream\n\n* fix kinesis offset discrepancy with kafka\n\n* kinesis record supplier disable getPosition\n\n* pr comments\n\n* mock for kinesis tests and remove docker dependency for unit tests\n\n* PR comments\n\n* avg lag in kafkasupervisor #6587\n\n* refacotred SequenceMetadata in taskRunners\n\n* small fix\n\n* more small fix\n\n* recordsupplier resource leak\n\n* revert .travis.yml formatting\n\n* fix style\n\n* kinesis docs\n\n* doc part2\n\n* more docs\n\n* comments\n\n* comments*2\n\n* revert string replace changes\n\n* comments\n\n* teamcity\n\n* comments part 1\n\n* comments part 2\n\n* comments part 3\n\n* merge #6754\n\n* fix injection binding\n\n* comments\n\n* KinesisRegion refactor\n\n* comments part idk lol\n\n* can't think of a commit msg anymore\n\n* remove possiblyResetDataSourceMetadata() for IncrementalPublishingTaskRunner\n\n* commmmmmmmmmments\n\n* extra error handling in KinesisRecordSupplier getRecords\n\n* comments\n\n* quickfix\n\n* typo\n\n* oof\n","date":"2018-12-22 03:49:24","modifiedFileCount":"22","status":"M","submitter":"Joshua Sun"},{"authorTime":"2019-02-19 03:50:08","codes":[{"authorDate":"2019-02-19 03:50:08","commitOrder":6,"curCode":"  public void testStopGracefully() throws Exception\n  {\n    final TaskLocation location1 = new TaskLocation(\"testHost\", 1234, -1);\n    final TaskLocation location2 = new TaskLocation(\"testHost2\", 145, -1);\n    final DateTime startTime = DateTimes.nowUtc();\n\n    supervisor = getSupervisor(2, 1, true, \"PT1H\", null, null);\n    addSomeEvents(1);\n\n    Task id1 = createKafkaIndexTask(\n        \"id1\",\n        DATASOURCE,\n        0,\n        new SeekableStreamPartitions<>(\"topic\", ImmutableMap.of(0, 0L, 1, 0L, 2, 0L)),\n        new SeekableStreamPartitions<>(\n            \"topic\",\n            ImmutableMap.of(0, Long.MAX_VALUE, 1, Long.MAX_VALUE, 2, Long.MAX_VALUE)\n        ),\n        null,\n        null\n    );\n\n    Task id2 = createKafkaIndexTask(\n        \"id2\",\n        DATASOURCE,\n        0,\n        new SeekableStreamPartitions<>(\"topic\", ImmutableMap.of(0, 10L, 1, 20L, 2, 30L)),\n        new SeekableStreamPartitions<>(\n            \"topic\",\n            ImmutableMap.of(0, Long.MAX_VALUE, 1, Long.MAX_VALUE, 2, Long.MAX_VALUE)\n        ),\n        null,\n        null\n    );\n\n    Task id3 = createKafkaIndexTask(\n        \"id3\",\n        DATASOURCE,\n        0,\n        new SeekableStreamPartitions<>(\"topic\", ImmutableMap.of(0, 10L, 1, 20L, 2, 30L)),\n        new SeekableStreamPartitions<>(\n            \"topic\",\n            ImmutableMap.of(0, Long.MAX_VALUE, 1, Long.MAX_VALUE, 2, Long.MAX_VALUE)\n        ),\n        null,\n        null\n    );\n\n    Collection workItems = new ArrayList<>();\n    workItems.add(new TestTaskRunnerWorkItem(id1, null, location1));\n    workItems.add(new TestTaskRunnerWorkItem(id2, null, location2));\n\n    expect(taskMaster.getTaskQueue()).andReturn(Optional.of(taskQueue)).anyTimes();\n    expect(taskMaster.getTaskRunner()).andReturn(Optional.of(taskRunner)).anyTimes();\n    expect(taskRunner.getRunningTasks()).andReturn(workItems).anyTimes();\n    expect(taskStorage.getActiveTasks()).andReturn(ImmutableList.of(id1, id2, id3)).anyTimes();\n    expect(taskStorage.getStatus(\"id1\")).andReturn(Optional.of(TaskStatus.running(\"id1\"))).anyTimes();\n    expect(taskStorage.getStatus(\"id2\")).andReturn(Optional.of(TaskStatus.running(\"id2\"))).anyTimes();\n    expect(taskStorage.getStatus(\"id3\")).andReturn(Optional.of(TaskStatus.running(\"id3\"))).anyTimes();\n    expect(taskStorage.getTask(\"id1\")).andReturn(Optional.of(id1)).anyTimes();\n    expect(taskStorage.getTask(\"id2\")).andReturn(Optional.of(id2)).anyTimes();\n    expect(taskStorage.getTask(\"id3\")).andReturn(Optional.of(id3)).anyTimes();\n    expect(indexerMetadataStorageCoordinator.getDataSourceMetadata(DATASOURCE)).andReturn(\n        new KafkaDataSourceMetadata(\n            null\n        )\n    ).anyTimes();\n    expect(taskClient.getStatusAsync(\"id1\")).andReturn(Futures.immediateFuture(Status.PUBLISHING));\n    expect(taskClient.getStatusAsync(\"id2\")).andReturn(Futures.immediateFuture(Status.READING));\n    expect(taskClient.getStatusAsync(\"id3\")).andReturn(Futures.immediateFuture(Status.READING));\n    expect(taskClient.getStartTimeAsync(\"id2\")).andReturn(Futures.immediateFuture(startTime));\n    expect(taskClient.getStartTimeAsync(\"id3\")).andReturn(Futures.immediateFuture(startTime));\n    expect(taskClient.getEndOffsets(\"id1\")).andReturn(ImmutableMap.of(0, 10L, 1, 20L, 2, 30L));\n\n    \r\n    TreeMap<Integer, Map<Integer, Long>> checkpoints = new TreeMap<>();\n    checkpoints.put(0, ImmutableMap.of(0, 10L, 1, 20L, 2, 30L));\n    expect(taskClient.getCheckpointsAsync(EasyMock.contains(\"id2\"), EasyMock.anyBoolean()))\n        .andReturn(Futures.immediateFuture(checkpoints))\n        .times(1);\n    expect(taskClient.getCheckpointsAsync(EasyMock.contains(\"id3\"), EasyMock.anyBoolean()))\n        .andReturn(Futures.immediateFuture(checkpoints))\n        .times(1);\n\n    taskRunner.registerListener(anyObject(TaskRunnerListener.class), anyObject(Executor.class));\n    replayAll();\n\n    supervisor.start();\n    supervisor.runInternal();\n    verifyAll();\n\n    EasyMock.reset(taskRunner, taskClient, taskQueue);\n    expect(taskRunner.getRunningTasks()).andReturn(workItems).anyTimes();\n    expect(taskClient.pauseAsync(\"id2\"))\n        .andReturn(Futures.immediateFuture((Map<Integer, Long>) ImmutableMap.of(0, 15L, 1, 25L, 2, 30L)));\n    expect(taskClient.setEndOffsetsAsync(\"id2\", ImmutableMap.of(0, 15L, 1, 25L, 2, 30L), true))\n        .andReturn(Futures.immediateFuture(true));\n    taskQueue.shutdown(\"id3\", \"Killing task for graceful shutdown\");\n    expectLastCall().times(1);\n    taskQueue.shutdown(\"id3\", \"Killing task [%s] which hasn't been assigned to a worker\", \"id3\");\n    expectLastCall().times(1);\n\n    replay(taskRunner, taskClient, taskQueue);\n\n    supervisor.gracefulShutdownInternal();\n    verifyAll();\n  }\n","date":"2019-02-19 03:50:08","endLine":1853,"groupId":"16026","id":9,"instanceNumber":1,"isCurCommit":0,"methodName":"testStopGracefully","params":"()","path":"/mnt/clonedata/CloneManagementServer/ManagementServer/consistResult/result-druid-10-0.7/blobInfo/CC_OUT/blobs/f2/db280c51eb85380271ce661d1c415120afa0e0.src","preCode":"  public void testStopGracefully() throws Exception\n  {\n    final TaskLocation location1 = new TaskLocation(\"testHost\", 1234, -1);\n    final TaskLocation location2 = new TaskLocation(\"testHost2\", 145, -1);\n    final DateTime startTime = DateTimes.nowUtc();\n\n    supervisor = getSupervisor(2, 1, true, \"PT1H\", null, null, false);\n    addSomeEvents(1);\n\n    Task id1 = createKafkaIndexTask(\n        \"id1\",\n        DATASOURCE,\n        0,\n        new SeekableStreamPartitions<>(\"topic\", ImmutableMap.of(0, 0L, 1, 0L, 2, 0L)),\n        new SeekableStreamPartitions<>(\n            \"topic\",\n            ImmutableMap.of(0, Long.MAX_VALUE, 1, Long.MAX_VALUE, 2, Long.MAX_VALUE)\n        ),\n        null,\n        null\n    );\n\n    Task id2 = createKafkaIndexTask(\n        \"id2\",\n        DATASOURCE,\n        0,\n        new SeekableStreamPartitions<>(\"topic\", ImmutableMap.of(0, 10L, 1, 20L, 2, 30L)),\n        new SeekableStreamPartitions<>(\n            \"topic\",\n            ImmutableMap.of(0, Long.MAX_VALUE, 1, Long.MAX_VALUE, 2, Long.MAX_VALUE)\n        ),\n        null,\n        null\n    );\n\n    Task id3 = createKafkaIndexTask(\n        \"id3\",\n        DATASOURCE,\n        0,\n        new SeekableStreamPartitions<>(\"topic\", ImmutableMap.of(0, 10L, 1, 20L, 2, 30L)),\n        new SeekableStreamPartitions<>(\n            \"topic\",\n            ImmutableMap.of(0, Long.MAX_VALUE, 1, Long.MAX_VALUE, 2, Long.MAX_VALUE)\n        ),\n        null,\n        null\n    );\n\n    Collection workItems = new ArrayList<>();\n    workItems.add(new TestTaskRunnerWorkItem(id1, null, location1));\n    workItems.add(new TestTaskRunnerWorkItem(id2, null, location2));\n\n    expect(taskMaster.getTaskQueue()).andReturn(Optional.of(taskQueue)).anyTimes();\n    expect(taskMaster.getTaskRunner()).andReturn(Optional.of(taskRunner)).anyTimes();\n    expect(taskRunner.getRunningTasks()).andReturn(workItems).anyTimes();\n    expect(taskStorage.getActiveTasks()).andReturn(ImmutableList.of(id1, id2, id3)).anyTimes();\n    expect(taskStorage.getStatus(\"id1\")).andReturn(Optional.of(TaskStatus.running(\"id1\"))).anyTimes();\n    expect(taskStorage.getStatus(\"id2\")).andReturn(Optional.of(TaskStatus.running(\"id2\"))).anyTimes();\n    expect(taskStorage.getStatus(\"id3\")).andReturn(Optional.of(TaskStatus.running(\"id3\"))).anyTimes();\n    expect(taskStorage.getTask(\"id1\")).andReturn(Optional.of(id1)).anyTimes();\n    expect(taskStorage.getTask(\"id2\")).andReturn(Optional.of(id2)).anyTimes();\n    expect(taskStorage.getTask(\"id3\")).andReturn(Optional.of(id3)).anyTimes();\n    expect(indexerMetadataStorageCoordinator.getDataSourceMetadata(DATASOURCE)).andReturn(\n        new KafkaDataSourceMetadata(\n            null\n        )\n    ).anyTimes();\n    expect(taskClient.getStatusAsync(\"id1\")).andReturn(Futures.immediateFuture(Status.PUBLISHING));\n    expect(taskClient.getStatusAsync(\"id2\")).andReturn(Futures.immediateFuture(Status.READING));\n    expect(taskClient.getStatusAsync(\"id3\")).andReturn(Futures.immediateFuture(Status.READING));\n    expect(taskClient.getStartTimeAsync(\"id2\")).andReturn(Futures.immediateFuture(startTime));\n    expect(taskClient.getStartTimeAsync(\"id3\")).andReturn(Futures.immediateFuture(startTime));\n    expect(taskClient.getEndOffsets(\"id1\")).andReturn(ImmutableMap.of(0, 10L, 1, 20L, 2, 30L));\n\n    \r\n    TreeMap<Integer, Map<Integer, Long>> checkpoints = new TreeMap<>();\n    checkpoints.put(0, ImmutableMap.of(0, 10L, 1, 20L, 2, 30L));\n    expect(taskClient.getCheckpointsAsync(EasyMock.contains(\"id2\"), EasyMock.anyBoolean()))\n        .andReturn(Futures.immediateFuture(checkpoints))\n        .times(1);\n    expect(taskClient.getCheckpointsAsync(EasyMock.contains(\"id3\"), EasyMock.anyBoolean()))\n        .andReturn(Futures.immediateFuture(checkpoints))\n        .times(1);\n\n    taskRunner.registerListener(anyObject(TaskRunnerListener.class), anyObject(Executor.class));\n    replayAll();\n\n    supervisor.start();\n    supervisor.runInternal();\n    verifyAll();\n\n    EasyMock.reset(taskRunner, taskClient, taskQueue);\n    expect(taskRunner.getRunningTasks()).andReturn(workItems).anyTimes();\n    expect(taskClient.pauseAsync(\"id2\"))\n        .andReturn(Futures.immediateFuture((Map<Integer, Long>) ImmutableMap.of(0, 15L, 1, 25L, 2, 30L)));\n    expect(taskClient.setEndOffsetsAsync(\"id2\", ImmutableMap.of(0, 15L, 1, 25L, 2, 30L), true))\n        .andReturn(Futures.immediateFuture(true));\n    taskQueue.shutdown(\"id3\", \"Killing task for graceful shutdown\");\n    expectLastCall().times(1);\n    taskQueue.shutdown(\"id3\", \"Killing task [%s] which hasn't been assigned to a worker\", \"id3\");\n    expectLastCall().times(1);\n\n    replay(taskRunner, taskClient, taskQueue);\n\n    supervisor.gracefulShutdownInternal();\n    verifyAll();\n  }\n","realPath":"extensions-core/kafka-indexing-service/src/test/java/org/apache/druid/indexing/kafka/supervisor/KafkaSupervisorTest.java","repoName":"druid","snippetEndLine":0,"snippetStartLine":0,"startLine":1747,"status":"M"},{"authorDate":"2019-02-19 03:50:08","commitOrder":6,"curCode":"  public void testSuspendedRunningTasks() throws Exception\n  {\n    \r\n\n    final TaskLocation location1 = new TaskLocation(\"testHost\", 1234, -1);\n    final TaskLocation location2 = new TaskLocation(\"testHost2\", 145, -1);\n    final DateTime startTime = DateTimes.nowUtc();\n\n    supervisor = getSupervisor(2, 1, true, \"PT1H\", null, null, true, kafkaHost);\n    addSomeEvents(1);\n\n    Task id1 = createKafkaIndexTask(\n        \"id1\",\n        DATASOURCE,\n        0,\n        new SeekableStreamPartitions<>(\"topic\", ImmutableMap.of(0, 0L, 1, 0L, 2, 0L)),\n        new SeekableStreamPartitions<>(\n            \"topic\",\n            ImmutableMap.of(0, Long.MAX_VALUE, 1, Long.MAX_VALUE, 2, Long.MAX_VALUE)\n        ),\n        null,\n        null\n    );\n\n    Task id2 = createKafkaIndexTask(\n        \"id2\",\n        DATASOURCE,\n        0,\n        new SeekableStreamPartitions<>(\"topic\", ImmutableMap.of(0, 10L, 1, 20L, 2, 30L)),\n        new SeekableStreamPartitions<>(\n            \"topic\",\n            ImmutableMap.of(0, Long.MAX_VALUE, 1, Long.MAX_VALUE, 2, Long.MAX_VALUE)\n        ),\n        null,\n        null\n    );\n\n    Task id3 = createKafkaIndexTask(\n        \"id3\",\n        DATASOURCE,\n        0,\n        new SeekableStreamPartitions<>(\"topic\", ImmutableMap.of(0, 10L, 1, 20L, 2, 30L)),\n        new SeekableStreamPartitions<>(\n            \"topic\",\n            ImmutableMap.of(0, Long.MAX_VALUE, 1, Long.MAX_VALUE, 2, Long.MAX_VALUE)\n        ),\n        null,\n        null\n    );\n\n    Collection workItems = new ArrayList<>();\n    workItems.add(new TestTaskRunnerWorkItem(id1, null, location1));\n    workItems.add(new TestTaskRunnerWorkItem(id2, null, location2));\n\n    expect(taskMaster.getTaskQueue()).andReturn(Optional.of(taskQueue)).anyTimes();\n    expect(taskMaster.getTaskRunner()).andReturn(Optional.of(taskRunner)).anyTimes();\n    expect(taskRunner.getRunningTasks()).andReturn(workItems).anyTimes();\n    expect(taskStorage.getActiveTasks()).andReturn(ImmutableList.of(id1, id2, id3)).anyTimes();\n    expect(taskStorage.getStatus(\"id1\")).andReturn(Optional.of(TaskStatus.running(\"id1\"))).anyTimes();\n    expect(taskStorage.getStatus(\"id2\")).andReturn(Optional.of(TaskStatus.running(\"id2\"))).anyTimes();\n    expect(taskStorage.getStatus(\"id3\")).andReturn(Optional.of(TaskStatus.running(\"id3\"))).anyTimes();\n    expect(taskStorage.getTask(\"id1\")).andReturn(Optional.of(id1)).anyTimes();\n    expect(taskStorage.getTask(\"id2\")).andReturn(Optional.of(id2)).anyTimes();\n    expect(taskStorage.getTask(\"id3\")).andReturn(Optional.of(id3)).anyTimes();\n    expect(indexerMetadataStorageCoordinator.getDataSourceMetadata(DATASOURCE)).andReturn(\n        new KafkaDataSourceMetadata(\n            null\n        )\n    ).anyTimes();\n    expect(taskClient.getStatusAsync(\"id1\"))\n        .andReturn(Futures.immediateFuture(Status.PUBLISHING));\n    expect(taskClient.getStatusAsync(\"id2\"))\n        .andReturn(Futures.immediateFuture(Status.READING));\n    expect(taskClient.getStatusAsync(\"id3\"))\n        .andReturn(Futures.immediateFuture(Status.READING));\n    expect(taskClient.getStartTimeAsync(\"id2\")).andReturn(Futures.immediateFuture(startTime));\n    expect(taskClient.getStartTimeAsync(\"id3\")).andReturn(Futures.immediateFuture(startTime));\n    expect(taskClient.getEndOffsets(\"id1\")).andReturn(ImmutableMap.of(0, 10L, 1, 20L, 2, 30L));\n\n    \r\n    TreeMap<Integer, Map<Integer, Long>> checkpoints = new TreeMap<>();\n    checkpoints.put(0, ImmutableMap.of(0, 10L, 1, 20L, 2, 30L));\n    expect(taskClient.getCheckpointsAsync(EasyMock.contains(\"id2\"), EasyMock.anyBoolean()))\n        .andReturn(Futures.immediateFuture(checkpoints))\n        .times(1);\n    expect(taskClient.getCheckpointsAsync(EasyMock.contains(\"id3\"), EasyMock.anyBoolean()))\n        .andReturn(Futures.immediateFuture(checkpoints))\n        .times(1);\n\n    taskRunner.registerListener(anyObject(TaskRunnerListener.class), anyObject(Executor.class));\n\n    expect(taskClient.pauseAsync(\"id2\"))\n        .andReturn(Futures.immediateFuture(ImmutableMap.of(0, 15L, 1, 25L, 2, 30L)));\n    expect(taskClient.setEndOffsetsAsync(\"id2\", ImmutableMap.of(0, 15L, 1, 25L, 2, 30L), true))\n        .andReturn(Futures.immediateFuture(true));\n    taskQueue.shutdown(\"id3\", \"Killing task for graceful shutdown\");\n    expectLastCall().times(1);\n    taskQueue.shutdown(\"id3\", \"Killing task [%s] which hasn't been assigned to a worker\", \"id3\");\n    expectLastCall().times(1);\n\n    replayAll();\n    supervisor.start();\n    supervisor.runInternal();\n    verifyAll();\n  }\n","date":"2019-02-19 03:50:08","endLine":2568,"groupId":"16026","id":10,"instanceNumber":2,"isCurCommit":0,"methodName":"testSuspendedRunningTasks","params":"()","path":"/mnt/clonedata/CloneManagementServer/ManagementServer/consistResult/result-druid-10-0.7/blobInfo/CC_OUT/blobs/f2/db280c51eb85380271ce661d1c415120afa0e0.src","preCode":"  public void testSuspendedRunningTasks() throws Exception\n  {\n    \r\n\n    final TaskLocation location1 = new TaskLocation(\"testHost\", 1234, -1);\n    final TaskLocation location2 = new TaskLocation(\"testHost2\", 145, -1);\n    final DateTime startTime = DateTimes.nowUtc();\n\n    supervisor = getSupervisor(2, 1, true, \"PT1H\", null, null, false, true, kafkaHost);\n    addSomeEvents(1);\n\n    Task id1 = createKafkaIndexTask(\n        \"id1\",\n        DATASOURCE,\n        0,\n        new SeekableStreamPartitions<>(\"topic\", ImmutableMap.of(0, 0L, 1, 0L, 2, 0L)),\n        new SeekableStreamPartitions<>(\n            \"topic\",\n            ImmutableMap.of(0, Long.MAX_VALUE, 1, Long.MAX_VALUE, 2, Long.MAX_VALUE)\n        ),\n        null,\n        null\n    );\n\n    Task id2 = createKafkaIndexTask(\n        \"id2\",\n        DATASOURCE,\n        0,\n        new SeekableStreamPartitions<>(\"topic\", ImmutableMap.of(0, 10L, 1, 20L, 2, 30L)),\n        new SeekableStreamPartitions<>(\n            \"topic\",\n            ImmutableMap.of(0, Long.MAX_VALUE, 1, Long.MAX_VALUE, 2, Long.MAX_VALUE)\n        ),\n        null,\n        null\n    );\n\n    Task id3 = createKafkaIndexTask(\n        \"id3\",\n        DATASOURCE,\n        0,\n        new SeekableStreamPartitions<>(\"topic\", ImmutableMap.of(0, 10L, 1, 20L, 2, 30L)),\n        new SeekableStreamPartitions<>(\n            \"topic\",\n            ImmutableMap.of(0, Long.MAX_VALUE, 1, Long.MAX_VALUE, 2, Long.MAX_VALUE)\n        ),\n        null,\n        null\n    );\n\n    Collection workItems = new ArrayList<>();\n    workItems.add(new TestTaskRunnerWorkItem(id1, null, location1));\n    workItems.add(new TestTaskRunnerWorkItem(id2, null, location2));\n\n    expect(taskMaster.getTaskQueue()).andReturn(Optional.of(taskQueue)).anyTimes();\n    expect(taskMaster.getTaskRunner()).andReturn(Optional.of(taskRunner)).anyTimes();\n    expect(taskRunner.getRunningTasks()).andReturn(workItems).anyTimes();\n    expect(taskStorage.getActiveTasks()).andReturn(ImmutableList.of(id1, id2, id3)).anyTimes();\n    expect(taskStorage.getStatus(\"id1\")).andReturn(Optional.of(TaskStatus.running(\"id1\"))).anyTimes();\n    expect(taskStorage.getStatus(\"id2\")).andReturn(Optional.of(TaskStatus.running(\"id2\"))).anyTimes();\n    expect(taskStorage.getStatus(\"id3\")).andReturn(Optional.of(TaskStatus.running(\"id3\"))).anyTimes();\n    expect(taskStorage.getTask(\"id1\")).andReturn(Optional.of(id1)).anyTimes();\n    expect(taskStorage.getTask(\"id2\")).andReturn(Optional.of(id2)).anyTimes();\n    expect(taskStorage.getTask(\"id3\")).andReturn(Optional.of(id3)).anyTimes();\n    expect(indexerMetadataStorageCoordinator.getDataSourceMetadata(DATASOURCE)).andReturn(\n        new KafkaDataSourceMetadata(\n            null\n        )\n    ).anyTimes();\n    expect(taskClient.getStatusAsync(\"id1\"))\n        .andReturn(Futures.immediateFuture(Status.PUBLISHING));\n    expect(taskClient.getStatusAsync(\"id2\"))\n        .andReturn(Futures.immediateFuture(Status.READING));\n    expect(taskClient.getStatusAsync(\"id3\"))\n        .andReturn(Futures.immediateFuture(Status.READING));\n    expect(taskClient.getStartTimeAsync(\"id2\")).andReturn(Futures.immediateFuture(startTime));\n    expect(taskClient.getStartTimeAsync(\"id3\")).andReturn(Futures.immediateFuture(startTime));\n    expect(taskClient.getEndOffsets(\"id1\")).andReturn(ImmutableMap.of(0, 10L, 1, 20L, 2, 30L));\n\n    \r\n    TreeMap<Integer, Map<Integer, Long>> checkpoints = new TreeMap<>();\n    checkpoints.put(0, ImmutableMap.of(0, 10L, 1, 20L, 2, 30L));\n    expect(taskClient.getCheckpointsAsync(EasyMock.contains(\"id2\"), EasyMock.anyBoolean()))\n        .andReturn(Futures.immediateFuture(checkpoints))\n        .times(1);\n    expect(taskClient.getCheckpointsAsync(EasyMock.contains(\"id3\"), EasyMock.anyBoolean()))\n        .andReturn(Futures.immediateFuture(checkpoints))\n        .times(1);\n\n    taskRunner.registerListener(anyObject(TaskRunnerListener.class), anyObject(Executor.class));\n\n    expect(taskClient.pauseAsync(\"id2\"))\n        .andReturn(Futures.immediateFuture(ImmutableMap.of(0, 15L, 1, 25L, 2, 30L)));\n    expect(taskClient.setEndOffsetsAsync(\"id2\", ImmutableMap.of(0, 15L, 1, 25L, 2, 30L), true))\n        .andReturn(Futures.immediateFuture(true));\n    taskQueue.shutdown(\"id3\", \"Killing task for graceful shutdown\");\n    expectLastCall().times(1);\n    taskQueue.shutdown(\"id3\", \"Killing task [%s] which hasn't been assigned to a worker\", \"id3\");\n    expectLastCall().times(1);\n\n    replayAll();\n    supervisor.start();\n    supervisor.runInternal();\n    verifyAll();\n  }\n","realPath":"extensions-core/kafka-indexing-service/src/test/java/org/apache/druid/indexing/kafka/supervisor/KafkaSupervisorTest.java","repoName":"druid","snippetEndLine":0,"snippetStartLine":0,"startLine":2464,"status":"M"}],"commitId":"80a2ef7be46c4fc807ac2a74507b8ba8b6a44049","commitMessage":"@@@Support kafka transactional topics (#5404) (#6496)\n\n* Support kafka transactional topics\n\n* update kafka to version 2.0.0\n* Remove the skipOffsetGaps option since it's not used anymore\n* Adjust kafka consumer to use transactional semantics\n* Update tests\n\n* Remove unused import from test\n\n* Fix compilation\n\n* Invoke transaction api to fix a unit test\n\n* temporary modification of travis.yml for debugging\n\n* another attempt to get travis tasklogs\n\n* update kafka to 2.0.1 at all places\n\n* Remove druid-kafka-eight dependency from integration-tests.  remove the kafka firehose test and deprecate kafka-eight classes\n\n* Add deprecated in docs for kafka-eight and kafka-simple extensions\n\n* Remove skipOffsetGaps and code changes for transaction support\n\n* Fix indentation\n\n* remove skipOffsetGaps from kinesis\n\n* Add transaction api to KafkaRecordSupplierTest\n\n* Fix indent\n\n* Fix test\n\n* update kafka version to 2.1.0\n","date":"2019-02-19 03:50:08","modifiedFileCount":"25","status":"M","submitter":"Surekha"},{"authorTime":"2019-03-22 04:12:22","codes":[{"authorDate":"2019-03-22 04:12:22","commitOrder":7,"curCode":"  public void testStopGracefully() throws Exception\n  {\n    final TaskLocation location1 = new TaskLocation(\"testHost\", 1234, -1);\n    final TaskLocation location2 = new TaskLocation(\"testHost2\", 145, -1);\n    final DateTime startTime = DateTimes.nowUtc();\n\n    supervisor = getSupervisor(2, 1, true, \"PT1H\", null, null);\n    addSomeEvents(1);\n\n    Task id1 = createKafkaIndexTask(\n        \"id1\",\n        DATASOURCE,\n        0,\n        new SeekableStreamStartSequenceNumbers<>(\"topic\", ImmutableMap.of(0, 0L, 1, 0L, 2, 0L), ImmutableSet.of()),\n        new SeekableStreamEndSequenceNumbers<>(\n            \"topic\",\n            ImmutableMap.of(0, Long.MAX_VALUE, 1, Long.MAX_VALUE, 2, Long.MAX_VALUE)\n        ),\n        null,\n        null\n    );\n\n    Task id2 = createKafkaIndexTask(\n        \"id2\",\n        DATASOURCE,\n        0,\n        new SeekableStreamStartSequenceNumbers<>(\"topic\", ImmutableMap.of(0, 10L, 1, 20L, 2, 30L), ImmutableSet.of()),\n        new SeekableStreamEndSequenceNumbers<>(\n            \"topic\",\n            ImmutableMap.of(0, Long.MAX_VALUE, 1, Long.MAX_VALUE, 2, Long.MAX_VALUE)\n        ),\n        null,\n        null\n    );\n\n    Task id3 = createKafkaIndexTask(\n        \"id3\",\n        DATASOURCE,\n        0,\n        new SeekableStreamStartSequenceNumbers<>(\"topic\", ImmutableMap.of(0, 10L, 1, 20L, 2, 30L), ImmutableSet.of()),\n        new SeekableStreamEndSequenceNumbers<>(\n            \"topic\",\n            ImmutableMap.of(0, Long.MAX_VALUE, 1, Long.MAX_VALUE, 2, Long.MAX_VALUE)\n        ),\n        null,\n        null\n    );\n\n    Collection workItems = new ArrayList<>();\n    workItems.add(new TestTaskRunnerWorkItem(id1, null, location1));\n    workItems.add(new TestTaskRunnerWorkItem(id2, null, location2));\n\n    expect(taskMaster.getTaskQueue()).andReturn(Optional.of(taskQueue)).anyTimes();\n    expect(taskMaster.getTaskRunner()).andReturn(Optional.of(taskRunner)).anyTimes();\n    expect(taskRunner.getRunningTasks()).andReturn(workItems).anyTimes();\n    expect(taskStorage.getActiveTasks()).andReturn(ImmutableList.of(id1, id2, id3)).anyTimes();\n    expect(taskStorage.getStatus(\"id1\")).andReturn(Optional.of(TaskStatus.running(\"id1\"))).anyTimes();\n    expect(taskStorage.getStatus(\"id2\")).andReturn(Optional.of(TaskStatus.running(\"id2\"))).anyTimes();\n    expect(taskStorage.getStatus(\"id3\")).andReturn(Optional.of(TaskStatus.running(\"id3\"))).anyTimes();\n    expect(taskStorage.getTask(\"id1\")).andReturn(Optional.of(id1)).anyTimes();\n    expect(taskStorage.getTask(\"id2\")).andReturn(Optional.of(id2)).anyTimes();\n    expect(taskStorage.getTask(\"id3\")).andReturn(Optional.of(id3)).anyTimes();\n    expect(indexerMetadataStorageCoordinator.getDataSourceMetadata(DATASOURCE)).andReturn(\n        new KafkaDataSourceMetadata(\n            null\n        )\n    ).anyTimes();\n    expect(taskClient.getStatusAsync(\"id1\")).andReturn(Futures.immediateFuture(Status.PUBLISHING));\n    expect(taskClient.getStatusAsync(\"id2\")).andReturn(Futures.immediateFuture(Status.READING));\n    expect(taskClient.getStatusAsync(\"id3\")).andReturn(Futures.immediateFuture(Status.READING));\n    expect(taskClient.getStartTimeAsync(\"id2\")).andReturn(Futures.immediateFuture(startTime));\n    expect(taskClient.getStartTimeAsync(\"id3\")).andReturn(Futures.immediateFuture(startTime));\n    expect(taskClient.getEndOffsets(\"id1\")).andReturn(ImmutableMap.of(0, 10L, 1, 20L, 2, 30L));\n\n    \r\n    TreeMap<Integer, Map<Integer, Long>> checkpoints = new TreeMap<>();\n    checkpoints.put(0, ImmutableMap.of(0, 10L, 1, 20L, 2, 30L));\n    expect(taskClient.getCheckpointsAsync(EasyMock.contains(\"id2\"), EasyMock.anyBoolean()))\n        .andReturn(Futures.immediateFuture(checkpoints))\n        .times(1);\n    expect(taskClient.getCheckpointsAsync(EasyMock.contains(\"id3\"), EasyMock.anyBoolean()))\n        .andReturn(Futures.immediateFuture(checkpoints))\n        .times(1);\n\n    taskRunner.registerListener(anyObject(TaskRunnerListener.class), anyObject(Executor.class));\n    replayAll();\n\n    supervisor.start();\n    supervisor.runInternal();\n    verifyAll();\n\n    EasyMock.reset(taskRunner, taskClient, taskQueue);\n    expect(taskRunner.getRunningTasks()).andReturn(workItems).anyTimes();\n    expect(taskClient.pauseAsync(\"id2\"))\n        .andReturn(Futures.immediateFuture((Map<Integer, Long>) ImmutableMap.of(0, 15L, 1, 25L, 2, 30L)));\n    expect(taskClient.setEndOffsetsAsync(\"id2\", ImmutableMap.of(0, 15L, 1, 25L, 2, 30L), true))\n        .andReturn(Futures.immediateFuture(true));\n    taskQueue.shutdown(\"id3\", \"Killing task for graceful shutdown\");\n    expectLastCall().times(1);\n    taskQueue.shutdown(\"id3\", \"Killing task [%s] which hasn't been assigned to a worker\", \"id3\");\n    expectLastCall().times(1);\n\n    replay(taskRunner, taskClient, taskQueue);\n\n    supervisor.gracefulShutdownInternal();\n    verifyAll();\n  }\n","date":"2019-03-22 04:12:22","endLine":1926,"groupId":"16026","id":11,"instanceNumber":1,"isCurCommit":0,"methodName":"testStopGracefully","params":"()","path":"/mnt/clonedata/CloneManagementServer/ManagementServer/consistResult/result-druid-10-0.7/blobInfo/CC_OUT/blobs/b3/0b75ee7886fa6f4c326a81770066d6778e2033.src","preCode":"  public void testStopGracefully() throws Exception\n  {\n    final TaskLocation location1 = new TaskLocation(\"testHost\", 1234, -1);\n    final TaskLocation location2 = new TaskLocation(\"testHost2\", 145, -1);\n    final DateTime startTime = DateTimes.nowUtc();\n\n    supervisor = getSupervisor(2, 1, true, \"PT1H\", null, null);\n    addSomeEvents(1);\n\n    Task id1 = createKafkaIndexTask(\n        \"id1\",\n        DATASOURCE,\n        0,\n        new SeekableStreamPartitions<>(\"topic\", ImmutableMap.of(0, 0L, 1, 0L, 2, 0L)),\n        new SeekableStreamPartitions<>(\n            \"topic\",\n            ImmutableMap.of(0, Long.MAX_VALUE, 1, Long.MAX_VALUE, 2, Long.MAX_VALUE)\n        ),\n        null,\n        null\n    );\n\n    Task id2 = createKafkaIndexTask(\n        \"id2\",\n        DATASOURCE,\n        0,\n        new SeekableStreamPartitions<>(\"topic\", ImmutableMap.of(0, 10L, 1, 20L, 2, 30L)),\n        new SeekableStreamPartitions<>(\n            \"topic\",\n            ImmutableMap.of(0, Long.MAX_VALUE, 1, Long.MAX_VALUE, 2, Long.MAX_VALUE)\n        ),\n        null,\n        null\n    );\n\n    Task id3 = createKafkaIndexTask(\n        \"id3\",\n        DATASOURCE,\n        0,\n        new SeekableStreamPartitions<>(\"topic\", ImmutableMap.of(0, 10L, 1, 20L, 2, 30L)),\n        new SeekableStreamPartitions<>(\n            \"topic\",\n            ImmutableMap.of(0, Long.MAX_VALUE, 1, Long.MAX_VALUE, 2, Long.MAX_VALUE)\n        ),\n        null,\n        null\n    );\n\n    Collection workItems = new ArrayList<>();\n    workItems.add(new TestTaskRunnerWorkItem(id1, null, location1));\n    workItems.add(new TestTaskRunnerWorkItem(id2, null, location2));\n\n    expect(taskMaster.getTaskQueue()).andReturn(Optional.of(taskQueue)).anyTimes();\n    expect(taskMaster.getTaskRunner()).andReturn(Optional.of(taskRunner)).anyTimes();\n    expect(taskRunner.getRunningTasks()).andReturn(workItems).anyTimes();\n    expect(taskStorage.getActiveTasks()).andReturn(ImmutableList.of(id1, id2, id3)).anyTimes();\n    expect(taskStorage.getStatus(\"id1\")).andReturn(Optional.of(TaskStatus.running(\"id1\"))).anyTimes();\n    expect(taskStorage.getStatus(\"id2\")).andReturn(Optional.of(TaskStatus.running(\"id2\"))).anyTimes();\n    expect(taskStorage.getStatus(\"id3\")).andReturn(Optional.of(TaskStatus.running(\"id3\"))).anyTimes();\n    expect(taskStorage.getTask(\"id1\")).andReturn(Optional.of(id1)).anyTimes();\n    expect(taskStorage.getTask(\"id2\")).andReturn(Optional.of(id2)).anyTimes();\n    expect(taskStorage.getTask(\"id3\")).andReturn(Optional.of(id3)).anyTimes();\n    expect(indexerMetadataStorageCoordinator.getDataSourceMetadata(DATASOURCE)).andReturn(\n        new KafkaDataSourceMetadata(\n            null\n        )\n    ).anyTimes();\n    expect(taskClient.getStatusAsync(\"id1\")).andReturn(Futures.immediateFuture(Status.PUBLISHING));\n    expect(taskClient.getStatusAsync(\"id2\")).andReturn(Futures.immediateFuture(Status.READING));\n    expect(taskClient.getStatusAsync(\"id3\")).andReturn(Futures.immediateFuture(Status.READING));\n    expect(taskClient.getStartTimeAsync(\"id2\")).andReturn(Futures.immediateFuture(startTime));\n    expect(taskClient.getStartTimeAsync(\"id3\")).andReturn(Futures.immediateFuture(startTime));\n    expect(taskClient.getEndOffsets(\"id1\")).andReturn(ImmutableMap.of(0, 10L, 1, 20L, 2, 30L));\n\n    \r\n    TreeMap<Integer, Map<Integer, Long>> checkpoints = new TreeMap<>();\n    checkpoints.put(0, ImmutableMap.of(0, 10L, 1, 20L, 2, 30L));\n    expect(taskClient.getCheckpointsAsync(EasyMock.contains(\"id2\"), EasyMock.anyBoolean()))\n        .andReturn(Futures.immediateFuture(checkpoints))\n        .times(1);\n    expect(taskClient.getCheckpointsAsync(EasyMock.contains(\"id3\"), EasyMock.anyBoolean()))\n        .andReturn(Futures.immediateFuture(checkpoints))\n        .times(1);\n\n    taskRunner.registerListener(anyObject(TaskRunnerListener.class), anyObject(Executor.class));\n    replayAll();\n\n    supervisor.start();\n    supervisor.runInternal();\n    verifyAll();\n\n    EasyMock.reset(taskRunner, taskClient, taskQueue);\n    expect(taskRunner.getRunningTasks()).andReturn(workItems).anyTimes();\n    expect(taskClient.pauseAsync(\"id2\"))\n        .andReturn(Futures.immediateFuture((Map<Integer, Long>) ImmutableMap.of(0, 15L, 1, 25L, 2, 30L)));\n    expect(taskClient.setEndOffsetsAsync(\"id2\", ImmutableMap.of(0, 15L, 1, 25L, 2, 30L), true))\n        .andReturn(Futures.immediateFuture(true));\n    taskQueue.shutdown(\"id3\", \"Killing task for graceful shutdown\");\n    expectLastCall().times(1);\n    taskQueue.shutdown(\"id3\", \"Killing task [%s] which hasn't been assigned to a worker\", \"id3\");\n    expectLastCall().times(1);\n\n    replay(taskRunner, taskClient, taskQueue);\n\n    supervisor.gracefulShutdownInternal();\n    verifyAll();\n  }\n","realPath":"extensions-core/kafka-indexing-service/src/test/java/org/apache/druid/indexing/kafka/supervisor/KafkaSupervisorTest.java","repoName":"druid","snippetEndLine":0,"snippetStartLine":0,"startLine":1820,"status":"M"},{"authorDate":"2019-03-22 04:12:22","commitOrder":7,"curCode":"  public void testSuspendedRunningTasks() throws Exception\n  {\n    \r\n\n    final TaskLocation location1 = new TaskLocation(\"testHost\", 1234, -1);\n    final TaskLocation location2 = new TaskLocation(\"testHost2\", 145, -1);\n    final DateTime startTime = DateTimes.nowUtc();\n\n    supervisor = getSupervisor(2, 1, true, \"PT1H\", null, null, true, kafkaHost);\n    addSomeEvents(1);\n\n    Task id1 = createKafkaIndexTask(\n        \"id1\",\n        DATASOURCE,\n        0,\n        new SeekableStreamStartSequenceNumbers<>(\"topic\", ImmutableMap.of(0, 0L, 1, 0L, 2, 0L), ImmutableSet.of()),\n        new SeekableStreamEndSequenceNumbers<>(\n            \"topic\",\n            ImmutableMap.of(0, Long.MAX_VALUE, 1, Long.MAX_VALUE, 2, Long.MAX_VALUE)\n        ),\n        null,\n        null\n    );\n\n    Task id2 = createKafkaIndexTask(\n        \"id2\",\n        DATASOURCE,\n        0,\n        new SeekableStreamStartSequenceNumbers<>(\"topic\", ImmutableMap.of(0, 10L, 1, 20L, 2, 30L), ImmutableSet.of()),\n        new SeekableStreamEndSequenceNumbers<>(\n            \"topic\",\n            ImmutableMap.of(0, Long.MAX_VALUE, 1, Long.MAX_VALUE, 2, Long.MAX_VALUE)\n        ),\n        null,\n        null\n    );\n\n    Task id3 = createKafkaIndexTask(\n        \"id3\",\n        DATASOURCE,\n        0,\n        new SeekableStreamStartSequenceNumbers<>(\"topic\", ImmutableMap.of(0, 10L, 1, 20L, 2, 30L), ImmutableSet.of()),\n        new SeekableStreamEndSequenceNumbers<>(\n            \"topic\",\n            ImmutableMap.of(0, Long.MAX_VALUE, 1, Long.MAX_VALUE, 2, Long.MAX_VALUE)\n        ),\n        null,\n        null\n    );\n\n    Collection workItems = new ArrayList<>();\n    workItems.add(new TestTaskRunnerWorkItem(id1, null, location1));\n    workItems.add(new TestTaskRunnerWorkItem(id2, null, location2));\n\n    expect(taskMaster.getTaskQueue()).andReturn(Optional.of(taskQueue)).anyTimes();\n    expect(taskMaster.getTaskRunner()).andReturn(Optional.of(taskRunner)).anyTimes();\n    expect(taskRunner.getRunningTasks()).andReturn(workItems).anyTimes();\n    expect(taskStorage.getActiveTasks()).andReturn(ImmutableList.of(id1, id2, id3)).anyTimes();\n    expect(taskStorage.getStatus(\"id1\")).andReturn(Optional.of(TaskStatus.running(\"id1\"))).anyTimes();\n    expect(taskStorage.getStatus(\"id2\")).andReturn(Optional.of(TaskStatus.running(\"id2\"))).anyTimes();\n    expect(taskStorage.getStatus(\"id3\")).andReturn(Optional.of(TaskStatus.running(\"id3\"))).anyTimes();\n    expect(taskStorage.getTask(\"id1\")).andReturn(Optional.of(id1)).anyTimes();\n    expect(taskStorage.getTask(\"id2\")).andReturn(Optional.of(id2)).anyTimes();\n    expect(taskStorage.getTask(\"id3\")).andReturn(Optional.of(id3)).anyTimes();\n    expect(indexerMetadataStorageCoordinator.getDataSourceMetadata(DATASOURCE)).andReturn(\n        new KafkaDataSourceMetadata(\n            null\n        )\n    ).anyTimes();\n    expect(taskClient.getStatusAsync(\"id1\"))\n        .andReturn(Futures.immediateFuture(Status.PUBLISHING));\n    expect(taskClient.getStatusAsync(\"id2\"))\n        .andReturn(Futures.immediateFuture(Status.READING));\n    expect(taskClient.getStatusAsync(\"id3\"))\n        .andReturn(Futures.immediateFuture(Status.READING));\n    expect(taskClient.getStartTimeAsync(\"id2\")).andReturn(Futures.immediateFuture(startTime));\n    expect(taskClient.getStartTimeAsync(\"id3\")).andReturn(Futures.immediateFuture(startTime));\n    expect(taskClient.getEndOffsets(\"id1\")).andReturn(ImmutableMap.of(0, 10L, 1, 20L, 2, 30L));\n\n    \r\n    TreeMap<Integer, Map<Integer, Long>> checkpoints = new TreeMap<>();\n    checkpoints.put(0, ImmutableMap.of(0, 10L, 1, 20L, 2, 30L));\n    expect(taskClient.getCheckpointsAsync(EasyMock.contains(\"id2\"), EasyMock.anyBoolean()))\n        .andReturn(Futures.immediateFuture(checkpoints))\n        .times(1);\n    expect(taskClient.getCheckpointsAsync(EasyMock.contains(\"id3\"), EasyMock.anyBoolean()))\n        .andReturn(Futures.immediateFuture(checkpoints))\n        .times(1);\n\n    taskRunner.registerListener(anyObject(TaskRunnerListener.class), anyObject(Executor.class));\n\n    expect(taskClient.pauseAsync(\"id2\"))\n        .andReturn(Futures.immediateFuture(ImmutableMap.of(0, 15L, 1, 25L, 2, 30L)));\n    expect(taskClient.setEndOffsetsAsync(\"id2\", ImmutableMap.of(0, 15L, 1, 25L, 2, 30L), true))\n        .andReturn(Futures.immediateFuture(true));\n    taskQueue.shutdown(\"id3\", \"Killing task for graceful shutdown\");\n    expectLastCall().times(1);\n    taskQueue.shutdown(\"id3\", \"Killing task [%s] which hasn't been assigned to a worker\", \"id3\");\n    expectLastCall().times(1);\n\n    replayAll();\n    supervisor.start();\n    supervisor.runInternal();\n    verifyAll();\n  }\n","date":"2019-03-22 04:12:22","endLine":2661,"groupId":"16026","id":12,"instanceNumber":2,"isCurCommit":0,"methodName":"testSuspendedRunningTasks","params":"()","path":"/mnt/clonedata/CloneManagementServer/ManagementServer/consistResult/result-druid-10-0.7/blobInfo/CC_OUT/blobs/b3/0b75ee7886fa6f4c326a81770066d6778e2033.src","preCode":"  public void testSuspendedRunningTasks() throws Exception\n  {\n    \r\n\n    final TaskLocation location1 = new TaskLocation(\"testHost\", 1234, -1);\n    final TaskLocation location2 = new TaskLocation(\"testHost2\", 145, -1);\n    final DateTime startTime = DateTimes.nowUtc();\n\n    supervisor = getSupervisor(2, 1, true, \"PT1H\", null, null, true, kafkaHost);\n    addSomeEvents(1);\n\n    Task id1 = createKafkaIndexTask(\n        \"id1\",\n        DATASOURCE,\n        0,\n        new SeekableStreamPartitions<>(\"topic\", ImmutableMap.of(0, 0L, 1, 0L, 2, 0L)),\n        new SeekableStreamPartitions<>(\n            \"topic\",\n            ImmutableMap.of(0, Long.MAX_VALUE, 1, Long.MAX_VALUE, 2, Long.MAX_VALUE)\n        ),\n        null,\n        null\n    );\n\n    Task id2 = createKafkaIndexTask(\n        \"id2\",\n        DATASOURCE,\n        0,\n        new SeekableStreamPartitions<>(\"topic\", ImmutableMap.of(0, 10L, 1, 20L, 2, 30L)),\n        new SeekableStreamPartitions<>(\n            \"topic\",\n            ImmutableMap.of(0, Long.MAX_VALUE, 1, Long.MAX_VALUE, 2, Long.MAX_VALUE)\n        ),\n        null,\n        null\n    );\n\n    Task id3 = createKafkaIndexTask(\n        \"id3\",\n        DATASOURCE,\n        0,\n        new SeekableStreamPartitions<>(\"topic\", ImmutableMap.of(0, 10L, 1, 20L, 2, 30L)),\n        new SeekableStreamPartitions<>(\n            \"topic\",\n            ImmutableMap.of(0, Long.MAX_VALUE, 1, Long.MAX_VALUE, 2, Long.MAX_VALUE)\n        ),\n        null,\n        null\n    );\n\n    Collection workItems = new ArrayList<>();\n    workItems.add(new TestTaskRunnerWorkItem(id1, null, location1));\n    workItems.add(new TestTaskRunnerWorkItem(id2, null, location2));\n\n    expect(taskMaster.getTaskQueue()).andReturn(Optional.of(taskQueue)).anyTimes();\n    expect(taskMaster.getTaskRunner()).andReturn(Optional.of(taskRunner)).anyTimes();\n    expect(taskRunner.getRunningTasks()).andReturn(workItems).anyTimes();\n    expect(taskStorage.getActiveTasks()).andReturn(ImmutableList.of(id1, id2, id3)).anyTimes();\n    expect(taskStorage.getStatus(\"id1\")).andReturn(Optional.of(TaskStatus.running(\"id1\"))).anyTimes();\n    expect(taskStorage.getStatus(\"id2\")).andReturn(Optional.of(TaskStatus.running(\"id2\"))).anyTimes();\n    expect(taskStorage.getStatus(\"id3\")).andReturn(Optional.of(TaskStatus.running(\"id3\"))).anyTimes();\n    expect(taskStorage.getTask(\"id1\")).andReturn(Optional.of(id1)).anyTimes();\n    expect(taskStorage.getTask(\"id2\")).andReturn(Optional.of(id2)).anyTimes();\n    expect(taskStorage.getTask(\"id3\")).andReturn(Optional.of(id3)).anyTimes();\n    expect(indexerMetadataStorageCoordinator.getDataSourceMetadata(DATASOURCE)).andReturn(\n        new KafkaDataSourceMetadata(\n            null\n        )\n    ).anyTimes();\n    expect(taskClient.getStatusAsync(\"id1\"))\n        .andReturn(Futures.immediateFuture(Status.PUBLISHING));\n    expect(taskClient.getStatusAsync(\"id2\"))\n        .andReturn(Futures.immediateFuture(Status.READING));\n    expect(taskClient.getStatusAsync(\"id3\"))\n        .andReturn(Futures.immediateFuture(Status.READING));\n    expect(taskClient.getStartTimeAsync(\"id2\")).andReturn(Futures.immediateFuture(startTime));\n    expect(taskClient.getStartTimeAsync(\"id3\")).andReturn(Futures.immediateFuture(startTime));\n    expect(taskClient.getEndOffsets(\"id1\")).andReturn(ImmutableMap.of(0, 10L, 1, 20L, 2, 30L));\n\n    \r\n    TreeMap<Integer, Map<Integer, Long>> checkpoints = new TreeMap<>();\n    checkpoints.put(0, ImmutableMap.of(0, 10L, 1, 20L, 2, 30L));\n    expect(taskClient.getCheckpointsAsync(EasyMock.contains(\"id2\"), EasyMock.anyBoolean()))\n        .andReturn(Futures.immediateFuture(checkpoints))\n        .times(1);\n    expect(taskClient.getCheckpointsAsync(EasyMock.contains(\"id3\"), EasyMock.anyBoolean()))\n        .andReturn(Futures.immediateFuture(checkpoints))\n        .times(1);\n\n    taskRunner.registerListener(anyObject(TaskRunnerListener.class), anyObject(Executor.class));\n\n    expect(taskClient.pauseAsync(\"id2\"))\n        .andReturn(Futures.immediateFuture(ImmutableMap.of(0, 15L, 1, 25L, 2, 30L)));\n    expect(taskClient.setEndOffsetsAsync(\"id2\", ImmutableMap.of(0, 15L, 1, 25L, 2, 30L), true))\n        .andReturn(Futures.immediateFuture(true));\n    taskQueue.shutdown(\"id3\", \"Killing task for graceful shutdown\");\n    expectLastCall().times(1);\n    taskQueue.shutdown(\"id3\", \"Killing task [%s] which hasn't been assigned to a worker\", \"id3\");\n    expectLastCall().times(1);\n\n    replayAll();\n    supervisor.start();\n    supervisor.runInternal();\n    verifyAll();\n  }\n","realPath":"extensions-core/kafka-indexing-service/src/test/java/org/apache/druid/indexing/kafka/supervisor/KafkaSupervisorTest.java","repoName":"druid","snippetEndLine":0,"snippetStartLine":0,"startLine":2557,"status":"M"}],"commitId":"0c5dcf5586e33607849e397209f3eb0b10661f1e","commitMessage":"@@@Fix exclusivity for start offset in kinesis indexing service & check exclusivity properly in IndexerSQLMetadataStorageCoordinator (#7291)\n\n* Fix exclusivity for start offset in kinesis indexing service\n\n* some adjustment\n\n* Fix SeekableStreamDataSourceMetadata\n\n* Add missing javadocs\n\n* Add missing comments and unit test\n\n* fix SeekableStreamStartSequenceNumbers.plus and add comments\n\n* remove extra exclusivePartitions in KafkaIOConfig and fix downgrade issue\n\n* Add javadocs\n\n* fix compilation\n\n* fix test\n\n* remove unused variable\n","date":"2019-03-22 04:12:22","modifiedFileCount":"30","status":"M","submitter":"Jihoon Son"},{"authorTime":"2019-04-11 09:16:38","codes":[{"authorDate":"2019-04-11 09:16:38","commitOrder":8,"curCode":"  public void testStopGracefully() throws Exception\n  {\n    final TaskLocation location1 = new TaskLocation(\"testHost\", 1234, -1);\n    final TaskLocation location2 = new TaskLocation(\"testHost2\", 145, -1);\n    final DateTime startTime = DateTimes.nowUtc();\n\n    supervisor = getTestableSupervisor(2, 1, true, \"PT1H\", null, null);\n    addSomeEvents(1);\n\n    Task id1 = createKafkaIndexTask(\n        \"id1\",\n        DATASOURCE,\n        0,\n        new SeekableStreamStartSequenceNumbers<>(\"topic\", ImmutableMap.of(0, 0L, 1, 0L, 2, 0L), ImmutableSet.of()),\n        new SeekableStreamEndSequenceNumbers<>(\n            \"topic\",\n            ImmutableMap.of(0, Long.MAX_VALUE, 1, Long.MAX_VALUE, 2, Long.MAX_VALUE)\n        ),\n        null,\n        null\n    );\n\n    Task id2 = createKafkaIndexTask(\n        \"id2\",\n        DATASOURCE,\n        0,\n        new SeekableStreamStartSequenceNumbers<>(\"topic\", ImmutableMap.of(0, 10L, 1, 20L, 2, 30L), ImmutableSet.of()),\n        new SeekableStreamEndSequenceNumbers<>(\n            \"topic\",\n            ImmutableMap.of(0, Long.MAX_VALUE, 1, Long.MAX_VALUE, 2, Long.MAX_VALUE)\n        ),\n        null,\n        null\n    );\n\n    Task id3 = createKafkaIndexTask(\n        \"id3\",\n        DATASOURCE,\n        0,\n        new SeekableStreamStartSequenceNumbers<>(\"topic\", ImmutableMap.of(0, 10L, 1, 20L, 2, 30L), ImmutableSet.of()),\n        new SeekableStreamEndSequenceNumbers<>(\n            \"topic\",\n            ImmutableMap.of(0, Long.MAX_VALUE, 1, Long.MAX_VALUE, 2, Long.MAX_VALUE)\n        ),\n        null,\n        null\n    );\n\n    Collection workItems = new ArrayList<>();\n    workItems.add(new TestTaskRunnerWorkItem(id1, null, location1));\n    workItems.add(new TestTaskRunnerWorkItem(id2, null, location2));\n\n    expect(taskMaster.getTaskQueue()).andReturn(Optional.of(taskQueue)).anyTimes();\n    expect(taskMaster.getTaskRunner()).andReturn(Optional.of(taskRunner)).anyTimes();\n    expect(taskRunner.getRunningTasks()).andReturn(workItems).anyTimes();\n    expect(taskStorage.getActiveTasks()).andReturn(ImmutableList.of(id1, id2, id3)).anyTimes();\n    expect(taskStorage.getStatus(\"id1\")).andReturn(Optional.of(TaskStatus.running(\"id1\"))).anyTimes();\n    expect(taskStorage.getStatus(\"id2\")).andReturn(Optional.of(TaskStatus.running(\"id2\"))).anyTimes();\n    expect(taskStorage.getStatus(\"id3\")).andReturn(Optional.of(TaskStatus.running(\"id3\"))).anyTimes();\n    expect(taskStorage.getTask(\"id1\")).andReturn(Optional.of(id1)).anyTimes();\n    expect(taskStorage.getTask(\"id2\")).andReturn(Optional.of(id2)).anyTimes();\n    expect(taskStorage.getTask(\"id3\")).andReturn(Optional.of(id3)).anyTimes();\n    expect(indexerMetadataStorageCoordinator.getDataSourceMetadata(DATASOURCE)).andReturn(\n        new KafkaDataSourceMetadata(\n            null\n        )\n    ).anyTimes();\n    expect(taskClient.getStatusAsync(\"id1\")).andReturn(Futures.immediateFuture(Status.PUBLISHING));\n    expect(taskClient.getStatusAsync(\"id2\")).andReturn(Futures.immediateFuture(Status.READING));\n    expect(taskClient.getStatusAsync(\"id3\")).andReturn(Futures.immediateFuture(Status.READING));\n    expect(taskClient.getStartTimeAsync(\"id2\")).andReturn(Futures.immediateFuture(startTime));\n    expect(taskClient.getStartTimeAsync(\"id3\")).andReturn(Futures.immediateFuture(startTime));\n    expect(taskClient.getEndOffsets(\"id1\")).andReturn(ImmutableMap.of(0, 10L, 1, 20L, 2, 30L));\n\n    \r\n    TreeMap<Integer, Map<Integer, Long>> checkpoints = new TreeMap<>();\n    checkpoints.put(0, ImmutableMap.of(0, 10L, 1, 20L, 2, 30L));\n    expect(taskClient.getCheckpointsAsync(EasyMock.contains(\"id2\"), EasyMock.anyBoolean()))\n        .andReturn(Futures.immediateFuture(checkpoints))\n        .times(1);\n    expect(taskClient.getCheckpointsAsync(EasyMock.contains(\"id3\"), EasyMock.anyBoolean()))\n        .andReturn(Futures.immediateFuture(checkpoints))\n        .times(1);\n\n    taskRunner.registerListener(anyObject(TaskRunnerListener.class), anyObject(Executor.class));\n    replayAll();\n\n    supervisor.start();\n    supervisor.runInternal();\n    verifyAll();\n\n    EasyMock.reset(taskRunner, taskClient, taskQueue);\n    expect(taskRunner.getRunningTasks()).andReturn(workItems).anyTimes();\n    expect(taskClient.pauseAsync(\"id2\"))\n        .andReturn(Futures.immediateFuture((Map<Integer, Long>) ImmutableMap.of(0, 15L, 1, 25L, 2, 30L)));\n    expect(taskClient.setEndOffsetsAsync(\"id2\", ImmutableMap.of(0, 15L, 1, 25L, 2, 30L), true))\n        .andReturn(Futures.immediateFuture(true));\n    taskQueue.shutdown(\"id3\", \"Killing task for graceful shutdown\");\n    expectLastCall().times(1);\n    taskQueue.shutdown(\"id3\", \"Killing task [%s] which hasn't been assigned to a worker\", \"id3\");\n    expectLastCall().times(1);\n\n    replay(taskRunner, taskClient, taskQueue);\n\n    supervisor.gracefulShutdownInternal();\n    verifyAll();\n  }\n","date":"2019-04-11 09:16:38","endLine":1890,"groupId":"16026","id":13,"instanceNumber":1,"isCurCommit":0,"methodName":"testStopGracefully","params":"()","path":"/mnt/clonedata/CloneManagementServer/ManagementServer/consistResult/result-druid-10-0.7/blobInfo/CC_OUT/blobs/b6/e7b3406771642225b6ce082b4b2bc68e735a67.src","preCode":"  public void testStopGracefully() throws Exception\n  {\n    final TaskLocation location1 = new TaskLocation(\"testHost\", 1234, -1);\n    final TaskLocation location2 = new TaskLocation(\"testHost2\", 145, -1);\n    final DateTime startTime = DateTimes.nowUtc();\n\n    supervisor = getSupervisor(2, 1, true, \"PT1H\", null, null);\n    addSomeEvents(1);\n\n    Task id1 = createKafkaIndexTask(\n        \"id1\",\n        DATASOURCE,\n        0,\n        new SeekableStreamStartSequenceNumbers<>(\"topic\", ImmutableMap.of(0, 0L, 1, 0L, 2, 0L), ImmutableSet.of()),\n        new SeekableStreamEndSequenceNumbers<>(\n            \"topic\",\n            ImmutableMap.of(0, Long.MAX_VALUE, 1, Long.MAX_VALUE, 2, Long.MAX_VALUE)\n        ),\n        null,\n        null\n    );\n\n    Task id2 = createKafkaIndexTask(\n        \"id2\",\n        DATASOURCE,\n        0,\n        new SeekableStreamStartSequenceNumbers<>(\"topic\", ImmutableMap.of(0, 10L, 1, 20L, 2, 30L), ImmutableSet.of()),\n        new SeekableStreamEndSequenceNumbers<>(\n            \"topic\",\n            ImmutableMap.of(0, Long.MAX_VALUE, 1, Long.MAX_VALUE, 2, Long.MAX_VALUE)\n        ),\n        null,\n        null\n    );\n\n    Task id3 = createKafkaIndexTask(\n        \"id3\",\n        DATASOURCE,\n        0,\n        new SeekableStreamStartSequenceNumbers<>(\"topic\", ImmutableMap.of(0, 10L, 1, 20L, 2, 30L), ImmutableSet.of()),\n        new SeekableStreamEndSequenceNumbers<>(\n            \"topic\",\n            ImmutableMap.of(0, Long.MAX_VALUE, 1, Long.MAX_VALUE, 2, Long.MAX_VALUE)\n        ),\n        null,\n        null\n    );\n\n    Collection workItems = new ArrayList<>();\n    workItems.add(new TestTaskRunnerWorkItem(id1, null, location1));\n    workItems.add(new TestTaskRunnerWorkItem(id2, null, location2));\n\n    expect(taskMaster.getTaskQueue()).andReturn(Optional.of(taskQueue)).anyTimes();\n    expect(taskMaster.getTaskRunner()).andReturn(Optional.of(taskRunner)).anyTimes();\n    expect(taskRunner.getRunningTasks()).andReturn(workItems).anyTimes();\n    expect(taskStorage.getActiveTasks()).andReturn(ImmutableList.of(id1, id2, id3)).anyTimes();\n    expect(taskStorage.getStatus(\"id1\")).andReturn(Optional.of(TaskStatus.running(\"id1\"))).anyTimes();\n    expect(taskStorage.getStatus(\"id2\")).andReturn(Optional.of(TaskStatus.running(\"id2\"))).anyTimes();\n    expect(taskStorage.getStatus(\"id3\")).andReturn(Optional.of(TaskStatus.running(\"id3\"))).anyTimes();\n    expect(taskStorage.getTask(\"id1\")).andReturn(Optional.of(id1)).anyTimes();\n    expect(taskStorage.getTask(\"id2\")).andReturn(Optional.of(id2)).anyTimes();\n    expect(taskStorage.getTask(\"id3\")).andReturn(Optional.of(id3)).anyTimes();\n    expect(indexerMetadataStorageCoordinator.getDataSourceMetadata(DATASOURCE)).andReturn(\n        new KafkaDataSourceMetadata(\n            null\n        )\n    ).anyTimes();\n    expect(taskClient.getStatusAsync(\"id1\")).andReturn(Futures.immediateFuture(Status.PUBLISHING));\n    expect(taskClient.getStatusAsync(\"id2\")).andReturn(Futures.immediateFuture(Status.READING));\n    expect(taskClient.getStatusAsync(\"id3\")).andReturn(Futures.immediateFuture(Status.READING));\n    expect(taskClient.getStartTimeAsync(\"id2\")).andReturn(Futures.immediateFuture(startTime));\n    expect(taskClient.getStartTimeAsync(\"id3\")).andReturn(Futures.immediateFuture(startTime));\n    expect(taskClient.getEndOffsets(\"id1\")).andReturn(ImmutableMap.of(0, 10L, 1, 20L, 2, 30L));\n\n    \r\n    TreeMap<Integer, Map<Integer, Long>> checkpoints = new TreeMap<>();\n    checkpoints.put(0, ImmutableMap.of(0, 10L, 1, 20L, 2, 30L));\n    expect(taskClient.getCheckpointsAsync(EasyMock.contains(\"id2\"), EasyMock.anyBoolean()))\n        .andReturn(Futures.immediateFuture(checkpoints))\n        .times(1);\n    expect(taskClient.getCheckpointsAsync(EasyMock.contains(\"id3\"), EasyMock.anyBoolean()))\n        .andReturn(Futures.immediateFuture(checkpoints))\n        .times(1);\n\n    taskRunner.registerListener(anyObject(TaskRunnerListener.class), anyObject(Executor.class));\n    replayAll();\n\n    supervisor.start();\n    supervisor.runInternal();\n    verifyAll();\n\n    EasyMock.reset(taskRunner, taskClient, taskQueue);\n    expect(taskRunner.getRunningTasks()).andReturn(workItems).anyTimes();\n    expect(taskClient.pauseAsync(\"id2\"))\n        .andReturn(Futures.immediateFuture((Map<Integer, Long>) ImmutableMap.of(0, 15L, 1, 25L, 2, 30L)));\n    expect(taskClient.setEndOffsetsAsync(\"id2\", ImmutableMap.of(0, 15L, 1, 25L, 2, 30L), true))\n        .andReturn(Futures.immediateFuture(true));\n    taskQueue.shutdown(\"id3\", \"Killing task for graceful shutdown\");\n    expectLastCall().times(1);\n    taskQueue.shutdown(\"id3\", \"Killing task [%s] which hasn't been assigned to a worker\", \"id3\");\n    expectLastCall().times(1);\n\n    replay(taskRunner, taskClient, taskQueue);\n\n    supervisor.gracefulShutdownInternal();\n    verifyAll();\n  }\n","realPath":"extensions-core/kafka-indexing-service/src/test/java/org/apache/druid/indexing/kafka/supervisor/KafkaSupervisorTest.java","repoName":"druid","snippetEndLine":0,"snippetStartLine":0,"startLine":1784,"status":"M"},{"authorDate":"2019-04-11 09:16:38","commitOrder":8,"curCode":"  public void testSuspendedRunningTasks() throws Exception\n  {\n    \r\n\n    final TaskLocation location1 = new TaskLocation(\"testHost\", 1234, -1);\n    final TaskLocation location2 = new TaskLocation(\"testHost2\", 145, -1);\n    final DateTime startTime = DateTimes.nowUtc();\n\n    supervisor = getTestableSupervisor(2, 1, true, \"PT1H\", null, null, true, kafkaHost);\n    addSomeEvents(1);\n\n    Task id1 = createKafkaIndexTask(\n        \"id1\",\n        DATASOURCE,\n        0,\n        new SeekableStreamStartSequenceNumbers<>(\"topic\", ImmutableMap.of(0, 0L, 1, 0L, 2, 0L), ImmutableSet.of()),\n        new SeekableStreamEndSequenceNumbers<>(\n            \"topic\",\n            ImmutableMap.of(0, Long.MAX_VALUE, 1, Long.MAX_VALUE, 2, Long.MAX_VALUE)\n        ),\n        null,\n        null\n    );\n\n    Task id2 = createKafkaIndexTask(\n        \"id2\",\n        DATASOURCE,\n        0,\n        new SeekableStreamStartSequenceNumbers<>(\"topic\", ImmutableMap.of(0, 10L, 1, 20L, 2, 30L), ImmutableSet.of()),\n        new SeekableStreamEndSequenceNumbers<>(\n            \"topic\",\n            ImmutableMap.of(0, Long.MAX_VALUE, 1, Long.MAX_VALUE, 2, Long.MAX_VALUE)\n        ),\n        null,\n        null\n    );\n\n    Task id3 = createKafkaIndexTask(\n        \"id3\",\n        DATASOURCE,\n        0,\n        new SeekableStreamStartSequenceNumbers<>(\"topic\", ImmutableMap.of(0, 10L, 1, 20L, 2, 30L), ImmutableSet.of()),\n        new SeekableStreamEndSequenceNumbers<>(\n            \"topic\",\n            ImmutableMap.of(0, Long.MAX_VALUE, 1, Long.MAX_VALUE, 2, Long.MAX_VALUE)\n        ),\n        null,\n        null\n    );\n\n    Collection workItems = new ArrayList<>();\n    workItems.add(new TestTaskRunnerWorkItem(id1, null, location1));\n    workItems.add(new TestTaskRunnerWorkItem(id2, null, location2));\n\n    expect(taskMaster.getTaskQueue()).andReturn(Optional.of(taskQueue)).anyTimes();\n    expect(taskMaster.getTaskRunner()).andReturn(Optional.of(taskRunner)).anyTimes();\n    expect(taskRunner.getRunningTasks()).andReturn(workItems).anyTimes();\n    expect(taskStorage.getActiveTasks()).andReturn(ImmutableList.of(id1, id2, id3)).anyTimes();\n    expect(taskStorage.getStatus(\"id1\")).andReturn(Optional.of(TaskStatus.running(\"id1\"))).anyTimes();\n    expect(taskStorage.getStatus(\"id2\")).andReturn(Optional.of(TaskStatus.running(\"id2\"))).anyTimes();\n    expect(taskStorage.getStatus(\"id3\")).andReturn(Optional.of(TaskStatus.running(\"id3\"))).anyTimes();\n    expect(taskStorage.getTask(\"id1\")).andReturn(Optional.of(id1)).anyTimes();\n    expect(taskStorage.getTask(\"id2\")).andReturn(Optional.of(id2)).anyTimes();\n    expect(taskStorage.getTask(\"id3\")).andReturn(Optional.of(id3)).anyTimes();\n    expect(indexerMetadataStorageCoordinator.getDataSourceMetadata(DATASOURCE)).andReturn(\n        new KafkaDataSourceMetadata(\n            null\n        )\n    ).anyTimes();\n    expect(taskClient.getStatusAsync(\"id1\"))\n        .andReturn(Futures.immediateFuture(Status.PUBLISHING));\n    expect(taskClient.getStatusAsync(\"id2\"))\n        .andReturn(Futures.immediateFuture(Status.READING));\n    expect(taskClient.getStatusAsync(\"id3\"))\n        .andReturn(Futures.immediateFuture(Status.READING));\n    expect(taskClient.getStartTimeAsync(\"id2\")).andReturn(Futures.immediateFuture(startTime));\n    expect(taskClient.getStartTimeAsync(\"id3\")).andReturn(Futures.immediateFuture(startTime));\n    expect(taskClient.getEndOffsets(\"id1\")).andReturn(ImmutableMap.of(0, 10L, 1, 20L, 2, 30L));\n\n    \r\n    TreeMap<Integer, Map<Integer, Long>> checkpoints = new TreeMap<>();\n    checkpoints.put(0, ImmutableMap.of(0, 10L, 1, 20L, 2, 30L));\n    expect(taskClient.getCheckpointsAsync(EasyMock.contains(\"id2\"), EasyMock.anyBoolean()))\n        .andReturn(Futures.immediateFuture(checkpoints))\n        .times(1);\n    expect(taskClient.getCheckpointsAsync(EasyMock.contains(\"id3\"), EasyMock.anyBoolean()))\n        .andReturn(Futures.immediateFuture(checkpoints))\n        .times(1);\n\n    taskRunner.registerListener(anyObject(TaskRunnerListener.class), anyObject(Executor.class));\n\n    expect(taskClient.pauseAsync(\"id2\"))\n        .andReturn(Futures.immediateFuture(ImmutableMap.of(0, 15L, 1, 25L, 2, 30L)));\n    expect(taskClient.setEndOffsetsAsync(\"id2\", ImmutableMap.of(0, 15L, 1, 25L, 2, 30L), true))\n        .andReturn(Futures.immediateFuture(true));\n    taskQueue.shutdown(\"id3\", \"Killing task for graceful shutdown\");\n    expectLastCall().times(1);\n    taskQueue.shutdown(\"id3\", \"Killing task [%s] which hasn't been assigned to a worker\", \"id3\");\n    expectLastCall().times(1);\n\n    replayAll();\n    supervisor.start();\n    supervisor.runInternal();\n    verifyAll();\n  }\n","date":"2019-04-11 09:16:38","endLine":2649,"groupId":"16026","id":14,"instanceNumber":2,"isCurCommit":0,"methodName":"testSuspendedRunningTasks","params":"()","path":"/mnt/clonedata/CloneManagementServer/ManagementServer/consistResult/result-druid-10-0.7/blobInfo/CC_OUT/blobs/b6/e7b3406771642225b6ce082b4b2bc68e735a67.src","preCode":"  public void testSuspendedRunningTasks() throws Exception\n  {\n    \r\n\n    final TaskLocation location1 = new TaskLocation(\"testHost\", 1234, -1);\n    final TaskLocation location2 = new TaskLocation(\"testHost2\", 145, -1);\n    final DateTime startTime = DateTimes.nowUtc();\n\n    supervisor = getSupervisor(2, 1, true, \"PT1H\", null, null, true, kafkaHost);\n    addSomeEvents(1);\n\n    Task id1 = createKafkaIndexTask(\n        \"id1\",\n        DATASOURCE,\n        0,\n        new SeekableStreamStartSequenceNumbers<>(\"topic\", ImmutableMap.of(0, 0L, 1, 0L, 2, 0L), ImmutableSet.of()),\n        new SeekableStreamEndSequenceNumbers<>(\n            \"topic\",\n            ImmutableMap.of(0, Long.MAX_VALUE, 1, Long.MAX_VALUE, 2, Long.MAX_VALUE)\n        ),\n        null,\n        null\n    );\n\n    Task id2 = createKafkaIndexTask(\n        \"id2\",\n        DATASOURCE,\n        0,\n        new SeekableStreamStartSequenceNumbers<>(\"topic\", ImmutableMap.of(0, 10L, 1, 20L, 2, 30L), ImmutableSet.of()),\n        new SeekableStreamEndSequenceNumbers<>(\n            \"topic\",\n            ImmutableMap.of(0, Long.MAX_VALUE, 1, Long.MAX_VALUE, 2, Long.MAX_VALUE)\n        ),\n        null,\n        null\n    );\n\n    Task id3 = createKafkaIndexTask(\n        \"id3\",\n        DATASOURCE,\n        0,\n        new SeekableStreamStartSequenceNumbers<>(\"topic\", ImmutableMap.of(0, 10L, 1, 20L, 2, 30L), ImmutableSet.of()),\n        new SeekableStreamEndSequenceNumbers<>(\n            \"topic\",\n            ImmutableMap.of(0, Long.MAX_VALUE, 1, Long.MAX_VALUE, 2, Long.MAX_VALUE)\n        ),\n        null,\n        null\n    );\n\n    Collection workItems = new ArrayList<>();\n    workItems.add(new TestTaskRunnerWorkItem(id1, null, location1));\n    workItems.add(new TestTaskRunnerWorkItem(id2, null, location2));\n\n    expect(taskMaster.getTaskQueue()).andReturn(Optional.of(taskQueue)).anyTimes();\n    expect(taskMaster.getTaskRunner()).andReturn(Optional.of(taskRunner)).anyTimes();\n    expect(taskRunner.getRunningTasks()).andReturn(workItems).anyTimes();\n    expect(taskStorage.getActiveTasks()).andReturn(ImmutableList.of(id1, id2, id3)).anyTimes();\n    expect(taskStorage.getStatus(\"id1\")).andReturn(Optional.of(TaskStatus.running(\"id1\"))).anyTimes();\n    expect(taskStorage.getStatus(\"id2\")).andReturn(Optional.of(TaskStatus.running(\"id2\"))).anyTimes();\n    expect(taskStorage.getStatus(\"id3\")).andReturn(Optional.of(TaskStatus.running(\"id3\"))).anyTimes();\n    expect(taskStorage.getTask(\"id1\")).andReturn(Optional.of(id1)).anyTimes();\n    expect(taskStorage.getTask(\"id2\")).andReturn(Optional.of(id2)).anyTimes();\n    expect(taskStorage.getTask(\"id3\")).andReturn(Optional.of(id3)).anyTimes();\n    expect(indexerMetadataStorageCoordinator.getDataSourceMetadata(DATASOURCE)).andReturn(\n        new KafkaDataSourceMetadata(\n            null\n        )\n    ).anyTimes();\n    expect(taskClient.getStatusAsync(\"id1\"))\n        .andReturn(Futures.immediateFuture(Status.PUBLISHING));\n    expect(taskClient.getStatusAsync(\"id2\"))\n        .andReturn(Futures.immediateFuture(Status.READING));\n    expect(taskClient.getStatusAsync(\"id3\"))\n        .andReturn(Futures.immediateFuture(Status.READING));\n    expect(taskClient.getStartTimeAsync(\"id2\")).andReturn(Futures.immediateFuture(startTime));\n    expect(taskClient.getStartTimeAsync(\"id3\")).andReturn(Futures.immediateFuture(startTime));\n    expect(taskClient.getEndOffsets(\"id1\")).andReturn(ImmutableMap.of(0, 10L, 1, 20L, 2, 30L));\n\n    \r\n    TreeMap<Integer, Map<Integer, Long>> checkpoints = new TreeMap<>();\n    checkpoints.put(0, ImmutableMap.of(0, 10L, 1, 20L, 2, 30L));\n    expect(taskClient.getCheckpointsAsync(EasyMock.contains(\"id2\"), EasyMock.anyBoolean()))\n        .andReturn(Futures.immediateFuture(checkpoints))\n        .times(1);\n    expect(taskClient.getCheckpointsAsync(EasyMock.contains(\"id3\"), EasyMock.anyBoolean()))\n        .andReturn(Futures.immediateFuture(checkpoints))\n        .times(1);\n\n    taskRunner.registerListener(anyObject(TaskRunnerListener.class), anyObject(Executor.class));\n\n    expect(taskClient.pauseAsync(\"id2\"))\n        .andReturn(Futures.immediateFuture(ImmutableMap.of(0, 15L, 1, 25L, 2, 30L)));\n    expect(taskClient.setEndOffsetsAsync(\"id2\", ImmutableMap.of(0, 15L, 1, 25L, 2, 30L), true))\n        .andReturn(Futures.immediateFuture(true));\n    taskQueue.shutdown(\"id3\", \"Killing task for graceful shutdown\");\n    expectLastCall().times(1);\n    taskQueue.shutdown(\"id3\", \"Killing task [%s] which hasn't been assigned to a worker\", \"id3\");\n    expectLastCall().times(1);\n\n    replayAll();\n    supervisor.start();\n    supervisor.runInternal();\n    verifyAll();\n  }\n","realPath":"extensions-core/kafka-indexing-service/src/test/java/org/apache/druid/indexing/kafka/supervisor/KafkaSupervisorTest.java","repoName":"druid","snippetEndLine":0,"snippetStartLine":0,"startLine":2545,"status":"M"}],"commitId":"2771ed50b0f07b0ee519da72ed9f4877466f8be4","commitMessage":"@@@Support Kafka supervisor adopting running tasks between versions  (#7212)\n\n* Recompute hash in isTaskCurrent() and added tests\n\n* Fixed checkstyle stuff\n\n* Fixed failing tests\n\n* Make TestableKafkaSupervisorWithCustomIsTaskCurrent static\n\n* Add doc\n\n* baseSequenceName change\n\n* Added comment\n\n* WIP\n\n* Fixed imports\n\n* Undid lambda change for diff sake\n\n* Cleanup\n\n* Added comment\n\n* Reinsert Kafka tests\n\n* Readded kinesis test\n\n* Readd bad partition assignment in kinesis supervisor test\n\n* Nit\n\n* Misnamed var\n","date":"2019-04-11 09:16:38","modifiedFileCount":"6","status":"M","submitter":"Justin Borromeo"},{"authorTime":"2019-07-07 00:33:12","codes":[{"authorDate":"2019-07-07 00:33:12","commitOrder":9,"curCode":"  public void testStopGracefully() throws Exception\n  {\n    final TaskLocation location1 = new TaskLocation(\"testHost\", 1234, -1);\n    final TaskLocation location2 = new TaskLocation(\"testHost2\", 145, -1);\n    final DateTime startTime = DateTimes.nowUtc();\n\n    supervisor = getTestableSupervisor(2, 1, true, \"PT1H\", null, null);\n    addSomeEvents(1);\n\n    Task id1 = createKafkaIndexTask(\n        \"id1\",\n        DATASOURCE,\n        0,\n        new SeekableStreamStartSequenceNumbers<>(\"topic\", ImmutableMap.of(0, 0L, 1, 0L, 2, 0L), ImmutableSet.of()),\n        new SeekableStreamEndSequenceNumbers<>(\n            \"topic\",\n            ImmutableMap.of(0, Long.MAX_VALUE, 1, Long.MAX_VALUE, 2, Long.MAX_VALUE)\n        ),\n        null,\n        null\n    );\n\n    Task id2 = createKafkaIndexTask(\n        \"id2\",\n        DATASOURCE,\n        0,\n        new SeekableStreamStartSequenceNumbers<>(\"topic\", ImmutableMap.of(0, 10L, 1, 20L, 2, 30L), ImmutableSet.of()),\n        new SeekableStreamEndSequenceNumbers<>(\n            \"topic\",\n            ImmutableMap.of(0, Long.MAX_VALUE, 1, Long.MAX_VALUE, 2, Long.MAX_VALUE)\n        ),\n        null,\n        null\n    );\n\n    Task id3 = createKafkaIndexTask(\n        \"id3\",\n        DATASOURCE,\n        0,\n        new SeekableStreamStartSequenceNumbers<>(\"topic\", ImmutableMap.of(0, 10L, 1, 20L, 2, 30L), ImmutableSet.of()),\n        new SeekableStreamEndSequenceNumbers<>(\n            \"topic\",\n            ImmutableMap.of(0, Long.MAX_VALUE, 1, Long.MAX_VALUE, 2, Long.MAX_VALUE)\n        ),\n        null,\n        null\n    );\n\n    Collection workItems = new ArrayList<>();\n    workItems.add(new TestTaskRunnerWorkItem(id1, null, location1));\n    workItems.add(new TestTaskRunnerWorkItem(id2, null, location2));\n\n    EasyMock.expect(taskMaster.getTaskQueue()).andReturn(Optional.of(taskQueue)).anyTimes();\n    EasyMock.expect(taskMaster.getTaskRunner()).andReturn(Optional.of(taskRunner)).anyTimes();\n    EasyMock.expect(taskRunner.getRunningTasks()).andReturn(workItems).anyTimes();\n    EasyMock.expect(taskStorage.getActiveTasks()).andReturn(ImmutableList.of(id1, id2, id3)).anyTimes();\n    EasyMock.expect(taskStorage.getStatus(\"id1\")).andReturn(Optional.of(TaskStatus.running(\"id1\"))).anyTimes();\n    EasyMock.expect(taskStorage.getStatus(\"id2\")).andReturn(Optional.of(TaskStatus.running(\"id2\"))).anyTimes();\n    EasyMock.expect(taskStorage.getStatus(\"id3\")).andReturn(Optional.of(TaskStatus.running(\"id3\"))).anyTimes();\n    EasyMock.expect(taskStorage.getTask(\"id1\")).andReturn(Optional.of(id1)).anyTimes();\n    EasyMock.expect(taskStorage.getTask(\"id2\")).andReturn(Optional.of(id2)).anyTimes();\n    EasyMock.expect(taskStorage.getTask(\"id3\")).andReturn(Optional.of(id3)).anyTimes();\n    EasyMock.expect(indexerMetadataStorageCoordinator.getDataSourceMetadata(DATASOURCE)).andReturn(\n        new KafkaDataSourceMetadata(\n            null\n        )\n    ).anyTimes();\n    EasyMock.expect(taskClient.getStatusAsync(\"id1\")).andReturn(Futures.immediateFuture(Status.PUBLISHING));\n    EasyMock.expect(taskClient.getStatusAsync(\"id2\")).andReturn(Futures.immediateFuture(Status.READING));\n    EasyMock.expect(taskClient.getStatusAsync(\"id3\")).andReturn(Futures.immediateFuture(Status.READING));\n    EasyMock.expect(taskClient.getStartTimeAsync(\"id2\")).andReturn(Futures.immediateFuture(startTime));\n    EasyMock.expect(taskClient.getStartTimeAsync(\"id3\")).andReturn(Futures.immediateFuture(startTime));\n    EasyMock.expect(taskClient.getEndOffsets(\"id1\")).andReturn(ImmutableMap.of(0, 10L, 1, 20L, 2, 30L));\n\n    \r\n    TreeMap<Integer, Map<Integer, Long>> checkpoints = new TreeMap<>();\n    checkpoints.put(0, ImmutableMap.of(0, 10L, 1, 20L, 2, 30L));\n    EasyMock.expect(taskClient.getCheckpointsAsync(EasyMock.contains(\"id2\"), EasyMock.anyBoolean()))\n            .andReturn(Futures.immediateFuture(checkpoints))\n            .times(1);\n    EasyMock.expect(taskClient.getCheckpointsAsync(EasyMock.contains(\"id3\"), EasyMock.anyBoolean()))\n            .andReturn(Futures.immediateFuture(checkpoints))\n            .times(1);\n\n    taskRunner.registerListener(EasyMock.anyObject(TaskRunnerListener.class), EasyMock.anyObject(Executor.class));\n    replayAll();\n\n    supervisor.start();\n    supervisor.runInternal();\n    verifyAll();\n\n    EasyMock.reset(taskRunner, taskClient, taskQueue);\n    EasyMock.expect(taskRunner.getRunningTasks()).andReturn(workItems).anyTimes();\n    EasyMock.expect(taskClient.pauseAsync(\"id2\"))\n            .andReturn(Futures.immediateFuture(ImmutableMap.of(0, 15L, 1, 25L, 2, 30L)));\n    EasyMock.expect(taskClient.setEndOffsetsAsync(\"id2\", ImmutableMap.of(0, 15L, 1, 25L, 2, 30L), true))\n            .andReturn(Futures.immediateFuture(true));\n    taskQueue.shutdown(\"id3\", \"Killing task for graceful shutdown\");\n    EasyMock.expectLastCall().times(1);\n    taskQueue.shutdown(\"id3\", \"Killing task [%s] which hasn't been assigned to a worker\", \"id3\");\n    EasyMock.expectLastCall().times(1);\n\n    EasyMock.replay(taskRunner, taskClient, taskQueue);\n\n    supervisor.gracefulShutdownInternal();\n    verifyAll();\n  }\n","date":"2019-07-07 00:33:12","endLine":1894,"groupId":"10584","id":15,"instanceNumber":1,"isCurCommit":0,"methodName":"testStopGracefully","params":"()","path":"/mnt/clonedata/CloneManagementServer/ManagementServer/consistResult/result-druid-10-0.7/blobInfo/CC_OUT/blobs/af/f5639cd0e0e7c6a18b14f694e3cf2c6cb73dec.src","preCode":"  public void testStopGracefully() throws Exception\n  {\n    final TaskLocation location1 = new TaskLocation(\"testHost\", 1234, -1);\n    final TaskLocation location2 = new TaskLocation(\"testHost2\", 145, -1);\n    final DateTime startTime = DateTimes.nowUtc();\n\n    supervisor = getTestableSupervisor(2, 1, true, \"PT1H\", null, null);\n    addSomeEvents(1);\n\n    Task id1 = createKafkaIndexTask(\n        \"id1\",\n        DATASOURCE,\n        0,\n        new SeekableStreamStartSequenceNumbers<>(\"topic\", ImmutableMap.of(0, 0L, 1, 0L, 2, 0L), ImmutableSet.of()),\n        new SeekableStreamEndSequenceNumbers<>(\n            \"topic\",\n            ImmutableMap.of(0, Long.MAX_VALUE, 1, Long.MAX_VALUE, 2, Long.MAX_VALUE)\n        ),\n        null,\n        null\n    );\n\n    Task id2 = createKafkaIndexTask(\n        \"id2\",\n        DATASOURCE,\n        0,\n        new SeekableStreamStartSequenceNumbers<>(\"topic\", ImmutableMap.of(0, 10L, 1, 20L, 2, 30L), ImmutableSet.of()),\n        new SeekableStreamEndSequenceNumbers<>(\n            \"topic\",\n            ImmutableMap.of(0, Long.MAX_VALUE, 1, Long.MAX_VALUE, 2, Long.MAX_VALUE)\n        ),\n        null,\n        null\n    );\n\n    Task id3 = createKafkaIndexTask(\n        \"id3\",\n        DATASOURCE,\n        0,\n        new SeekableStreamStartSequenceNumbers<>(\"topic\", ImmutableMap.of(0, 10L, 1, 20L, 2, 30L), ImmutableSet.of()),\n        new SeekableStreamEndSequenceNumbers<>(\n            \"topic\",\n            ImmutableMap.of(0, Long.MAX_VALUE, 1, Long.MAX_VALUE, 2, Long.MAX_VALUE)\n        ),\n        null,\n        null\n    );\n\n    Collection workItems = new ArrayList<>();\n    workItems.add(new TestTaskRunnerWorkItem(id1, null, location1));\n    workItems.add(new TestTaskRunnerWorkItem(id2, null, location2));\n\n    expect(taskMaster.getTaskQueue()).andReturn(Optional.of(taskQueue)).anyTimes();\n    expect(taskMaster.getTaskRunner()).andReturn(Optional.of(taskRunner)).anyTimes();\n    expect(taskRunner.getRunningTasks()).andReturn(workItems).anyTimes();\n    expect(taskStorage.getActiveTasks()).andReturn(ImmutableList.of(id1, id2, id3)).anyTimes();\n    expect(taskStorage.getStatus(\"id1\")).andReturn(Optional.of(TaskStatus.running(\"id1\"))).anyTimes();\n    expect(taskStorage.getStatus(\"id2\")).andReturn(Optional.of(TaskStatus.running(\"id2\"))).anyTimes();\n    expect(taskStorage.getStatus(\"id3\")).andReturn(Optional.of(TaskStatus.running(\"id3\"))).anyTimes();\n    expect(taskStorage.getTask(\"id1\")).andReturn(Optional.of(id1)).anyTimes();\n    expect(taskStorage.getTask(\"id2\")).andReturn(Optional.of(id2)).anyTimes();\n    expect(taskStorage.getTask(\"id3\")).andReturn(Optional.of(id3)).anyTimes();\n    expect(indexerMetadataStorageCoordinator.getDataSourceMetadata(DATASOURCE)).andReturn(\n        new KafkaDataSourceMetadata(\n            null\n        )\n    ).anyTimes();\n    expect(taskClient.getStatusAsync(\"id1\")).andReturn(Futures.immediateFuture(Status.PUBLISHING));\n    expect(taskClient.getStatusAsync(\"id2\")).andReturn(Futures.immediateFuture(Status.READING));\n    expect(taskClient.getStatusAsync(\"id3\")).andReturn(Futures.immediateFuture(Status.READING));\n    expect(taskClient.getStartTimeAsync(\"id2\")).andReturn(Futures.immediateFuture(startTime));\n    expect(taskClient.getStartTimeAsync(\"id3\")).andReturn(Futures.immediateFuture(startTime));\n    expect(taskClient.getEndOffsets(\"id1\")).andReturn(ImmutableMap.of(0, 10L, 1, 20L, 2, 30L));\n\n    \r\n    TreeMap<Integer, Map<Integer, Long>> checkpoints = new TreeMap<>();\n    checkpoints.put(0, ImmutableMap.of(0, 10L, 1, 20L, 2, 30L));\n    expect(taskClient.getCheckpointsAsync(EasyMock.contains(\"id2\"), EasyMock.anyBoolean()))\n        .andReturn(Futures.immediateFuture(checkpoints))\n        .times(1);\n    expect(taskClient.getCheckpointsAsync(EasyMock.contains(\"id3\"), EasyMock.anyBoolean()))\n        .andReturn(Futures.immediateFuture(checkpoints))\n        .times(1);\n\n    taskRunner.registerListener(anyObject(TaskRunnerListener.class), anyObject(Executor.class));\n    replayAll();\n\n    supervisor.start();\n    supervisor.runInternal();\n    verifyAll();\n\n    EasyMock.reset(taskRunner, taskClient, taskQueue);\n    expect(taskRunner.getRunningTasks()).andReturn(workItems).anyTimes();\n    expect(taskClient.pauseAsync(\"id2\"))\n        .andReturn(Futures.immediateFuture((Map<Integer, Long>) ImmutableMap.of(0, 15L, 1, 25L, 2, 30L)));\n    expect(taskClient.setEndOffsetsAsync(\"id2\", ImmutableMap.of(0, 15L, 1, 25L, 2, 30L), true))\n        .andReturn(Futures.immediateFuture(true));\n    taskQueue.shutdown(\"id3\", \"Killing task for graceful shutdown\");\n    expectLastCall().times(1);\n    taskQueue.shutdown(\"id3\", \"Killing task [%s] which hasn't been assigned to a worker\", \"id3\");\n    expectLastCall().times(1);\n\n    replay(taskRunner, taskClient, taskQueue);\n\n    supervisor.gracefulShutdownInternal();\n    verifyAll();\n  }\n","realPath":"extensions-core/kafka-indexing-service/src/test/java/org/apache/druid/indexing/kafka/supervisor/KafkaSupervisorTest.java","repoName":"druid","snippetEndLine":0,"snippetStartLine":0,"startLine":1788,"status":"M"},{"authorDate":"2019-07-07 00:33:12","commitOrder":9,"curCode":"  public void testSuspendedRunningTasks() throws Exception\n  {\n    \r\n\n    final TaskLocation location1 = new TaskLocation(\"testHost\", 1234, -1);\n    final TaskLocation location2 = new TaskLocation(\"testHost2\", 145, -1);\n    final DateTime startTime = DateTimes.nowUtc();\n\n    supervisor = getTestableSupervisor(2, 1, true, \"PT1H\", null, null, true, kafkaHost);\n    addSomeEvents(1);\n\n    Task id1 = createKafkaIndexTask(\n        \"id1\",\n        DATASOURCE,\n        0,\n        new SeekableStreamStartSequenceNumbers<>(\"topic\", ImmutableMap.of(0, 0L, 1, 0L, 2, 0L), ImmutableSet.of()),\n        new SeekableStreamEndSequenceNumbers<>(\n            \"topic\",\n            ImmutableMap.of(0, Long.MAX_VALUE, 1, Long.MAX_VALUE, 2, Long.MAX_VALUE)\n        ),\n        null,\n        null\n    );\n\n    Task id2 = createKafkaIndexTask(\n        \"id2\",\n        DATASOURCE,\n        0,\n        new SeekableStreamStartSequenceNumbers<>(\"topic\", ImmutableMap.of(0, 10L, 1, 20L, 2, 30L), ImmutableSet.of()),\n        new SeekableStreamEndSequenceNumbers<>(\n            \"topic\",\n            ImmutableMap.of(0, Long.MAX_VALUE, 1, Long.MAX_VALUE, 2, Long.MAX_VALUE)\n        ),\n        null,\n        null\n    );\n\n    Task id3 = createKafkaIndexTask(\n        \"id3\",\n        DATASOURCE,\n        0,\n        new SeekableStreamStartSequenceNumbers<>(\"topic\", ImmutableMap.of(0, 10L, 1, 20L, 2, 30L), ImmutableSet.of()),\n        new SeekableStreamEndSequenceNumbers<>(\n            \"topic\",\n            ImmutableMap.of(0, Long.MAX_VALUE, 1, Long.MAX_VALUE, 2, Long.MAX_VALUE)\n        ),\n        null,\n        null\n    );\n\n    Collection workItems = new ArrayList<>();\n    workItems.add(new TestTaskRunnerWorkItem(id1, null, location1));\n    workItems.add(new TestTaskRunnerWorkItem(id2, null, location2));\n\n    EasyMock.expect(taskMaster.getTaskQueue()).andReturn(Optional.of(taskQueue)).anyTimes();\n    EasyMock.expect(taskMaster.getTaskRunner()).andReturn(Optional.of(taskRunner)).anyTimes();\n    EasyMock.expect(taskRunner.getRunningTasks()).andReturn(workItems).anyTimes();\n    EasyMock.expect(taskStorage.getActiveTasks()).andReturn(ImmutableList.of(id1, id2, id3)).anyTimes();\n    EasyMock.expect(taskStorage.getStatus(\"id1\")).andReturn(Optional.of(TaskStatus.running(\"id1\"))).anyTimes();\n    EasyMock.expect(taskStorage.getStatus(\"id2\")).andReturn(Optional.of(TaskStatus.running(\"id2\"))).anyTimes();\n    EasyMock.expect(taskStorage.getStatus(\"id3\")).andReturn(Optional.of(TaskStatus.running(\"id3\"))).anyTimes();\n    EasyMock.expect(taskStorage.getTask(\"id1\")).andReturn(Optional.of(id1)).anyTimes();\n    EasyMock.expect(taskStorage.getTask(\"id2\")).andReturn(Optional.of(id2)).anyTimes();\n    EasyMock.expect(taskStorage.getTask(\"id3\")).andReturn(Optional.of(id3)).anyTimes();\n    EasyMock.expect(indexerMetadataStorageCoordinator.getDataSourceMetadata(DATASOURCE)).andReturn(\n        new KafkaDataSourceMetadata(\n            null\n        )\n    ).anyTimes();\n    EasyMock.expect(taskClient.getStatusAsync(\"id1\"))\n            .andReturn(Futures.immediateFuture(Status.PUBLISHING));\n    EasyMock.expect(taskClient.getStatusAsync(\"id2\"))\n            .andReturn(Futures.immediateFuture(Status.READING));\n    EasyMock.expect(taskClient.getStatusAsync(\"id3\"))\n            .andReturn(Futures.immediateFuture(Status.READING));\n    EasyMock.expect(taskClient.getStartTimeAsync(\"id2\")).andReturn(Futures.immediateFuture(startTime));\n    EasyMock.expect(taskClient.getStartTimeAsync(\"id3\")).andReturn(Futures.immediateFuture(startTime));\n    EasyMock.expect(taskClient.getEndOffsets(\"id1\")).andReturn(ImmutableMap.of(0, 10L, 1, 20L, 2, 30L));\n\n    \r\n    TreeMap<Integer, Map<Integer, Long>> checkpoints = new TreeMap<>();\n    checkpoints.put(0, ImmutableMap.of(0, 10L, 1, 20L, 2, 30L));\n    EasyMock.expect(taskClient.getCheckpointsAsync(EasyMock.contains(\"id2\"), EasyMock.anyBoolean()))\n            .andReturn(Futures.immediateFuture(checkpoints))\n            .times(1);\n    EasyMock.expect(taskClient.getCheckpointsAsync(EasyMock.contains(\"id3\"), EasyMock.anyBoolean()))\n            .andReturn(Futures.immediateFuture(checkpoints))\n            .times(1);\n\n    taskRunner.registerListener(EasyMock.anyObject(TaskRunnerListener.class), EasyMock.anyObject(Executor.class));\n\n    EasyMock.expect(taskClient.pauseAsync(\"id2\"))\n            .andReturn(Futures.immediateFuture(ImmutableMap.of(0, 15L, 1, 25L, 2, 30L)));\n    EasyMock.expect(taskClient.setEndOffsetsAsync(\"id2\", ImmutableMap.of(0, 15L, 1, 25L, 2, 30L), true))\n            .andReturn(Futures.immediateFuture(true));\n    taskQueue.shutdown(\"id3\", \"Killing task for graceful shutdown\");\n    EasyMock.expectLastCall().times(1);\n    taskQueue.shutdown(\"id3\", \"Killing task [%s] which hasn't been assigned to a worker\", \"id3\");\n    EasyMock.expectLastCall().times(1);\n\n    replayAll();\n    supervisor.start();\n    supervisor.runInternal();\n    verifyAll();\n  }\n","date":"2019-07-07 00:33:12","endLine":2659,"groupId":"10584","id":16,"instanceNumber":2,"isCurCommit":0,"methodName":"testSuspendedRunningTasks","params":"()","path":"/mnt/clonedata/CloneManagementServer/ManagementServer/consistResult/result-druid-10-0.7/blobInfo/CC_OUT/blobs/af/f5639cd0e0e7c6a18b14f694e3cf2c6cb73dec.src","preCode":"  public void testSuspendedRunningTasks() throws Exception\n  {\n    \r\n\n    final TaskLocation location1 = new TaskLocation(\"testHost\", 1234, -1);\n    final TaskLocation location2 = new TaskLocation(\"testHost2\", 145, -1);\n    final DateTime startTime = DateTimes.nowUtc();\n\n    supervisor = getTestableSupervisor(2, 1, true, \"PT1H\", null, null, true, kafkaHost);\n    addSomeEvents(1);\n\n    Task id1 = createKafkaIndexTask(\n        \"id1\",\n        DATASOURCE,\n        0,\n        new SeekableStreamStartSequenceNumbers<>(\"topic\", ImmutableMap.of(0, 0L, 1, 0L, 2, 0L), ImmutableSet.of()),\n        new SeekableStreamEndSequenceNumbers<>(\n            \"topic\",\n            ImmutableMap.of(0, Long.MAX_VALUE, 1, Long.MAX_VALUE, 2, Long.MAX_VALUE)\n        ),\n        null,\n        null\n    );\n\n    Task id2 = createKafkaIndexTask(\n        \"id2\",\n        DATASOURCE,\n        0,\n        new SeekableStreamStartSequenceNumbers<>(\"topic\", ImmutableMap.of(0, 10L, 1, 20L, 2, 30L), ImmutableSet.of()),\n        new SeekableStreamEndSequenceNumbers<>(\n            \"topic\",\n            ImmutableMap.of(0, Long.MAX_VALUE, 1, Long.MAX_VALUE, 2, Long.MAX_VALUE)\n        ),\n        null,\n        null\n    );\n\n    Task id3 = createKafkaIndexTask(\n        \"id3\",\n        DATASOURCE,\n        0,\n        new SeekableStreamStartSequenceNumbers<>(\"topic\", ImmutableMap.of(0, 10L, 1, 20L, 2, 30L), ImmutableSet.of()),\n        new SeekableStreamEndSequenceNumbers<>(\n            \"topic\",\n            ImmutableMap.of(0, Long.MAX_VALUE, 1, Long.MAX_VALUE, 2, Long.MAX_VALUE)\n        ),\n        null,\n        null\n    );\n\n    Collection workItems = new ArrayList<>();\n    workItems.add(new TestTaskRunnerWorkItem(id1, null, location1));\n    workItems.add(new TestTaskRunnerWorkItem(id2, null, location2));\n\n    expect(taskMaster.getTaskQueue()).andReturn(Optional.of(taskQueue)).anyTimes();\n    expect(taskMaster.getTaskRunner()).andReturn(Optional.of(taskRunner)).anyTimes();\n    expect(taskRunner.getRunningTasks()).andReturn(workItems).anyTimes();\n    expect(taskStorage.getActiveTasks()).andReturn(ImmutableList.of(id1, id2, id3)).anyTimes();\n    expect(taskStorage.getStatus(\"id1\")).andReturn(Optional.of(TaskStatus.running(\"id1\"))).anyTimes();\n    expect(taskStorage.getStatus(\"id2\")).andReturn(Optional.of(TaskStatus.running(\"id2\"))).anyTimes();\n    expect(taskStorage.getStatus(\"id3\")).andReturn(Optional.of(TaskStatus.running(\"id3\"))).anyTimes();\n    expect(taskStorage.getTask(\"id1\")).andReturn(Optional.of(id1)).anyTimes();\n    expect(taskStorage.getTask(\"id2\")).andReturn(Optional.of(id2)).anyTimes();\n    expect(taskStorage.getTask(\"id3\")).andReturn(Optional.of(id3)).anyTimes();\n    expect(indexerMetadataStorageCoordinator.getDataSourceMetadata(DATASOURCE)).andReturn(\n        new KafkaDataSourceMetadata(\n            null\n        )\n    ).anyTimes();\n    expect(taskClient.getStatusAsync(\"id1\"))\n        .andReturn(Futures.immediateFuture(Status.PUBLISHING));\n    expect(taskClient.getStatusAsync(\"id2\"))\n        .andReturn(Futures.immediateFuture(Status.READING));\n    expect(taskClient.getStatusAsync(\"id3\"))\n        .andReturn(Futures.immediateFuture(Status.READING));\n    expect(taskClient.getStartTimeAsync(\"id2\")).andReturn(Futures.immediateFuture(startTime));\n    expect(taskClient.getStartTimeAsync(\"id3\")).andReturn(Futures.immediateFuture(startTime));\n    expect(taskClient.getEndOffsets(\"id1\")).andReturn(ImmutableMap.of(0, 10L, 1, 20L, 2, 30L));\n\n    \r\n    TreeMap<Integer, Map<Integer, Long>> checkpoints = new TreeMap<>();\n    checkpoints.put(0, ImmutableMap.of(0, 10L, 1, 20L, 2, 30L));\n    expect(taskClient.getCheckpointsAsync(EasyMock.contains(\"id2\"), EasyMock.anyBoolean()))\n        .andReturn(Futures.immediateFuture(checkpoints))\n        .times(1);\n    expect(taskClient.getCheckpointsAsync(EasyMock.contains(\"id3\"), EasyMock.anyBoolean()))\n        .andReturn(Futures.immediateFuture(checkpoints))\n        .times(1);\n\n    taskRunner.registerListener(anyObject(TaskRunnerListener.class), anyObject(Executor.class));\n\n    expect(taskClient.pauseAsync(\"id2\"))\n        .andReturn(Futures.immediateFuture(ImmutableMap.of(0, 15L, 1, 25L, 2, 30L)));\n    expect(taskClient.setEndOffsetsAsync(\"id2\", ImmutableMap.of(0, 15L, 1, 25L, 2, 30L), true))\n        .andReturn(Futures.immediateFuture(true));\n    taskQueue.shutdown(\"id3\", \"Killing task for graceful shutdown\");\n    expectLastCall().times(1);\n    taskQueue.shutdown(\"id3\", \"Killing task [%s] which hasn't been assigned to a worker\", \"id3\");\n    expectLastCall().times(1);\n\n    replayAll();\n    supervisor.start();\n    supervisor.runInternal();\n    verifyAll();\n  }\n","realPath":"extensions-core/kafka-indexing-service/src/test/java/org/apache/druid/indexing/kafka/supervisor/KafkaSupervisorTest.java","repoName":"druid","snippetEndLine":0,"snippetStartLine":0,"startLine":2555,"status":"M"}],"commitId":"1166bbcb75d432817715fdd429737f86730b5591","commitMessage":"@@@Remove static imports from tests (#8036)\n\nMake static imports forbidden in tests and remove all occurrences to be\nconsistent with the non-test code.\n\nAlso.  various changes to files affected by above:\n- Reformat to adhere to druid style guide\n- Fix various IntelliJ warnings\n- Fix various SonarLint warnings (e.g..  the expected/actual args to\n  Assert.assertEquals() were flipped)","date":"2019-07-07 00:33:12","modifiedFileCount":"98","status":"M","submitter":"Chi Cao Minh"},{"authorTime":"2019-08-23 05:51:25","codes":[{"authorDate":"2019-08-23 05:51:25","commitOrder":10,"curCode":"  public void testStopGracefully() throws Exception\n  {\n    final TaskLocation location1 = new TaskLocation(\"testHost\", 1234, -1);\n    final TaskLocation location2 = new TaskLocation(\"testHost2\", 145, -1);\n    final DateTime startTime = DateTimes.nowUtc();\n\n    supervisor = getTestableSupervisor(2, 1, true, \"PT1H\", null, null);\n    final KafkaSupervisorTuningConfig tuningConfig = supervisor.getTuningConfig();\n    addSomeEvents(1);\n\n    Task id1 = createKafkaIndexTask(\n        \"id1\",\n        DATASOURCE,\n        0,\n        new SeekableStreamStartSequenceNumbers<>(\"topic\", ImmutableMap.of(0, 0L, 1, 0L, 2, 0L), ImmutableSet.of()),\n        new SeekableStreamEndSequenceNumbers<>(\n            \"topic\",\n            ImmutableMap.of(0, Long.MAX_VALUE, 1, Long.MAX_VALUE, 2, Long.MAX_VALUE)\n        ),\n        null,\n        null,\n        tuningConfig\n    );\n\n    Task id2 = createKafkaIndexTask(\n        \"id2\",\n        DATASOURCE,\n        0,\n        new SeekableStreamStartSequenceNumbers<>(\"topic\", ImmutableMap.of(0, 10L, 1, 20L, 2, 30L), ImmutableSet.of()),\n        new SeekableStreamEndSequenceNumbers<>(\n            \"topic\",\n            ImmutableMap.of(0, Long.MAX_VALUE, 1, Long.MAX_VALUE, 2, Long.MAX_VALUE)\n        ),\n        null,\n        null,\n        tuningConfig\n    );\n\n    Task id3 = createKafkaIndexTask(\n        \"id3\",\n        DATASOURCE,\n        0,\n        new SeekableStreamStartSequenceNumbers<>(\"topic\", ImmutableMap.of(0, 10L, 1, 20L, 2, 30L), ImmutableSet.of()),\n        new SeekableStreamEndSequenceNumbers<>(\n            \"topic\",\n            ImmutableMap.of(0, Long.MAX_VALUE, 1, Long.MAX_VALUE, 2, Long.MAX_VALUE)\n        ),\n        null,\n        null,\n        tuningConfig\n    );\n\n    Collection workItems = new ArrayList<>();\n    workItems.add(new TestTaskRunnerWorkItem(id1, null, location1));\n    workItems.add(new TestTaskRunnerWorkItem(id2, null, location2));\n\n    EasyMock.expect(taskMaster.getTaskQueue()).andReturn(Optional.of(taskQueue)).anyTimes();\n    EasyMock.expect(taskMaster.getTaskRunner()).andReturn(Optional.of(taskRunner)).anyTimes();\n    EasyMock.expect(taskRunner.getRunningTasks()).andReturn(workItems).anyTimes();\n    EasyMock.expect(taskStorage.getActiveTasks()).andReturn(ImmutableList.of(id1, id2, id3)).anyTimes();\n    EasyMock.expect(taskStorage.getStatus(\"id1\")).andReturn(Optional.of(TaskStatus.running(\"id1\"))).anyTimes();\n    EasyMock.expect(taskStorage.getStatus(\"id2\")).andReturn(Optional.of(TaskStatus.running(\"id2\"))).anyTimes();\n    EasyMock.expect(taskStorage.getStatus(\"id3\")).andReturn(Optional.of(TaskStatus.running(\"id3\"))).anyTimes();\n    EasyMock.expect(taskStorage.getTask(\"id1\")).andReturn(Optional.of(id1)).anyTimes();\n    EasyMock.expect(taskStorage.getTask(\"id2\")).andReturn(Optional.of(id2)).anyTimes();\n    EasyMock.expect(taskStorage.getTask(\"id3\")).andReturn(Optional.of(id3)).anyTimes();\n    EasyMock.expect(indexerMetadataStorageCoordinator.getDataSourceMetadata(DATASOURCE)).andReturn(\n        new KafkaDataSourceMetadata(\n            null\n        )\n    ).anyTimes();\n    EasyMock.expect(taskClient.getStatusAsync(\"id1\")).andReturn(Futures.immediateFuture(Status.PUBLISHING));\n    EasyMock.expect(taskClient.getStatusAsync(\"id2\")).andReturn(Futures.immediateFuture(Status.READING));\n    EasyMock.expect(taskClient.getStatusAsync(\"id3\")).andReturn(Futures.immediateFuture(Status.READING));\n    EasyMock.expect(taskClient.getStartTimeAsync(\"id2\")).andReturn(Futures.immediateFuture(startTime));\n    EasyMock.expect(taskClient.getStartTimeAsync(\"id3\")).andReturn(Futures.immediateFuture(startTime));\n    EasyMock.expect(taskClient.getEndOffsets(\"id1\")).andReturn(ImmutableMap.of(0, 10L, 1, 20L, 2, 30L));\n\n    \r\n    TreeMap<Integer, Map<Integer, Long>> checkpoints = new TreeMap<>();\n    checkpoints.put(0, ImmutableMap.of(0, 10L, 1, 20L, 2, 30L));\n    EasyMock.expect(taskClient.getCheckpointsAsync(EasyMock.contains(\"id2\"), EasyMock.anyBoolean()))\n            .andReturn(Futures.immediateFuture(checkpoints))\n            .times(1);\n    EasyMock.expect(taskClient.getCheckpointsAsync(EasyMock.contains(\"id3\"), EasyMock.anyBoolean()))\n            .andReturn(Futures.immediateFuture(checkpoints))\n            .times(1);\n\n    taskRunner.registerListener(EasyMock.anyObject(TaskRunnerListener.class), EasyMock.anyObject(Executor.class));\n    replayAll();\n\n    supervisor.start();\n    supervisor.runInternal();\n    verifyAll();\n\n    EasyMock.reset(taskRunner, taskClient, taskQueue);\n    EasyMock.expect(taskRunner.getRunningTasks()).andReturn(workItems).anyTimes();\n    EasyMock.expect(taskClient.pauseAsync(\"id2\"))\n            .andReturn(Futures.immediateFuture(ImmutableMap.of(0, 15L, 1, 25L, 2, 30L)));\n    EasyMock.expect(taskClient.setEndOffsetsAsync(\"id2\", ImmutableMap.of(0, 15L, 1, 25L, 2, 30L), true))\n            .andReturn(Futures.immediateFuture(true));\n    taskQueue.shutdown(\"id3\", \"Killing task for graceful shutdown\");\n    EasyMock.expectLastCall().times(1);\n    taskQueue.shutdown(\"id3\", \"Killing task [%s] which hasn't been assigned to a worker\", \"id3\");\n    EasyMock.expectLastCall().times(1);\n\n    EasyMock.replay(taskRunner, taskClient, taskQueue);\n\n    supervisor.gracefulShutdownInternal();\n    verifyAll();\n  }\n","date":"2019-08-23 05:51:25","endLine":1929,"groupId":"10584","id":17,"instanceNumber":1,"isCurCommit":0,"methodName":"testStopGracefully","params":"()","path":"/mnt/clonedata/CloneManagementServer/ManagementServer/consistResult/result-druid-10-0.7/blobInfo/CC_OUT/blobs/0d/255b66d24d58735694d9fe14274069e9ba5ee9.src","preCode":"  public void testStopGracefully() throws Exception\n  {\n    final TaskLocation location1 = new TaskLocation(\"testHost\", 1234, -1);\n    final TaskLocation location2 = new TaskLocation(\"testHost2\", 145, -1);\n    final DateTime startTime = DateTimes.nowUtc();\n\n    supervisor = getTestableSupervisor(2, 1, true, \"PT1H\", null, null);\n    addSomeEvents(1);\n\n    Task id1 = createKafkaIndexTask(\n        \"id1\",\n        DATASOURCE,\n        0,\n        new SeekableStreamStartSequenceNumbers<>(\"topic\", ImmutableMap.of(0, 0L, 1, 0L, 2, 0L), ImmutableSet.of()),\n        new SeekableStreamEndSequenceNumbers<>(\n            \"topic\",\n            ImmutableMap.of(0, Long.MAX_VALUE, 1, Long.MAX_VALUE, 2, Long.MAX_VALUE)\n        ),\n        null,\n        null\n    );\n\n    Task id2 = createKafkaIndexTask(\n        \"id2\",\n        DATASOURCE,\n        0,\n        new SeekableStreamStartSequenceNumbers<>(\"topic\", ImmutableMap.of(0, 10L, 1, 20L, 2, 30L), ImmutableSet.of()),\n        new SeekableStreamEndSequenceNumbers<>(\n            \"topic\",\n            ImmutableMap.of(0, Long.MAX_VALUE, 1, Long.MAX_VALUE, 2, Long.MAX_VALUE)\n        ),\n        null,\n        null\n    );\n\n    Task id3 = createKafkaIndexTask(\n        \"id3\",\n        DATASOURCE,\n        0,\n        new SeekableStreamStartSequenceNumbers<>(\"topic\", ImmutableMap.of(0, 10L, 1, 20L, 2, 30L), ImmutableSet.of()),\n        new SeekableStreamEndSequenceNumbers<>(\n            \"topic\",\n            ImmutableMap.of(0, Long.MAX_VALUE, 1, Long.MAX_VALUE, 2, Long.MAX_VALUE)\n        ),\n        null,\n        null\n    );\n\n    Collection workItems = new ArrayList<>();\n    workItems.add(new TestTaskRunnerWorkItem(id1, null, location1));\n    workItems.add(new TestTaskRunnerWorkItem(id2, null, location2));\n\n    EasyMock.expect(taskMaster.getTaskQueue()).andReturn(Optional.of(taskQueue)).anyTimes();\n    EasyMock.expect(taskMaster.getTaskRunner()).andReturn(Optional.of(taskRunner)).anyTimes();\n    EasyMock.expect(taskRunner.getRunningTasks()).andReturn(workItems).anyTimes();\n    EasyMock.expect(taskStorage.getActiveTasks()).andReturn(ImmutableList.of(id1, id2, id3)).anyTimes();\n    EasyMock.expect(taskStorage.getStatus(\"id1\")).andReturn(Optional.of(TaskStatus.running(\"id1\"))).anyTimes();\n    EasyMock.expect(taskStorage.getStatus(\"id2\")).andReturn(Optional.of(TaskStatus.running(\"id2\"))).anyTimes();\n    EasyMock.expect(taskStorage.getStatus(\"id3\")).andReturn(Optional.of(TaskStatus.running(\"id3\"))).anyTimes();\n    EasyMock.expect(taskStorage.getTask(\"id1\")).andReturn(Optional.of(id1)).anyTimes();\n    EasyMock.expect(taskStorage.getTask(\"id2\")).andReturn(Optional.of(id2)).anyTimes();\n    EasyMock.expect(taskStorage.getTask(\"id3\")).andReturn(Optional.of(id3)).anyTimes();\n    EasyMock.expect(indexerMetadataStorageCoordinator.getDataSourceMetadata(DATASOURCE)).andReturn(\n        new KafkaDataSourceMetadata(\n            null\n        )\n    ).anyTimes();\n    EasyMock.expect(taskClient.getStatusAsync(\"id1\")).andReturn(Futures.immediateFuture(Status.PUBLISHING));\n    EasyMock.expect(taskClient.getStatusAsync(\"id2\")).andReturn(Futures.immediateFuture(Status.READING));\n    EasyMock.expect(taskClient.getStatusAsync(\"id3\")).andReturn(Futures.immediateFuture(Status.READING));\n    EasyMock.expect(taskClient.getStartTimeAsync(\"id2\")).andReturn(Futures.immediateFuture(startTime));\n    EasyMock.expect(taskClient.getStartTimeAsync(\"id3\")).andReturn(Futures.immediateFuture(startTime));\n    EasyMock.expect(taskClient.getEndOffsets(\"id1\")).andReturn(ImmutableMap.of(0, 10L, 1, 20L, 2, 30L));\n\n    \r\n    TreeMap<Integer, Map<Integer, Long>> checkpoints = new TreeMap<>();\n    checkpoints.put(0, ImmutableMap.of(0, 10L, 1, 20L, 2, 30L));\n    EasyMock.expect(taskClient.getCheckpointsAsync(EasyMock.contains(\"id2\"), EasyMock.anyBoolean()))\n            .andReturn(Futures.immediateFuture(checkpoints))\n            .times(1);\n    EasyMock.expect(taskClient.getCheckpointsAsync(EasyMock.contains(\"id3\"), EasyMock.anyBoolean()))\n            .andReturn(Futures.immediateFuture(checkpoints))\n            .times(1);\n\n    taskRunner.registerListener(EasyMock.anyObject(TaskRunnerListener.class), EasyMock.anyObject(Executor.class));\n    replayAll();\n\n    supervisor.start();\n    supervisor.runInternal();\n    verifyAll();\n\n    EasyMock.reset(taskRunner, taskClient, taskQueue);\n    EasyMock.expect(taskRunner.getRunningTasks()).andReturn(workItems).anyTimes();\n    EasyMock.expect(taskClient.pauseAsync(\"id2\"))\n            .andReturn(Futures.immediateFuture(ImmutableMap.of(0, 15L, 1, 25L, 2, 30L)));\n    EasyMock.expect(taskClient.setEndOffsetsAsync(\"id2\", ImmutableMap.of(0, 15L, 1, 25L, 2, 30L), true))\n            .andReturn(Futures.immediateFuture(true));\n    taskQueue.shutdown(\"id3\", \"Killing task for graceful shutdown\");\n    EasyMock.expectLastCall().times(1);\n    taskQueue.shutdown(\"id3\", \"Killing task [%s] which hasn't been assigned to a worker\", \"id3\");\n    EasyMock.expectLastCall().times(1);\n\n    EasyMock.replay(taskRunner, taskClient, taskQueue);\n\n    supervisor.gracefulShutdownInternal();\n    verifyAll();\n  }\n","realPath":"extensions-core/kafka-indexing-service/src/test/java/org/apache/druid/indexing/kafka/supervisor/KafkaSupervisorTest.java","repoName":"druid","snippetEndLine":0,"snippetStartLine":0,"startLine":1819,"status":"M"},{"authorDate":"2019-08-23 05:51:25","commitOrder":10,"curCode":"  public void testSuspendedRunningTasks() throws Exception\n  {\n    \r\n\n    final TaskLocation location1 = new TaskLocation(\"testHost\", 1234, -1);\n    final TaskLocation location2 = new TaskLocation(\"testHost2\", 145, -1);\n    final DateTime startTime = DateTimes.nowUtc();\n\n    supervisor = getTestableSupervisor(2, 1, true, \"PT1H\", null, null, true, kafkaHost);\n    final KafkaSupervisorTuningConfig tuningConfig = supervisor.getTuningConfig();\n    addSomeEvents(1);\n\n    Task id1 = createKafkaIndexTask(\n        \"id1\",\n        DATASOURCE,\n        0,\n        new SeekableStreamStartSequenceNumbers<>(\"topic\", ImmutableMap.of(0, 0L, 1, 0L, 2, 0L), ImmutableSet.of()),\n        new SeekableStreamEndSequenceNumbers<>(\n            \"topic\",\n            ImmutableMap.of(0, Long.MAX_VALUE, 1, Long.MAX_VALUE, 2, Long.MAX_VALUE)\n        ),\n        null,\n        null,\n        tuningConfig\n    );\n\n    Task id2 = createKafkaIndexTask(\n        \"id2\",\n        DATASOURCE,\n        0,\n        new SeekableStreamStartSequenceNumbers<>(\"topic\", ImmutableMap.of(0, 10L, 1, 20L, 2, 30L), ImmutableSet.of()),\n        new SeekableStreamEndSequenceNumbers<>(\n            \"topic\",\n            ImmutableMap.of(0, Long.MAX_VALUE, 1, Long.MAX_VALUE, 2, Long.MAX_VALUE)\n        ),\n        null,\n        null,\n        tuningConfig\n    );\n\n    Task id3 = createKafkaIndexTask(\n        \"id3\",\n        DATASOURCE,\n        0,\n        new SeekableStreamStartSequenceNumbers<>(\"topic\", ImmutableMap.of(0, 10L, 1, 20L, 2, 30L), ImmutableSet.of()),\n        new SeekableStreamEndSequenceNumbers<>(\n            \"topic\",\n            ImmutableMap.of(0, Long.MAX_VALUE, 1, Long.MAX_VALUE, 2, Long.MAX_VALUE)\n        ),\n        null,\n        null,\n        tuningConfig\n    );\n\n    Collection workItems = new ArrayList<>();\n    workItems.add(new TestTaskRunnerWorkItem(id1, null, location1));\n    workItems.add(new TestTaskRunnerWorkItem(id2, null, location2));\n\n    EasyMock.expect(taskMaster.getTaskQueue()).andReturn(Optional.of(taskQueue)).anyTimes();\n    EasyMock.expect(taskMaster.getTaskRunner()).andReturn(Optional.of(taskRunner)).anyTimes();\n    EasyMock.expect(taskRunner.getRunningTasks()).andReturn(workItems).anyTimes();\n    EasyMock.expect(taskStorage.getActiveTasks()).andReturn(ImmutableList.of(id1, id2, id3)).anyTimes();\n    EasyMock.expect(taskStorage.getStatus(\"id1\")).andReturn(Optional.of(TaskStatus.running(\"id1\"))).anyTimes();\n    EasyMock.expect(taskStorage.getStatus(\"id2\")).andReturn(Optional.of(TaskStatus.running(\"id2\"))).anyTimes();\n    EasyMock.expect(taskStorage.getStatus(\"id3\")).andReturn(Optional.of(TaskStatus.running(\"id3\"))).anyTimes();\n    EasyMock.expect(taskStorage.getTask(\"id1\")).andReturn(Optional.of(id1)).anyTimes();\n    EasyMock.expect(taskStorage.getTask(\"id2\")).andReturn(Optional.of(id2)).anyTimes();\n    EasyMock.expect(taskStorage.getTask(\"id3\")).andReturn(Optional.of(id3)).anyTimes();\n    EasyMock.expect(indexerMetadataStorageCoordinator.getDataSourceMetadata(DATASOURCE)).andReturn(\n        new KafkaDataSourceMetadata(\n            null\n        )\n    ).anyTimes();\n    EasyMock.expect(taskClient.getStatusAsync(\"id1\"))\n            .andReturn(Futures.immediateFuture(Status.PUBLISHING));\n    EasyMock.expect(taskClient.getStatusAsync(\"id2\"))\n            .andReturn(Futures.immediateFuture(Status.READING));\n    EasyMock.expect(taskClient.getStatusAsync(\"id3\"))\n            .andReturn(Futures.immediateFuture(Status.READING));\n    EasyMock.expect(taskClient.getStartTimeAsync(\"id2\")).andReturn(Futures.immediateFuture(startTime));\n    EasyMock.expect(taskClient.getStartTimeAsync(\"id3\")).andReturn(Futures.immediateFuture(startTime));\n    EasyMock.expect(taskClient.getEndOffsets(\"id1\")).andReturn(ImmutableMap.of(0, 10L, 1, 20L, 2, 30L));\n\n    \r\n    TreeMap<Integer, Map<Integer, Long>> checkpoints = new TreeMap<>();\n    checkpoints.put(0, ImmutableMap.of(0, 10L, 1, 20L, 2, 30L));\n    EasyMock.expect(taskClient.getCheckpointsAsync(EasyMock.contains(\"id2\"), EasyMock.anyBoolean()))\n            .andReturn(Futures.immediateFuture(checkpoints))\n            .times(1);\n    EasyMock.expect(taskClient.getCheckpointsAsync(EasyMock.contains(\"id3\"), EasyMock.anyBoolean()))\n            .andReturn(Futures.immediateFuture(checkpoints))\n            .times(1);\n\n    taskRunner.registerListener(EasyMock.anyObject(TaskRunnerListener.class), EasyMock.anyObject(Executor.class));\n\n    EasyMock.expect(taskClient.pauseAsync(\"id2\"))\n            .andReturn(Futures.immediateFuture(ImmutableMap.of(0, 15L, 1, 25L, 2, 30L)));\n    EasyMock.expect(taskClient.setEndOffsetsAsync(\"id2\", ImmutableMap.of(0, 15L, 1, 25L, 2, 30L), true))\n            .andReturn(Futures.immediateFuture(true));\n    taskQueue.shutdown(\"id3\", \"Killing task for graceful shutdown\");\n    EasyMock.expectLastCall().times(1);\n    taskQueue.shutdown(\"id3\", \"Killing task [%s] which hasn't been assigned to a worker\", \"id3\");\n    EasyMock.expectLastCall().times(1);\n\n    replayAll();\n    supervisor.start();\n    supervisor.runInternal();\n    verifyAll();\n  }\n","date":"2019-08-23 05:51:25","endLine":2734,"groupId":"10584","id":18,"instanceNumber":2,"isCurCommit":0,"methodName":"testSuspendedRunningTasks","params":"()","path":"/mnt/clonedata/CloneManagementServer/ManagementServer/consistResult/result-druid-10-0.7/blobInfo/CC_OUT/blobs/0d/255b66d24d58735694d9fe14274069e9ba5ee9.src","preCode":"  public void testSuspendedRunningTasks() throws Exception\n  {\n    \r\n\n    final TaskLocation location1 = new TaskLocation(\"testHost\", 1234, -1);\n    final TaskLocation location2 = new TaskLocation(\"testHost2\", 145, -1);\n    final DateTime startTime = DateTimes.nowUtc();\n\n    supervisor = getTestableSupervisor(2, 1, true, \"PT1H\", null, null, true, kafkaHost);\n    addSomeEvents(1);\n\n    Task id1 = createKafkaIndexTask(\n        \"id1\",\n        DATASOURCE,\n        0,\n        new SeekableStreamStartSequenceNumbers<>(\"topic\", ImmutableMap.of(0, 0L, 1, 0L, 2, 0L), ImmutableSet.of()),\n        new SeekableStreamEndSequenceNumbers<>(\n            \"topic\",\n            ImmutableMap.of(0, Long.MAX_VALUE, 1, Long.MAX_VALUE, 2, Long.MAX_VALUE)\n        ),\n        null,\n        null\n    );\n\n    Task id2 = createKafkaIndexTask(\n        \"id2\",\n        DATASOURCE,\n        0,\n        new SeekableStreamStartSequenceNumbers<>(\"topic\", ImmutableMap.of(0, 10L, 1, 20L, 2, 30L), ImmutableSet.of()),\n        new SeekableStreamEndSequenceNumbers<>(\n            \"topic\",\n            ImmutableMap.of(0, Long.MAX_VALUE, 1, Long.MAX_VALUE, 2, Long.MAX_VALUE)\n        ),\n        null,\n        null\n    );\n\n    Task id3 = createKafkaIndexTask(\n        \"id3\",\n        DATASOURCE,\n        0,\n        new SeekableStreamStartSequenceNumbers<>(\"topic\", ImmutableMap.of(0, 10L, 1, 20L, 2, 30L), ImmutableSet.of()),\n        new SeekableStreamEndSequenceNumbers<>(\n            \"topic\",\n            ImmutableMap.of(0, Long.MAX_VALUE, 1, Long.MAX_VALUE, 2, Long.MAX_VALUE)\n        ),\n        null,\n        null\n    );\n\n    Collection workItems = new ArrayList<>();\n    workItems.add(new TestTaskRunnerWorkItem(id1, null, location1));\n    workItems.add(new TestTaskRunnerWorkItem(id2, null, location2));\n\n    EasyMock.expect(taskMaster.getTaskQueue()).andReturn(Optional.of(taskQueue)).anyTimes();\n    EasyMock.expect(taskMaster.getTaskRunner()).andReturn(Optional.of(taskRunner)).anyTimes();\n    EasyMock.expect(taskRunner.getRunningTasks()).andReturn(workItems).anyTimes();\n    EasyMock.expect(taskStorage.getActiveTasks()).andReturn(ImmutableList.of(id1, id2, id3)).anyTimes();\n    EasyMock.expect(taskStorage.getStatus(\"id1\")).andReturn(Optional.of(TaskStatus.running(\"id1\"))).anyTimes();\n    EasyMock.expect(taskStorage.getStatus(\"id2\")).andReturn(Optional.of(TaskStatus.running(\"id2\"))).anyTimes();\n    EasyMock.expect(taskStorage.getStatus(\"id3\")).andReturn(Optional.of(TaskStatus.running(\"id3\"))).anyTimes();\n    EasyMock.expect(taskStorage.getTask(\"id1\")).andReturn(Optional.of(id1)).anyTimes();\n    EasyMock.expect(taskStorage.getTask(\"id2\")).andReturn(Optional.of(id2)).anyTimes();\n    EasyMock.expect(taskStorage.getTask(\"id3\")).andReturn(Optional.of(id3)).anyTimes();\n    EasyMock.expect(indexerMetadataStorageCoordinator.getDataSourceMetadata(DATASOURCE)).andReturn(\n        new KafkaDataSourceMetadata(\n            null\n        )\n    ).anyTimes();\n    EasyMock.expect(taskClient.getStatusAsync(\"id1\"))\n            .andReturn(Futures.immediateFuture(Status.PUBLISHING));\n    EasyMock.expect(taskClient.getStatusAsync(\"id2\"))\n            .andReturn(Futures.immediateFuture(Status.READING));\n    EasyMock.expect(taskClient.getStatusAsync(\"id3\"))\n            .andReturn(Futures.immediateFuture(Status.READING));\n    EasyMock.expect(taskClient.getStartTimeAsync(\"id2\")).andReturn(Futures.immediateFuture(startTime));\n    EasyMock.expect(taskClient.getStartTimeAsync(\"id3\")).andReturn(Futures.immediateFuture(startTime));\n    EasyMock.expect(taskClient.getEndOffsets(\"id1\")).andReturn(ImmutableMap.of(0, 10L, 1, 20L, 2, 30L));\n\n    \r\n    TreeMap<Integer, Map<Integer, Long>> checkpoints = new TreeMap<>();\n    checkpoints.put(0, ImmutableMap.of(0, 10L, 1, 20L, 2, 30L));\n    EasyMock.expect(taskClient.getCheckpointsAsync(EasyMock.contains(\"id2\"), EasyMock.anyBoolean()))\n            .andReturn(Futures.immediateFuture(checkpoints))\n            .times(1);\n    EasyMock.expect(taskClient.getCheckpointsAsync(EasyMock.contains(\"id3\"), EasyMock.anyBoolean()))\n            .andReturn(Futures.immediateFuture(checkpoints))\n            .times(1);\n\n    taskRunner.registerListener(EasyMock.anyObject(TaskRunnerListener.class), EasyMock.anyObject(Executor.class));\n\n    EasyMock.expect(taskClient.pauseAsync(\"id2\"))\n            .andReturn(Futures.immediateFuture(ImmutableMap.of(0, 15L, 1, 25L, 2, 30L)));\n    EasyMock.expect(taskClient.setEndOffsetsAsync(\"id2\", ImmutableMap.of(0, 15L, 1, 25L, 2, 30L), true))\n            .andReturn(Futures.immediateFuture(true));\n    taskQueue.shutdown(\"id3\", \"Killing task for graceful shutdown\");\n    EasyMock.expectLastCall().times(1);\n    taskQueue.shutdown(\"id3\", \"Killing task [%s] which hasn't been assigned to a worker\", \"id3\");\n    EasyMock.expectLastCall().times(1);\n\n    replayAll();\n    supervisor.start();\n    supervisor.runInternal();\n    verifyAll();\n  }\n","realPath":"extensions-core/kafka-indexing-service/src/test/java/org/apache/druid/indexing/kafka/supervisor/KafkaSupervisorTest.java","repoName":"druid","snippetEndLine":0,"snippetStartLine":0,"startLine":2626,"status":"M"}],"commitId":"fba92ae469b512cca6cdf86ffc1c1a2090808453","commitMessage":"@@@Fix to always use end sequenceNumber for reset (#8305)\n\n* Fix to always use end sequenceNumber for reset\n\n* fix checkstyle\n\n* fix style and add log\n","date":"2019-08-23 05:51:25","modifiedFileCount":"8","status":"M","submitter":"Jihoon Son"},{"authorTime":"2019-09-27 07:15:24","codes":[{"authorDate":"2019-09-27 07:15:24","commitOrder":11,"curCode":"  public void testStopGracefully() throws Exception\n  {\n    final TaskLocation location1 = new TaskLocation(\"testHost\", 1234, -1);\n    final TaskLocation location2 = new TaskLocation(\"testHost2\", 145, -1);\n    final DateTime startTime = DateTimes.nowUtc();\n\n    supervisor = getTestableSupervisor(2, 1, true, \"PT1H\", null, null);\n    final KafkaSupervisorTuningConfig tuningConfig = supervisor.getTuningConfig();\n    addSomeEvents(1);\n\n    Task id1 = createKafkaIndexTask(\n        \"id1\",\n        DATASOURCE,\n        0,\n        new SeekableStreamStartSequenceNumbers<>(\"topic\", ImmutableMap.of(0, 0L, 1, 0L, 2, 0L), ImmutableSet.of()),\n        new SeekableStreamEndSequenceNumbers<>(\n            \"topic\",\n            ImmutableMap.of(0, Long.MAX_VALUE, 1, Long.MAX_VALUE, 2, Long.MAX_VALUE)\n        ),\n        null,\n        null,\n        tuningConfig\n    );\n\n    Task id2 = createKafkaIndexTask(\n        \"id2\",\n        DATASOURCE,\n        0,\n        new SeekableStreamStartSequenceNumbers<>(\"topic\", ImmutableMap.of(0, 10L, 1, 20L, 2, 30L), ImmutableSet.of()),\n        new SeekableStreamEndSequenceNumbers<>(\n            \"topic\",\n            ImmutableMap.of(0, Long.MAX_VALUE, 1, Long.MAX_VALUE, 2, Long.MAX_VALUE)\n        ),\n        null,\n        null,\n        tuningConfig\n    );\n\n    Task id3 = createKafkaIndexTask(\n        \"id3\",\n        DATASOURCE,\n        0,\n        new SeekableStreamStartSequenceNumbers<>(\"topic\", ImmutableMap.of(0, 10L, 1, 20L, 2, 30L), ImmutableSet.of()),\n        new SeekableStreamEndSequenceNumbers<>(\n            \"topic\",\n            ImmutableMap.of(0, Long.MAX_VALUE, 1, Long.MAX_VALUE, 2, Long.MAX_VALUE)\n        ),\n        null,\n        null,\n        tuningConfig\n    );\n\n    Collection workItems = new ArrayList<>();\n    workItems.add(new TestTaskRunnerWorkItem(id1, null, location1));\n    workItems.add(new TestTaskRunnerWorkItem(id2, null, location2));\n\n    EasyMock.expect(taskMaster.getTaskQueue()).andReturn(Optional.of(taskQueue)).anyTimes();\n    EasyMock.expect(taskMaster.getTaskRunner()).andReturn(Optional.of(taskRunner)).anyTimes();\n    EasyMock.expect(taskRunner.getRunningTasks()).andReturn(workItems).anyTimes();\n    EasyMock.expect(taskStorage.getActiveTasksByDatasource(DATASOURCE)).andReturn(ImmutableList.of(id1, id2, id3)).anyTimes();\n    EasyMock.expect(taskStorage.getStatus(\"id1\")).andReturn(Optional.of(TaskStatus.running(\"id1\"))).anyTimes();\n    EasyMock.expect(taskStorage.getStatus(\"id2\")).andReturn(Optional.of(TaskStatus.running(\"id2\"))).anyTimes();\n    EasyMock.expect(taskStorage.getStatus(\"id3\")).andReturn(Optional.of(TaskStatus.running(\"id3\"))).anyTimes();\n    EasyMock.expect(taskStorage.getTask(\"id1\")).andReturn(Optional.of(id1)).anyTimes();\n    EasyMock.expect(taskStorage.getTask(\"id2\")).andReturn(Optional.of(id2)).anyTimes();\n    EasyMock.expect(taskStorage.getTask(\"id3\")).andReturn(Optional.of(id3)).anyTimes();\n    EasyMock.expect(indexerMetadataStorageCoordinator.getDataSourceMetadata(DATASOURCE)).andReturn(\n        new KafkaDataSourceMetadata(\n            null\n        )\n    ).anyTimes();\n    EasyMock.expect(taskClient.getStatusAsync(\"id1\")).andReturn(Futures.immediateFuture(Status.PUBLISHING));\n    EasyMock.expect(taskClient.getStatusAsync(\"id2\")).andReturn(Futures.immediateFuture(Status.READING));\n    EasyMock.expect(taskClient.getStatusAsync(\"id3\")).andReturn(Futures.immediateFuture(Status.READING));\n    EasyMock.expect(taskClient.getStartTimeAsync(\"id2\")).andReturn(Futures.immediateFuture(startTime));\n    EasyMock.expect(taskClient.getStartTimeAsync(\"id3\")).andReturn(Futures.immediateFuture(startTime));\n    EasyMock.expect(taskClient.getEndOffsets(\"id1\")).andReturn(ImmutableMap.of(0, 10L, 1, 20L, 2, 30L));\n\n    \r\n    TreeMap<Integer, Map<Integer, Long>> checkpoints = new TreeMap<>();\n    checkpoints.put(0, ImmutableMap.of(0, 10L, 1, 20L, 2, 30L));\n    EasyMock.expect(taskClient.getCheckpointsAsync(EasyMock.contains(\"id2\"), EasyMock.anyBoolean()))\n            .andReturn(Futures.immediateFuture(checkpoints))\n            .times(1);\n    EasyMock.expect(taskClient.getCheckpointsAsync(EasyMock.contains(\"id3\"), EasyMock.anyBoolean()))\n            .andReturn(Futures.immediateFuture(checkpoints))\n            .times(1);\n\n    taskRunner.registerListener(EasyMock.anyObject(TaskRunnerListener.class), EasyMock.anyObject(Executor.class));\n    replayAll();\n\n    supervisor.start();\n    supervisor.runInternal();\n    verifyAll();\n\n    EasyMock.reset(taskRunner, taskClient, taskQueue);\n    EasyMock.expect(taskRunner.getRunningTasks()).andReturn(workItems).anyTimes();\n    EasyMock.expect(taskClient.pauseAsync(\"id2\"))\n            .andReturn(Futures.immediateFuture(ImmutableMap.of(0, 15L, 1, 25L, 2, 30L)));\n    EasyMock.expect(taskClient.setEndOffsetsAsync(\"id2\", ImmutableMap.of(0, 15L, 1, 25L, 2, 30L), true))\n            .andReturn(Futures.immediateFuture(true));\n    taskQueue.shutdown(\"id3\", \"Killing task for graceful shutdown\");\n    EasyMock.expectLastCall().times(1);\n    taskQueue.shutdown(\"id3\", \"Killing task [%s] which hasn't been assigned to a worker\", \"id3\");\n    EasyMock.expectLastCall().times(1);\n\n    EasyMock.replay(taskRunner, taskClient, taskQueue);\n\n    supervisor.gracefulShutdownInternal();\n    verifyAll();\n  }\n","date":"2019-09-27 07:15:24","endLine":1917,"groupId":"10584","id":19,"instanceNumber":1,"isCurCommit":0,"methodName":"testStopGracefully","params":"()","path":"/mnt/clonedata/CloneManagementServer/ManagementServer/consistResult/result-druid-10-0.7/blobInfo/CC_OUT/blobs/75/9182feac2928302ae8f3579d872a1883a232c5.src","preCode":"  public void testStopGracefully() throws Exception\n  {\n    final TaskLocation location1 = new TaskLocation(\"testHost\", 1234, -1);\n    final TaskLocation location2 = new TaskLocation(\"testHost2\", 145, -1);\n    final DateTime startTime = DateTimes.nowUtc();\n\n    supervisor = getTestableSupervisor(2, 1, true, \"PT1H\", null, null);\n    final KafkaSupervisorTuningConfig tuningConfig = supervisor.getTuningConfig();\n    addSomeEvents(1);\n\n    Task id1 = createKafkaIndexTask(\n        \"id1\",\n        DATASOURCE,\n        0,\n        new SeekableStreamStartSequenceNumbers<>(\"topic\", ImmutableMap.of(0, 0L, 1, 0L, 2, 0L), ImmutableSet.of()),\n        new SeekableStreamEndSequenceNumbers<>(\n            \"topic\",\n            ImmutableMap.of(0, Long.MAX_VALUE, 1, Long.MAX_VALUE, 2, Long.MAX_VALUE)\n        ),\n        null,\n        null,\n        tuningConfig\n    );\n\n    Task id2 = createKafkaIndexTask(\n        \"id2\",\n        DATASOURCE,\n        0,\n        new SeekableStreamStartSequenceNumbers<>(\"topic\", ImmutableMap.of(0, 10L, 1, 20L, 2, 30L), ImmutableSet.of()),\n        new SeekableStreamEndSequenceNumbers<>(\n            \"topic\",\n            ImmutableMap.of(0, Long.MAX_VALUE, 1, Long.MAX_VALUE, 2, Long.MAX_VALUE)\n        ),\n        null,\n        null,\n        tuningConfig\n    );\n\n    Task id3 = createKafkaIndexTask(\n        \"id3\",\n        DATASOURCE,\n        0,\n        new SeekableStreamStartSequenceNumbers<>(\"topic\", ImmutableMap.of(0, 10L, 1, 20L, 2, 30L), ImmutableSet.of()),\n        new SeekableStreamEndSequenceNumbers<>(\n            \"topic\",\n            ImmutableMap.of(0, Long.MAX_VALUE, 1, Long.MAX_VALUE, 2, Long.MAX_VALUE)\n        ),\n        null,\n        null,\n        tuningConfig\n    );\n\n    Collection workItems = new ArrayList<>();\n    workItems.add(new TestTaskRunnerWorkItem(id1, null, location1));\n    workItems.add(new TestTaskRunnerWorkItem(id2, null, location2));\n\n    EasyMock.expect(taskMaster.getTaskQueue()).andReturn(Optional.of(taskQueue)).anyTimes();\n    EasyMock.expect(taskMaster.getTaskRunner()).andReturn(Optional.of(taskRunner)).anyTimes();\n    EasyMock.expect(taskRunner.getRunningTasks()).andReturn(workItems).anyTimes();\n    EasyMock.expect(taskStorage.getActiveTasks()).andReturn(ImmutableList.of(id1, id2, id3)).anyTimes();\n    EasyMock.expect(taskStorage.getStatus(\"id1\")).andReturn(Optional.of(TaskStatus.running(\"id1\"))).anyTimes();\n    EasyMock.expect(taskStorage.getStatus(\"id2\")).andReturn(Optional.of(TaskStatus.running(\"id2\"))).anyTimes();\n    EasyMock.expect(taskStorage.getStatus(\"id3\")).andReturn(Optional.of(TaskStatus.running(\"id3\"))).anyTimes();\n    EasyMock.expect(taskStorage.getTask(\"id1\")).andReturn(Optional.of(id1)).anyTimes();\n    EasyMock.expect(taskStorage.getTask(\"id2\")).andReturn(Optional.of(id2)).anyTimes();\n    EasyMock.expect(taskStorage.getTask(\"id3\")).andReturn(Optional.of(id3)).anyTimes();\n    EasyMock.expect(indexerMetadataStorageCoordinator.getDataSourceMetadata(DATASOURCE)).andReturn(\n        new KafkaDataSourceMetadata(\n            null\n        )\n    ).anyTimes();\n    EasyMock.expect(taskClient.getStatusAsync(\"id1\")).andReturn(Futures.immediateFuture(Status.PUBLISHING));\n    EasyMock.expect(taskClient.getStatusAsync(\"id2\")).andReturn(Futures.immediateFuture(Status.READING));\n    EasyMock.expect(taskClient.getStatusAsync(\"id3\")).andReturn(Futures.immediateFuture(Status.READING));\n    EasyMock.expect(taskClient.getStartTimeAsync(\"id2\")).andReturn(Futures.immediateFuture(startTime));\n    EasyMock.expect(taskClient.getStartTimeAsync(\"id3\")).andReturn(Futures.immediateFuture(startTime));\n    EasyMock.expect(taskClient.getEndOffsets(\"id1\")).andReturn(ImmutableMap.of(0, 10L, 1, 20L, 2, 30L));\n\n    \r\n    TreeMap<Integer, Map<Integer, Long>> checkpoints = new TreeMap<>();\n    checkpoints.put(0, ImmutableMap.of(0, 10L, 1, 20L, 2, 30L));\n    EasyMock.expect(taskClient.getCheckpointsAsync(EasyMock.contains(\"id2\"), EasyMock.anyBoolean()))\n            .andReturn(Futures.immediateFuture(checkpoints))\n            .times(1);\n    EasyMock.expect(taskClient.getCheckpointsAsync(EasyMock.contains(\"id3\"), EasyMock.anyBoolean()))\n            .andReturn(Futures.immediateFuture(checkpoints))\n            .times(1);\n\n    taskRunner.registerListener(EasyMock.anyObject(TaskRunnerListener.class), EasyMock.anyObject(Executor.class));\n    replayAll();\n\n    supervisor.start();\n    supervisor.runInternal();\n    verifyAll();\n\n    EasyMock.reset(taskRunner, taskClient, taskQueue);\n    EasyMock.expect(taskRunner.getRunningTasks()).andReturn(workItems).anyTimes();\n    EasyMock.expect(taskClient.pauseAsync(\"id2\"))\n            .andReturn(Futures.immediateFuture(ImmutableMap.of(0, 15L, 1, 25L, 2, 30L)));\n    EasyMock.expect(taskClient.setEndOffsetsAsync(\"id2\", ImmutableMap.of(0, 15L, 1, 25L, 2, 30L), true))\n            .andReturn(Futures.immediateFuture(true));\n    taskQueue.shutdown(\"id3\", \"Killing task for graceful shutdown\");\n    EasyMock.expectLastCall().times(1);\n    taskQueue.shutdown(\"id3\", \"Killing task [%s] which hasn't been assigned to a worker\", \"id3\");\n    EasyMock.expectLastCall().times(1);\n\n    EasyMock.replay(taskRunner, taskClient, taskQueue);\n\n    supervisor.gracefulShutdownInternal();\n    verifyAll();\n  }\n","realPath":"extensions-core/kafka-indexing-service/src/test/java/org/apache/druid/indexing/kafka/supervisor/KafkaSupervisorTest.java","repoName":"druid","snippetEndLine":0,"snippetStartLine":0,"startLine":1807,"status":"M"},{"authorDate":"2019-09-27 07:15:24","commitOrder":11,"curCode":"  public void testSuspendedRunningTasks() throws Exception\n  {\n    \r\n\n    final TaskLocation location1 = new TaskLocation(\"testHost\", 1234, -1);\n    final TaskLocation location2 = new TaskLocation(\"testHost2\", 145, -1);\n    final DateTime startTime = DateTimes.nowUtc();\n\n    supervisor = getTestableSupervisor(2, 1, true, \"PT1H\", null, null, true, kafkaHost);\n    final KafkaSupervisorTuningConfig tuningConfig = supervisor.getTuningConfig();\n    addSomeEvents(1);\n\n    Task id1 = createKafkaIndexTask(\n        \"id1\",\n        DATASOURCE,\n        0,\n        new SeekableStreamStartSequenceNumbers<>(\"topic\", ImmutableMap.of(0, 0L, 1, 0L, 2, 0L), ImmutableSet.of()),\n        new SeekableStreamEndSequenceNumbers<>(\n            \"topic\",\n            ImmutableMap.of(0, Long.MAX_VALUE, 1, Long.MAX_VALUE, 2, Long.MAX_VALUE)\n        ),\n        null,\n        null,\n        tuningConfig\n    );\n\n    Task id2 = createKafkaIndexTask(\n        \"id2\",\n        DATASOURCE,\n        0,\n        new SeekableStreamStartSequenceNumbers<>(\"topic\", ImmutableMap.of(0, 10L, 1, 20L, 2, 30L), ImmutableSet.of()),\n        new SeekableStreamEndSequenceNumbers<>(\n            \"topic\",\n            ImmutableMap.of(0, Long.MAX_VALUE, 1, Long.MAX_VALUE, 2, Long.MAX_VALUE)\n        ),\n        null,\n        null,\n        tuningConfig\n    );\n\n    Task id3 = createKafkaIndexTask(\n        \"id3\",\n        DATASOURCE,\n        0,\n        new SeekableStreamStartSequenceNumbers<>(\"topic\", ImmutableMap.of(0, 10L, 1, 20L, 2, 30L), ImmutableSet.of()),\n        new SeekableStreamEndSequenceNumbers<>(\n            \"topic\",\n            ImmutableMap.of(0, Long.MAX_VALUE, 1, Long.MAX_VALUE, 2, Long.MAX_VALUE)\n        ),\n        null,\n        null,\n        tuningConfig\n    );\n\n    Collection workItems = new ArrayList<>();\n    workItems.add(new TestTaskRunnerWorkItem(id1, null, location1));\n    workItems.add(new TestTaskRunnerWorkItem(id2, null, location2));\n\n    EasyMock.expect(taskMaster.getTaskQueue()).andReturn(Optional.of(taskQueue)).anyTimes();\n    EasyMock.expect(taskMaster.getTaskRunner()).andReturn(Optional.of(taskRunner)).anyTimes();\n    EasyMock.expect(taskRunner.getRunningTasks()).andReturn(workItems).anyTimes();\n    EasyMock.expect(taskStorage.getActiveTasksByDatasource(DATASOURCE)).andReturn(ImmutableList.of(id1, id2, id3)).anyTimes();\n    EasyMock.expect(taskStorage.getStatus(\"id1\")).andReturn(Optional.of(TaskStatus.running(\"id1\"))).anyTimes();\n    EasyMock.expect(taskStorage.getStatus(\"id2\")).andReturn(Optional.of(TaskStatus.running(\"id2\"))).anyTimes();\n    EasyMock.expect(taskStorage.getStatus(\"id3\")).andReturn(Optional.of(TaskStatus.running(\"id3\"))).anyTimes();\n    EasyMock.expect(taskStorage.getTask(\"id1\")).andReturn(Optional.of(id1)).anyTimes();\n    EasyMock.expect(taskStorage.getTask(\"id2\")).andReturn(Optional.of(id2)).anyTimes();\n    EasyMock.expect(taskStorage.getTask(\"id3\")).andReturn(Optional.of(id3)).anyTimes();\n    EasyMock.expect(indexerMetadataStorageCoordinator.getDataSourceMetadata(DATASOURCE)).andReturn(\n        new KafkaDataSourceMetadata(\n            null\n        )\n    ).anyTimes();\n    EasyMock.expect(taskClient.getStatusAsync(\"id1\"))\n            .andReturn(Futures.immediateFuture(Status.PUBLISHING));\n    EasyMock.expect(taskClient.getStatusAsync(\"id2\"))\n            .andReturn(Futures.immediateFuture(Status.READING));\n    EasyMock.expect(taskClient.getStatusAsync(\"id3\"))\n            .andReturn(Futures.immediateFuture(Status.READING));\n    EasyMock.expect(taskClient.getStartTimeAsync(\"id2\")).andReturn(Futures.immediateFuture(startTime));\n    EasyMock.expect(taskClient.getStartTimeAsync(\"id3\")).andReturn(Futures.immediateFuture(startTime));\n    EasyMock.expect(taskClient.getEndOffsets(\"id1\")).andReturn(ImmutableMap.of(0, 10L, 1, 20L, 2, 30L));\n\n    \r\n    TreeMap<Integer, Map<Integer, Long>> checkpoints = new TreeMap<>();\n    checkpoints.put(0, ImmutableMap.of(0, 10L, 1, 20L, 2, 30L));\n    EasyMock.expect(taskClient.getCheckpointsAsync(EasyMock.contains(\"id2\"), EasyMock.anyBoolean()))\n            .andReturn(Futures.immediateFuture(checkpoints))\n            .times(1);\n    EasyMock.expect(taskClient.getCheckpointsAsync(EasyMock.contains(\"id3\"), EasyMock.anyBoolean()))\n            .andReturn(Futures.immediateFuture(checkpoints))\n            .times(1);\n\n    taskRunner.registerListener(EasyMock.anyObject(TaskRunnerListener.class), EasyMock.anyObject(Executor.class));\n\n    EasyMock.expect(taskClient.pauseAsync(\"id2\"))\n            .andReturn(Futures.immediateFuture(ImmutableMap.of(0, 15L, 1, 25L, 2, 30L)));\n    EasyMock.expect(taskClient.setEndOffsetsAsync(\"id2\", ImmutableMap.of(0, 15L, 1, 25L, 2, 30L), true))\n            .andReturn(Futures.immediateFuture(true));\n    taskQueue.shutdown(\"id3\", \"Killing task for graceful shutdown\");\n    EasyMock.expectLastCall().times(1);\n    taskQueue.shutdown(\"id3\", \"Killing task [%s] which hasn't been assigned to a worker\", \"id3\");\n    EasyMock.expectLastCall().times(1);\n\n    replayAll();\n    supervisor.start();\n    supervisor.runInternal();\n    verifyAll();\n  }\n","date":"2019-09-27 07:15:24","endLine":2722,"groupId":"10584","id":20,"instanceNumber":2,"isCurCommit":0,"methodName":"testSuspendedRunningTasks","params":"()","path":"/mnt/clonedata/CloneManagementServer/ManagementServer/consistResult/result-druid-10-0.7/blobInfo/CC_OUT/blobs/75/9182feac2928302ae8f3579d872a1883a232c5.src","preCode":"  public void testSuspendedRunningTasks() throws Exception\n  {\n    \r\n\n    final TaskLocation location1 = new TaskLocation(\"testHost\", 1234, -1);\n    final TaskLocation location2 = new TaskLocation(\"testHost2\", 145, -1);\n    final DateTime startTime = DateTimes.nowUtc();\n\n    supervisor = getTestableSupervisor(2, 1, true, \"PT1H\", null, null, true, kafkaHost);\n    final KafkaSupervisorTuningConfig tuningConfig = supervisor.getTuningConfig();\n    addSomeEvents(1);\n\n    Task id1 = createKafkaIndexTask(\n        \"id1\",\n        DATASOURCE,\n        0,\n        new SeekableStreamStartSequenceNumbers<>(\"topic\", ImmutableMap.of(0, 0L, 1, 0L, 2, 0L), ImmutableSet.of()),\n        new SeekableStreamEndSequenceNumbers<>(\n            \"topic\",\n            ImmutableMap.of(0, Long.MAX_VALUE, 1, Long.MAX_VALUE, 2, Long.MAX_VALUE)\n        ),\n        null,\n        null,\n        tuningConfig\n    );\n\n    Task id2 = createKafkaIndexTask(\n        \"id2\",\n        DATASOURCE,\n        0,\n        new SeekableStreamStartSequenceNumbers<>(\"topic\", ImmutableMap.of(0, 10L, 1, 20L, 2, 30L), ImmutableSet.of()),\n        new SeekableStreamEndSequenceNumbers<>(\n            \"topic\",\n            ImmutableMap.of(0, Long.MAX_VALUE, 1, Long.MAX_VALUE, 2, Long.MAX_VALUE)\n        ),\n        null,\n        null,\n        tuningConfig\n    );\n\n    Task id3 = createKafkaIndexTask(\n        \"id3\",\n        DATASOURCE,\n        0,\n        new SeekableStreamStartSequenceNumbers<>(\"topic\", ImmutableMap.of(0, 10L, 1, 20L, 2, 30L), ImmutableSet.of()),\n        new SeekableStreamEndSequenceNumbers<>(\n            \"topic\",\n            ImmutableMap.of(0, Long.MAX_VALUE, 1, Long.MAX_VALUE, 2, Long.MAX_VALUE)\n        ),\n        null,\n        null,\n        tuningConfig\n    );\n\n    Collection workItems = new ArrayList<>();\n    workItems.add(new TestTaskRunnerWorkItem(id1, null, location1));\n    workItems.add(new TestTaskRunnerWorkItem(id2, null, location2));\n\n    EasyMock.expect(taskMaster.getTaskQueue()).andReturn(Optional.of(taskQueue)).anyTimes();\n    EasyMock.expect(taskMaster.getTaskRunner()).andReturn(Optional.of(taskRunner)).anyTimes();\n    EasyMock.expect(taskRunner.getRunningTasks()).andReturn(workItems).anyTimes();\n    EasyMock.expect(taskStorage.getActiveTasks()).andReturn(ImmutableList.of(id1, id2, id3)).anyTimes();\n    EasyMock.expect(taskStorage.getStatus(\"id1\")).andReturn(Optional.of(TaskStatus.running(\"id1\"))).anyTimes();\n    EasyMock.expect(taskStorage.getStatus(\"id2\")).andReturn(Optional.of(TaskStatus.running(\"id2\"))).anyTimes();\n    EasyMock.expect(taskStorage.getStatus(\"id3\")).andReturn(Optional.of(TaskStatus.running(\"id3\"))).anyTimes();\n    EasyMock.expect(taskStorage.getTask(\"id1\")).andReturn(Optional.of(id1)).anyTimes();\n    EasyMock.expect(taskStorage.getTask(\"id2\")).andReturn(Optional.of(id2)).anyTimes();\n    EasyMock.expect(taskStorage.getTask(\"id3\")).andReturn(Optional.of(id3)).anyTimes();\n    EasyMock.expect(indexerMetadataStorageCoordinator.getDataSourceMetadata(DATASOURCE)).andReturn(\n        new KafkaDataSourceMetadata(\n            null\n        )\n    ).anyTimes();\n    EasyMock.expect(taskClient.getStatusAsync(\"id1\"))\n            .andReturn(Futures.immediateFuture(Status.PUBLISHING));\n    EasyMock.expect(taskClient.getStatusAsync(\"id2\"))\n            .andReturn(Futures.immediateFuture(Status.READING));\n    EasyMock.expect(taskClient.getStatusAsync(\"id3\"))\n            .andReturn(Futures.immediateFuture(Status.READING));\n    EasyMock.expect(taskClient.getStartTimeAsync(\"id2\")).andReturn(Futures.immediateFuture(startTime));\n    EasyMock.expect(taskClient.getStartTimeAsync(\"id3\")).andReturn(Futures.immediateFuture(startTime));\n    EasyMock.expect(taskClient.getEndOffsets(\"id1\")).andReturn(ImmutableMap.of(0, 10L, 1, 20L, 2, 30L));\n\n    \r\n    TreeMap<Integer, Map<Integer, Long>> checkpoints = new TreeMap<>();\n    checkpoints.put(0, ImmutableMap.of(0, 10L, 1, 20L, 2, 30L));\n    EasyMock.expect(taskClient.getCheckpointsAsync(EasyMock.contains(\"id2\"), EasyMock.anyBoolean()))\n            .andReturn(Futures.immediateFuture(checkpoints))\n            .times(1);\n    EasyMock.expect(taskClient.getCheckpointsAsync(EasyMock.contains(\"id3\"), EasyMock.anyBoolean()))\n            .andReturn(Futures.immediateFuture(checkpoints))\n            .times(1);\n\n    taskRunner.registerListener(EasyMock.anyObject(TaskRunnerListener.class), EasyMock.anyObject(Executor.class));\n\n    EasyMock.expect(taskClient.pauseAsync(\"id2\"))\n            .andReturn(Futures.immediateFuture(ImmutableMap.of(0, 15L, 1, 25L, 2, 30L)));\n    EasyMock.expect(taskClient.setEndOffsetsAsync(\"id2\", ImmutableMap.of(0, 15L, 1, 25L, 2, 30L), true))\n            .andReturn(Futures.immediateFuture(true));\n    taskQueue.shutdown(\"id3\", \"Killing task for graceful shutdown\");\n    EasyMock.expectLastCall().times(1);\n    taskQueue.shutdown(\"id3\", \"Killing task [%s] which hasn't been assigned to a worker\", \"id3\");\n    EasyMock.expectLastCall().times(1);\n\n    replayAll();\n    supervisor.start();\n    supervisor.runInternal();\n    verifyAll();\n  }\n","realPath":"extensions-core/kafka-indexing-service/src/test/java/org/apache/druid/indexing/kafka/supervisor/KafkaSupervisorTest.java","repoName":"druid","snippetEndLine":0,"snippetStartLine":0,"startLine":2614,"status":"M"}],"commitId":"7f2b6577ef19f18523e8353336ad496e8dc4a270","commitMessage":"@@@get active task by datasource when supervisor discover tasks (#8450)\n\n* get active task by datasource when supervisor discover tasks\n\n* fix ut\n\n* fix ut\n\n* fix ut\n\n* remove unnecessary condition check\n\n* fix ut\n\n* remove stream in hot loop\n","date":"2019-09-27 07:15:24","modifiedFileCount":"7","status":"M","submitter":"elloooooo"},{"authorTime":"2020-01-28 03:24:29","codes":[{"authorDate":"2020-01-28 03:24:29","commitOrder":12,"curCode":"  public void testStopGracefully() throws Exception\n  {\n    final TaskLocation location1 = new TaskLocation(\"testHost\", 1234, -1);\n    final TaskLocation location2 = new TaskLocation(\"testHost2\", 145, -1);\n    final DateTime startTime = DateTimes.nowUtc();\n\n    supervisor = getTestableSupervisor(2, 1, true, \"PT1H\", null, null);\n    final KafkaSupervisorTuningConfig tuningConfig = supervisor.getTuningConfig();\n    addSomeEvents(1);\n\n    Task id1 = createKafkaIndexTask(\n        \"id1\",\n        DATASOURCE,\n        0,\n        new SeekableStreamStartSequenceNumbers<>(\"topic\", ImmutableMap.of(0, 0L, 1, 0L, 2, 0L), ImmutableSet.of()),\n        new SeekableStreamEndSequenceNumbers<>(\n            \"topic\",\n            ImmutableMap.of(0, Long.MAX_VALUE, 1, Long.MAX_VALUE, 2, Long.MAX_VALUE)\n        ),\n        null,\n        null,\n        tuningConfig\n    );\n\n    Task id2 = createKafkaIndexTask(\n        \"id2\",\n        DATASOURCE,\n        0,\n        new SeekableStreamStartSequenceNumbers<>(\"topic\", ImmutableMap.of(0, 10L, 1, 20L, 2, 30L), ImmutableSet.of()),\n        new SeekableStreamEndSequenceNumbers<>(\n            \"topic\",\n            ImmutableMap.of(0, Long.MAX_VALUE, 1, Long.MAX_VALUE, 2, Long.MAX_VALUE)\n        ),\n        null,\n        null,\n        tuningConfig\n    );\n\n    Task id3 = createKafkaIndexTask(\n        \"id3\",\n        DATASOURCE,\n        0,\n        new SeekableStreamStartSequenceNumbers<>(\"topic\", ImmutableMap.of(0, 10L, 1, 20L, 2, 30L), ImmutableSet.of()),\n        new SeekableStreamEndSequenceNumbers<>(\n            \"topic\",\n            ImmutableMap.of(0, Long.MAX_VALUE, 1, Long.MAX_VALUE, 2, Long.MAX_VALUE)\n        ),\n        null,\n        null,\n        tuningConfig\n    );\n\n    Collection workItems = new ArrayList<>();\n    workItems.add(new TestTaskRunnerWorkItem(id1, null, location1));\n    workItems.add(new TestTaskRunnerWorkItem(id2, null, location2));\n\n    EasyMock.expect(taskMaster.getTaskQueue()).andReturn(Optional.of(taskQueue)).anyTimes();\n    EasyMock.expect(taskMaster.getTaskRunner()).andReturn(Optional.of(taskRunner)).anyTimes();\n    EasyMock.expect(taskRunner.getRunningTasks()).andReturn(workItems).anyTimes();\n    EasyMock.expect(taskStorage.getActiveTasksByDatasource(DATASOURCE))\n            .andReturn(ImmutableList.of(id1, id2, id3))\n            .anyTimes();\n    EasyMock.expect(taskStorage.getStatus(\"id1\")).andReturn(Optional.of(TaskStatus.running(\"id1\"))).anyTimes();\n    EasyMock.expect(taskStorage.getStatus(\"id2\")).andReturn(Optional.of(TaskStatus.running(\"id2\"))).anyTimes();\n    EasyMock.expect(taskStorage.getStatus(\"id3\")).andReturn(Optional.of(TaskStatus.running(\"id3\"))).anyTimes();\n    EasyMock.expect(taskStorage.getTask(\"id1\")).andReturn(Optional.of(id1)).anyTimes();\n    EasyMock.expect(taskStorage.getTask(\"id2\")).andReturn(Optional.of(id2)).anyTimes();\n    EasyMock.expect(taskStorage.getTask(\"id3\")).andReturn(Optional.of(id3)).anyTimes();\n    EasyMock.expect(indexerMetadataStorageCoordinator.retrieveDataSourceMetadata(DATASOURCE)).andReturn(\n        new KafkaDataSourceMetadata(\n            null\n        )\n    ).anyTimes();\n    EasyMock.expect(taskClient.getStatusAsync(\"id1\")).andReturn(Futures.immediateFuture(Status.PUBLISHING));\n    EasyMock.expect(taskClient.getStatusAsync(\"id2\")).andReturn(Futures.immediateFuture(Status.READING));\n    EasyMock.expect(taskClient.getStatusAsync(\"id3\")).andReturn(Futures.immediateFuture(Status.READING));\n    EasyMock.expect(taskClient.getStartTimeAsync(\"id2\")).andReturn(Futures.immediateFuture(startTime));\n    EasyMock.expect(taskClient.getStartTimeAsync(\"id3\")).andReturn(Futures.immediateFuture(startTime));\n    EasyMock.expect(taskClient.getEndOffsets(\"id1\")).andReturn(ImmutableMap.of(0, 10L, 1, 20L, 2, 30L));\n\n    \r\n    TreeMap<Integer, Map<Integer, Long>> checkpoints = new TreeMap<>();\n    checkpoints.put(0, ImmutableMap.of(0, 10L, 1, 20L, 2, 30L));\n    EasyMock.expect(taskClient.getCheckpointsAsync(EasyMock.contains(\"id2\"), EasyMock.anyBoolean()))\n            .andReturn(Futures.immediateFuture(checkpoints))\n            .times(1);\n    EasyMock.expect(taskClient.getCheckpointsAsync(EasyMock.contains(\"id3\"), EasyMock.anyBoolean()))\n            .andReturn(Futures.immediateFuture(checkpoints))\n            .times(1);\n\n    taskRunner.registerListener(EasyMock.anyObject(TaskRunnerListener.class), EasyMock.anyObject(Executor.class));\n    replayAll();\n\n    supervisor.start();\n    supervisor.runInternal();\n    verifyAll();\n\n    EasyMock.reset(taskRunner, taskClient, taskQueue);\n    EasyMock.expect(taskRunner.getRunningTasks()).andReturn(workItems).anyTimes();\n    EasyMock.expect(taskClient.pauseAsync(\"id2\"))\n            .andReturn(Futures.immediateFuture(ImmutableMap.of(0, 15L, 1, 25L, 2, 30L)));\n    EasyMock.expect(taskClient.setEndOffsetsAsync(\"id2\", ImmutableMap.of(0, 15L, 1, 25L, 2, 30L), true))\n            .andReturn(Futures.immediateFuture(true));\n    taskQueue.shutdown(\"id3\", \"Killing task for graceful shutdown\");\n    EasyMock.expectLastCall().times(1);\n    taskQueue.shutdown(\"id3\", \"Killing task [%s] which hasn't been assigned to a worker\", \"id3\");\n    EasyMock.expectLastCall().times(1);\n\n    EasyMock.replay(taskRunner, taskClient, taskQueue);\n\n    supervisor.gracefulShutdownInternal();\n    verifyAll();\n  }\n","date":"2020-01-28 03:24:29","endLine":2037,"groupId":"102490","id":21,"instanceNumber":1,"isCurCommit":0,"methodName":"testStopGracefully","params":"()","path":"/mnt/clonedata/CloneManagementServer/ManagementServer/consistResult/result-druid-10-0.7/blobInfo/CC_OUT/blobs/0b/29b65a628bb6f0503d980831a84ce2d009150d.src","preCode":"  public void testStopGracefully() throws Exception\n  {\n    final TaskLocation location1 = new TaskLocation(\"testHost\", 1234, -1);\n    final TaskLocation location2 = new TaskLocation(\"testHost2\", 145, -1);\n    final DateTime startTime = DateTimes.nowUtc();\n\n    supervisor = getTestableSupervisor(2, 1, true, \"PT1H\", null, null);\n    final KafkaSupervisorTuningConfig tuningConfig = supervisor.getTuningConfig();\n    addSomeEvents(1);\n\n    Task id1 = createKafkaIndexTask(\n        \"id1\",\n        DATASOURCE,\n        0,\n        new SeekableStreamStartSequenceNumbers<>(\"topic\", ImmutableMap.of(0, 0L, 1, 0L, 2, 0L), ImmutableSet.of()),\n        new SeekableStreamEndSequenceNumbers<>(\n            \"topic\",\n            ImmutableMap.of(0, Long.MAX_VALUE, 1, Long.MAX_VALUE, 2, Long.MAX_VALUE)\n        ),\n        null,\n        null,\n        tuningConfig\n    );\n\n    Task id2 = createKafkaIndexTask(\n        \"id2\",\n        DATASOURCE,\n        0,\n        new SeekableStreamStartSequenceNumbers<>(\"topic\", ImmutableMap.of(0, 10L, 1, 20L, 2, 30L), ImmutableSet.of()),\n        new SeekableStreamEndSequenceNumbers<>(\n            \"topic\",\n            ImmutableMap.of(0, Long.MAX_VALUE, 1, Long.MAX_VALUE, 2, Long.MAX_VALUE)\n        ),\n        null,\n        null,\n        tuningConfig\n    );\n\n    Task id3 = createKafkaIndexTask(\n        \"id3\",\n        DATASOURCE,\n        0,\n        new SeekableStreamStartSequenceNumbers<>(\"topic\", ImmutableMap.of(0, 10L, 1, 20L, 2, 30L), ImmutableSet.of()),\n        new SeekableStreamEndSequenceNumbers<>(\n            \"topic\",\n            ImmutableMap.of(0, Long.MAX_VALUE, 1, Long.MAX_VALUE, 2, Long.MAX_VALUE)\n        ),\n        null,\n        null,\n        tuningConfig\n    );\n\n    Collection workItems = new ArrayList<>();\n    workItems.add(new TestTaskRunnerWorkItem(id1, null, location1));\n    workItems.add(new TestTaskRunnerWorkItem(id2, null, location2));\n\n    EasyMock.expect(taskMaster.getTaskQueue()).andReturn(Optional.of(taskQueue)).anyTimes();\n    EasyMock.expect(taskMaster.getTaskRunner()).andReturn(Optional.of(taskRunner)).anyTimes();\n    EasyMock.expect(taskRunner.getRunningTasks()).andReturn(workItems).anyTimes();\n    EasyMock.expect(taskStorage.getActiveTasksByDatasource(DATASOURCE))\n            .andReturn(ImmutableList.of(id1, id2, id3))\n            .anyTimes();\n    EasyMock.expect(taskStorage.getStatus(\"id1\")).andReturn(Optional.of(TaskStatus.running(\"id1\"))).anyTimes();\n    EasyMock.expect(taskStorage.getStatus(\"id2\")).andReturn(Optional.of(TaskStatus.running(\"id2\"))).anyTimes();\n    EasyMock.expect(taskStorage.getStatus(\"id3\")).andReturn(Optional.of(TaskStatus.running(\"id3\"))).anyTimes();\n    EasyMock.expect(taskStorage.getTask(\"id1\")).andReturn(Optional.of(id1)).anyTimes();\n    EasyMock.expect(taskStorage.getTask(\"id2\")).andReturn(Optional.of(id2)).anyTimes();\n    EasyMock.expect(taskStorage.getTask(\"id3\")).andReturn(Optional.of(id3)).anyTimes();\n    EasyMock.expect(indexerMetadataStorageCoordinator.getDataSourceMetadata(DATASOURCE)).andReturn(\n        new KafkaDataSourceMetadata(\n            null\n        )\n    ).anyTimes();\n    EasyMock.expect(taskClient.getStatusAsync(\"id1\")).andReturn(Futures.immediateFuture(Status.PUBLISHING));\n    EasyMock.expect(taskClient.getStatusAsync(\"id2\")).andReturn(Futures.immediateFuture(Status.READING));\n    EasyMock.expect(taskClient.getStatusAsync(\"id3\")).andReturn(Futures.immediateFuture(Status.READING));\n    EasyMock.expect(taskClient.getStartTimeAsync(\"id2\")).andReturn(Futures.immediateFuture(startTime));\n    EasyMock.expect(taskClient.getStartTimeAsync(\"id3\")).andReturn(Futures.immediateFuture(startTime));\n    EasyMock.expect(taskClient.getEndOffsets(\"id1\")).andReturn(ImmutableMap.of(0, 10L, 1, 20L, 2, 30L));\n\n    \r\n    TreeMap<Integer, Map<Integer, Long>> checkpoints = new TreeMap<>();\n    checkpoints.put(0, ImmutableMap.of(0, 10L, 1, 20L, 2, 30L));\n    EasyMock.expect(taskClient.getCheckpointsAsync(EasyMock.contains(\"id2\"), EasyMock.anyBoolean()))\n            .andReturn(Futures.immediateFuture(checkpoints))\n            .times(1);\n    EasyMock.expect(taskClient.getCheckpointsAsync(EasyMock.contains(\"id3\"), EasyMock.anyBoolean()))\n            .andReturn(Futures.immediateFuture(checkpoints))\n            .times(1);\n\n    taskRunner.registerListener(EasyMock.anyObject(TaskRunnerListener.class), EasyMock.anyObject(Executor.class));\n    replayAll();\n\n    supervisor.start();\n    supervisor.runInternal();\n    verifyAll();\n\n    EasyMock.reset(taskRunner, taskClient, taskQueue);\n    EasyMock.expect(taskRunner.getRunningTasks()).andReturn(workItems).anyTimes();\n    EasyMock.expect(taskClient.pauseAsync(\"id2\"))\n            .andReturn(Futures.immediateFuture(ImmutableMap.of(0, 15L, 1, 25L, 2, 30L)));\n    EasyMock.expect(taskClient.setEndOffsetsAsync(\"id2\", ImmutableMap.of(0, 15L, 1, 25L, 2, 30L), true))\n            .andReturn(Futures.immediateFuture(true));\n    taskQueue.shutdown(\"id3\", \"Killing task for graceful shutdown\");\n    EasyMock.expectLastCall().times(1);\n    taskQueue.shutdown(\"id3\", \"Killing task [%s] which hasn't been assigned to a worker\", \"id3\");\n    EasyMock.expectLastCall().times(1);\n\n    EasyMock.replay(taskRunner, taskClient, taskQueue);\n\n    supervisor.gracefulShutdownInternal();\n    verifyAll();\n  }\n","realPath":"extensions-core/kafka-indexing-service/src/test/java/org/apache/druid/indexing/kafka/supervisor/KafkaSupervisorTest.java","repoName":"druid","snippetEndLine":0,"snippetStartLine":0,"startLine":1925,"status":"M"},{"authorDate":"2020-01-28 03:24:29","commitOrder":12,"curCode":"  public void testSuspendedRunningTasks() throws Exception\n  {\n    \r\n\n    final TaskLocation location1 = new TaskLocation(\"testHost\", 1234, -1);\n    final TaskLocation location2 = new TaskLocation(\"testHost2\", 145, -1);\n    final DateTime startTime = DateTimes.nowUtc();\n\n    supervisor = getTestableSupervisor(2, 1, true, \"PT1H\", null, null, true, kafkaHost);\n    final KafkaSupervisorTuningConfig tuningConfig = supervisor.getTuningConfig();\n    addSomeEvents(1);\n\n    Task id1 = createKafkaIndexTask(\n        \"id1\",\n        DATASOURCE,\n        0,\n        new SeekableStreamStartSequenceNumbers<>(\"topic\", ImmutableMap.of(0, 0L, 1, 0L, 2, 0L), ImmutableSet.of()),\n        new SeekableStreamEndSequenceNumbers<>(\n            \"topic\",\n            ImmutableMap.of(0, Long.MAX_VALUE, 1, Long.MAX_VALUE, 2, Long.MAX_VALUE)\n        ),\n        null,\n        null,\n        tuningConfig\n    );\n\n    Task id2 = createKafkaIndexTask(\n        \"id2\",\n        DATASOURCE,\n        0,\n        new SeekableStreamStartSequenceNumbers<>(\"topic\", ImmutableMap.of(0, 10L, 1, 20L, 2, 30L), ImmutableSet.of()),\n        new SeekableStreamEndSequenceNumbers<>(\n            \"topic\",\n            ImmutableMap.of(0, Long.MAX_VALUE, 1, Long.MAX_VALUE, 2, Long.MAX_VALUE)\n        ),\n        null,\n        null,\n        tuningConfig\n    );\n\n    Task id3 = createKafkaIndexTask(\n        \"id3\",\n        DATASOURCE,\n        0,\n        new SeekableStreamStartSequenceNumbers<>(\"topic\", ImmutableMap.of(0, 10L, 1, 20L, 2, 30L), ImmutableSet.of()),\n        new SeekableStreamEndSequenceNumbers<>(\n            \"topic\",\n            ImmutableMap.of(0, Long.MAX_VALUE, 1, Long.MAX_VALUE, 2, Long.MAX_VALUE)\n        ),\n        null,\n        null,\n        tuningConfig\n    );\n\n    Collection workItems = new ArrayList<>();\n    workItems.add(new TestTaskRunnerWorkItem(id1, null, location1));\n    workItems.add(new TestTaskRunnerWorkItem(id2, null, location2));\n\n    EasyMock.expect(taskMaster.getTaskQueue()).andReturn(Optional.of(taskQueue)).anyTimes();\n    EasyMock.expect(taskMaster.getTaskRunner()).andReturn(Optional.of(taskRunner)).anyTimes();\n    EasyMock.expect(taskRunner.getRunningTasks()).andReturn(workItems).anyTimes();\n    EasyMock.expect(taskStorage.getActiveTasksByDatasource(DATASOURCE))\n            .andReturn(ImmutableList.of(id1, id2, id3))\n            .anyTimes();\n    EasyMock.expect(taskStorage.getStatus(\"id1\")).andReturn(Optional.of(TaskStatus.running(\"id1\"))).anyTimes();\n    EasyMock.expect(taskStorage.getStatus(\"id2\")).andReturn(Optional.of(TaskStatus.running(\"id2\"))).anyTimes();\n    EasyMock.expect(taskStorage.getStatus(\"id3\")).andReturn(Optional.of(TaskStatus.running(\"id3\"))).anyTimes();\n    EasyMock.expect(taskStorage.getTask(\"id1\")).andReturn(Optional.of(id1)).anyTimes();\n    EasyMock.expect(taskStorage.getTask(\"id2\")).andReturn(Optional.of(id2)).anyTimes();\n    EasyMock.expect(taskStorage.getTask(\"id3\")).andReturn(Optional.of(id3)).anyTimes();\n    EasyMock.expect(indexerMetadataStorageCoordinator.retrieveDataSourceMetadata(DATASOURCE)).andReturn(\n        new KafkaDataSourceMetadata(\n            null\n        )\n    ).anyTimes();\n    EasyMock.expect(taskClient.getStatusAsync(\"id1\"))\n            .andReturn(Futures.immediateFuture(Status.PUBLISHING));\n    EasyMock.expect(taskClient.getStatusAsync(\"id2\"))\n            .andReturn(Futures.immediateFuture(Status.READING));\n    EasyMock.expect(taskClient.getStatusAsync(\"id3\"))\n            .andReturn(Futures.immediateFuture(Status.READING));\n    EasyMock.expect(taskClient.getStartTimeAsync(\"id2\")).andReturn(Futures.immediateFuture(startTime));\n    EasyMock.expect(taskClient.getStartTimeAsync(\"id3\")).andReturn(Futures.immediateFuture(startTime));\n    EasyMock.expect(taskClient.getEndOffsets(\"id1\")).andReturn(ImmutableMap.of(0, 10L, 1, 20L, 2, 30L));\n\n    \r\n    TreeMap<Integer, Map<Integer, Long>> checkpoints = new TreeMap<>();\n    checkpoints.put(0, ImmutableMap.of(0, 10L, 1, 20L, 2, 30L));\n    EasyMock.expect(taskClient.getCheckpointsAsync(EasyMock.contains(\"id2\"), EasyMock.anyBoolean()))\n            .andReturn(Futures.immediateFuture(checkpoints))\n            .times(1);\n    EasyMock.expect(taskClient.getCheckpointsAsync(EasyMock.contains(\"id3\"), EasyMock.anyBoolean()))\n            .andReturn(Futures.immediateFuture(checkpoints))\n            .times(1);\n\n    taskRunner.registerListener(EasyMock.anyObject(TaskRunnerListener.class), EasyMock.anyObject(Executor.class));\n\n    EasyMock.expect(taskClient.pauseAsync(\"id2\"))\n            .andReturn(Futures.immediateFuture(ImmutableMap.of(0, 15L, 1, 25L, 2, 30L)));\n    EasyMock.expect(taskClient.setEndOffsetsAsync(\"id2\", ImmutableMap.of(0, 15L, 1, 25L, 2, 30L), true))\n            .andReturn(Futures.immediateFuture(true));\n    taskQueue.shutdown(\"id3\", \"Killing task for graceful shutdown\");\n    EasyMock.expectLastCall().times(1);\n    taskQueue.shutdown(\"id3\", \"Killing task [%s] which hasn't been assigned to a worker\", \"id3\");\n    EasyMock.expectLastCall().times(1);\n\n    replayAll();\n    supervisor.start();\n    supervisor.runInternal();\n    verifyAll();\n  }\n","date":"2020-01-28 03:24:29","endLine":2754,"groupId":"102490","id":22,"instanceNumber":2,"isCurCommit":0,"methodName":"testSuspendedRunningTasks","params":"()","path":"/mnt/clonedata/CloneManagementServer/ManagementServer/consistResult/result-druid-10-0.7/blobInfo/CC_OUT/blobs/0b/29b65a628bb6f0503d980831a84ce2d009150d.src","preCode":"  public void testSuspendedRunningTasks() throws Exception\n  {\n    \r\n\n    final TaskLocation location1 = new TaskLocation(\"testHost\", 1234, -1);\n    final TaskLocation location2 = new TaskLocation(\"testHost2\", 145, -1);\n    final DateTime startTime = DateTimes.nowUtc();\n\n    supervisor = getTestableSupervisor(2, 1, true, \"PT1H\", null, null, true, kafkaHost);\n    final KafkaSupervisorTuningConfig tuningConfig = supervisor.getTuningConfig();\n    addSomeEvents(1);\n\n    Task id1 = createKafkaIndexTask(\n        \"id1\",\n        DATASOURCE,\n        0,\n        new SeekableStreamStartSequenceNumbers<>(\"topic\", ImmutableMap.of(0, 0L, 1, 0L, 2, 0L), ImmutableSet.of()),\n        new SeekableStreamEndSequenceNumbers<>(\n            \"topic\",\n            ImmutableMap.of(0, Long.MAX_VALUE, 1, Long.MAX_VALUE, 2, Long.MAX_VALUE)\n        ),\n        null,\n        null,\n        tuningConfig\n    );\n\n    Task id2 = createKafkaIndexTask(\n        \"id2\",\n        DATASOURCE,\n        0,\n        new SeekableStreamStartSequenceNumbers<>(\"topic\", ImmutableMap.of(0, 10L, 1, 20L, 2, 30L), ImmutableSet.of()),\n        new SeekableStreamEndSequenceNumbers<>(\n            \"topic\",\n            ImmutableMap.of(0, Long.MAX_VALUE, 1, Long.MAX_VALUE, 2, Long.MAX_VALUE)\n        ),\n        null,\n        null,\n        tuningConfig\n    );\n\n    Task id3 = createKafkaIndexTask(\n        \"id3\",\n        DATASOURCE,\n        0,\n        new SeekableStreamStartSequenceNumbers<>(\"topic\", ImmutableMap.of(0, 10L, 1, 20L, 2, 30L), ImmutableSet.of()),\n        new SeekableStreamEndSequenceNumbers<>(\n            \"topic\",\n            ImmutableMap.of(0, Long.MAX_VALUE, 1, Long.MAX_VALUE, 2, Long.MAX_VALUE)\n        ),\n        null,\n        null,\n        tuningConfig\n    );\n\n    Collection workItems = new ArrayList<>();\n    workItems.add(new TestTaskRunnerWorkItem(id1, null, location1));\n    workItems.add(new TestTaskRunnerWorkItem(id2, null, location2));\n\n    EasyMock.expect(taskMaster.getTaskQueue()).andReturn(Optional.of(taskQueue)).anyTimes();\n    EasyMock.expect(taskMaster.getTaskRunner()).andReturn(Optional.of(taskRunner)).anyTimes();\n    EasyMock.expect(taskRunner.getRunningTasks()).andReturn(workItems).anyTimes();\n    EasyMock.expect(taskStorage.getActiveTasksByDatasource(DATASOURCE))\n            .andReturn(ImmutableList.of(id1, id2, id3))\n            .anyTimes();\n    EasyMock.expect(taskStorage.getStatus(\"id1\")).andReturn(Optional.of(TaskStatus.running(\"id1\"))).anyTimes();\n    EasyMock.expect(taskStorage.getStatus(\"id2\")).andReturn(Optional.of(TaskStatus.running(\"id2\"))).anyTimes();\n    EasyMock.expect(taskStorage.getStatus(\"id3\")).andReturn(Optional.of(TaskStatus.running(\"id3\"))).anyTimes();\n    EasyMock.expect(taskStorage.getTask(\"id1\")).andReturn(Optional.of(id1)).anyTimes();\n    EasyMock.expect(taskStorage.getTask(\"id2\")).andReturn(Optional.of(id2)).anyTimes();\n    EasyMock.expect(taskStorage.getTask(\"id3\")).andReturn(Optional.of(id3)).anyTimes();\n    EasyMock.expect(indexerMetadataStorageCoordinator.getDataSourceMetadata(DATASOURCE)).andReturn(\n        new KafkaDataSourceMetadata(\n            null\n        )\n    ).anyTimes();\n    EasyMock.expect(taskClient.getStatusAsync(\"id1\"))\n            .andReturn(Futures.immediateFuture(Status.PUBLISHING));\n    EasyMock.expect(taskClient.getStatusAsync(\"id2\"))\n            .andReturn(Futures.immediateFuture(Status.READING));\n    EasyMock.expect(taskClient.getStatusAsync(\"id3\"))\n            .andReturn(Futures.immediateFuture(Status.READING));\n    EasyMock.expect(taskClient.getStartTimeAsync(\"id2\")).andReturn(Futures.immediateFuture(startTime));\n    EasyMock.expect(taskClient.getStartTimeAsync(\"id3\")).andReturn(Futures.immediateFuture(startTime));\n    EasyMock.expect(taskClient.getEndOffsets(\"id1\")).andReturn(ImmutableMap.of(0, 10L, 1, 20L, 2, 30L));\n\n    \r\n    TreeMap<Integer, Map<Integer, Long>> checkpoints = new TreeMap<>();\n    checkpoints.put(0, ImmutableMap.of(0, 10L, 1, 20L, 2, 30L));\n    EasyMock.expect(taskClient.getCheckpointsAsync(EasyMock.contains(\"id2\"), EasyMock.anyBoolean()))\n            .andReturn(Futures.immediateFuture(checkpoints))\n            .times(1);\n    EasyMock.expect(taskClient.getCheckpointsAsync(EasyMock.contains(\"id3\"), EasyMock.anyBoolean()))\n            .andReturn(Futures.immediateFuture(checkpoints))\n            .times(1);\n\n    taskRunner.registerListener(EasyMock.anyObject(TaskRunnerListener.class), EasyMock.anyObject(Executor.class));\n\n    EasyMock.expect(taskClient.pauseAsync(\"id2\"))\n            .andReturn(Futures.immediateFuture(ImmutableMap.of(0, 15L, 1, 25L, 2, 30L)));\n    EasyMock.expect(taskClient.setEndOffsetsAsync(\"id2\", ImmutableMap.of(0, 15L, 1, 25L, 2, 30L), true))\n            .andReturn(Futures.immediateFuture(true));\n    taskQueue.shutdown(\"id3\", \"Killing task for graceful shutdown\");\n    EasyMock.expectLastCall().times(1);\n    taskQueue.shutdown(\"id3\", \"Killing task [%s] which hasn't been assigned to a worker\", \"id3\");\n    EasyMock.expectLastCall().times(1);\n\n    replayAll();\n    supervisor.start();\n    supervisor.runInternal();\n    verifyAll();\n  }\n","realPath":"extensions-core/kafka-indexing-service/src/test/java/org/apache/druid/indexing/kafka/supervisor/KafkaSupervisorTest.java","repoName":"druid","snippetEndLine":0,"snippetStartLine":0,"startLine":2644,"status":"M"}],"commitId":"b9186f8f9ff2ff52aceda42bc5f24ffd47a7d17e","commitMessage":"@@@Reconcile terminology and method naming to 'used/unused segments'; Rename MetadataSegmentManager to MetadataSegmentsManager (#7306)\n\n* Reconcile terminology and method naming to 'used/unused segments'; Don't use terms 'enable/disable data source'; Rename MetadataSegmentManager to MetadataSegments; Make REST API methods which mark segments as used/unused to return server error instead of an empty response in case of error\n\n* Fix brace\n\n* Import order\n\n* Rename withKillDataSourceWhitelist to withSpecificDataSourcesToKill\n\n* Fix tests\n\n* Fix tests by adding proper methods without interval parameters to IndexerMetadataStorageCoordinator instead of hacking with Intervals.ETERNITY\n\n* More aligned names of DruidCoordinatorHelpers.  rename several CoordinatorDynamicConfig parameters\n\n* Rename ClientCompactTaskQuery to ClientCompactionTaskQuery for consistency with CompactionTask; ClientCompactQueryTuningConfig to ClientCompactionTaskQueryTuningConfig\n\n* More variable and method renames\n\n* Rename MetadataSegments to SegmentsMetadata\n\n* Javadoc update\n\n* Simplify SegmentsMetadata.getUnusedSegmentIntervals().  more javadocs\n\n* Update Javadoc of VersionedIntervalTimeline.iterateAllObjects()\n\n* Reorder imports\n\n* Rename SegmentsMetadata.tryMark... methods to mark... and make them to return boolean and the numbers of segments changed and relay exceptions to callers\n\n* Complete merge\n\n* Add CollectionUtils.newTreeSet(); Refactor DruidCoordinatorRuntimeParams creation in tests\n\n* Remove MetadataSegmentManager\n\n* Rename millisLagSinceCoordinatorBecomesLeaderBeforeCanMarkAsUnusedOvershadowedSegments to leadingTimeMillisBeforeCanMarkAsUnusedOvershadowedSegments\n\n* Fix tests.  refactor DruidCluster creation in tests into DruidClusterBuilder\n\n* Fix inspections\n\n* Fix SQLMetadataSegmentManagerEmptyTest and rename it to SqlSegmentsMetadataEmptyTest\n\n* Rename SegmentsAndMetadata to SegmentsAndCommitMetadata to reduce the similarity with SegmentsMetadata; Rename some methods\n\n* Rename DruidCoordinatorHelper to CoordinatorDuty.  refactor DruidCoordinator\n\n* Unused import\n\n* Optimize imports\n\n* Rename IndexerSQLMetadataStorageCoordinator.getDataSourceMetadata() to retrieveDataSourceMetadata()\n\n* Unused import\n\n* Update terminology in datasource-view.tsx\n\n* Fix label in datasource-view.spec.tsx.snap\n\n* Fix lint errors in datasource-view.tsx\n\n* Doc improvements\n\n* Another attempt to please TSLint\n\n* Another attempt to please TSLint\n\n* Style fixes\n\n* Fix IndexerSQLMetadataStorageCoordinator.createUsedSegmentsSqlQueryForIntervals() (wrong merge)\n\n* Try to fix docs build issue\n\n* Javadoc and spelling fixes\n\n* Rename SegmentsMetadata to SegmentsMetadataManager.  address other comments\n\n* Address more comments\n","date":"2020-01-28 03:24:29","modifiedFileCount":"127","status":"M","submitter":"Roman Leventov"}]
