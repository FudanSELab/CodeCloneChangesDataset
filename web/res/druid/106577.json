[{"authorTime":"2018-08-31 00:56:26","codes":[{"authorDate":"2018-08-31 00:56:26","commitOrder":1,"curCode":"  public void setup() throws Exception\n  {\n    tmpDir = Files.createTempDir();\n\n    InputRow row;\n    List<String> dimNames = Arrays.asList(\"dimA\", \"metA\");\n    Map<String, Object> event;\n\n    final IncrementalIndex indexA = makeIncIndex(false);\n    incrementalIndices.add(indexA);\n\n    event = new HashMap<>();\n    event.put(\"dimA\", \"hello\");\n    event.put(\"metA\", 100);\n    row = new MapBasedInputRow(1000, dimNames, event);\n    indexA.add(row);\n\n    event = new HashMap<>();\n    event.put(\"dimA\", \"mango\");\n    event.put(\"metA\", 95);\n    row = new MapBasedInputRow(1000, dimNames, event);\n    indexA.add(row);\n\n    event = new HashMap<>();\n    event.put(\"dimA\", \"world\");\n    event.put(\"metA\", 75);\n    row = new MapBasedInputRow(1000, dimNames, event);\n    indexA.add(row);\n\n    event = new HashMap<>();\n    event.put(\"dimA\", \"fubaz\");\n    event.put(\"metA\", 75);\n    row = new MapBasedInputRow(1000, dimNames, event);\n    indexA.add(row);\n\n    event = new HashMap<>();\n    event.put(\"dimA\", \"zortaxx\");\n    event.put(\"metA\", 999);\n    row = new MapBasedInputRow(1000, dimNames, event);\n    indexA.add(row);\n\n    event = new HashMap<>();\n    event.put(\"dimA\", \"blarg\");\n    event.put(\"metA\", 125);\n    row = new MapBasedInputRow(1000, dimNames, event);\n    indexA.add(row);\n\n    event = new HashMap<>();\n    event.put(\"dimA\", \"blerg\");\n    event.put(\"metA\", 130);\n    row = new MapBasedInputRow(1000, dimNames, event);\n    indexA.add(row);\n\n\n    final File fileA = INDEX_MERGER_V9.persist(\n        indexA,\n        new File(tmpDir, \"A\"),\n        new IndexSpec(),\n        OffHeapMemorySegmentWriteOutMediumFactory.instance()\n    );\n    QueryableIndex qindexA = INDEX_IO.loadIndex(fileA);\n\n\n    final IncrementalIndex indexB = makeIncIndex(false);\n    incrementalIndices.add(indexB);\n\n    event = new HashMap<>();\n    event.put(\"dimA\", \"foo\");\n    event.put(\"metA\", 200);\n    row = new MapBasedInputRow(1000, dimNames, event);\n    indexB.add(row);\n\n    event = new HashMap<>();\n    event.put(\"dimA\", \"world\");\n    event.put(\"metA\", 75);\n    row = new MapBasedInputRow(1000, dimNames, event);\n    indexB.add(row);\n\n    event = new HashMap<>();\n    event.put(\"dimA\", \"mango\");\n    event.put(\"metA\", 95);\n    row = new MapBasedInputRow(1000, dimNames, event);\n    indexB.add(row);\n\n    event = new HashMap<>();\n    event.put(\"dimA\", \"zebra\");\n    event.put(\"metA\", 180);\n    row = new MapBasedInputRow(1000, dimNames, event);\n    indexB.add(row);\n\n    event = new HashMap<>();\n    event.put(\"dimA\", \"blorg\");\n    event.put(\"metA\", 120);\n    row = new MapBasedInputRow(1000, dimNames, event);\n    indexB.add(row);\n\n    final File fileB = INDEX_MERGER_V9.persist(\n        indexB,\n        new File(tmpDir, \"B\"),\n        new IndexSpec(),\n        OffHeapMemorySegmentWriteOutMediumFactory.instance()\n    );\n    QueryableIndex qindexB = INDEX_IO.loadIndex(fileB);\n\n    groupByIndices = Arrays.asList(qindexA, qindexB);\n    resourceCloser = Closer.create();\n    setupGroupByFactory();\n  }\n","date":"2018-08-31 00:56:26","endLine":266,"groupId":"15521","id":1,"instanceNumber":1,"isCurCommit":0,"methodName":"setup","params":"()","path":"/mnt/clonedata/CloneManagementServer/ManagementServer/consistResult/result-druid-10-0.7/blobInfo/CC_OUT/blobs/99/a8b646a6615a7f64b98f7ac65a3d4bcd25e353.src","preCode":"  public void setup() throws Exception\n  {\n    tmpDir = Files.createTempDir();\n\n    InputRow row;\n    List<String> dimNames = Arrays.asList(\"dimA\", \"metA\");\n    Map<String, Object> event;\n\n    final IncrementalIndex indexA = makeIncIndex(false);\n    incrementalIndices.add(indexA);\n\n    event = new HashMap<>();\n    event.put(\"dimA\", \"hello\");\n    event.put(\"metA\", 100);\n    row = new MapBasedInputRow(1000, dimNames, event);\n    indexA.add(row);\n\n    event = new HashMap<>();\n    event.put(\"dimA\", \"mango\");\n    event.put(\"metA\", 95);\n    row = new MapBasedInputRow(1000, dimNames, event);\n    indexA.add(row);\n\n    event = new HashMap<>();\n    event.put(\"dimA\", \"world\");\n    event.put(\"metA\", 75);\n    row = new MapBasedInputRow(1000, dimNames, event);\n    indexA.add(row);\n\n    event = new HashMap<>();\n    event.put(\"dimA\", \"fubaz\");\n    event.put(\"metA\", 75);\n    row = new MapBasedInputRow(1000, dimNames, event);\n    indexA.add(row);\n\n    event = new HashMap<>();\n    event.put(\"dimA\", \"zortaxx\");\n    event.put(\"metA\", 999);\n    row = new MapBasedInputRow(1000, dimNames, event);\n    indexA.add(row);\n\n    event = new HashMap<>();\n    event.put(\"dimA\", \"blarg\");\n    event.put(\"metA\", 125);\n    row = new MapBasedInputRow(1000, dimNames, event);\n    indexA.add(row);\n\n    event = new HashMap<>();\n    event.put(\"dimA\", \"blerg\");\n    event.put(\"metA\", 130);\n    row = new MapBasedInputRow(1000, dimNames, event);\n    indexA.add(row);\n\n\n    final File fileA = INDEX_MERGER_V9.persist(\n        indexA,\n        new File(tmpDir, \"A\"),\n        new IndexSpec(),\n        OffHeapMemorySegmentWriteOutMediumFactory.instance()\n    );\n    QueryableIndex qindexA = INDEX_IO.loadIndex(fileA);\n\n\n    final IncrementalIndex indexB = makeIncIndex(false);\n    incrementalIndices.add(indexB);\n\n    event = new HashMap<>();\n    event.put(\"dimA\", \"foo\");\n    event.put(\"metA\", 200);\n    row = new MapBasedInputRow(1000, dimNames, event);\n    indexB.add(row);\n\n    event = new HashMap<>();\n    event.put(\"dimA\", \"world\");\n    event.put(\"metA\", 75);\n    row = new MapBasedInputRow(1000, dimNames, event);\n    indexB.add(row);\n\n    event = new HashMap<>();\n    event.put(\"dimA\", \"mango\");\n    event.put(\"metA\", 95);\n    row = new MapBasedInputRow(1000, dimNames, event);\n    indexB.add(row);\n\n    event = new HashMap<>();\n    event.put(\"dimA\", \"zebra\");\n    event.put(\"metA\", 180);\n    row = new MapBasedInputRow(1000, dimNames, event);\n    indexB.add(row);\n\n    event = new HashMap<>();\n    event.put(\"dimA\", \"blorg\");\n    event.put(\"metA\", 120);\n    row = new MapBasedInputRow(1000, dimNames, event);\n    indexB.add(row);\n\n    final File fileB = INDEX_MERGER_V9.persist(\n        indexB,\n        new File(tmpDir, \"B\"),\n        new IndexSpec(),\n        OffHeapMemorySegmentWriteOutMediumFactory.instance()\n    );\n    QueryableIndex qindexB = INDEX_IO.loadIndex(fileB);\n\n    groupByIndices = Arrays.asList(qindexA, qindexB);\n    resourceCloser = Closer.create();\n    setupGroupByFactory();\n  }\n","realPath":"processing/src/test/java/org/apache/druid/query/groupby/GroupByLimitPushDownInsufficientBufferTest.java","repoName":"druid","snippetEndLine":0,"snippetStartLine":0,"startLine":159,"status":"B"},{"authorDate":"2018-08-31 00:56:26","commitOrder":1,"curCode":"  public void setup() throws Exception\n  {\n    tmpDir = Files.createTempDir();\n\n    InputRow row;\n    List<String> dimNames = Arrays.asList(\"dimA\", \"metA\");\n    Map<String, Object> event;\n\n    final IncrementalIndex indexA = makeIncIndex(false);\n    incrementalIndices.add(indexA);\n\n    event = new HashMap<>();\n    event.put(\"dimA\", \"pomegranate\");\n    event.put(\"metA\", 2395L);\n    row = new MapBasedInputRow(1505260888888L, dimNames, event);\n    indexA.add(row);\n\n    event = new HashMap<>();\n    event.put(\"dimA\", \"mango\");\n    event.put(\"metA\", 8L);\n    row = new MapBasedInputRow(1505260800000L, dimNames, event);\n    indexA.add(row);\n\n    event = new HashMap<>();\n    event.put(\"dimA\", \"pomegranate\");\n    event.put(\"metA\", 5028L);\n    row = new MapBasedInputRow(1505264400000L, dimNames, event);\n    indexA.add(row);\n\n    event = new HashMap<>();\n    event.put(\"dimA\", \"mango\");\n    event.put(\"metA\", 7L);\n    row = new MapBasedInputRow(1505264400400L, dimNames, event);\n    indexA.add(row);\n\n    final File fileA = INDEX_MERGER_V9.persist(\n        indexA,\n        new File(tmpDir, \"A\"),\n        new IndexSpec(),\n        null\n    );\n    QueryableIndex qindexA = INDEX_IO.loadIndex(fileA);\n\n\n    final IncrementalIndex indexB = makeIncIndex(false);\n    incrementalIndices.add(indexB);\n\n    event = new HashMap<>();\n    event.put(\"dimA\", \"pomegranate\");\n    event.put(\"metA\", 4718L);\n    row = new MapBasedInputRow(1505260800000L, dimNames, event);\n    indexB.add(row);\n\n    event = new HashMap<>();\n    event.put(\"dimA\", \"mango\");\n    event.put(\"metA\", 18L);\n    row = new MapBasedInputRow(1505260800000L, dimNames, event);\n    indexB.add(row);\n\n    event = new HashMap<>();\n    event.put(\"dimA\", \"pomegranate\");\n    event.put(\"metA\", 2698L);\n    row = new MapBasedInputRow(1505264400000L, dimNames, event);\n    indexB.add(row);\n\n    event = new HashMap<>();\n    event.put(\"dimA\", \"mango\");\n    event.put(\"metA\", 3L);\n    row = new MapBasedInputRow(1505264400000L, dimNames, event);\n    indexB.add(row);\n\n    final File fileB = INDEX_MERGER_V9.persist(\n        indexB,\n        new File(tmpDir, \"B\"),\n        new IndexSpec(),\n        null\n    );\n    QueryableIndex qindexB = INDEX_IO.loadIndex(fileB);\n\n    final IncrementalIndex indexC = makeIncIndex(false);\n    incrementalIndices.add(indexC);\n\n    event = new HashMap<>();\n    event.put(\"dimA\", \"pomegranate\");\n    event.put(\"metA\", 2395L);\n    row = new MapBasedInputRow(1505260800000L, dimNames, event);\n    indexC.add(row);\n\n    event = new HashMap<>();\n    event.put(\"dimA\", \"mango\");\n    event.put(\"metA\", 8L);\n    row = new MapBasedInputRow(1605260800000L, dimNames, event);\n    indexC.add(row);\n\n    event = new HashMap<>();\n    event.put(\"dimA\", \"pomegranate\");\n    event.put(\"metA\", 5028L);\n    row = new MapBasedInputRow(1705264400000L, dimNames, event);\n    indexC.add(row);\n\n    event = new HashMap<>();\n    event.put(\"dimA\", \"mango\");\n    event.put(\"metA\", 7L);\n    row = new MapBasedInputRow(1805264400000L, dimNames, event);\n    indexC.add(row);\n\n    final File fileC = INDEX_MERGER_V9.persist(\n        indexC,\n        new File(tmpDir, \"C\"),\n        new IndexSpec(),\n        null\n    );\n    QueryableIndex qindexC = INDEX_IO.loadIndex(fileC);\n\n\n    final IncrementalIndex indexD = makeIncIndex(false);\n    incrementalIndices.add(indexD);\n\n    event = new HashMap<>();\n    event.put(\"dimA\", \"pomegranate\");\n    event.put(\"metA\", 4718L);\n    row = new MapBasedInputRow(1505260800000L, dimNames, event);\n    indexD.add(row);\n\n    event = new HashMap<>();\n    event.put(\"dimA\", \"mango\");\n    event.put(\"metA\", 18L);\n    row = new MapBasedInputRow(1605260800000L, dimNames, event);\n    indexD.add(row);\n\n    event = new HashMap<>();\n    event.put(\"dimA\", \"pomegranate\");\n    event.put(\"metA\", 2698L);\n    row = new MapBasedInputRow(1705264400000L, dimNames, event);\n    indexD.add(row);\n\n    event = new HashMap<>();\n    event.put(\"dimA\", \"mango\");\n    event.put(\"metA\", 3L);\n    row = new MapBasedInputRow(1805264400000L, dimNames, event);\n    indexD.add(row);\n\n    final File fileD = INDEX_MERGER_V9.persist(\n        indexD,\n        new File(tmpDir, \"D\"),\n        new IndexSpec(),\n        null\n    );\n    QueryableIndex qindexD = INDEX_IO.loadIndex(fileD);\n\n    groupByIndices = Arrays.asList(qindexA, qindexB, qindexC, qindexD);\n    resourceCloser = Closer.create();\n    setupGroupByFactory();\n  }\n","date":"2018-08-31 00:56:26","endLine":321,"groupId":"14771","id":2,"instanceNumber":2,"isCurCommit":0,"methodName":"setup","params":"()","path":"/mnt/clonedata/CloneManagementServer/ManagementServer/consistResult/result-druid-10-0.7/blobInfo/CC_OUT/blobs/6b/12b2d054151757f3602b9cbe44f7a8ac18efe0.src","preCode":"  public void setup() throws Exception\n  {\n    tmpDir = Files.createTempDir();\n\n    InputRow row;\n    List<String> dimNames = Arrays.asList(\"dimA\", \"metA\");\n    Map<String, Object> event;\n\n    final IncrementalIndex indexA = makeIncIndex(false);\n    incrementalIndices.add(indexA);\n\n    event = new HashMap<>();\n    event.put(\"dimA\", \"pomegranate\");\n    event.put(\"metA\", 2395L);\n    row = new MapBasedInputRow(1505260888888L, dimNames, event);\n    indexA.add(row);\n\n    event = new HashMap<>();\n    event.put(\"dimA\", \"mango\");\n    event.put(\"metA\", 8L);\n    row = new MapBasedInputRow(1505260800000L, dimNames, event);\n    indexA.add(row);\n\n    event = new HashMap<>();\n    event.put(\"dimA\", \"pomegranate\");\n    event.put(\"metA\", 5028L);\n    row = new MapBasedInputRow(1505264400000L, dimNames, event);\n    indexA.add(row);\n\n    event = new HashMap<>();\n    event.put(\"dimA\", \"mango\");\n    event.put(\"metA\", 7L);\n    row = new MapBasedInputRow(1505264400400L, dimNames, event);\n    indexA.add(row);\n\n    final File fileA = INDEX_MERGER_V9.persist(\n        indexA,\n        new File(tmpDir, \"A\"),\n        new IndexSpec(),\n        null\n    );\n    QueryableIndex qindexA = INDEX_IO.loadIndex(fileA);\n\n\n    final IncrementalIndex indexB = makeIncIndex(false);\n    incrementalIndices.add(indexB);\n\n    event = new HashMap<>();\n    event.put(\"dimA\", \"pomegranate\");\n    event.put(\"metA\", 4718L);\n    row = new MapBasedInputRow(1505260800000L, dimNames, event);\n    indexB.add(row);\n\n    event = new HashMap<>();\n    event.put(\"dimA\", \"mango\");\n    event.put(\"metA\", 18L);\n    row = new MapBasedInputRow(1505260800000L, dimNames, event);\n    indexB.add(row);\n\n    event = new HashMap<>();\n    event.put(\"dimA\", \"pomegranate\");\n    event.put(\"metA\", 2698L);\n    row = new MapBasedInputRow(1505264400000L, dimNames, event);\n    indexB.add(row);\n\n    event = new HashMap<>();\n    event.put(\"dimA\", \"mango\");\n    event.put(\"metA\", 3L);\n    row = new MapBasedInputRow(1505264400000L, dimNames, event);\n    indexB.add(row);\n\n    final File fileB = INDEX_MERGER_V9.persist(\n        indexB,\n        new File(tmpDir, \"B\"),\n        new IndexSpec(),\n        null\n    );\n    QueryableIndex qindexB = INDEX_IO.loadIndex(fileB);\n\n    final IncrementalIndex indexC = makeIncIndex(false);\n    incrementalIndices.add(indexC);\n\n    event = new HashMap<>();\n    event.put(\"dimA\", \"pomegranate\");\n    event.put(\"metA\", 2395L);\n    row = new MapBasedInputRow(1505260800000L, dimNames, event);\n    indexC.add(row);\n\n    event = new HashMap<>();\n    event.put(\"dimA\", \"mango\");\n    event.put(\"metA\", 8L);\n    row = new MapBasedInputRow(1605260800000L, dimNames, event);\n    indexC.add(row);\n\n    event = new HashMap<>();\n    event.put(\"dimA\", \"pomegranate\");\n    event.put(\"metA\", 5028L);\n    row = new MapBasedInputRow(1705264400000L, dimNames, event);\n    indexC.add(row);\n\n    event = new HashMap<>();\n    event.put(\"dimA\", \"mango\");\n    event.put(\"metA\", 7L);\n    row = new MapBasedInputRow(1805264400000L, dimNames, event);\n    indexC.add(row);\n\n    final File fileC = INDEX_MERGER_V9.persist(\n        indexC,\n        new File(tmpDir, \"C\"),\n        new IndexSpec(),\n        null\n    );\n    QueryableIndex qindexC = INDEX_IO.loadIndex(fileC);\n\n\n    final IncrementalIndex indexD = makeIncIndex(false);\n    incrementalIndices.add(indexD);\n\n    event = new HashMap<>();\n    event.put(\"dimA\", \"pomegranate\");\n    event.put(\"metA\", 4718L);\n    row = new MapBasedInputRow(1505260800000L, dimNames, event);\n    indexD.add(row);\n\n    event = new HashMap<>();\n    event.put(\"dimA\", \"mango\");\n    event.put(\"metA\", 18L);\n    row = new MapBasedInputRow(1605260800000L, dimNames, event);\n    indexD.add(row);\n\n    event = new HashMap<>();\n    event.put(\"dimA\", \"pomegranate\");\n    event.put(\"metA\", 2698L);\n    row = new MapBasedInputRow(1705264400000L, dimNames, event);\n    indexD.add(row);\n\n    event = new HashMap<>();\n    event.put(\"dimA\", \"mango\");\n    event.put(\"metA\", 3L);\n    row = new MapBasedInputRow(1805264400000L, dimNames, event);\n    indexD.add(row);\n\n    final File fileD = INDEX_MERGER_V9.persist(\n        indexD,\n        new File(tmpDir, \"D\"),\n        new IndexSpec(),\n        null\n    );\n    QueryableIndex qindexD = INDEX_IO.loadIndex(fileD);\n\n    groupByIndices = Arrays.asList(qindexA, qindexB, qindexC, qindexD);\n    resourceCloser = Closer.create();\n    setupGroupByFactory();\n  }\n","realPath":"processing/src/test/java/org/apache/druid/query/groupby/GroupByLimitPushDownMultiNodeMergeTest.java","repoName":"druid","snippetEndLine":0,"snippetStartLine":0,"startLine":168,"status":"B"}],"commitId":"431d3d8497f9079857c3baa7ae7ab4cb44a22355","commitMessage":"@@@Rename io.druid to org.apache.druid. (#6266)\n\n* Rename io.druid to org.apache.druid.\n\n* Fix META-INF files and remove some benchmark results.\n\n* MonitorsConfig update for metrics package migration.\n\n* Reorder some dimensions in inner queries for some reason.\n\n* Fix protobuf tests.\n","date":"2018-08-31 00:56:26","modifiedFileCount":"5","status":"B","submitter":"Gian Merlino"},{"authorTime":"2019-11-23 11:48:49","codes":[{"authorDate":"2019-11-23 11:48:49","commitOrder":2,"curCode":"  public void setup() throws Exception\n  {\n    tmpDir = FileUtils.createTempDir();\n\n    InputRow row;\n    List<String> dimNames = Arrays.asList(\"dimA\", \"metA\");\n    Map<String, Object> event;\n\n    final IncrementalIndex indexA = makeIncIndex(false);\n    incrementalIndices.add(indexA);\n\n    event = new HashMap<>();\n    event.put(\"dimA\", \"hello\");\n    event.put(\"metA\", 100);\n    row = new MapBasedInputRow(1000, dimNames, event);\n    indexA.add(row);\n\n    event = new HashMap<>();\n    event.put(\"dimA\", \"mango\");\n    event.put(\"metA\", 95);\n    row = new MapBasedInputRow(1000, dimNames, event);\n    indexA.add(row);\n\n    event = new HashMap<>();\n    event.put(\"dimA\", \"world\");\n    event.put(\"metA\", 75);\n    row = new MapBasedInputRow(1000, dimNames, event);\n    indexA.add(row);\n\n    event = new HashMap<>();\n    event.put(\"dimA\", \"fubaz\");\n    event.put(\"metA\", 75);\n    row = new MapBasedInputRow(1000, dimNames, event);\n    indexA.add(row);\n\n    event = new HashMap<>();\n    event.put(\"dimA\", \"zortaxx\");\n    event.put(\"metA\", 999);\n    row = new MapBasedInputRow(1000, dimNames, event);\n    indexA.add(row);\n\n    event = new HashMap<>();\n    event.put(\"dimA\", \"blarg\");\n    event.put(\"metA\", 125);\n    row = new MapBasedInputRow(1000, dimNames, event);\n    indexA.add(row);\n\n    event = new HashMap<>();\n    event.put(\"dimA\", \"blerg\");\n    event.put(\"metA\", 130);\n    row = new MapBasedInputRow(1000, dimNames, event);\n    indexA.add(row);\n\n\n    final File fileA = INDEX_MERGER_V9.persist(\n        indexA,\n        new File(tmpDir, \"A\"),\n        new IndexSpec(),\n        OffHeapMemorySegmentWriteOutMediumFactory.instance()\n    );\n    QueryableIndex qindexA = INDEX_IO.loadIndex(fileA);\n\n\n    final IncrementalIndex indexB = makeIncIndex(false);\n    incrementalIndices.add(indexB);\n\n    event = new HashMap<>();\n    event.put(\"dimA\", \"foo\");\n    event.put(\"metA\", 200);\n    row = new MapBasedInputRow(1000, dimNames, event);\n    indexB.add(row);\n\n    event = new HashMap<>();\n    event.put(\"dimA\", \"world\");\n    event.put(\"metA\", 75);\n    row = new MapBasedInputRow(1000, dimNames, event);\n    indexB.add(row);\n\n    event = new HashMap<>();\n    event.put(\"dimA\", \"mango\");\n    event.put(\"metA\", 95);\n    row = new MapBasedInputRow(1000, dimNames, event);\n    indexB.add(row);\n\n    event = new HashMap<>();\n    event.put(\"dimA\", \"zebra\");\n    event.put(\"metA\", 180);\n    row = new MapBasedInputRow(1000, dimNames, event);\n    indexB.add(row);\n\n    event = new HashMap<>();\n    event.put(\"dimA\", \"blorg\");\n    event.put(\"metA\", 120);\n    row = new MapBasedInputRow(1000, dimNames, event);\n    indexB.add(row);\n\n    final File fileB = INDEX_MERGER_V9.persist(\n        indexB,\n        new File(tmpDir, \"B\"),\n        new IndexSpec(),\n        OffHeapMemorySegmentWriteOutMediumFactory.instance()\n    );\n    QueryableIndex qindexB = INDEX_IO.loadIndex(fileB);\n\n    groupByIndices = Arrays.asList(qindexA, qindexB);\n    resourceCloser = Closer.create();\n    setupGroupByFactory();\n  }\n","date":"2019-11-23 11:48:49","endLine":265,"groupId":"15521","id":3,"instanceNumber":1,"isCurCommit":0,"methodName":"setup","params":"()","path":"/mnt/clonedata/CloneManagementServer/ManagementServer/consistResult/result-druid-10-0.7/blobInfo/CC_OUT/blobs/68/94f71ad55cb94590b863b0a45dc69dc5333aaf.src","preCode":"  public void setup() throws Exception\n  {\n    tmpDir = Files.createTempDir();\n\n    InputRow row;\n    List<String> dimNames = Arrays.asList(\"dimA\", \"metA\");\n    Map<String, Object> event;\n\n    final IncrementalIndex indexA = makeIncIndex(false);\n    incrementalIndices.add(indexA);\n\n    event = new HashMap<>();\n    event.put(\"dimA\", \"hello\");\n    event.put(\"metA\", 100);\n    row = new MapBasedInputRow(1000, dimNames, event);\n    indexA.add(row);\n\n    event = new HashMap<>();\n    event.put(\"dimA\", \"mango\");\n    event.put(\"metA\", 95);\n    row = new MapBasedInputRow(1000, dimNames, event);\n    indexA.add(row);\n\n    event = new HashMap<>();\n    event.put(\"dimA\", \"world\");\n    event.put(\"metA\", 75);\n    row = new MapBasedInputRow(1000, dimNames, event);\n    indexA.add(row);\n\n    event = new HashMap<>();\n    event.put(\"dimA\", \"fubaz\");\n    event.put(\"metA\", 75);\n    row = new MapBasedInputRow(1000, dimNames, event);\n    indexA.add(row);\n\n    event = new HashMap<>();\n    event.put(\"dimA\", \"zortaxx\");\n    event.put(\"metA\", 999);\n    row = new MapBasedInputRow(1000, dimNames, event);\n    indexA.add(row);\n\n    event = new HashMap<>();\n    event.put(\"dimA\", \"blarg\");\n    event.put(\"metA\", 125);\n    row = new MapBasedInputRow(1000, dimNames, event);\n    indexA.add(row);\n\n    event = new HashMap<>();\n    event.put(\"dimA\", \"blerg\");\n    event.put(\"metA\", 130);\n    row = new MapBasedInputRow(1000, dimNames, event);\n    indexA.add(row);\n\n\n    final File fileA = INDEX_MERGER_V9.persist(\n        indexA,\n        new File(tmpDir, \"A\"),\n        new IndexSpec(),\n        OffHeapMemorySegmentWriteOutMediumFactory.instance()\n    );\n    QueryableIndex qindexA = INDEX_IO.loadIndex(fileA);\n\n\n    final IncrementalIndex indexB = makeIncIndex(false);\n    incrementalIndices.add(indexB);\n\n    event = new HashMap<>();\n    event.put(\"dimA\", \"foo\");\n    event.put(\"metA\", 200);\n    row = new MapBasedInputRow(1000, dimNames, event);\n    indexB.add(row);\n\n    event = new HashMap<>();\n    event.put(\"dimA\", \"world\");\n    event.put(\"metA\", 75);\n    row = new MapBasedInputRow(1000, dimNames, event);\n    indexB.add(row);\n\n    event = new HashMap<>();\n    event.put(\"dimA\", \"mango\");\n    event.put(\"metA\", 95);\n    row = new MapBasedInputRow(1000, dimNames, event);\n    indexB.add(row);\n\n    event = new HashMap<>();\n    event.put(\"dimA\", \"zebra\");\n    event.put(\"metA\", 180);\n    row = new MapBasedInputRow(1000, dimNames, event);\n    indexB.add(row);\n\n    event = new HashMap<>();\n    event.put(\"dimA\", \"blorg\");\n    event.put(\"metA\", 120);\n    row = new MapBasedInputRow(1000, dimNames, event);\n    indexB.add(row);\n\n    final File fileB = INDEX_MERGER_V9.persist(\n        indexB,\n        new File(tmpDir, \"B\"),\n        new IndexSpec(),\n        OffHeapMemorySegmentWriteOutMediumFactory.instance()\n    );\n    QueryableIndex qindexB = INDEX_IO.loadIndex(fileB);\n\n    groupByIndices = Arrays.asList(qindexA, qindexB);\n    resourceCloser = Closer.create();\n    setupGroupByFactory();\n  }\n","realPath":"processing/src/test/java/org/apache/druid/query/groupby/GroupByLimitPushDownInsufficientBufferTest.java","repoName":"druid","snippetEndLine":0,"snippetStartLine":0,"startLine":158,"status":"M"},{"authorDate":"2019-11-23 11:48:49","commitOrder":2,"curCode":"  public void setup() throws Exception\n  {\n    tmpDir = FileUtils.createTempDir();\n\n    InputRow row;\n    List<String> dimNames = Arrays.asList(\"dimA\", \"metA\");\n    Map<String, Object> event;\n\n    final IncrementalIndex indexA = makeIncIndex(false);\n    incrementalIndices.add(indexA);\n\n    event = new HashMap<>();\n    event.put(\"dimA\", \"pomegranate\");\n    event.put(\"metA\", 2395L);\n    row = new MapBasedInputRow(1505260888888L, dimNames, event);\n    indexA.add(row);\n\n    event = new HashMap<>();\n    event.put(\"dimA\", \"mango\");\n    event.put(\"metA\", 8L);\n    row = new MapBasedInputRow(1505260800000L, dimNames, event);\n    indexA.add(row);\n\n    event = new HashMap<>();\n    event.put(\"dimA\", \"pomegranate\");\n    event.put(\"metA\", 5028L);\n    row = new MapBasedInputRow(1505264400000L, dimNames, event);\n    indexA.add(row);\n\n    event = new HashMap<>();\n    event.put(\"dimA\", \"mango\");\n    event.put(\"metA\", 7L);\n    row = new MapBasedInputRow(1505264400400L, dimNames, event);\n    indexA.add(row);\n\n    final File fileA = INDEX_MERGER_V9.persist(\n        indexA,\n        new File(tmpDir, \"A\"),\n        new IndexSpec(),\n        null\n    );\n    QueryableIndex qindexA = INDEX_IO.loadIndex(fileA);\n\n\n    final IncrementalIndex indexB = makeIncIndex(false);\n    incrementalIndices.add(indexB);\n\n    event = new HashMap<>();\n    event.put(\"dimA\", \"pomegranate\");\n    event.put(\"metA\", 4718L);\n    row = new MapBasedInputRow(1505260800000L, dimNames, event);\n    indexB.add(row);\n\n    event = new HashMap<>();\n    event.put(\"dimA\", \"mango\");\n    event.put(\"metA\", 18L);\n    row = new MapBasedInputRow(1505260800000L, dimNames, event);\n    indexB.add(row);\n\n    event = new HashMap<>();\n    event.put(\"dimA\", \"pomegranate\");\n    event.put(\"metA\", 2698L);\n    row = new MapBasedInputRow(1505264400000L, dimNames, event);\n    indexB.add(row);\n\n    event = new HashMap<>();\n    event.put(\"dimA\", \"mango\");\n    event.put(\"metA\", 3L);\n    row = new MapBasedInputRow(1505264400000L, dimNames, event);\n    indexB.add(row);\n\n    final File fileB = INDEX_MERGER_V9.persist(\n        indexB,\n        new File(tmpDir, \"B\"),\n        new IndexSpec(),\n        null\n    );\n    QueryableIndex qindexB = INDEX_IO.loadIndex(fileB);\n\n    final IncrementalIndex indexC = makeIncIndex(false);\n    incrementalIndices.add(indexC);\n\n    event = new HashMap<>();\n    event.put(\"dimA\", \"pomegranate\");\n    event.put(\"metA\", 2395L);\n    row = new MapBasedInputRow(1505260800000L, dimNames, event);\n    indexC.add(row);\n\n    event = new HashMap<>();\n    event.put(\"dimA\", \"mango\");\n    event.put(\"metA\", 8L);\n    row = new MapBasedInputRow(1605260800000L, dimNames, event);\n    indexC.add(row);\n\n    event = new HashMap<>();\n    event.put(\"dimA\", \"pomegranate\");\n    event.put(\"metA\", 5028L);\n    row = new MapBasedInputRow(1705264400000L, dimNames, event);\n    indexC.add(row);\n\n    event = new HashMap<>();\n    event.put(\"dimA\", \"mango\");\n    event.put(\"metA\", 7L);\n    row = new MapBasedInputRow(1805264400000L, dimNames, event);\n    indexC.add(row);\n\n    final File fileC = INDEX_MERGER_V9.persist(\n        indexC,\n        new File(tmpDir, \"C\"),\n        new IndexSpec(),\n        null\n    );\n    QueryableIndex qindexC = INDEX_IO.loadIndex(fileC);\n\n\n    final IncrementalIndex indexD = makeIncIndex(false);\n    incrementalIndices.add(indexD);\n\n    event = new HashMap<>();\n    event.put(\"dimA\", \"pomegranate\");\n    event.put(\"metA\", 4718L);\n    row = new MapBasedInputRow(1505260800000L, dimNames, event);\n    indexD.add(row);\n\n    event = new HashMap<>();\n    event.put(\"dimA\", \"mango\");\n    event.put(\"metA\", 18L);\n    row = new MapBasedInputRow(1605260800000L, dimNames, event);\n    indexD.add(row);\n\n    event = new HashMap<>();\n    event.put(\"dimA\", \"pomegranate\");\n    event.put(\"metA\", 2698L);\n    row = new MapBasedInputRow(1705264400000L, dimNames, event);\n    indexD.add(row);\n\n    event = new HashMap<>();\n    event.put(\"dimA\", \"mango\");\n    event.put(\"metA\", 3L);\n    row = new MapBasedInputRow(1805264400000L, dimNames, event);\n    indexD.add(row);\n\n    final File fileD = INDEX_MERGER_V9.persist(\n        indexD,\n        new File(tmpDir, \"D\"),\n        new IndexSpec(),\n        null\n    );\n    QueryableIndex qindexD = INDEX_IO.loadIndex(fileD);\n\n    groupByIndices = Arrays.asList(qindexA, qindexB, qindexC, qindexD);\n    resourceCloser = Closer.create();\n    setupGroupByFactory();\n  }\n","date":"2019-11-23 11:48:49","endLine":320,"groupId":"14771","id":4,"instanceNumber":2,"isCurCommit":0,"methodName":"setup","params":"()","path":"/mnt/clonedata/CloneManagementServer/ManagementServer/consistResult/result-druid-10-0.7/blobInfo/CC_OUT/blobs/40/44e2ff9d76ddcbf25a40020b7802a875f396b2.src","preCode":"  public void setup() throws Exception\n  {\n    tmpDir = Files.createTempDir();\n\n    InputRow row;\n    List<String> dimNames = Arrays.asList(\"dimA\", \"metA\");\n    Map<String, Object> event;\n\n    final IncrementalIndex indexA = makeIncIndex(false);\n    incrementalIndices.add(indexA);\n\n    event = new HashMap<>();\n    event.put(\"dimA\", \"pomegranate\");\n    event.put(\"metA\", 2395L);\n    row = new MapBasedInputRow(1505260888888L, dimNames, event);\n    indexA.add(row);\n\n    event = new HashMap<>();\n    event.put(\"dimA\", \"mango\");\n    event.put(\"metA\", 8L);\n    row = new MapBasedInputRow(1505260800000L, dimNames, event);\n    indexA.add(row);\n\n    event = new HashMap<>();\n    event.put(\"dimA\", \"pomegranate\");\n    event.put(\"metA\", 5028L);\n    row = new MapBasedInputRow(1505264400000L, dimNames, event);\n    indexA.add(row);\n\n    event = new HashMap<>();\n    event.put(\"dimA\", \"mango\");\n    event.put(\"metA\", 7L);\n    row = new MapBasedInputRow(1505264400400L, dimNames, event);\n    indexA.add(row);\n\n    final File fileA = INDEX_MERGER_V9.persist(\n        indexA,\n        new File(tmpDir, \"A\"),\n        new IndexSpec(),\n        null\n    );\n    QueryableIndex qindexA = INDEX_IO.loadIndex(fileA);\n\n\n    final IncrementalIndex indexB = makeIncIndex(false);\n    incrementalIndices.add(indexB);\n\n    event = new HashMap<>();\n    event.put(\"dimA\", \"pomegranate\");\n    event.put(\"metA\", 4718L);\n    row = new MapBasedInputRow(1505260800000L, dimNames, event);\n    indexB.add(row);\n\n    event = new HashMap<>();\n    event.put(\"dimA\", \"mango\");\n    event.put(\"metA\", 18L);\n    row = new MapBasedInputRow(1505260800000L, dimNames, event);\n    indexB.add(row);\n\n    event = new HashMap<>();\n    event.put(\"dimA\", \"pomegranate\");\n    event.put(\"metA\", 2698L);\n    row = new MapBasedInputRow(1505264400000L, dimNames, event);\n    indexB.add(row);\n\n    event = new HashMap<>();\n    event.put(\"dimA\", \"mango\");\n    event.put(\"metA\", 3L);\n    row = new MapBasedInputRow(1505264400000L, dimNames, event);\n    indexB.add(row);\n\n    final File fileB = INDEX_MERGER_V9.persist(\n        indexB,\n        new File(tmpDir, \"B\"),\n        new IndexSpec(),\n        null\n    );\n    QueryableIndex qindexB = INDEX_IO.loadIndex(fileB);\n\n    final IncrementalIndex indexC = makeIncIndex(false);\n    incrementalIndices.add(indexC);\n\n    event = new HashMap<>();\n    event.put(\"dimA\", \"pomegranate\");\n    event.put(\"metA\", 2395L);\n    row = new MapBasedInputRow(1505260800000L, dimNames, event);\n    indexC.add(row);\n\n    event = new HashMap<>();\n    event.put(\"dimA\", \"mango\");\n    event.put(\"metA\", 8L);\n    row = new MapBasedInputRow(1605260800000L, dimNames, event);\n    indexC.add(row);\n\n    event = new HashMap<>();\n    event.put(\"dimA\", \"pomegranate\");\n    event.put(\"metA\", 5028L);\n    row = new MapBasedInputRow(1705264400000L, dimNames, event);\n    indexC.add(row);\n\n    event = new HashMap<>();\n    event.put(\"dimA\", \"mango\");\n    event.put(\"metA\", 7L);\n    row = new MapBasedInputRow(1805264400000L, dimNames, event);\n    indexC.add(row);\n\n    final File fileC = INDEX_MERGER_V9.persist(\n        indexC,\n        new File(tmpDir, \"C\"),\n        new IndexSpec(),\n        null\n    );\n    QueryableIndex qindexC = INDEX_IO.loadIndex(fileC);\n\n\n    final IncrementalIndex indexD = makeIncIndex(false);\n    incrementalIndices.add(indexD);\n\n    event = new HashMap<>();\n    event.put(\"dimA\", \"pomegranate\");\n    event.put(\"metA\", 4718L);\n    row = new MapBasedInputRow(1505260800000L, dimNames, event);\n    indexD.add(row);\n\n    event = new HashMap<>();\n    event.put(\"dimA\", \"mango\");\n    event.put(\"metA\", 18L);\n    row = new MapBasedInputRow(1605260800000L, dimNames, event);\n    indexD.add(row);\n\n    event = new HashMap<>();\n    event.put(\"dimA\", \"pomegranate\");\n    event.put(\"metA\", 2698L);\n    row = new MapBasedInputRow(1705264400000L, dimNames, event);\n    indexD.add(row);\n\n    event = new HashMap<>();\n    event.put(\"dimA\", \"mango\");\n    event.put(\"metA\", 3L);\n    row = new MapBasedInputRow(1805264400000L, dimNames, event);\n    indexD.add(row);\n\n    final File fileD = INDEX_MERGER_V9.persist(\n        indexD,\n        new File(tmpDir, \"D\"),\n        new IndexSpec(),\n        null\n    );\n    QueryableIndex qindexD = INDEX_IO.loadIndex(fileD);\n\n    groupByIndices = Arrays.asList(qindexA, qindexB, qindexC, qindexD);\n    resourceCloser = Closer.create();\n    setupGroupByFactory();\n  }\n","realPath":"processing/src/test/java/org/apache/druid/query/groupby/GroupByLimitPushDownMultiNodeMergeTest.java","repoName":"druid","snippetEndLine":0,"snippetStartLine":0,"startLine":167,"status":"M"}],"commitId":"e0eb85ace72322c80a7f2f0555d411f2067f83ea","commitMessage":"@@@Add FileUtils.createTempDir() and enforce its usage. (#8932)\n\n* Add FileUtils.createTempDir() and enforce its usage.\n\nThe purpose of this is to improve error messages. Previously.  the error\nmessage on a nonexistent or unwritable temp directory would be\n\"Failed to create directory within 10. 000 attempts\".\n\n* Further updates.\n\n* Another update.\n\n* Remove commons-io from benchmark.\n\n* Fix tests.\n","date":"2019-11-23 11:48:49","modifiedFileCount":"71","status":"M","submitter":"Gian Merlino"},{"authorTime":"2021-09-02 12:19:38","codes":[{"authorDate":"2019-11-23 11:48:49","commitOrder":3,"curCode":"  public void setup() throws Exception\n  {\n    tmpDir = FileUtils.createTempDir();\n\n    InputRow row;\n    List<String> dimNames = Arrays.asList(\"dimA\", \"metA\");\n    Map<String, Object> event;\n\n    final IncrementalIndex indexA = makeIncIndex(false);\n    incrementalIndices.add(indexA);\n\n    event = new HashMap<>();\n    event.put(\"dimA\", \"hello\");\n    event.put(\"metA\", 100);\n    row = new MapBasedInputRow(1000, dimNames, event);\n    indexA.add(row);\n\n    event = new HashMap<>();\n    event.put(\"dimA\", \"mango\");\n    event.put(\"metA\", 95);\n    row = new MapBasedInputRow(1000, dimNames, event);\n    indexA.add(row);\n\n    event = new HashMap<>();\n    event.put(\"dimA\", \"world\");\n    event.put(\"metA\", 75);\n    row = new MapBasedInputRow(1000, dimNames, event);\n    indexA.add(row);\n\n    event = new HashMap<>();\n    event.put(\"dimA\", \"fubaz\");\n    event.put(\"metA\", 75);\n    row = new MapBasedInputRow(1000, dimNames, event);\n    indexA.add(row);\n\n    event = new HashMap<>();\n    event.put(\"dimA\", \"zortaxx\");\n    event.put(\"metA\", 999);\n    row = new MapBasedInputRow(1000, dimNames, event);\n    indexA.add(row);\n\n    event = new HashMap<>();\n    event.put(\"dimA\", \"blarg\");\n    event.put(\"metA\", 125);\n    row = new MapBasedInputRow(1000, dimNames, event);\n    indexA.add(row);\n\n    event = new HashMap<>();\n    event.put(\"dimA\", \"blerg\");\n    event.put(\"metA\", 130);\n    row = new MapBasedInputRow(1000, dimNames, event);\n    indexA.add(row);\n\n\n    final File fileA = INDEX_MERGER_V9.persist(\n        indexA,\n        new File(tmpDir, \"A\"),\n        new IndexSpec(),\n        OffHeapMemorySegmentWriteOutMediumFactory.instance()\n    );\n    QueryableIndex qindexA = INDEX_IO.loadIndex(fileA);\n\n\n    final IncrementalIndex indexB = makeIncIndex(false);\n    incrementalIndices.add(indexB);\n\n    event = new HashMap<>();\n    event.put(\"dimA\", \"foo\");\n    event.put(\"metA\", 200);\n    row = new MapBasedInputRow(1000, dimNames, event);\n    indexB.add(row);\n\n    event = new HashMap<>();\n    event.put(\"dimA\", \"world\");\n    event.put(\"metA\", 75);\n    row = new MapBasedInputRow(1000, dimNames, event);\n    indexB.add(row);\n\n    event = new HashMap<>();\n    event.put(\"dimA\", \"mango\");\n    event.put(\"metA\", 95);\n    row = new MapBasedInputRow(1000, dimNames, event);\n    indexB.add(row);\n\n    event = new HashMap<>();\n    event.put(\"dimA\", \"zebra\");\n    event.put(\"metA\", 180);\n    row = new MapBasedInputRow(1000, dimNames, event);\n    indexB.add(row);\n\n    event = new HashMap<>();\n    event.put(\"dimA\", \"blorg\");\n    event.put(\"metA\", 120);\n    row = new MapBasedInputRow(1000, dimNames, event);\n    indexB.add(row);\n\n    final File fileB = INDEX_MERGER_V9.persist(\n        indexB,\n        new File(tmpDir, \"B\"),\n        new IndexSpec(),\n        OffHeapMemorySegmentWriteOutMediumFactory.instance()\n    );\n    QueryableIndex qindexB = INDEX_IO.loadIndex(fileB);\n\n    groupByIndices = Arrays.asList(qindexA, qindexB);\n    resourceCloser = Closer.create();\n    setupGroupByFactory();\n  }\n","date":"2019-11-23 11:48:49","endLine":265,"groupId":"106577","id":5,"instanceNumber":1,"isCurCommit":0,"methodName":"setup","params":"()","path":"/mnt/clonedata/CloneManagementServer/ManagementServer/consistResult/result-druid-10-0.7/blobInfo/CC_OUT/blobs/68/94f71ad55cb94590b863b0a45dc69dc5333aaf.src","preCode":"  public void setup() throws Exception\n  {\n    tmpDir = FileUtils.createTempDir();\n\n    InputRow row;\n    List<String> dimNames = Arrays.asList(\"dimA\", \"metA\");\n    Map<String, Object> event;\n\n    final IncrementalIndex indexA = makeIncIndex(false);\n    incrementalIndices.add(indexA);\n\n    event = new HashMap<>();\n    event.put(\"dimA\", \"hello\");\n    event.put(\"metA\", 100);\n    row = new MapBasedInputRow(1000, dimNames, event);\n    indexA.add(row);\n\n    event = new HashMap<>();\n    event.put(\"dimA\", \"mango\");\n    event.put(\"metA\", 95);\n    row = new MapBasedInputRow(1000, dimNames, event);\n    indexA.add(row);\n\n    event = new HashMap<>();\n    event.put(\"dimA\", \"world\");\n    event.put(\"metA\", 75);\n    row = new MapBasedInputRow(1000, dimNames, event);\n    indexA.add(row);\n\n    event = new HashMap<>();\n    event.put(\"dimA\", \"fubaz\");\n    event.put(\"metA\", 75);\n    row = new MapBasedInputRow(1000, dimNames, event);\n    indexA.add(row);\n\n    event = new HashMap<>();\n    event.put(\"dimA\", \"zortaxx\");\n    event.put(\"metA\", 999);\n    row = new MapBasedInputRow(1000, dimNames, event);\n    indexA.add(row);\n\n    event = new HashMap<>();\n    event.put(\"dimA\", \"blarg\");\n    event.put(\"metA\", 125);\n    row = new MapBasedInputRow(1000, dimNames, event);\n    indexA.add(row);\n\n    event = new HashMap<>();\n    event.put(\"dimA\", \"blerg\");\n    event.put(\"metA\", 130);\n    row = new MapBasedInputRow(1000, dimNames, event);\n    indexA.add(row);\n\n\n    final File fileA = INDEX_MERGER_V9.persist(\n        indexA,\n        new File(tmpDir, \"A\"),\n        new IndexSpec(),\n        OffHeapMemorySegmentWriteOutMediumFactory.instance()\n    );\n    QueryableIndex qindexA = INDEX_IO.loadIndex(fileA);\n\n\n    final IncrementalIndex indexB = makeIncIndex(false);\n    incrementalIndices.add(indexB);\n\n    event = new HashMap<>();\n    event.put(\"dimA\", \"foo\");\n    event.put(\"metA\", 200);\n    row = new MapBasedInputRow(1000, dimNames, event);\n    indexB.add(row);\n\n    event = new HashMap<>();\n    event.put(\"dimA\", \"world\");\n    event.put(\"metA\", 75);\n    row = new MapBasedInputRow(1000, dimNames, event);\n    indexB.add(row);\n\n    event = new HashMap<>();\n    event.put(\"dimA\", \"mango\");\n    event.put(\"metA\", 95);\n    row = new MapBasedInputRow(1000, dimNames, event);\n    indexB.add(row);\n\n    event = new HashMap<>();\n    event.put(\"dimA\", \"zebra\");\n    event.put(\"metA\", 180);\n    row = new MapBasedInputRow(1000, dimNames, event);\n    indexB.add(row);\n\n    event = new HashMap<>();\n    event.put(\"dimA\", \"blorg\");\n    event.put(\"metA\", 120);\n    row = new MapBasedInputRow(1000, dimNames, event);\n    indexB.add(row);\n\n    final File fileB = INDEX_MERGER_V9.persist(\n        indexB,\n        new File(tmpDir, \"B\"),\n        new IndexSpec(),\n        OffHeapMemorySegmentWriteOutMediumFactory.instance()\n    );\n    QueryableIndex qindexB = INDEX_IO.loadIndex(fileB);\n\n    groupByIndices = Arrays.asList(qindexA, qindexB);\n    resourceCloser = Closer.create();\n    setupGroupByFactory();\n  }\n","realPath":"processing/src/test/java/org/apache/druid/query/groupby/GroupByLimitPushDownInsufficientBufferTest.java","repoName":"druid","snippetEndLine":0,"snippetStartLine":0,"startLine":158,"status":"N"},{"authorDate":"2021-09-02 12:19:38","commitOrder":3,"curCode":"  public void setup() throws Exception\n  {\n    tmpDir = FileUtils.createTempDir();\n\n    InputRow row;\n    List<String> dimNames = Arrays.asList(\"dimA\", \"metA\");\n    Map<String, Object> event;\n\n    final IncrementalIndex indexA = makeIncIndex(false);\n    incrementalIndices.add(indexA);\n\n    event = new HashMap<>();\n    event.put(\"dimA\", \"pomegranate\");\n    event.put(\"metA\", 2395L);\n    row = new MapBasedInputRow(1505260888888L, dimNames, event);\n    indexA.add(row);\n\n    event = new HashMap<>();\n    event.put(\"dimA\", \"mango\");\n    event.put(\"metA\", 8L);\n    row = new MapBasedInputRow(1505260800000L, dimNames, event);\n    indexA.add(row);\n\n    event = new HashMap<>();\n    event.put(\"dimA\", \"pomegranate\");\n    event.put(\"metA\", 5028L);\n    row = new MapBasedInputRow(1505264400000L, dimNames, event);\n    indexA.add(row);\n\n    event = new HashMap<>();\n    event.put(\"dimA\", \"mango\");\n    event.put(\"metA\", 7L);\n    row = new MapBasedInputRow(1505264400400L, dimNames, event);\n    indexA.add(row);\n\n    final File fileA = INDEX_MERGER_V9.persist(\n        indexA,\n        new File(tmpDir, \"A\"),\n        new IndexSpec(),\n        null\n    );\n    QueryableIndex qindexA = INDEX_IO.loadIndex(fileA);\n\n\n    final IncrementalIndex indexB = makeIncIndex(false);\n    incrementalIndices.add(indexB);\n\n    event = new HashMap<>();\n    event.put(\"dimA\", \"pomegranate\");\n    event.put(\"metA\", 4718L);\n    row = new MapBasedInputRow(1505260800000L, dimNames, event);\n    indexB.add(row);\n\n    event = new HashMap<>();\n    event.put(\"dimA\", \"mango\");\n    event.put(\"metA\", 18L);\n    row = new MapBasedInputRow(1505260800000L, dimNames, event);\n    indexB.add(row);\n\n    event = new HashMap<>();\n    event.put(\"dimA\", \"pomegranate\");\n    event.put(\"metA\", 2698L);\n    row = new MapBasedInputRow(1505264400000L, dimNames, event);\n    indexB.add(row);\n\n    event = new HashMap<>();\n    event.put(\"dimA\", \"mango\");\n    event.put(\"metA\", 3L);\n    row = new MapBasedInputRow(1505264400000L, dimNames, event);\n    indexB.add(row);\n\n    final File fileB = INDEX_MERGER_V9.persist(\n        indexB,\n        new File(tmpDir, \"B\"),\n        new IndexSpec(),\n        null\n    );\n    QueryableIndex qindexB = INDEX_IO.loadIndex(fileB);\n\n    final IncrementalIndex indexC = makeIncIndex(false);\n    incrementalIndices.add(indexC);\n\n    event = new HashMap<>();\n    event.put(\"dimA\", \"pomegranate\");\n    event.put(\"metA\", 2395L);\n    row = new MapBasedInputRow(1505260800000L, dimNames, event);\n    indexC.add(row);\n\n    event = new HashMap<>();\n    event.put(\"dimA\", \"mango\");\n    event.put(\"metA\", 8L);\n    row = new MapBasedInputRow(1605260800000L, dimNames, event);\n    indexC.add(row);\n\n    event = new HashMap<>();\n    event.put(\"dimA\", \"pomegranate\");\n    event.put(\"metA\", 5028L);\n    row = new MapBasedInputRow(1705264400000L, dimNames, event);\n    indexC.add(row);\n\n    event = new HashMap<>();\n    event.put(\"dimA\", \"mango\");\n    event.put(\"metA\", 7L);\n    row = new MapBasedInputRow(1805264400000L, dimNames, event);\n    indexC.add(row);\n\n    final File fileC = INDEX_MERGER_V9.persist(\n        indexC,\n        new File(tmpDir, \"C\"),\n        new IndexSpec(),\n        null\n    );\n    QueryableIndex qindexC = INDEX_IO.loadIndex(fileC);\n\n\n    final IncrementalIndex indexD = makeIncIndex(false);\n    incrementalIndices.add(indexD);\n\n    event = new HashMap<>();\n    event.put(\"dimA\", \"pomegranate\");\n    event.put(\"metA\", 4718L);\n    row = new MapBasedInputRow(1505260800000L, dimNames, event);\n    indexD.add(row);\n\n    event = new HashMap<>();\n    event.put(\"dimA\", \"mango\");\n    event.put(\"metA\", 18L);\n    row = new MapBasedInputRow(1605260800000L, dimNames, event);\n    indexD.add(row);\n\n    event = new HashMap<>();\n    event.put(\"dimA\", \"pomegranate\");\n    event.put(\"metA\", 2698L);\n    row = new MapBasedInputRow(1705264400000L, dimNames, event);\n    indexD.add(row);\n\n    event = new HashMap<>();\n    event.put(\"dimA\", \"mango\");\n    event.put(\"metA\", 3L);\n    row = new MapBasedInputRow(1805264400000L, dimNames, event);\n    indexD.add(row);\n\n    final File fileD = INDEX_MERGER_V9.persist(\n        indexD,\n        new File(tmpDir, \"D\"),\n        new IndexSpec(),\n        null\n    );\n    QueryableIndex qindexD = INDEX_IO.loadIndex(fileD);\n\n    List<String> dimNames2 = Arrays.asList(\"dimA\", \"dimB\", \"metA\");\n    List<DimensionSchema> dimensions = Arrays.asList(\n        new StringDimensionSchema(\"dimA\"),\n        new StringDimensionSchema(\"dimB\"),\n        new LongDimensionSchema(\"metA\")\n    );\n    final IncrementalIndex indexE = makeIncIndex(false, dimensions);\n    incrementalIndices.add(indexE);\n\n    event = new HashMap<>();\n    event.put(\"dimA\", \"pomegranate\");\n    event.put(\"dimB\", \"raw\");\n    event.put(\"metA\", 5L);\n    row = new MapBasedInputRow(1505260800000L, dimNames2, event);\n    indexE.add(row);\n\n    event = new HashMap<>();\n    event.put(\"dimA\", \"mango\");\n    event.put(\"dimB\", \"ripe\");\n    event.put(\"metA\", 9L);\n    row = new MapBasedInputRow(1605260800000L, dimNames2, event);\n    indexE.add(row);\n\n    event = new HashMap<>();\n    event.put(\"dimA\", \"pomegranate\");\n    event.put(\"dimB\", \"raw\");\n    event.put(\"metA\", 3L);\n    row = new MapBasedInputRow(1705264400000L, dimNames2, event);\n    indexE.add(row);\n\n    event = new HashMap<>();\n    event.put(\"dimA\", \"mango\");\n    event.put(\"dimB\", \"ripe\");\n    event.put(\"metA\", 7L);\n    row = new MapBasedInputRow(1805264400000L, dimNames2, event);\n    indexE.add(row);\n\n    event = new HashMap<>();\n    event.put(\"dimA\", \"grape\");\n    event.put(\"dimB\", \"raw\");\n    event.put(\"metA\", 5L);\n    row = new MapBasedInputRow(1805264400000L, dimNames2, event);\n    indexE.add(row);\n\n    event = new HashMap<>();\n    event.put(\"dimA\", \"apple\");\n    event.put(\"dimB\", \"ripe\");\n    event.put(\"metA\", 3L);\n    row = new MapBasedInputRow(1805264400000L, dimNames2, event);\n    indexE.add(row);\n\n    event = new HashMap<>();\n    event.put(\"dimA\", \"apple\");\n    event.put(\"dimB\", \"raw\");\n    event.put(\"metA\", 1L);\n    row = new MapBasedInputRow(1805264400000L, dimNames2, event);\n    indexE.add(row);\n\n    event = new HashMap<>();\n    event.put(\"dimA\", \"apple\");\n    event.put(\"dimB\", \"ripe\");\n    event.put(\"metA\", 4L);\n    row = new MapBasedInputRow(1805264400000L, dimNames2, event);\n    indexE.add(row);\n\n    event = new HashMap<>();\n    event.put(\"dimA\", \"apple\");\n    event.put(\"dimB\", \"raw\");\n    event.put(\"metA\", 1L);\n    row = new MapBasedInputRow(1805264400000L, dimNames2, event);\n    indexE.add(row);\n\n    event = new HashMap<>();\n    event.put(\"dimA\", \"banana\");\n    event.put(\"dimB\", \"ripe\");\n    event.put(\"metA\", 4L);\n    row = new MapBasedInputRow(1805264400000L, dimNames2, event);\n    indexE.add(row);\n\n    event = new HashMap<>();\n    event.put(\"dimA\", \"orange\");\n    event.put(\"dimB\", \"raw\");\n    event.put(\"metA\", 9L);\n    row = new MapBasedInputRow(1805264400000L, dimNames2, event);\n    indexE.add(row);\n\n    event = new HashMap<>();\n    event.put(\"dimA\", \"peach\");\n    event.put(\"dimB\", \"ripe\");\n    event.put(\"metA\", 7L);\n    row = new MapBasedInputRow(1805264400000L, dimNames2, event);\n    indexE.add(row);\n\n    event = new HashMap<>();\n    event.put(\"dimA\", \"orange\");\n    event.put(\"dimB\", \"raw\");\n    event.put(\"metA\", 2L);\n    row = new MapBasedInputRow(1805264400000L, dimNames2, event);\n    indexE.add(row);\n\n    event = new HashMap<>();\n    event.put(\"dimA\", \"strawberry\");\n    event.put(\"dimB\", \"ripe\");\n    event.put(\"metA\", 10L);\n    row = new MapBasedInputRow(1805264400000L, dimNames2, event);\n    indexE.add(row);\n\n    final File fileE = INDEX_MERGER_V9.persist(\n        indexE,\n        new File(tmpDir, \"E\"),\n        new IndexSpec(),\n        null\n    );\n    QueryableIndex qindexE = INDEX_IO.loadIndex(fileE);\n\n    final IncrementalIndex indexF = makeIncIndex(false, dimensions);\n    incrementalIndices.add(indexF);\n\n    event = new HashMap<>();\n    event.put(\"dimA\", \"kiwi\");\n    event.put(\"dimB\", \"raw\");\n    event.put(\"metA\", 7L);\n    row = new MapBasedInputRow(1505260800000L, dimNames2, event);\n    indexF.add(row);\n\n    event = new HashMap<>();\n    event.put(\"dimA\", \"watermelon\");\n    event.put(\"dimB\", \"ripe\");\n    event.put(\"metA\", 14L);\n    row = new MapBasedInputRow(1605260800000L, dimNames2, event);\n    indexF.add(row);\n\n    event = new HashMap<>();\n    event.put(\"dimA\", \"kiwi\");\n    event.put(\"dimB\", \"raw\");\n    event.put(\"metA\", 8L);\n    row = new MapBasedInputRow(1705264400000L, dimNames2, event);\n    indexF.add(row);\n\n    event = new HashMap<>();\n    event.put(\"dimA\", \"kiwi\");\n    event.put(\"dimB\", \"ripe\");\n    event.put(\"metA\", 8L);\n    row = new MapBasedInputRow(1805264400000L, dimNames2, event);\n    indexF.add(row);\n\n    event = new HashMap<>();\n    event.put(\"dimA\", \"lemon\");\n    event.put(\"dimB\", \"raw\");\n    event.put(\"metA\", 3L);\n    row = new MapBasedInputRow(1805264400000L, dimNames2, event);\n    indexF.add(row);\n\n    event = new HashMap<>();\n    event.put(\"dimA\", \"cherry\");\n    event.put(\"dimB\", \"ripe\");\n    event.put(\"metA\", 2L);\n    row = new MapBasedInputRow(1805264400000L, dimNames2, event);\n    indexF.add(row);\n\n    event = new HashMap<>();\n    event.put(\"dimA\", \"cherry\");\n    event.put(\"dimB\", \"raw\");\n    event.put(\"metA\", 7L);\n    row = new MapBasedInputRow(1805264400000L, dimNames2, event);\n    indexF.add(row);\n\n    event = new HashMap<>();\n    event.put(\"dimA\", \"avocado\");\n    event.put(\"dimB\", \"ripe\");\n    event.put(\"metA\", 12L);\n    row = new MapBasedInputRow(1805264400000L, dimNames2, event);\n    indexF.add(row);\n\n    event = new HashMap<>();\n    event.put(\"dimA\", \"cherry\");\n    event.put(\"dimB\", \"raw\");\n    event.put(\"metA\", 3L);\n    row = new MapBasedInputRow(1805264400000L, dimNames2, event);\n    indexF.add(row);\n\n    event = new HashMap<>();\n    event.put(\"dimA\", \"plum\");\n    event.put(\"dimB\", \"ripe\");\n    event.put(\"metA\", 5L);\n    row = new MapBasedInputRow(1805264400000L, dimNames2, event);\n    indexF.add(row);\n\n    event = new HashMap<>();\n    event.put(\"dimA\", \"plum\");\n    event.put(\"dimB\", \"raw\");\n    event.put(\"metA\", 3L);\n    row = new MapBasedInputRow(1805264400000L, dimNames2, event);\n    indexF.add(row);\n\n    event = new HashMap<>();\n    event.put(\"dimA\", \"lime\");\n    event.put(\"dimB\", \"ripe\");\n    event.put(\"metA\", 7L);\n    row = new MapBasedInputRow(1805264400000L, dimNames2, event);\n    indexF.add(row);\n\n    final File fileF = INDEX_MERGER_V9.persist(\n        indexF,\n        new File(tmpDir, \"F\"),\n        new IndexSpec(),\n        null\n    );\n    QueryableIndex qindexF = INDEX_IO.loadIndex(fileF);\n\n    groupByIndices = Arrays.asList(qindexA, qindexB, qindexC, qindexD, qindexE, qindexF);\n    resourceCloser = Closer.create();\n    setupGroupByFactory();\n  }\n","date":"2021-09-02 12:19:38","endLine":536,"groupId":"106577","id":6,"instanceNumber":2,"isCurCommit":1,"methodName":"setup","params":"()","path":"/mnt/clonedata/CloneManagementServer/ManagementServer/consistResult/result-druid-10-0.7/blobInfo/CC_OUT/blobs/7c/f7e65d2bb29f5586caad3f54e2bc27b745cad3.src","preCode":"  public void setup() throws Exception\n  {\n    tmpDir = FileUtils.createTempDir();\n\n    InputRow row;\n    List<String> dimNames = Arrays.asList(\"dimA\", \"metA\");\n    Map<String, Object> event;\n\n    final IncrementalIndex indexA = makeIncIndex(false);\n    incrementalIndices.add(indexA);\n\n    event = new HashMap<>();\n    event.put(\"dimA\", \"pomegranate\");\n    event.put(\"metA\", 2395L);\n    row = new MapBasedInputRow(1505260888888L, dimNames, event);\n    indexA.add(row);\n\n    event = new HashMap<>();\n    event.put(\"dimA\", \"mango\");\n    event.put(\"metA\", 8L);\n    row = new MapBasedInputRow(1505260800000L, dimNames, event);\n    indexA.add(row);\n\n    event = new HashMap<>();\n    event.put(\"dimA\", \"pomegranate\");\n    event.put(\"metA\", 5028L);\n    row = new MapBasedInputRow(1505264400000L, dimNames, event);\n    indexA.add(row);\n\n    event = new HashMap<>();\n    event.put(\"dimA\", \"mango\");\n    event.put(\"metA\", 7L);\n    row = new MapBasedInputRow(1505264400400L, dimNames, event);\n    indexA.add(row);\n\n    final File fileA = INDEX_MERGER_V9.persist(\n        indexA,\n        new File(tmpDir, \"A\"),\n        new IndexSpec(),\n        null\n    );\n    QueryableIndex qindexA = INDEX_IO.loadIndex(fileA);\n\n\n    final IncrementalIndex indexB = makeIncIndex(false);\n    incrementalIndices.add(indexB);\n\n    event = new HashMap<>();\n    event.put(\"dimA\", \"pomegranate\");\n    event.put(\"metA\", 4718L);\n    row = new MapBasedInputRow(1505260800000L, dimNames, event);\n    indexB.add(row);\n\n    event = new HashMap<>();\n    event.put(\"dimA\", \"mango\");\n    event.put(\"metA\", 18L);\n    row = new MapBasedInputRow(1505260800000L, dimNames, event);\n    indexB.add(row);\n\n    event = new HashMap<>();\n    event.put(\"dimA\", \"pomegranate\");\n    event.put(\"metA\", 2698L);\n    row = new MapBasedInputRow(1505264400000L, dimNames, event);\n    indexB.add(row);\n\n    event = new HashMap<>();\n    event.put(\"dimA\", \"mango\");\n    event.put(\"metA\", 3L);\n    row = new MapBasedInputRow(1505264400000L, dimNames, event);\n    indexB.add(row);\n\n    final File fileB = INDEX_MERGER_V9.persist(\n        indexB,\n        new File(tmpDir, \"B\"),\n        new IndexSpec(),\n        null\n    );\n    QueryableIndex qindexB = INDEX_IO.loadIndex(fileB);\n\n    final IncrementalIndex indexC = makeIncIndex(false);\n    incrementalIndices.add(indexC);\n\n    event = new HashMap<>();\n    event.put(\"dimA\", \"pomegranate\");\n    event.put(\"metA\", 2395L);\n    row = new MapBasedInputRow(1505260800000L, dimNames, event);\n    indexC.add(row);\n\n    event = new HashMap<>();\n    event.put(\"dimA\", \"mango\");\n    event.put(\"metA\", 8L);\n    row = new MapBasedInputRow(1605260800000L, dimNames, event);\n    indexC.add(row);\n\n    event = new HashMap<>();\n    event.put(\"dimA\", \"pomegranate\");\n    event.put(\"metA\", 5028L);\n    row = new MapBasedInputRow(1705264400000L, dimNames, event);\n    indexC.add(row);\n\n    event = new HashMap<>();\n    event.put(\"dimA\", \"mango\");\n    event.put(\"metA\", 7L);\n    row = new MapBasedInputRow(1805264400000L, dimNames, event);\n    indexC.add(row);\n\n    final File fileC = INDEX_MERGER_V9.persist(\n        indexC,\n        new File(tmpDir, \"C\"),\n        new IndexSpec(),\n        null\n    );\n    QueryableIndex qindexC = INDEX_IO.loadIndex(fileC);\n\n\n    final IncrementalIndex indexD = makeIncIndex(false);\n    incrementalIndices.add(indexD);\n\n    event = new HashMap<>();\n    event.put(\"dimA\", \"pomegranate\");\n    event.put(\"metA\", 4718L);\n    row = new MapBasedInputRow(1505260800000L, dimNames, event);\n    indexD.add(row);\n\n    event = new HashMap<>();\n    event.put(\"dimA\", \"mango\");\n    event.put(\"metA\", 18L);\n    row = new MapBasedInputRow(1605260800000L, dimNames, event);\n    indexD.add(row);\n\n    event = new HashMap<>();\n    event.put(\"dimA\", \"pomegranate\");\n    event.put(\"metA\", 2698L);\n    row = new MapBasedInputRow(1705264400000L, dimNames, event);\n    indexD.add(row);\n\n    event = new HashMap<>();\n    event.put(\"dimA\", \"mango\");\n    event.put(\"metA\", 3L);\n    row = new MapBasedInputRow(1805264400000L, dimNames, event);\n    indexD.add(row);\n\n    final File fileD = INDEX_MERGER_V9.persist(\n        indexD,\n        new File(tmpDir, \"D\"),\n        new IndexSpec(),\n        null\n    );\n    QueryableIndex qindexD = INDEX_IO.loadIndex(fileD);\n\n    groupByIndices = Arrays.asList(qindexA, qindexB, qindexC, qindexD);\n    resourceCloser = Closer.create();\n    setupGroupByFactory();\n  }\n","realPath":"processing/src/test/java/org/apache/druid/query/groupby/GroupByLimitPushDownMultiNodeMergeTest.java","repoName":"druid","snippetEndLine":0,"snippetStartLine":0,"startLine":173,"status":"M"}],"commitId":"3ff1c2b8ce7fbcf6e184b0e25fe81691cca18e6c","commitMessage":"@@@Fix bug which produces vastly inaccurate query results when forceLimitPushDown is enabled and order by clause has non grouping fields (#11097)\n\n","date":"2021-09-02 12:19:38","modifiedFileCount":"4","status":"M","submitter":"Jian Wang"}]
