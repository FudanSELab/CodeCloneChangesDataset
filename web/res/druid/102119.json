[{"authorTime":"2019-04-10 00:03:26","codes":[{"authorDate":"2019-04-10 00:03:26","commitOrder":1,"curCode":"  public void testOrcSplitElim() throws IOException, InterruptedException\n  {\n    \r\n\n    \r\n\r\n\r\n\r\n\n    HadoopDruidIndexerConfig config = loadHadoopDruidIndexerConfig(\"example/orc_split_elim_hadoop_job.json\");\n    Job job = Job.getInstance(new Configuration());\n    config.intoConfiguration(job);\n\n    OrcStruct data = getFirstRow(job, ((StaticPathSpec) config.getPathSpec()).getPaths());\n    List<InputRow> rows = (List<InputRow>) config.getParser().parseBatch(data);\n    assertEquals(4, rows.get(0).getDimensions().size());\n    assertEquals(\"2\", rows.get(0).getDimension(\"userid\").get(0));\n    assertEquals(\"foo\", rows.get(0).getDimension(\"string1\").get(0));\n    assertEquals(\"0.8\", rows.get(0).getDimension(\"subtype\").get(0));\n    assertEquals(\"1.2\", rows.get(0).getDimension(\"decimal1\").get(0));\n    assertEquals(DateTimes.of(\"1969-12-31T16:00:00.0Z\"), rows.get(0).getTimestamp());\n  }\n","date":"2019-04-10 00:03:26","endLine":176,"groupId":"14010","id":1,"instanceNumber":1,"isCurCommit":0,"methodName":"testOrcSplitElim","params":"()","path":"/mnt/clonedata/CloneManagementServer/ManagementServer/consistResult/result-druid-10-0.7/blobInfo/CC_OUT/blobs/8d/d399fa0dd12d4cb5f0ae40c6e8c06c506ede65.src","preCode":"  public void testOrcSplitElim() throws IOException, InterruptedException\n  {\n    \r\n\n    \r\n\r\n\r\n\r\n\n    HadoopDruidIndexerConfig config = loadHadoopDruidIndexerConfig(\"example/orc_split_elim_hadoop_job.json\");\n    Job job = Job.getInstance(new Configuration());\n    config.intoConfiguration(job);\n\n    OrcStruct data = getFirstRow(job, ((StaticPathSpec) config.getPathSpec()).getPaths());\n    List<InputRow> rows = (List<InputRow>) config.getParser().parseBatch(data);\n    assertEquals(4, rows.get(0).getDimensions().size());\n    assertEquals(\"2\", rows.get(0).getDimension(\"userid\").get(0));\n    assertEquals(\"foo\", rows.get(0).getDimension(\"string1\").get(0));\n    assertEquals(\"0.8\", rows.get(0).getDimension(\"subtype\").get(0));\n    assertEquals(\"1.2\", rows.get(0).getDimension(\"decimal1\").get(0));\n    assertEquals(DateTimes.of(\"1969-12-31T16:00:00.0Z\"), rows.get(0).getTimestamp());\n  }\n","realPath":"extensions-core/orc-extensions/src/test/java/org/apache/druid/data/input/orc/OrcHadoopInputRowParserTest.java","repoName":"druid","snippetEndLine":0,"snippetStartLine":0,"startLine":155,"status":"B"},{"authorDate":"2019-04-10 00:03:26","commitOrder":1,"curCode":"  public void testDate1900() throws IOException, InterruptedException\n  {\n    \r\n\r\n\r\n\r\n\n    HadoopDruidIndexerConfig config = loadHadoopDruidIndexerConfig(\"example/testDate1900_hadoop_job.json\");\n    Job job = Job.getInstance(new Configuration());\n    config.intoConfiguration(job);\n\n    OrcStruct data = getFirstRow(job, ((StaticPathSpec) config.getPathSpec()).getPaths());\n    List<InputRow> rows = (List<InputRow>) config.getParser().parseBatch(data);\n    assertEquals(1, rows.get(0).getDimensions().size());\n    assertEquals(\"1900-12-25T00:00:00.000Z\", rows.get(0).getDimension(\"date\").get(0));\n    assertEquals(DateTimes.of(\"1900-05-05T12:34:56.1Z\"), rows.get(0).getTimestamp());\n  }\n","date":"2019-04-10 00:03:26","endLine":195,"groupId":"14010","id":2,"instanceNumber":2,"isCurCommit":0,"methodName":"testDate1900","params":"()","path":"/mnt/clonedata/CloneManagementServer/ManagementServer/consistResult/result-druid-10-0.7/blobInfo/CC_OUT/blobs/8d/d399fa0dd12d4cb5f0ae40c6e8c06c506ede65.src","preCode":"  public void testDate1900() throws IOException, InterruptedException\n  {\n    \r\n\r\n\r\n\r\n\n    HadoopDruidIndexerConfig config = loadHadoopDruidIndexerConfig(\"example/testDate1900_hadoop_job.json\");\n    Job job = Job.getInstance(new Configuration());\n    config.intoConfiguration(job);\n\n    OrcStruct data = getFirstRow(job, ((StaticPathSpec) config.getPathSpec()).getPaths());\n    List<InputRow> rows = (List<InputRow>) config.getParser().parseBatch(data);\n    assertEquals(1, rows.get(0).getDimensions().size());\n    assertEquals(\"1900-12-25T00:00:00.000Z\", rows.get(0).getDimension(\"date\").get(0));\n    assertEquals(DateTimes.of(\"1900-05-05T12:34:56.1Z\"), rows.get(0).getTimestamp());\n  }\n","realPath":"extensions-core/orc-extensions/src/test/java/org/apache/druid/data/input/orc/OrcHadoopInputRowParserTest.java","repoName":"druid","snippetEndLine":0,"snippetStartLine":0,"startLine":179,"status":"B"}],"commitId":"89bb43f382643e7bad05681956c6a08778cf0165","commitMessage":"@@@'core' ORC extension (#7138)\n\n* orc extension reworked to use apache orc map-reduce lib.  moved to core extensions.  support for flattenSpec.  tests.  docs\n\n* change binary handling to be compatible with avro and parquet.  Rows.objectToStrings now converts byte[] to base64.  change date handling\n\n* better docs and tests\n\n* fix it\n\n* formatting\n\n* doc fix\n\n* fix it\n\n* exclude redundant dependencies\n\n* use latest orc-mapreduce.  add hadoop jobProperties recommendations to docs\n\n* doc fix\n\n* review stuff and fix binaryAsString\n\n* cache for root level fields\n\n* more better\n","date":"2019-04-10 00:03:26","modifiedFileCount":"1","status":"B","submitter":"Clint Wylie"},{"authorTime":"2019-07-07 00:33:12","codes":[{"authorDate":"2019-07-07 00:33:12","commitOrder":2,"curCode":"  public void testOrcSplitElim() throws IOException, InterruptedException\n  {\n    \r\n\n    \r\n\r\n\r\n\r\n\n    HadoopDruidIndexerConfig config = loadHadoopDruidIndexerConfig(\"example/orc_split_elim_hadoop_job.json\");\n    Job job = Job.getInstance(new Configuration());\n    config.intoConfiguration(job);\n\n    OrcStruct data = getFirstRow(job, ((StaticPathSpec) config.getPathSpec()).getPaths());\n    List<InputRow> rows = (List<InputRow>) config.getParser().parseBatch(data);\n    Assert.assertEquals(4, rows.get(0).getDimensions().size());\n    Assert.assertEquals(\"2\", rows.get(0).getDimension(\"userid\").get(0));\n    Assert.assertEquals(\"foo\", rows.get(0).getDimension(\"string1\").get(0));\n    Assert.assertEquals(\"0.8\", rows.get(0).getDimension(\"subtype\").get(0));\n    Assert.assertEquals(\"1.2\", rows.get(0).getDimension(\"decimal1\").get(0));\n    Assert.assertEquals(DateTimes.of(\"1969-12-31T16:00:00.0Z\"), rows.get(0).getTimestamp());\n  }\n","date":"2019-07-07 00:33:12","endLine":175,"groupId":"19513","id":3,"instanceNumber":1,"isCurCommit":0,"methodName":"testOrcSplitElim","params":"()","path":"/mnt/clonedata/CloneManagementServer/ManagementServer/consistResult/result-druid-10-0.7/blobInfo/CC_OUT/blobs/e5/88e60bdee1c21afb3b14290ca4689a52a0b392.src","preCode":"  public void testOrcSplitElim() throws IOException, InterruptedException\n  {\n    \r\n\n    \r\n\r\n\r\n\r\n\n    HadoopDruidIndexerConfig config = loadHadoopDruidIndexerConfig(\"example/orc_split_elim_hadoop_job.json\");\n    Job job = Job.getInstance(new Configuration());\n    config.intoConfiguration(job);\n\n    OrcStruct data = getFirstRow(job, ((StaticPathSpec) config.getPathSpec()).getPaths());\n    List<InputRow> rows = (List<InputRow>) config.getParser().parseBatch(data);\n    assertEquals(4, rows.get(0).getDimensions().size());\n    assertEquals(\"2\", rows.get(0).getDimension(\"userid\").get(0));\n    assertEquals(\"foo\", rows.get(0).getDimension(\"string1\").get(0));\n    assertEquals(\"0.8\", rows.get(0).getDimension(\"subtype\").get(0));\n    assertEquals(\"1.2\", rows.get(0).getDimension(\"decimal1\").get(0));\n    assertEquals(DateTimes.of(\"1969-12-31T16:00:00.0Z\"), rows.get(0).getTimestamp());\n  }\n","realPath":"extensions-core/orc-extensions/src/test/java/org/apache/druid/data/input/orc/OrcHadoopInputRowParserTest.java","repoName":"druid","snippetEndLine":0,"snippetStartLine":0,"startLine":154,"status":"M"},{"authorDate":"2019-07-07 00:33:12","commitOrder":2,"curCode":"  public void testDate1900() throws IOException, InterruptedException\n  {\n    \r\n\r\n\r\n\r\n\n    HadoopDruidIndexerConfig config = loadHadoopDruidIndexerConfig(\"example/testDate1900_hadoop_job.json\");\n    Job job = Job.getInstance(new Configuration());\n    config.intoConfiguration(job);\n\n    OrcStruct data = getFirstRow(job, ((StaticPathSpec) config.getPathSpec()).getPaths());\n    List<InputRow> rows = (List<InputRow>) config.getParser().parseBatch(data);\n    Assert.assertEquals(1, rows.get(0).getDimensions().size());\n    Assert.assertEquals(\"1900-12-25T00:00:00.000Z\", rows.get(0).getDimension(\"date\").get(0));\n    Assert.assertEquals(DateTimes.of(\"1900-05-05T12:34:56.1Z\"), rows.get(0).getTimestamp());\n  }\n","date":"2019-07-07 00:33:12","endLine":194,"groupId":"19513","id":4,"instanceNumber":2,"isCurCommit":0,"methodName":"testDate1900","params":"()","path":"/mnt/clonedata/CloneManagementServer/ManagementServer/consistResult/result-druid-10-0.7/blobInfo/CC_OUT/blobs/e5/88e60bdee1c21afb3b14290ca4689a52a0b392.src","preCode":"  public void testDate1900() throws IOException, InterruptedException\n  {\n    \r\n\r\n\r\n\r\n\n    HadoopDruidIndexerConfig config = loadHadoopDruidIndexerConfig(\"example/testDate1900_hadoop_job.json\");\n    Job job = Job.getInstance(new Configuration());\n    config.intoConfiguration(job);\n\n    OrcStruct data = getFirstRow(job, ((StaticPathSpec) config.getPathSpec()).getPaths());\n    List<InputRow> rows = (List<InputRow>) config.getParser().parseBatch(data);\n    assertEquals(1, rows.get(0).getDimensions().size());\n    assertEquals(\"1900-12-25T00:00:00.000Z\", rows.get(0).getDimension(\"date\").get(0));\n    assertEquals(DateTimes.of(\"1900-05-05T12:34:56.1Z\"), rows.get(0).getTimestamp());\n  }\n","realPath":"extensions-core/orc-extensions/src/test/java/org/apache/druid/data/input/orc/OrcHadoopInputRowParserTest.java","repoName":"druid","snippetEndLine":0,"snippetStartLine":0,"startLine":178,"status":"M"}],"commitId":"1166bbcb75d432817715fdd429737f86730b5591","commitMessage":"@@@Remove static imports from tests (#8036)\n\nMake static imports forbidden in tests and remove all occurrences to be\nconsistent with the non-test code.\n\nAlso.  various changes to files affected by above:\n- Reformat to adhere to druid style guide\n- Fix various IntelliJ warnings\n- Fix various SonarLint warnings (e.g..  the expected/actual args to\n  Assert.assertEquals() were flipped)","date":"2019-07-07 00:33:12","modifiedFileCount":"98","status":"M","submitter":"Chi Cao Minh"},{"authorTime":"2019-11-29 04:45:24","codes":[{"authorDate":"2019-11-29 04:45:24","commitOrder":3,"curCode":"  public void testOrcSplitElim() throws IOException\n  {\n    \r\n\n    \r\n\r\n\r\n\r\n\n    HadoopDruidIndexerConfig config = loadHadoopDruidIndexerConfig(\"example/orc_split_elim_hadoop_job.json\");\n    Job job = Job.getInstance(new Configuration());\n    config.intoConfiguration(job);\n\n    OrcStruct data = getFirstRow(job, ((StaticPathSpec) config.getPathSpec()).getPaths());\n    List<InputRow> rows = (List<InputRow>) config.getParser().parseBatch(data);\n    Assert.assertEquals(4, rows.get(0).getDimensions().size());\n    Assert.assertEquals(\"2\", rows.get(0).getDimension(\"userid\").get(0));\n    Assert.assertEquals(\"foo\", rows.get(0).getDimension(\"string1\").get(0));\n    Assert.assertEquals(\"0.8\", rows.get(0).getDimension(\"subtype\").get(0));\n    Assert.assertEquals(\"1.2\", rows.get(0).getDimension(\"decimal1\").get(0));\n    Assert.assertEquals(DateTimes.of(\"1969-12-31T16:00:00.0Z\"), rows.get(0).getTimestamp());\n  }\n","date":"2019-11-29 04:45:24","endLine":175,"groupId":"102119","id":5,"instanceNumber":1,"isCurCommit":1,"methodName":"testOrcSplitElim","params":"()","path":"/mnt/clonedata/CloneManagementServer/ManagementServer/consistResult/result-druid-10-0.7/blobInfo/CC_OUT/blobs/38/eb5da8d5b39c764c845b9a44970d15cdb906c8.src","preCode":"  public void testOrcSplitElim() throws IOException, InterruptedException\n  {\n    \r\n\n    \r\n\r\n\r\n\r\n\n    HadoopDruidIndexerConfig config = loadHadoopDruidIndexerConfig(\"example/orc_split_elim_hadoop_job.json\");\n    Job job = Job.getInstance(new Configuration());\n    config.intoConfiguration(job);\n\n    OrcStruct data = getFirstRow(job, ((StaticPathSpec) config.getPathSpec()).getPaths());\n    List<InputRow> rows = (List<InputRow>) config.getParser().parseBatch(data);\n    Assert.assertEquals(4, rows.get(0).getDimensions().size());\n    Assert.assertEquals(\"2\", rows.get(0).getDimension(\"userid\").get(0));\n    Assert.assertEquals(\"foo\", rows.get(0).getDimension(\"string1\").get(0));\n    Assert.assertEquals(\"0.8\", rows.get(0).getDimension(\"subtype\").get(0));\n    Assert.assertEquals(\"1.2\", rows.get(0).getDimension(\"decimal1\").get(0));\n    Assert.assertEquals(DateTimes.of(\"1969-12-31T16:00:00.0Z\"), rows.get(0).getTimestamp());\n  }\n","realPath":"extensions-core/orc-extensions/src/test/java/org/apache/druid/data/input/orc/OrcHadoopInputRowParserTest.java","repoName":"druid","snippetEndLine":0,"snippetStartLine":0,"startLine":154,"status":"M"},{"authorDate":"2019-11-29 04:45:24","commitOrder":3,"curCode":"  public void testDate1900() throws IOException\n  {\n    \r\n\r\n\r\n\r\n\n    HadoopDruidIndexerConfig config = loadHadoopDruidIndexerConfig(\"example/testDate1900_hadoop_job.json\");\n    Job job = Job.getInstance(new Configuration());\n    config.intoConfiguration(job);\n\n    OrcStruct data = getFirstRow(job, ((StaticPathSpec) config.getPathSpec()).getPaths());\n    List<InputRow> rows = (List<InputRow>) config.getParser().parseBatch(data);\n    Assert.assertEquals(1, rows.get(0).getDimensions().size());\n    Assert.assertEquals(\"1900-12-25T00:00:00.000Z\", rows.get(0).getDimension(\"date\").get(0));\n    Assert.assertEquals(DateTimes.of(\"1900-05-05T12:34:56.1Z\"), rows.get(0).getTimestamp());\n  }\n","date":"2019-11-29 04:45:24","endLine":194,"groupId":"102119","id":6,"instanceNumber":2,"isCurCommit":1,"methodName":"testDate1900","params":"()","path":"/mnt/clonedata/CloneManagementServer/ManagementServer/consistResult/result-druid-10-0.7/blobInfo/CC_OUT/blobs/38/eb5da8d5b39c764c845b9a44970d15cdb906c8.src","preCode":"  public void testDate1900() throws IOException, InterruptedException\n  {\n    \r\n\r\n\r\n\r\n\n    HadoopDruidIndexerConfig config = loadHadoopDruidIndexerConfig(\"example/testDate1900_hadoop_job.json\");\n    Job job = Job.getInstance(new Configuration());\n    config.intoConfiguration(job);\n\n    OrcStruct data = getFirstRow(job, ((StaticPathSpec) config.getPathSpec()).getPaths());\n    List<InputRow> rows = (List<InputRow>) config.getParser().parseBatch(data);\n    Assert.assertEquals(1, rows.get(0).getDimensions().size());\n    Assert.assertEquals(\"1900-12-25T00:00:00.000Z\", rows.get(0).getDimension(\"date\").get(0));\n    Assert.assertEquals(DateTimes.of(\"1900-05-05T12:34:56.1Z\"), rows.get(0).getTimestamp());\n  }\n","realPath":"extensions-core/orc-extensions/src/test/java/org/apache/druid/data/input/orc/OrcHadoopInputRowParserTest.java","repoName":"druid","snippetEndLine":0,"snippetStartLine":0,"startLine":178,"status":"M"}],"commitId":"86e8903523fc3d2d177963d2f95fd56441605102","commitMessage":"@@@Support orc format for native batch ingestion (#8950)\n\n* Support orc format for native batch ingestion\n\n* fix pom and remove wrong comment\n\n* fix unnecessary condition check\n\n* use flatMap back to handle exception properly\n\n* move exceptionThrowingIterator to intermediateRowParsingReader\n\n* runtime\n","date":"2019-11-29 04:45:24","modifiedFileCount":"5","status":"M","submitter":"Jihoon Son"}]
