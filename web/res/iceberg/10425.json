[{"authorTime":"2020-04-16 07:28:17","codes":[{"authorDate":"2020-04-16 07:28:17","commitOrder":1,"curCode":"  public void testMetadataFolderIsIntact() throws InterruptedException {\n    \r\n    Map<String, String> props = Maps.newHashMap();\n    props.put(TableProperties.WRITE_NEW_DATA_LOCATION, tableLocation);\n    Table table = TABLES.create(SCHEMA, SPEC, props, tableLocation);\n\n    List<ThreeColumnRecord> records = Lists.newArrayList(\n        new ThreeColumnRecord(1, \"AAAAAAAAAA\", \"AAAA\")\n    );\n    Dataset<Row> df = spark.createDataFrame(records, ThreeColumnRecord.class).coalesce(1);\n\n    df.select(\"c1\", \"c2\", \"c3\")\n        .write()\n        .format(\"iceberg\")\n        .mode(\"append\")\n        .save(tableLocation);\n\n    df.write().mode(\"append\").parquet(tableLocation + \"/c2_trunc=AA/c3=AAAA\");\n\n    \r\n    Thread.sleep(1000);\n\n    Actions actions = Actions.forTable(table);\n\n    List<String> result = actions.removeOrphanFiles()\n        .olderThan(System.currentTimeMillis())\n        .execute();\n\n    Assert.assertEquals(\"Should delete 1 file\", 1, result.size());\n\n    Dataset<Row> resultDF = spark.read().format(\"iceberg\").load(tableLocation);\n    List<ThreeColumnRecord> actualRecords = resultDF\n        .as(Encoders.bean(ThreeColumnRecord.class))\n        .collectAsList();\n    Assert.assertEquals(\"Rows must match\", records, actualRecords);\n  }\n","date":"2020-04-16 07:28:17","endLine":332,"groupId":"2862","id":1,"instanceNumber":1,"isCurCommit":0,"methodName":"testMetadataFolderIsIntact","params":"()","path":"/mnt/clonedata/CloneManagementServer/ManagementServer/consistResult/result-iceberg-10-0.7/blobInfo/CC_OUT/blobs/e0/245b5bf65a190fa7844962fae4d4f94e2195eb.src","preCode":"  public void testMetadataFolderIsIntact() throws InterruptedException {\n    \r\n    Map<String, String> props = Maps.newHashMap();\n    props.put(TableProperties.WRITE_NEW_DATA_LOCATION, tableLocation);\n    Table table = TABLES.create(SCHEMA, SPEC, props, tableLocation);\n\n    List<ThreeColumnRecord> records = Lists.newArrayList(\n        new ThreeColumnRecord(1, \"AAAAAAAAAA\", \"AAAA\")\n    );\n    Dataset<Row> df = spark.createDataFrame(records, ThreeColumnRecord.class).coalesce(1);\n\n    df.select(\"c1\", \"c2\", \"c3\")\n        .write()\n        .format(\"iceberg\")\n        .mode(\"append\")\n        .save(tableLocation);\n\n    df.write().mode(\"append\").parquet(tableLocation + \"/c2_trunc=AA/c3=AAAA\");\n\n    \r\n    Thread.sleep(1000);\n\n    Actions actions = Actions.forTable(table);\n\n    List<String> result = actions.removeOrphanFiles()\n        .olderThan(System.currentTimeMillis())\n        .execute();\n\n    Assert.assertEquals(\"Should delete 1 file\", 1, result.size());\n\n    Dataset<Row> resultDF = spark.read().format(\"iceberg\").load(tableLocation);\n    List<ThreeColumnRecord> actualRecords = resultDF\n        .as(Encoders.bean(ThreeColumnRecord.class))\n        .collectAsList();\n    Assert.assertEquals(\"Rows must match\", records, actualRecords);\n  }\n","realPath":"spark/src/test/java/org/apache/iceberg/actions/TestRemoveOrphanFilesAction.java","repoName":"iceberg","snippetEndLine":0,"snippetStartLine":0,"startLine":297,"status":"B"},{"authorDate":"2020-04-16 07:28:17","commitOrder":1,"curCode":"  public void testManyTopLevelPartitions() throws InterruptedException {\n    Table table = TABLES.create(SCHEMA, SPEC, Maps.newHashMap(), tableLocation);\n\n    List<ThreeColumnRecord> records = Lists.newArrayList();\n    for (int i = 0; i < 100; i++) {\n      records.add(new ThreeColumnRecord(i, String.valueOf(i), String.valueOf(i)));\n    }\n\n    Dataset<Row> df = spark.createDataFrame(records, ThreeColumnRecord.class);\n\n    df.select(\"c1\", \"c2\", \"c3\")\n        .write()\n        .format(\"iceberg\")\n        .mode(\"append\")\n        .save(tableLocation);\n\n    \r\n    Thread.sleep(1000);\n\n    Actions actions = Actions.forTable(table);\n\n    List<String> result = actions.removeOrphanFiles()\n        .olderThan(System.currentTimeMillis())\n        .execute();\n\n    Assert.assertTrue(\"Should not delete any files\", result.isEmpty());\n\n    Dataset<Row> resultDF = spark.read().format(\"iceberg\").load(tableLocation);\n    List<ThreeColumnRecord> actualRecords = resultDF\n        .as(Encoders.bean(ThreeColumnRecord.class))\n        .collectAsList();\n    Assert.assertEquals(\"Rows must match\", records, actualRecords);\n  }\n","date":"2020-04-16 07:28:17","endLine":449,"groupId":"4742","id":2,"instanceNumber":2,"isCurCommit":0,"methodName":"testManyTopLevelPartitions","params":"()","path":"/mnt/clonedata/CloneManagementServer/ManagementServer/consistResult/result-iceberg-10-0.7/blobInfo/CC_OUT/blobs/e0/245b5bf65a190fa7844962fae4d4f94e2195eb.src","preCode":"  public void testManyTopLevelPartitions() throws InterruptedException {\n    Table table = TABLES.create(SCHEMA, SPEC, Maps.newHashMap(), tableLocation);\n\n    List<ThreeColumnRecord> records = Lists.newArrayList();\n    for (int i = 0; i < 100; i++) {\n      records.add(new ThreeColumnRecord(i, String.valueOf(i), String.valueOf(i)));\n    }\n\n    Dataset<Row> df = spark.createDataFrame(records, ThreeColumnRecord.class);\n\n    df.select(\"c1\", \"c2\", \"c3\")\n        .write()\n        .format(\"iceberg\")\n        .mode(\"append\")\n        .save(tableLocation);\n\n    \r\n    Thread.sleep(1000);\n\n    Actions actions = Actions.forTable(table);\n\n    List<String> result = actions.removeOrphanFiles()\n        .olderThan(System.currentTimeMillis())\n        .execute();\n\n    Assert.assertTrue(\"Should not delete any files\", result.isEmpty());\n\n    Dataset<Row> resultDF = spark.read().format(\"iceberg\").load(tableLocation);\n    List<ThreeColumnRecord> actualRecords = resultDF\n        .as(Encoders.bean(ThreeColumnRecord.class))\n        .collectAsList();\n    Assert.assertEquals(\"Rows must match\", records, actualRecords);\n  }\n","realPath":"spark/src/test/java/org/apache/iceberg/actions/TestRemoveOrphanFilesAction.java","repoName":"iceberg","snippetEndLine":0,"snippetStartLine":0,"startLine":417,"status":"B"}],"commitId":"8b27fae275baca8b4623dffa142089f6aed50acc","commitMessage":"@@@Spark: move actions to a separate package (#927)\n\n","date":"2020-04-16 07:28:17","modifiedFileCount":"1","status":"B","submitter":"Anton Okolnychyi"},{"authorTime":"2020-04-16 07:28:17","codes":[{"authorDate":"2021-08-24 00:27:42","commitOrder":2,"curCode":"  public void testMetadataFolderIsIntact() throws InterruptedException {\n    \r\n    Map<String, String> props = Maps.newHashMap();\n    props.put(TableProperties.WRITE_FOLDER_STORAGE_LOCATION, tableLocation);\n    Table table = TABLES.create(SCHEMA, SPEC, props, tableLocation);\n\n    List<ThreeColumnRecord> records = Lists.newArrayList(\n        new ThreeColumnRecord(1, \"AAAAAAAAAA\", \"AAAA\")\n    );\n    Dataset<Row> df = spark.createDataFrame(records, ThreeColumnRecord.class).coalesce(1);\n\n    df.select(\"c1\", \"c2\", \"c3\")\n        .write()\n        .format(\"iceberg\")\n        .mode(\"append\")\n        .save(tableLocation);\n\n    df.write().mode(\"append\").parquet(tableLocation + \"/c2_trunc=AA/c3=AAAA\");\n\n    \r\n    Thread.sleep(1000);\n\n    Actions actions = Actions.forTable(table);\n\n    List<String> result = actions.removeOrphanFiles()\n        .olderThan(System.currentTimeMillis())\n        .execute();\n\n    Assert.assertEquals(\"Should delete 1 file\", 1, result.size());\n\n    Dataset<Row> resultDF = spark.read().format(\"iceberg\").load(tableLocation);\n    List<ThreeColumnRecord> actualRecords = resultDF\n        .as(Encoders.bean(ThreeColumnRecord.class))\n        .collectAsList();\n    Assert.assertEquals(\"Rows must match\", records, actualRecords);\n  }\n","date":"2021-08-24 00:27:42","endLine":320,"groupId":"2862","id":3,"instanceNumber":1,"isCurCommit":0,"methodName":"testMetadataFolderIsIntact","params":"()","path":"/mnt/clonedata/CloneManagementServer/ManagementServer/consistResult/result-iceberg-10-0.7/blobInfo/CC_OUT/blobs/7c/2a325eb7833869d957422fbf84350a9fa219c1.src","preCode":"  public void testMetadataFolderIsIntact() throws InterruptedException {\n    \r\n    Map<String, String> props = Maps.newHashMap();\n    props.put(TableProperties.WRITE_NEW_DATA_LOCATION, tableLocation);\n    Table table = TABLES.create(SCHEMA, SPEC, props, tableLocation);\n\n    List<ThreeColumnRecord> records = Lists.newArrayList(\n        new ThreeColumnRecord(1, \"AAAAAAAAAA\", \"AAAA\")\n    );\n    Dataset<Row> df = spark.createDataFrame(records, ThreeColumnRecord.class).coalesce(1);\n\n    df.select(\"c1\", \"c2\", \"c3\")\n        .write()\n        .format(\"iceberg\")\n        .mode(\"append\")\n        .save(tableLocation);\n\n    df.write().mode(\"append\").parquet(tableLocation + \"/c2_trunc=AA/c3=AAAA\");\n\n    \r\n    Thread.sleep(1000);\n\n    Actions actions = Actions.forTable(table);\n\n    List<String> result = actions.removeOrphanFiles()\n        .olderThan(System.currentTimeMillis())\n        .execute();\n\n    Assert.assertEquals(\"Should delete 1 file\", 1, result.size());\n\n    Dataset<Row> resultDF = spark.read().format(\"iceberg\").load(tableLocation);\n    List<ThreeColumnRecord> actualRecords = resultDF\n        .as(Encoders.bean(ThreeColumnRecord.class))\n        .collectAsList();\n    Assert.assertEquals(\"Rows must match\", records, actualRecords);\n  }\n","realPath":"spark/src/test/java/org/apache/iceberg/actions/TestRemoveOrphanFilesAction.java","repoName":"iceberg","snippetEndLine":0,"snippetStartLine":0,"startLine":285,"status":"M"},{"authorDate":"2020-04-16 07:28:17","commitOrder":2,"curCode":"  public void testManyTopLevelPartitions() throws InterruptedException {\n    Table table = TABLES.create(SCHEMA, SPEC, Maps.newHashMap(), tableLocation);\n\n    List<ThreeColumnRecord> records = Lists.newArrayList();\n    for (int i = 0; i < 100; i++) {\n      records.add(new ThreeColumnRecord(i, String.valueOf(i), String.valueOf(i)));\n    }\n\n    Dataset<Row> df = spark.createDataFrame(records, ThreeColumnRecord.class);\n\n    df.select(\"c1\", \"c2\", \"c3\")\n        .write()\n        .format(\"iceberg\")\n        .mode(\"append\")\n        .save(tableLocation);\n\n    \r\n    Thread.sleep(1000);\n\n    Actions actions = Actions.forTable(table);\n\n    List<String> result = actions.removeOrphanFiles()\n        .olderThan(System.currentTimeMillis())\n        .execute();\n\n    Assert.assertTrue(\"Should not delete any files\", result.isEmpty());\n\n    Dataset<Row> resultDF = spark.read().format(\"iceberg\").load(tableLocation);\n    List<ThreeColumnRecord> actualRecords = resultDF\n        .as(Encoders.bean(ThreeColumnRecord.class))\n        .collectAsList();\n    Assert.assertEquals(\"Rows must match\", records, actualRecords);\n  }\n","date":"2020-04-16 07:28:17","endLine":449,"groupId":"4742","id":4,"instanceNumber":2,"isCurCommit":0,"methodName":"testManyTopLevelPartitions","params":"()","path":"/mnt/clonedata/CloneManagementServer/ManagementServer/consistResult/result-iceberg-10-0.7/blobInfo/CC_OUT/blobs/e0/245b5bf65a190fa7844962fae4d4f94e2195eb.src","preCode":"  public void testManyTopLevelPartitions() throws InterruptedException {\n    Table table = TABLES.create(SCHEMA, SPEC, Maps.newHashMap(), tableLocation);\n\n    List<ThreeColumnRecord> records = Lists.newArrayList();\n    for (int i = 0; i < 100; i++) {\n      records.add(new ThreeColumnRecord(i, String.valueOf(i), String.valueOf(i)));\n    }\n\n    Dataset<Row> df = spark.createDataFrame(records, ThreeColumnRecord.class);\n\n    df.select(\"c1\", \"c2\", \"c3\")\n        .write()\n        .format(\"iceberg\")\n        .mode(\"append\")\n        .save(tableLocation);\n\n    \r\n    Thread.sleep(1000);\n\n    Actions actions = Actions.forTable(table);\n\n    List<String> result = actions.removeOrphanFiles()\n        .olderThan(System.currentTimeMillis())\n        .execute();\n\n    Assert.assertTrue(\"Should not delete any files\", result.isEmpty());\n\n    Dataset<Row> resultDF = spark.read().format(\"iceberg\").load(tableLocation);\n    List<ThreeColumnRecord> actualRecords = resultDF\n        .as(Encoders.bean(ThreeColumnRecord.class))\n        .collectAsList();\n    Assert.assertEquals(\"Rows must match\", records, actualRecords);\n  }\n","realPath":"spark/src/test/java/org/apache/iceberg/actions/TestRemoveOrphanFilesAction.java","repoName":"iceberg","snippetEndLine":0,"snippetStartLine":0,"startLine":417,"status":"N"}],"commitId":"aef12c0e702ef8c306826581893963e3dc5f6ba3","commitMessage":"@@@Core: Rename WRITE_NEW_DATA_LOCATION to WRITE_FOLDER_STORAGE_LOCATION (#2965)\n\n","date":"2021-08-24 00:27:42","modifiedFileCount":"7","status":"M","submitter":"Jack Ye"},{"authorTime":"2020-04-16 07:28:17","codes":[{"authorDate":"2021-09-23 02:18:43","commitOrder":3,"curCode":"  public void testMetadataFolderIsIntact() throws InterruptedException {\n    \r\n    Map<String, String> props = Maps.newHashMap();\n    props.put(TableProperties.WRITE_DATA_LOCATION, tableLocation);\n    Table table = TABLES.create(SCHEMA, SPEC, props, tableLocation);\n\n    List<ThreeColumnRecord> records = Lists.newArrayList(\n        new ThreeColumnRecord(1, \"AAAAAAAAAA\", \"AAAA\")\n    );\n    Dataset<Row> df = spark.createDataFrame(records, ThreeColumnRecord.class).coalesce(1);\n\n    df.select(\"c1\", \"c2\", \"c3\")\n        .write()\n        .format(\"iceberg\")\n        .mode(\"append\")\n        .save(tableLocation);\n\n    df.write().mode(\"append\").parquet(tableLocation + \"/c2_trunc=AA/c3=AAAA\");\n\n    \r\n    Thread.sleep(1000);\n\n    Actions actions = Actions.forTable(table);\n\n    List<String> result = actions.removeOrphanFiles()\n        .olderThan(System.currentTimeMillis())\n        .execute();\n\n    Assert.assertEquals(\"Should delete 1 file\", 1, result.size());\n\n    Dataset<Row> resultDF = spark.read().format(\"iceberg\").load(tableLocation);\n    List<ThreeColumnRecord> actualRecords = resultDF\n        .as(Encoders.bean(ThreeColumnRecord.class))\n        .collectAsList();\n    Assert.assertEquals(\"Rows must match\", records, actualRecords);\n  }\n","date":"2021-09-23 02:18:43","endLine":320,"groupId":"10425","id":5,"instanceNumber":1,"isCurCommit":1,"methodName":"testMetadataFolderIsIntact","params":"()","path":"/mnt/clonedata/CloneManagementServer/ManagementServer/consistResult/result-iceberg-10-0.7/blobInfo/CC_OUT/blobs/cf/85bc85042017593a68b05702717aa58b420250.src","preCode":"  public void testMetadataFolderIsIntact() throws InterruptedException {\n    \r\n    Map<String, String> props = Maps.newHashMap();\n    props.put(TableProperties.WRITE_FOLDER_STORAGE_LOCATION, tableLocation);\n    Table table = TABLES.create(SCHEMA, SPEC, props, tableLocation);\n\n    List<ThreeColumnRecord> records = Lists.newArrayList(\n        new ThreeColumnRecord(1, \"AAAAAAAAAA\", \"AAAA\")\n    );\n    Dataset<Row> df = spark.createDataFrame(records, ThreeColumnRecord.class).coalesce(1);\n\n    df.select(\"c1\", \"c2\", \"c3\")\n        .write()\n        .format(\"iceberg\")\n        .mode(\"append\")\n        .save(tableLocation);\n\n    df.write().mode(\"append\").parquet(tableLocation + \"/c2_trunc=AA/c3=AAAA\");\n\n    \r\n    Thread.sleep(1000);\n\n    Actions actions = Actions.forTable(table);\n\n    List<String> result = actions.removeOrphanFiles()\n        .olderThan(System.currentTimeMillis())\n        .execute();\n\n    Assert.assertEquals(\"Should delete 1 file\", 1, result.size());\n\n    Dataset<Row> resultDF = spark.read().format(\"iceberg\").load(tableLocation);\n    List<ThreeColumnRecord> actualRecords = resultDF\n        .as(Encoders.bean(ThreeColumnRecord.class))\n        .collectAsList();\n    Assert.assertEquals(\"Rows must match\", records, actualRecords);\n  }\n","realPath":"spark/src/test/java/org/apache/iceberg/actions/TestRemoveOrphanFilesAction.java","repoName":"iceberg","snippetEndLine":0,"snippetStartLine":0,"startLine":285,"status":"M"},{"authorDate":"2020-04-16 07:28:17","commitOrder":3,"curCode":"  public void testManyTopLevelPartitions() throws InterruptedException {\n    Table table = TABLES.create(SCHEMA, SPEC, Maps.newHashMap(), tableLocation);\n\n    List<ThreeColumnRecord> records = Lists.newArrayList();\n    for (int i = 0; i < 100; i++) {\n      records.add(new ThreeColumnRecord(i, String.valueOf(i), String.valueOf(i)));\n    }\n\n    Dataset<Row> df = spark.createDataFrame(records, ThreeColumnRecord.class);\n\n    df.select(\"c1\", \"c2\", \"c3\")\n        .write()\n        .format(\"iceberg\")\n        .mode(\"append\")\n        .save(tableLocation);\n\n    \r\n    Thread.sleep(1000);\n\n    Actions actions = Actions.forTable(table);\n\n    List<String> result = actions.removeOrphanFiles()\n        .olderThan(System.currentTimeMillis())\n        .execute();\n\n    Assert.assertTrue(\"Should not delete any files\", result.isEmpty());\n\n    Dataset<Row> resultDF = spark.read().format(\"iceberg\").load(tableLocation);\n    List<ThreeColumnRecord> actualRecords = resultDF\n        .as(Encoders.bean(ThreeColumnRecord.class))\n        .collectAsList();\n    Assert.assertEquals(\"Rows must match\", records, actualRecords);\n  }\n","date":"2020-04-16 07:28:17","endLine":449,"groupId":"10425","id":6,"instanceNumber":2,"isCurCommit":0,"methodName":"testManyTopLevelPartitions","params":"()","path":"/mnt/clonedata/CloneManagementServer/ManagementServer/consistResult/result-iceberg-10-0.7/blobInfo/CC_OUT/blobs/e0/245b5bf65a190fa7844962fae4d4f94e2195eb.src","preCode":"  public void testManyTopLevelPartitions() throws InterruptedException {\n    Table table = TABLES.create(SCHEMA, SPEC, Maps.newHashMap(), tableLocation);\n\n    List<ThreeColumnRecord> records = Lists.newArrayList();\n    for (int i = 0; i < 100; i++) {\n      records.add(new ThreeColumnRecord(i, String.valueOf(i), String.valueOf(i)));\n    }\n\n    Dataset<Row> df = spark.createDataFrame(records, ThreeColumnRecord.class);\n\n    df.select(\"c1\", \"c2\", \"c3\")\n        .write()\n        .format(\"iceberg\")\n        .mode(\"append\")\n        .save(tableLocation);\n\n    \r\n    Thread.sleep(1000);\n\n    Actions actions = Actions.forTable(table);\n\n    List<String> result = actions.removeOrphanFiles()\n        .olderThan(System.currentTimeMillis())\n        .execute();\n\n    Assert.assertTrue(\"Should not delete any files\", result.isEmpty());\n\n    Dataset<Row> resultDF = spark.read().format(\"iceberg\").load(tableLocation);\n    List<ThreeColumnRecord> actualRecords = resultDF\n        .as(Encoders.bean(ThreeColumnRecord.class))\n        .collectAsList();\n    Assert.assertEquals(\"Rows must match\", records, actualRecords);\n  }\n","realPath":"spark/src/test/java/org/apache/iceberg/actions/TestRemoveOrphanFilesAction.java","repoName":"iceberg","snippetEndLine":0,"snippetStartLine":0,"startLine":417,"status":"N"}],"commitId":"12e30ebaeb6dc0ff7e6bb01c0d9f8ae8a5f06ddf","commitMessage":"@@@Core: Prefer write.data.path to write.folder-storage.path or write.object-storage.path (#3094)\n\n","date":"2021-09-23 02:18:43","modifiedFileCount":"7","status":"M","submitter":"Yufei Gu"}]
