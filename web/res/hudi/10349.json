[{"authorTime":"2020-08-06 12:34:55","codes":[{"authorDate":"2020-08-06 12:34:55","commitOrder":1,"curCode":"  public static void createMORTable(String commitTime, String deltaCommitTime, int numberOfPartitions,\n      boolean createDeltaCommit, boolean useSchemaFromCommitMetadata)\n      throws IOException, URISyntaxException, InterruptedException {\n    Path path = new Path(hiveSyncConfig.basePath);\n    FileIOUtils.deleteDirectory(new File(hiveSyncConfig.basePath));\n    HoodieTableMetaClient.initTableType(configuration, hiveSyncConfig.basePath, HoodieTableType.MERGE_ON_READ,\n        hiveSyncConfig.tableName, HoodieAvroPayload.class.getName());\n\n    boolean result = fileSystem.mkdirs(path);\n    checkResult(result);\n    DateTime dateTime = DateTime.now();\n    HoodieCommitMetadata commitMetadata = createPartitions(numberOfPartitions, true,\n        useSchemaFromCommitMetadata, dateTime, commitTime);\n    createdTablesSet.add(hiveSyncConfig.databaseName + \".\" + hiveSyncConfig.tableName);\n    createdTablesSet\n        .add(hiveSyncConfig.databaseName + \".\" + hiveSyncConfig.tableName + HiveSyncTool.SUFFIX_SNAPSHOT_TABLE);\n    HoodieCommitMetadata compactionMetadata = new HoodieCommitMetadata();\n    commitMetadata.getPartitionToWriteStats()\n        .forEach((key, value) -> value.forEach(l -> compactionMetadata.addWriteStat(key, l)));\n    addSchemaToCommitMetadata(compactionMetadata, commitMetadata.getMetadata(HoodieCommitMetadata.SCHEMA_KEY),\n                             useSchemaFromCommitMetadata);\n    createCompactionCommitFile(compactionMetadata, commitTime);\n    if (createDeltaCommit) {\n      \r\n      HoodieCommitMetadata deltaMetadata = createLogFiles(commitMetadata.getPartitionToWriteStats(), true,\n                                                          useSchemaFromCommitMetadata);\n      createDeltaCommitFile(deltaMetadata, deltaCommitTime);\n    }\n  }\n","date":"2020-08-06 12:34:55","endLine":203,"groupId":"2194","id":1,"instanceNumber":1,"isCurCommit":0,"methodName":"createMORTable","params":"(StringcommitTime@StringdeltaCommitTime@intnumberOfPartitions@booleancreateDeltaCommit@booleanuseSchemaFromCommitMetadata)","path":"/mnt/clonedata/CloneManagementServer/ManagementServer/consistResult/result-hudi-10-0.7/blobInfo/CC_OUT/blobs/d0/d1b667aea20b9592b845de9833ace73a3ed40a.src","preCode":"  public static void createMORTable(String commitTime, String deltaCommitTime, int numberOfPartitions,\n      boolean createDeltaCommit, boolean useSchemaFromCommitMetadata)\n      throws IOException, URISyntaxException, InterruptedException {\n    Path path = new Path(hiveSyncConfig.basePath);\n    FileIOUtils.deleteDirectory(new File(hiveSyncConfig.basePath));\n    HoodieTableMetaClient.initTableType(configuration, hiveSyncConfig.basePath, HoodieTableType.MERGE_ON_READ,\n        hiveSyncConfig.tableName, HoodieAvroPayload.class.getName());\n\n    boolean result = fileSystem.mkdirs(path);\n    checkResult(result);\n    DateTime dateTime = DateTime.now();\n    HoodieCommitMetadata commitMetadata = createPartitions(numberOfPartitions, true,\n        useSchemaFromCommitMetadata, dateTime, commitTime);\n    createdTablesSet.add(hiveSyncConfig.databaseName + \".\" + hiveSyncConfig.tableName);\n    createdTablesSet\n        .add(hiveSyncConfig.databaseName + \".\" + hiveSyncConfig.tableName + HiveSyncTool.SUFFIX_SNAPSHOT_TABLE);\n    HoodieCommitMetadata compactionMetadata = new HoodieCommitMetadata();\n    commitMetadata.getPartitionToWriteStats()\n        .forEach((key, value) -> value.forEach(l -> compactionMetadata.addWriteStat(key, l)));\n    addSchemaToCommitMetadata(compactionMetadata, commitMetadata.getMetadata(HoodieCommitMetadata.SCHEMA_KEY),\n                             useSchemaFromCommitMetadata);\n    createCompactionCommitFile(compactionMetadata, commitTime);\n    if (createDeltaCommit) {\n      \r\n      HoodieCommitMetadata deltaMetadata = createLogFiles(commitMetadata.getPartitionToWriteStats(), true,\n                                                          useSchemaFromCommitMetadata);\n      createDeltaCommitFile(deltaMetadata, deltaCommitTime);\n    }\n  }\n","realPath":"hudi-sync/hudi-hive-sync/src/test/java/org/apache/hudi/hive/testutils/HiveTestUtil.java","repoName":"hudi","snippetEndLine":0,"snippetStartLine":0,"startLine":175,"status":"B"},{"authorDate":"2020-08-06 12:34:55","commitOrder":1,"curCode":"  public static void addMORPartitions(int numberOfPartitions, boolean isParquetSchemaSimple, boolean isLogSchemaSimple,\n      boolean useSchemaFromCommitMetadata, DateTime startFrom, String instantTime, String deltaCommitTime)\n      throws IOException, URISyntaxException, InterruptedException {\n    HoodieCommitMetadata commitMetadata = createPartitions(numberOfPartitions, isParquetSchemaSimple,\n        useSchemaFromCommitMetadata, startFrom, instantTime);\n    createdTablesSet.add(hiveSyncConfig.databaseName + \".\" + hiveSyncConfig.tableName + HiveSyncTool.SUFFIX_READ_OPTIMIZED_TABLE);\n    createdTablesSet.add(hiveSyncConfig.databaseName + \".\" + hiveSyncConfig.tableName + HiveSyncTool.SUFFIX_SNAPSHOT_TABLE);\n    HoodieCommitMetadata compactionMetadata = new HoodieCommitMetadata();\n    commitMetadata.getPartitionToWriteStats()\n        .forEach((key, value) -> value.forEach(l -> compactionMetadata.addWriteStat(key, l)));\n    addSchemaToCommitMetadata(compactionMetadata, commitMetadata.getMetadata(HoodieCommitMetadata.SCHEMA_KEY),\n                             useSchemaFromCommitMetadata);\n    createCompactionCommitFile(compactionMetadata, instantTime);\n    HoodieCommitMetadata deltaMetadata = createLogFiles(commitMetadata.getPartitionToWriteStats(), isLogSchemaSimple,\n        useSchemaFromCommitMetadata);\n    createDeltaCommitFile(deltaMetadata, deltaCommitTime);\n  }\n","date":"2020-08-06 12:34:55","endLine":229,"groupId":"5366","id":2,"instanceNumber":2,"isCurCommit":0,"methodName":"addMORPartitions","params":"(intnumberOfPartitions@booleanisParquetSchemaSimple@booleanisLogSchemaSimple@booleanuseSchemaFromCommitMetadata@DateTimestartFrom@StringinstantTime@StringdeltaCommitTime)","path":"/mnt/clonedata/CloneManagementServer/ManagementServer/consistResult/result-hudi-10-0.7/blobInfo/CC_OUT/blobs/d0/d1b667aea20b9592b845de9833ace73a3ed40a.src","preCode":"  public static void addMORPartitions(int numberOfPartitions, boolean isParquetSchemaSimple, boolean isLogSchemaSimple,\n      boolean useSchemaFromCommitMetadata, DateTime startFrom, String instantTime, String deltaCommitTime)\n      throws IOException, URISyntaxException, InterruptedException {\n    HoodieCommitMetadata commitMetadata = createPartitions(numberOfPartitions, isParquetSchemaSimple,\n        useSchemaFromCommitMetadata, startFrom, instantTime);\n    createdTablesSet.add(hiveSyncConfig.databaseName + \".\" + hiveSyncConfig.tableName + HiveSyncTool.SUFFIX_READ_OPTIMIZED_TABLE);\n    createdTablesSet.add(hiveSyncConfig.databaseName + \".\" + hiveSyncConfig.tableName + HiveSyncTool.SUFFIX_SNAPSHOT_TABLE);\n    HoodieCommitMetadata compactionMetadata = new HoodieCommitMetadata();\n    commitMetadata.getPartitionToWriteStats()\n        .forEach((key, value) -> value.forEach(l -> compactionMetadata.addWriteStat(key, l)));\n    addSchemaToCommitMetadata(compactionMetadata, commitMetadata.getMetadata(HoodieCommitMetadata.SCHEMA_KEY),\n                             useSchemaFromCommitMetadata);\n    createCompactionCommitFile(compactionMetadata, instantTime);\n    HoodieCommitMetadata deltaMetadata = createLogFiles(commitMetadata.getPartitionToWriteStats(), isLogSchemaSimple,\n        useSchemaFromCommitMetadata);\n    createDeltaCommitFile(deltaMetadata, deltaCommitTime);\n  }\n","realPath":"hudi-sync/hudi-hive-sync/src/test/java/org/apache/hudi/hive/testutils/HiveTestUtil.java","repoName":"hudi","snippetEndLine":0,"snippetStartLine":0,"startLine":213,"status":"B"}],"commitId":"51ea27d665d8053895dd047ca85e3338b357a81d","commitMessage":"@@@[HUDI-875] Abstract hudi-sync-common.  and support hudi-hive-sync.  hudi-dla-sync (#1810)\n\n- Generalize the hive-sync module for syncing to multiple metastores\n- Added new options for datasource\n- Added new command line for delta streamer \n\nCo-authored-by: Vinoth Chandar <vinoth@apache.org>","date":"2020-08-06 12:34:55","modifiedFileCount":"3","status":"B","submitter":"lw0090"},{"authorTime":"2020-08-06 12:34:55","codes":[{"authorDate":"2021-03-05 14:10:27","commitOrder":2,"curCode":"  public static void createMORTable(String commitTime, String deltaCommitTime, int numberOfPartitions,\n      boolean createDeltaCommit, boolean useSchemaFromCommitMetadata)\n      throws IOException, URISyntaxException, InterruptedException {\n    Path path = new Path(hiveSyncConfig.basePath);\n    FileIOUtils.deleteDirectory(new File(hiveSyncConfig.basePath));\n    HoodieTableMetaClient.withPropertyBuilder()\n      .setTableType(HoodieTableType.MERGE_ON_READ)\n      .setTableName(hiveSyncConfig.tableName)\n      .setPayloadClass(HoodieAvroPayload.class)\n      .initTable(configuration, hiveSyncConfig.basePath);\n\n    boolean result = fileSystem.mkdirs(path);\n    checkResult(result);\n    DateTime dateTime = DateTime.now();\n    HoodieCommitMetadata commitMetadata = createPartitions(numberOfPartitions, true,\n        useSchemaFromCommitMetadata, dateTime, commitTime);\n    createdTablesSet.add(hiveSyncConfig.databaseName + \".\" + hiveSyncConfig.tableName);\n    createdTablesSet\n        .add(hiveSyncConfig.databaseName + \".\" + hiveSyncConfig.tableName + HiveSyncTool.SUFFIX_SNAPSHOT_TABLE);\n    HoodieCommitMetadata compactionMetadata = new HoodieCommitMetadata();\n    commitMetadata.getPartitionToWriteStats()\n        .forEach((key, value) -> value.forEach(l -> compactionMetadata.addWriteStat(key, l)));\n    addSchemaToCommitMetadata(compactionMetadata, commitMetadata.getMetadata(HoodieCommitMetadata.SCHEMA_KEY),\n                             useSchemaFromCommitMetadata);\n    createCompactionCommitFile(compactionMetadata, commitTime);\n    if (createDeltaCommit) {\n      \r\n      HoodieCommitMetadata deltaMetadata = createLogFiles(commitMetadata.getPartitionToWriteStats(), true,\n                                                          useSchemaFromCommitMetadata);\n      createDeltaCommitFile(deltaMetadata, deltaCommitTime);\n    }\n  }\n","date":"2021-03-05 14:10:27","endLine":213,"groupId":"2194","id":3,"instanceNumber":1,"isCurCommit":0,"methodName":"createMORTable","params":"(StringcommitTime@StringdeltaCommitTime@intnumberOfPartitions@booleancreateDeltaCommit@booleanuseSchemaFromCommitMetadata)","path":"/mnt/clonedata/CloneManagementServer/ManagementServer/consistResult/result-hudi-10-0.7/blobInfo/CC_OUT/blobs/5f/eca25b134be3f2c8d1578bcdb6721cdf4035af.src","preCode":"  public static void createMORTable(String commitTime, String deltaCommitTime, int numberOfPartitions,\n      boolean createDeltaCommit, boolean useSchemaFromCommitMetadata)\n      throws IOException, URISyntaxException, InterruptedException {\n    Path path = new Path(hiveSyncConfig.basePath);\n    FileIOUtils.deleteDirectory(new File(hiveSyncConfig.basePath));\n    HoodieTableMetaClient.initTableType(configuration, hiveSyncConfig.basePath, HoodieTableType.MERGE_ON_READ,\n        hiveSyncConfig.tableName, HoodieAvroPayload.class.getName());\n\n    boolean result = fileSystem.mkdirs(path);\n    checkResult(result);\n    DateTime dateTime = DateTime.now();\n    HoodieCommitMetadata commitMetadata = createPartitions(numberOfPartitions, true,\n        useSchemaFromCommitMetadata, dateTime, commitTime);\n    createdTablesSet.add(hiveSyncConfig.databaseName + \".\" + hiveSyncConfig.tableName);\n    createdTablesSet\n        .add(hiveSyncConfig.databaseName + \".\" + hiveSyncConfig.tableName + HiveSyncTool.SUFFIX_SNAPSHOT_TABLE);\n    HoodieCommitMetadata compactionMetadata = new HoodieCommitMetadata();\n    commitMetadata.getPartitionToWriteStats()\n        .forEach((key, value) -> value.forEach(l -> compactionMetadata.addWriteStat(key, l)));\n    addSchemaToCommitMetadata(compactionMetadata, commitMetadata.getMetadata(HoodieCommitMetadata.SCHEMA_KEY),\n                             useSchemaFromCommitMetadata);\n    createCompactionCommitFile(compactionMetadata, commitTime);\n    if (createDeltaCommit) {\n      \r\n      HoodieCommitMetadata deltaMetadata = createLogFiles(commitMetadata.getPartitionToWriteStats(), true,\n                                                          useSchemaFromCommitMetadata);\n      createDeltaCommitFile(deltaMetadata, deltaCommitTime);\n    }\n  }\n","realPath":"hudi-sync/hudi-hive-sync/src/test/java/org/apache/hudi/hive/testutils/HiveTestUtil.java","repoName":"hudi","snippetEndLine":0,"snippetStartLine":0,"startLine":182,"status":"M"},{"authorDate":"2020-08-06 12:34:55","commitOrder":2,"curCode":"  public static void addMORPartitions(int numberOfPartitions, boolean isParquetSchemaSimple, boolean isLogSchemaSimple,\n      boolean useSchemaFromCommitMetadata, DateTime startFrom, String instantTime, String deltaCommitTime)\n      throws IOException, URISyntaxException, InterruptedException {\n    HoodieCommitMetadata commitMetadata = createPartitions(numberOfPartitions, isParquetSchemaSimple,\n        useSchemaFromCommitMetadata, startFrom, instantTime);\n    createdTablesSet.add(hiveSyncConfig.databaseName + \".\" + hiveSyncConfig.tableName + HiveSyncTool.SUFFIX_READ_OPTIMIZED_TABLE);\n    createdTablesSet.add(hiveSyncConfig.databaseName + \".\" + hiveSyncConfig.tableName + HiveSyncTool.SUFFIX_SNAPSHOT_TABLE);\n    HoodieCommitMetadata compactionMetadata = new HoodieCommitMetadata();\n    commitMetadata.getPartitionToWriteStats()\n        .forEach((key, value) -> value.forEach(l -> compactionMetadata.addWriteStat(key, l)));\n    addSchemaToCommitMetadata(compactionMetadata, commitMetadata.getMetadata(HoodieCommitMetadata.SCHEMA_KEY),\n                             useSchemaFromCommitMetadata);\n    createCompactionCommitFile(compactionMetadata, instantTime);\n    HoodieCommitMetadata deltaMetadata = createLogFiles(commitMetadata.getPartitionToWriteStats(), isLogSchemaSimple,\n        useSchemaFromCommitMetadata);\n    createDeltaCommitFile(deltaMetadata, deltaCommitTime);\n  }\n","date":"2020-08-06 12:34:55","endLine":229,"groupId":"5366","id":4,"instanceNumber":2,"isCurCommit":0,"methodName":"addMORPartitions","params":"(intnumberOfPartitions@booleanisParquetSchemaSimple@booleanisLogSchemaSimple@booleanuseSchemaFromCommitMetadata@DateTimestartFrom@StringinstantTime@StringdeltaCommitTime)","path":"/mnt/clonedata/CloneManagementServer/ManagementServer/consistResult/result-hudi-10-0.7/blobInfo/CC_OUT/blobs/d0/d1b667aea20b9592b845de9833ace73a3ed40a.src","preCode":"  public static void addMORPartitions(int numberOfPartitions, boolean isParquetSchemaSimple, boolean isLogSchemaSimple,\n      boolean useSchemaFromCommitMetadata, DateTime startFrom, String instantTime, String deltaCommitTime)\n      throws IOException, URISyntaxException, InterruptedException {\n    HoodieCommitMetadata commitMetadata = createPartitions(numberOfPartitions, isParquetSchemaSimple,\n        useSchemaFromCommitMetadata, startFrom, instantTime);\n    createdTablesSet.add(hiveSyncConfig.databaseName + \".\" + hiveSyncConfig.tableName + HiveSyncTool.SUFFIX_READ_OPTIMIZED_TABLE);\n    createdTablesSet.add(hiveSyncConfig.databaseName + \".\" + hiveSyncConfig.tableName + HiveSyncTool.SUFFIX_SNAPSHOT_TABLE);\n    HoodieCommitMetadata compactionMetadata = new HoodieCommitMetadata();\n    commitMetadata.getPartitionToWriteStats()\n        .forEach((key, value) -> value.forEach(l -> compactionMetadata.addWriteStat(key, l)));\n    addSchemaToCommitMetadata(compactionMetadata, commitMetadata.getMetadata(HoodieCommitMetadata.SCHEMA_KEY),\n                             useSchemaFromCommitMetadata);\n    createCompactionCommitFile(compactionMetadata, instantTime);\n    HoodieCommitMetadata deltaMetadata = createLogFiles(commitMetadata.getPartitionToWriteStats(), isLogSchemaSimple,\n        useSchemaFromCommitMetadata);\n    createDeltaCommitFile(deltaMetadata, deltaCommitTime);\n  }\n","realPath":"hudi-sync/hudi-hive-sync/src/test/java/org/apache/hudi/hive/testutils/HiveTestUtil.java","repoName":"hudi","snippetEndLine":0,"snippetStartLine":0,"startLine":213,"status":"N"}],"commitId":"bc883db5de5832fa429bbb04a35d3606fdacdb2a","commitMessage":"@@@[HUDI-1636] Support Builder Pattern To Build Table Properties For HoodieTableConfig (#2596)\n\n","date":"2021-03-05 14:10:27","modifiedFileCount":"19","status":"M","submitter":"pengzhiwei"},{"authorTime":"2020-08-06 12:34:55","codes":[{"authorDate":"2021-06-17 19:18:21","commitOrder":3,"curCode":"  public static void createMORTable(String commitTime, String deltaCommitTime, int numberOfPartitions,\n      boolean createDeltaCommit, boolean useSchemaFromCommitMetadata)\n      throws IOException, URISyntaxException, InterruptedException {\n    Path path = new Path(hiveSyncConfig.basePath);\n    FileIOUtils.deleteDirectory(new File(hiveSyncConfig.basePath));\n    HoodieTableMetaClient.withPropertyBuilder()\n      .setTableType(HoodieTableType.MERGE_ON_READ)\n      .setTableName(hiveSyncConfig.tableName)\n      .setPayloadClass(HoodieAvroPayload.class)\n      .initTable(configuration, hiveSyncConfig.basePath);\n\n    boolean result = fileSystem.mkdirs(path);\n    checkResult(result);\n    DateTime dateTime = DateTime.now();\n    HoodieCommitMetadata commitMetadata = createPartitions(numberOfPartitions, true,\n        useSchemaFromCommitMetadata, dateTime, commitTime);\n    createdTablesSet\n      .add(hiveSyncConfig.databaseName + \".\" + hiveSyncConfig.tableName + HiveSyncTool.SUFFIX_READ_OPTIMIZED_TABLE);\n    createdTablesSet\n        .add(hiveSyncConfig.databaseName + \".\" + hiveSyncConfig.tableName + HiveSyncTool.SUFFIX_SNAPSHOT_TABLE);\n    HoodieCommitMetadata compactionMetadata = new HoodieCommitMetadata();\n    commitMetadata.getPartitionToWriteStats()\n        .forEach((key, value) -> value.forEach(l -> compactionMetadata.addWriteStat(key, l)));\n    addSchemaToCommitMetadata(compactionMetadata, commitMetadata.getMetadata(HoodieCommitMetadata.SCHEMA_KEY),\n                             useSchemaFromCommitMetadata);\n    createCompactionCommitFile(compactionMetadata, commitTime);\n    if (createDeltaCommit) {\n      \r\n      HoodieCommitMetadata deltaMetadata = createLogFiles(commitMetadata.getPartitionToWriteStats(), true,\n                                                          useSchemaFromCommitMetadata);\n      createDeltaCommitFile(deltaMetadata, deltaCommitTime);\n    }\n  }\n","date":"2021-06-17 19:18:21","endLine":207,"groupId":"5366","id":5,"instanceNumber":1,"isCurCommit":0,"methodName":"createMORTable","params":"(StringcommitTime@StringdeltaCommitTime@intnumberOfPartitions@booleancreateDeltaCommit@booleanuseSchemaFromCommitMetadata)","path":"/mnt/clonedata/CloneManagementServer/ManagementServer/consistResult/result-hudi-10-0.7/blobInfo/CC_OUT/blobs/1d/6bfb4426462b93bd61d502aae941038d15cc6f.src","preCode":"  public static void createMORTable(String commitTime, String deltaCommitTime, int numberOfPartitions,\n      boolean createDeltaCommit, boolean useSchemaFromCommitMetadata)\n      throws IOException, URISyntaxException, InterruptedException {\n    Path path = new Path(hiveSyncConfig.basePath);\n    FileIOUtils.deleteDirectory(new File(hiveSyncConfig.basePath));\n    HoodieTableMetaClient.withPropertyBuilder()\n      .setTableType(HoodieTableType.MERGE_ON_READ)\n      .setTableName(hiveSyncConfig.tableName)\n      .setPayloadClass(HoodieAvroPayload.class)\n      .initTable(configuration, hiveSyncConfig.basePath);\n\n    boolean result = fileSystem.mkdirs(path);\n    checkResult(result);\n    DateTime dateTime = DateTime.now();\n    HoodieCommitMetadata commitMetadata = createPartitions(numberOfPartitions, true,\n        useSchemaFromCommitMetadata, dateTime, commitTime);\n    createdTablesSet.add(hiveSyncConfig.databaseName + \".\" + hiveSyncConfig.tableName);\n    createdTablesSet\n        .add(hiveSyncConfig.databaseName + \".\" + hiveSyncConfig.tableName + HiveSyncTool.SUFFIX_SNAPSHOT_TABLE);\n    HoodieCommitMetadata compactionMetadata = new HoodieCommitMetadata();\n    commitMetadata.getPartitionToWriteStats()\n        .forEach((key, value) -> value.forEach(l -> compactionMetadata.addWriteStat(key, l)));\n    addSchemaToCommitMetadata(compactionMetadata, commitMetadata.getMetadata(HoodieCommitMetadata.SCHEMA_KEY),\n                             useSchemaFromCommitMetadata);\n    createCompactionCommitFile(compactionMetadata, commitTime);\n    if (createDeltaCommit) {\n      \r\n      HoodieCommitMetadata deltaMetadata = createLogFiles(commitMetadata.getPartitionToWriteStats(), true,\n                                                          useSchemaFromCommitMetadata);\n      createDeltaCommitFile(deltaMetadata, deltaCommitTime);\n    }\n  }\n","realPath":"hudi-sync/hudi-hive-sync/src/test/java/org/apache/hudi/hive/testutils/HiveTestUtil.java","repoName":"hudi","snippetEndLine":0,"snippetStartLine":0,"startLine":175,"status":"M"},{"authorDate":"2020-08-06 12:34:55","commitOrder":3,"curCode":"  public static void addMORPartitions(int numberOfPartitions, boolean isParquetSchemaSimple, boolean isLogSchemaSimple,\n      boolean useSchemaFromCommitMetadata, DateTime startFrom, String instantTime, String deltaCommitTime)\n      throws IOException, URISyntaxException, InterruptedException {\n    HoodieCommitMetadata commitMetadata = createPartitions(numberOfPartitions, isParquetSchemaSimple,\n        useSchemaFromCommitMetadata, startFrom, instantTime);\n    createdTablesSet.add(hiveSyncConfig.databaseName + \".\" + hiveSyncConfig.tableName + HiveSyncTool.SUFFIX_READ_OPTIMIZED_TABLE);\n    createdTablesSet.add(hiveSyncConfig.databaseName + \".\" + hiveSyncConfig.tableName + HiveSyncTool.SUFFIX_SNAPSHOT_TABLE);\n    HoodieCommitMetadata compactionMetadata = new HoodieCommitMetadata();\n    commitMetadata.getPartitionToWriteStats()\n        .forEach((key, value) -> value.forEach(l -> compactionMetadata.addWriteStat(key, l)));\n    addSchemaToCommitMetadata(compactionMetadata, commitMetadata.getMetadata(HoodieCommitMetadata.SCHEMA_KEY),\n                             useSchemaFromCommitMetadata);\n    createCompactionCommitFile(compactionMetadata, instantTime);\n    HoodieCommitMetadata deltaMetadata = createLogFiles(commitMetadata.getPartitionToWriteStats(), isLogSchemaSimple,\n        useSchemaFromCommitMetadata);\n    createDeltaCommitFile(deltaMetadata, deltaCommitTime);\n  }\n","date":"2020-08-06 12:34:55","endLine":229,"groupId":"5366","id":6,"instanceNumber":2,"isCurCommit":0,"methodName":"addMORPartitions","params":"(intnumberOfPartitions@booleanisParquetSchemaSimple@booleanisLogSchemaSimple@booleanuseSchemaFromCommitMetadata@DateTimestartFrom@StringinstantTime@StringdeltaCommitTime)","path":"/mnt/clonedata/CloneManagementServer/ManagementServer/consistResult/result-hudi-10-0.7/blobInfo/CC_OUT/blobs/d0/d1b667aea20b9592b845de9833ace73a3ed40a.src","preCode":"  public static void addMORPartitions(int numberOfPartitions, boolean isParquetSchemaSimple, boolean isLogSchemaSimple,\n      boolean useSchemaFromCommitMetadata, DateTime startFrom, String instantTime, String deltaCommitTime)\n      throws IOException, URISyntaxException, InterruptedException {\n    HoodieCommitMetadata commitMetadata = createPartitions(numberOfPartitions, isParquetSchemaSimple,\n        useSchemaFromCommitMetadata, startFrom, instantTime);\n    createdTablesSet.add(hiveSyncConfig.databaseName + \".\" + hiveSyncConfig.tableName + HiveSyncTool.SUFFIX_READ_OPTIMIZED_TABLE);\n    createdTablesSet.add(hiveSyncConfig.databaseName + \".\" + hiveSyncConfig.tableName + HiveSyncTool.SUFFIX_SNAPSHOT_TABLE);\n    HoodieCommitMetadata compactionMetadata = new HoodieCommitMetadata();\n    commitMetadata.getPartitionToWriteStats()\n        .forEach((key, value) -> value.forEach(l -> compactionMetadata.addWriteStat(key, l)));\n    addSchemaToCommitMetadata(compactionMetadata, commitMetadata.getMetadata(HoodieCommitMetadata.SCHEMA_KEY),\n                             useSchemaFromCommitMetadata);\n    createCompactionCommitFile(compactionMetadata, instantTime);\n    HoodieCommitMetadata deltaMetadata = createLogFiles(commitMetadata.getPartitionToWriteStats(), isLogSchemaSimple,\n        useSchemaFromCommitMetadata);\n    createDeltaCommitFile(deltaMetadata, deltaCommitTime);\n  }\n","realPath":"hudi-sync/hudi-hive-sync/src/test/java/org/apache/hudi/hive/testutils/HiveTestUtil.java","repoName":"hudi","snippetEndLine":0,"snippetStartLine":0,"startLine":213,"status":"N"}],"commitId":"ad53cf450ef01806ff8b2cfe8ff76fa350a7b4c5","commitMessage":"@@@[HUDI-1879] Fix RO Tables Returning Snapshot Result (#2925)\n\n","date":"2021-06-17 19:18:21","modifiedFileCount":"4","status":"M","submitter":"pengzhiwei"},{"authorTime":"2021-08-11 11:25:41","codes":[{"authorDate":"2021-08-11 11:25:41","commitOrder":4,"curCode":"  public static void createMORTable(String commitTime, String deltaCommitTime, int numberOfPartitions,\n                                    boolean createDeltaCommit, boolean useSchemaFromCommitMetadata)\n      throws IOException, URISyntaxException, InterruptedException {\n    Path path = new Path(hiveSyncConfig.basePath);\n    FileIOUtils.deleteDirectory(new File(hiveSyncConfig.basePath));\n    HoodieTableMetaClient.withPropertyBuilder()\n        .setTableType(HoodieTableType.MERGE_ON_READ)\n        .setTableName(hiveSyncConfig.tableName)\n        .setPayloadClass(HoodieAvroPayload.class)\n        .initTable(configuration, hiveSyncConfig.basePath);\n\n    boolean result = fileSystem.mkdirs(path);\n    checkResult(result);\n    ZonedDateTime dateTime = ZonedDateTime.now();\n    HoodieCommitMetadata commitMetadata = createPartitions(numberOfPartitions, true,\n        useSchemaFromCommitMetadata, dateTime, commitTime);\n    createdTablesSet\n        .add(hiveSyncConfig.databaseName + \".\" + hiveSyncConfig.tableName + HiveSyncTool.SUFFIX_READ_OPTIMIZED_TABLE);\n    createdTablesSet\n        .add(hiveSyncConfig.databaseName + \".\" + hiveSyncConfig.tableName + HiveSyncTool.SUFFIX_SNAPSHOT_TABLE);\n    HoodieCommitMetadata compactionMetadata = new HoodieCommitMetadata();\n    commitMetadata.getPartitionToWriteStats()\n        .forEach((key, value) -> value.forEach(l -> compactionMetadata.addWriteStat(key, l)));\n    addSchemaToCommitMetadata(compactionMetadata, commitMetadata.getMetadata(HoodieCommitMetadata.SCHEMA_KEY),\n        useSchemaFromCommitMetadata);\n    createCompactionCommitFile(compactionMetadata, commitTime);\n    if (createDeltaCommit) {\n      \r\n      HoodieCommitMetadata deltaMetadata = createLogFiles(commitMetadata.getPartitionToWriteStats(), true,\n          useSchemaFromCommitMetadata);\n      createDeltaCommitFile(deltaMetadata, deltaCommitTime);\n    }\n  }\n","date":"2021-08-11 11:25:41","endLine":245,"groupId":"10349","id":7,"instanceNumber":1,"isCurCommit":1,"methodName":"createMORTable","params":"(StringcommitTime@StringdeltaCommitTime@intnumberOfPartitions@booleancreateDeltaCommit@booleanuseSchemaFromCommitMetadata)","path":"/mnt/clonedata/CloneManagementServer/ManagementServer/consistResult/result-hudi-10-0.7/blobInfo/CC_OUT/blobs/a3/bc2268dcac24706513384a719971574dd0b48b.src","preCode":"  public static void createMORTable(String commitTime, String deltaCommitTime, int numberOfPartitions,\n                                    boolean createDeltaCommit, boolean useSchemaFromCommitMetadata)\n      throws IOException, URISyntaxException, InterruptedException {\n    Path path = new Path(hiveSyncConfig.basePath);\n    FileIOUtils.deleteDirectory(new File(hiveSyncConfig.basePath));\n    HoodieTableMetaClient.withPropertyBuilder()\n        .setTableType(HoodieTableType.MERGE_ON_READ)\n        .setTableName(hiveSyncConfig.tableName)\n        .setPayloadClass(HoodieAvroPayload.class)\n        .initTable(configuration, hiveSyncConfig.basePath);\n\n    boolean result = fileSystem.mkdirs(path);\n    checkResult(result);\n    DateTime dateTime = DateTime.now();\n    HoodieCommitMetadata commitMetadata = createPartitions(numberOfPartitions, true,\n        useSchemaFromCommitMetadata, dateTime, commitTime);\n    createdTablesSet\n        .add(hiveSyncConfig.databaseName + \".\" + hiveSyncConfig.tableName + HiveSyncTool.SUFFIX_READ_OPTIMIZED_TABLE);\n    createdTablesSet\n        .add(hiveSyncConfig.databaseName + \".\" + hiveSyncConfig.tableName + HiveSyncTool.SUFFIX_SNAPSHOT_TABLE);\n    HoodieCommitMetadata compactionMetadata = new HoodieCommitMetadata();\n    commitMetadata.getPartitionToWriteStats()\n        .forEach((key, value) -> value.forEach(l -> compactionMetadata.addWriteStat(key, l)));\n    addSchemaToCommitMetadata(compactionMetadata, commitMetadata.getMetadata(HoodieCommitMetadata.SCHEMA_KEY),\n        useSchemaFromCommitMetadata);\n    createCompactionCommitFile(compactionMetadata, commitTime);\n    if (createDeltaCommit) {\n      \r\n      HoodieCommitMetadata deltaMetadata = createLogFiles(commitMetadata.getPartitionToWriteStats(), true,\n          useSchemaFromCommitMetadata);\n      createDeltaCommitFile(deltaMetadata, deltaCommitTime);\n    }\n  }\n","realPath":"hudi-sync/hudi-hive-sync/src/test/java/org/apache/hudi/hive/testutils/HiveTestUtil.java","repoName":"hudi","snippetEndLine":0,"snippetStartLine":0,"startLine":213,"status":"M"},{"authorDate":"2021-08-11 11:25:41","commitOrder":4,"curCode":"  public static void addMORPartitions(int numberOfPartitions, boolean isParquetSchemaSimple, boolean isLogSchemaSimple,\n                                      boolean useSchemaFromCommitMetadata, ZonedDateTime startFrom, String instantTime, String deltaCommitTime)\n      throws IOException, URISyntaxException, InterruptedException {\n    HoodieCommitMetadata commitMetadata = createPartitions(numberOfPartitions, isParquetSchemaSimple,\n        useSchemaFromCommitMetadata, startFrom, instantTime);\n    createdTablesSet.add(hiveSyncConfig.databaseName + \".\" + hiveSyncConfig.tableName + HiveSyncTool.SUFFIX_READ_OPTIMIZED_TABLE);\n    createdTablesSet.add(hiveSyncConfig.databaseName + \".\" + hiveSyncConfig.tableName + HiveSyncTool.SUFFIX_SNAPSHOT_TABLE);\n    HoodieCommitMetadata compactionMetadata = new HoodieCommitMetadata();\n    commitMetadata.getPartitionToWriteStats()\n        .forEach((key, value) -> value.forEach(l -> compactionMetadata.addWriteStat(key, l)));\n    addSchemaToCommitMetadata(compactionMetadata, commitMetadata.getMetadata(HoodieCommitMetadata.SCHEMA_KEY),\n        useSchemaFromCommitMetadata);\n    createCompactionCommitFile(compactionMetadata, instantTime);\n    HoodieCommitMetadata deltaMetadata = createLogFiles(commitMetadata.getPartitionToWriteStats(), isLogSchemaSimple,\n        useSchemaFromCommitMetadata);\n    createDeltaCommitFile(deltaMetadata, deltaCommitTime);\n  }\n","date":"2021-08-11 11:25:41","endLine":279,"groupId":"10349","id":8,"instanceNumber":2,"isCurCommit":1,"methodName":"addMORPartitions","params":"(intnumberOfPartitions@booleanisParquetSchemaSimple@booleanisLogSchemaSimple@booleanuseSchemaFromCommitMetadata@ZonedDateTimestartFrom@StringinstantTime@StringdeltaCommitTime)","path":"/mnt/clonedata/CloneManagementServer/ManagementServer/consistResult/result-hudi-10-0.7/blobInfo/CC_OUT/blobs/a3/bc2268dcac24706513384a719971574dd0b48b.src","preCode":"  public static void addMORPartitions(int numberOfPartitions, boolean isParquetSchemaSimple, boolean isLogSchemaSimple,\n                                      boolean useSchemaFromCommitMetadata, DateTime startFrom, String instantTime, String deltaCommitTime)\n      throws IOException, URISyntaxException, InterruptedException {\n    HoodieCommitMetadata commitMetadata = createPartitions(numberOfPartitions, isParquetSchemaSimple,\n        useSchemaFromCommitMetadata, startFrom, instantTime);\n    createdTablesSet.add(hiveSyncConfig.databaseName + \".\" + hiveSyncConfig.tableName + HiveSyncTool.SUFFIX_READ_OPTIMIZED_TABLE);\n    createdTablesSet.add(hiveSyncConfig.databaseName + \".\" + hiveSyncConfig.tableName + HiveSyncTool.SUFFIX_SNAPSHOT_TABLE);\n    HoodieCommitMetadata compactionMetadata = new HoodieCommitMetadata();\n    commitMetadata.getPartitionToWriteStats()\n        .forEach((key, value) -> value.forEach(l -> compactionMetadata.addWriteStat(key, l)));\n    addSchemaToCommitMetadata(compactionMetadata, commitMetadata.getMetadata(HoodieCommitMetadata.SCHEMA_KEY),\n        useSchemaFromCommitMetadata);\n    createCompactionCommitFile(compactionMetadata, instantTime);\n    HoodieCommitMetadata deltaMetadata = createLogFiles(commitMetadata.getPartitionToWriteStats(), isLogSchemaSimple,\n        useSchemaFromCommitMetadata);\n    createDeltaCommitFile(deltaMetadata, deltaCommitTime);\n  }\n","realPath":"hudi-sync/hudi-hive-sync/src/test/java/org/apache/hudi/hive/testutils/HiveTestUtil.java","repoName":"hudi","snippetEndLine":0,"snippetStartLine":0,"startLine":263,"status":"M"}],"commitId":"8255a86cb4d7f2173f0adcf0d752096b0b4df78c","commitMessage":"@@@[HUDI-1939] remove joda time in hivesync module (#3430)\n\n","date":"2021-08-11 11:25:41","modifiedFileCount":"5","status":"M","submitter":"Raymond Xu"}]
