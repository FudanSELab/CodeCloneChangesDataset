[{"authorTime":"2017-02-16 20:32:45","codes":[{"authorDate":"2017-02-16 20:32:45","commitOrder":1,"curCode":"  public void writeWithNewAPIHadoopFile() {\n    String outputDir = new File(tempDir, \"output\").getAbsolutePath();\n    List<Tuple2<Integer, String>> pairs = Arrays.asList(\n      new Tuple2<>(1, \"a\"),\n      new Tuple2<>(2, \"aa\"),\n      new Tuple2<>(3, \"aaa\")\n    );\n    JavaPairRDD<Integer, String> rdd = sc.parallelizePairs(pairs);\n\n    rdd.mapToPair(new PairFunction<Tuple2<Integer, String>, IntWritable, Text>() {\n      @Override\n      public Tuple2<IntWritable, Text> call(Tuple2<Integer, String> pair) {\n        return new Tuple2<>(new IntWritable(pair._1()), new Text(pair._2()));\n      }\n    }).saveAsNewAPIHadoopFile(\n        outputDir, IntWritable.class, Text.class,\n        org.apache.hadoop.mapreduce.lib.output.SequenceFileOutputFormat.class);\n\n    JavaPairRDD<IntWritable, Text> output =\n        sc.sequenceFile(outputDir, IntWritable.class, Text.class);\n    assertEquals(pairs.toString(), output.map(new Function<Tuple2<IntWritable, Text>, String>() {\n      @Override\n      public String call(Tuple2<IntWritable, Text> x) {\n        return x.toString();\n      }\n    }).collect().toString());\n  }\n","date":"2017-02-16 20:32:45","endLine":1249,"groupId":"3231","id":1,"instanceNumber":1,"isCurCommit":0,"methodName":"writeWithNewAPIHadoopFile","params":"()","path":"/mnt/clonedata/CloneManagementServer/ManagementServer/consistResult/result-spark-10-0.7/blobInfo/CC_OUT/blobs/80/aab100aced46cbe2866850ce4da7935d650b60.src","preCode":"  public void writeWithNewAPIHadoopFile() {\n    String outputDir = new File(tempDir, \"output\").getAbsolutePath();\n    List<Tuple2<Integer, String>> pairs = Arrays.asList(\n      new Tuple2<>(1, \"a\"),\n      new Tuple2<>(2, \"aa\"),\n      new Tuple2<>(3, \"aaa\")\n    );\n    JavaPairRDD<Integer, String> rdd = sc.parallelizePairs(pairs);\n\n    rdd.mapToPair(new PairFunction<Tuple2<Integer, String>, IntWritable, Text>() {\n      @Override\n      public Tuple2<IntWritable, Text> call(Tuple2<Integer, String> pair) {\n        return new Tuple2<>(new IntWritable(pair._1()), new Text(pair._2()));\n      }\n    }).saveAsNewAPIHadoopFile(\n        outputDir, IntWritable.class, Text.class,\n        org.apache.hadoop.mapreduce.lib.output.SequenceFileOutputFormat.class);\n\n    JavaPairRDD<IntWritable, Text> output =\n        sc.sequenceFile(outputDir, IntWritable.class, Text.class);\n    assertEquals(pairs.toString(), output.map(new Function<Tuple2<IntWritable, Text>, String>() {\n      @Override\n      public String call(Tuple2<IntWritable, Text> x) {\n        return x.toString();\n      }\n    }).collect().toString());\n  }\n","realPath":"core/src/test/java/test/org/apache/spark/JavaAPISuite.java","repoName":"spark","snippetEndLine":0,"snippetStartLine":0,"startLine":1223,"status":"B"},{"authorDate":"2017-02-16 20:32:45","commitOrder":1,"curCode":"  public void readWithNewAPIHadoopFile() throws IOException {\n    String outputDir = new File(tempDir, \"output\").getAbsolutePath();\n    List<Tuple2<Integer, String>> pairs = Arrays.asList(\n      new Tuple2<>(1, \"a\"),\n      new Tuple2<>(2, \"aa\"),\n      new Tuple2<>(3, \"aaa\")\n    );\n    JavaPairRDD<Integer, String> rdd = sc.parallelizePairs(pairs);\n\n    rdd.mapToPair(new PairFunction<Tuple2<Integer, String>, IntWritable, Text>() {\n      @Override\n      public Tuple2<IntWritable, Text> call(Tuple2<Integer, String> pair) {\n        return new Tuple2<>(new IntWritable(pair._1()), new Text(pair._2()));\n      }\n    }).saveAsHadoopFile(outputDir, IntWritable.class, Text.class, SequenceFileOutputFormat.class);\n\n    JavaPairRDD<IntWritable, Text> output = sc.newAPIHadoopFile(outputDir,\n        org.apache.hadoop.mapreduce.lib.input.SequenceFileInputFormat.class,\n        IntWritable.class, Text.class, Job.getInstance().getConfiguration());\n    assertEquals(pairs.toString(), output.map(new Function<Tuple2<IntWritable, Text>, String>() {\n      @Override\n      public String call(Tuple2<IntWritable, Text> x) {\n        return x.toString();\n      }\n    }).collect().toString());\n  }\n","date":"2017-02-16 20:32:45","endLine":1278,"groupId":"984","id":2,"instanceNumber":2,"isCurCommit":0,"methodName":"readWithNewAPIHadoopFile","params":"()","path":"/mnt/clonedata/CloneManagementServer/ManagementServer/consistResult/result-spark-10-0.7/blobInfo/CC_OUT/blobs/80/aab100aced46cbe2866850ce4da7935d650b60.src","preCode":"  public void readWithNewAPIHadoopFile() throws IOException {\n    String outputDir = new File(tempDir, \"output\").getAbsolutePath();\n    List<Tuple2<Integer, String>> pairs = Arrays.asList(\n      new Tuple2<>(1, \"a\"),\n      new Tuple2<>(2, \"aa\"),\n      new Tuple2<>(3, \"aaa\")\n    );\n    JavaPairRDD<Integer, String> rdd = sc.parallelizePairs(pairs);\n\n    rdd.mapToPair(new PairFunction<Tuple2<Integer, String>, IntWritable, Text>() {\n      @Override\n      public Tuple2<IntWritable, Text> call(Tuple2<Integer, String> pair) {\n        return new Tuple2<>(new IntWritable(pair._1()), new Text(pair._2()));\n      }\n    }).saveAsHadoopFile(outputDir, IntWritable.class, Text.class, SequenceFileOutputFormat.class);\n\n    JavaPairRDD<IntWritable, Text> output = sc.newAPIHadoopFile(outputDir,\n        org.apache.hadoop.mapreduce.lib.input.SequenceFileInputFormat.class,\n        IntWritable.class, Text.class, Job.getInstance().getConfiguration());\n    assertEquals(pairs.toString(), output.map(new Function<Tuple2<IntWritable, Text>, String>() {\n      @Override\n      public String call(Tuple2<IntWritable, Text> x) {\n        return x.toString();\n      }\n    }).collect().toString());\n  }\n","realPath":"core/src/test/java/test/org/apache/spark/JavaAPISuite.java","repoName":"spark","snippetEndLine":0,"snippetStartLine":0,"startLine":1253,"status":"B"}],"commitId":"0e2405490f2056728d1353abbac6f3ea177ae533","commitMessage":"@@@[SPARK-19550][BUILD][CORE][WIP] Remove Java 7 support\n\n- Move external/java8-tests tests into core.  streaming.  sql and remove\n- Remove MaxPermGen and related options\n- Fix some reflection / TODOs around Java 8+ methods\n- Update doc references to 1.7/1.8 differences\n- Remove Java 7/8 related build profiles\n- Update some plugins for better Java 8 compatibility\n- Fix a few Java-related warnings\n\nFor the future:\n\n- Update Java 8 examples to fully use Java 8\n- Update Java tests to use lambdas for simplicity\n- Update Java internal implementations to use lambdas\n\n## How was this patch tested?\n\nExisting tests\n\nAuthor: Sean Owen <sowen@cloudera.com>\n\nCloses #16871 from srowen/SPARK-19493.\n","date":"2017-02-16 20:32:45","modifiedFileCount":"51","status":"B","submitter":"Sean Owen"},{"authorTime":"2017-02-20 01:42:50","codes":[{"authorDate":"2017-02-20 01:42:50","commitOrder":2,"curCode":"  public void writeWithNewAPIHadoopFile() {\n    String outputDir = new File(tempDir, \"output\").getAbsolutePath();\n    List<Tuple2<Integer, String>> pairs = Arrays.asList(\n      new Tuple2<>(1, \"a\"),\n      new Tuple2<>(2, \"aa\"),\n      new Tuple2<>(3, \"aaa\")\n    );\n    JavaPairRDD<Integer, String> rdd = sc.parallelizePairs(pairs);\n\n    rdd.mapToPair(pair -> new Tuple2<>(new IntWritable(pair._1()), new Text(pair._2())))\n        .saveAsNewAPIHadoopFile(outputDir, IntWritable.class, Text.class,\n        org.apache.hadoop.mapreduce.lib.output.SequenceFileOutputFormat.class);\n\n    JavaPairRDD<IntWritable, Text> output =\n        sc.sequenceFile(outputDir, IntWritable.class, Text.class);\n    assertEquals(pairs.toString(), output.map(Tuple2::toString).collect().toString());\n  }\n","date":"2017-02-20 01:42:50","endLine":1077,"groupId":"10546","id":3,"instanceNumber":1,"isCurCommit":0,"methodName":"writeWithNewAPIHadoopFile","params":"()","path":"/mnt/clonedata/CloneManagementServer/ManagementServer/consistResult/result-spark-10-0.7/blobInfo/CC_OUT/blobs/51/2149127d72f4f8f7ed7438b9ba9015a07e254f.src","preCode":"  public void writeWithNewAPIHadoopFile() {\n    String outputDir = new File(tempDir, \"output\").getAbsolutePath();\n    List<Tuple2<Integer, String>> pairs = Arrays.asList(\n      new Tuple2<>(1, \"a\"),\n      new Tuple2<>(2, \"aa\"),\n      new Tuple2<>(3, \"aaa\")\n    );\n    JavaPairRDD<Integer, String> rdd = sc.parallelizePairs(pairs);\n\n    rdd.mapToPair(new PairFunction<Tuple2<Integer, String>, IntWritable, Text>() {\n      @Override\n      public Tuple2<IntWritable, Text> call(Tuple2<Integer, String> pair) {\n        return new Tuple2<>(new IntWritable(pair._1()), new Text(pair._2()));\n      }\n    }).saveAsNewAPIHadoopFile(\n        outputDir, IntWritable.class, Text.class,\n        org.apache.hadoop.mapreduce.lib.output.SequenceFileOutputFormat.class);\n\n    JavaPairRDD<IntWritable, Text> output =\n        sc.sequenceFile(outputDir, IntWritable.class, Text.class);\n    assertEquals(pairs.toString(), output.map(new Function<Tuple2<IntWritable, Text>, String>() {\n      @Override\n      public String call(Tuple2<IntWritable, Text> x) {\n        return x.toString();\n      }\n    }).collect().toString());\n  }\n","realPath":"core/src/test/java/test/org/apache/spark/JavaAPISuite.java","repoName":"spark","snippetEndLine":0,"snippetStartLine":0,"startLine":1061,"status":"M"},{"authorDate":"2017-02-20 01:42:50","commitOrder":2,"curCode":"  public void readWithNewAPIHadoopFile() throws IOException {\n    String outputDir = new File(tempDir, \"output\").getAbsolutePath();\n    List<Tuple2<Integer, String>> pairs = Arrays.asList(\n      new Tuple2<>(1, \"a\"),\n      new Tuple2<>(2, \"aa\"),\n      new Tuple2<>(3, \"aaa\")\n    );\n    JavaPairRDD<Integer, String> rdd = sc.parallelizePairs(pairs);\n\n    rdd.mapToPair(pair -> new Tuple2<>(new IntWritable(pair._1()), new Text(pair._2())))\n        .saveAsHadoopFile(outputDir, IntWritable.class, Text.class, SequenceFileOutputFormat.class);\n\n    JavaPairRDD<IntWritable, Text> output = sc.newAPIHadoopFile(outputDir,\n        org.apache.hadoop.mapreduce.lib.input.SequenceFileInputFormat.class,\n        IntWritable.class, Text.class, Job.getInstance().getConfiguration());\n    assertEquals(pairs.toString(), output.map(Tuple2::toString).collect().toString());\n  }\n","date":"2017-02-20 01:42:50","endLine":1097,"groupId":"10546","id":4,"instanceNumber":2,"isCurCommit":0,"methodName":"readWithNewAPIHadoopFile","params":"()","path":"/mnt/clonedata/CloneManagementServer/ManagementServer/consistResult/result-spark-10-0.7/blobInfo/CC_OUT/blobs/51/2149127d72f4f8f7ed7438b9ba9015a07e254f.src","preCode":"  public void readWithNewAPIHadoopFile() throws IOException {\n    String outputDir = new File(tempDir, \"output\").getAbsolutePath();\n    List<Tuple2<Integer, String>> pairs = Arrays.asList(\n      new Tuple2<>(1, \"a\"),\n      new Tuple2<>(2, \"aa\"),\n      new Tuple2<>(3, \"aaa\")\n    );\n    JavaPairRDD<Integer, String> rdd = sc.parallelizePairs(pairs);\n\n    rdd.mapToPair(new PairFunction<Tuple2<Integer, String>, IntWritable, Text>() {\n      @Override\n      public Tuple2<IntWritable, Text> call(Tuple2<Integer, String> pair) {\n        return new Tuple2<>(new IntWritable(pair._1()), new Text(pair._2()));\n      }\n    }).saveAsHadoopFile(outputDir, IntWritable.class, Text.class, SequenceFileOutputFormat.class);\n\n    JavaPairRDD<IntWritable, Text> output = sc.newAPIHadoopFile(outputDir,\n        org.apache.hadoop.mapreduce.lib.input.SequenceFileInputFormat.class,\n        IntWritable.class, Text.class, Job.getInstance().getConfiguration());\n    assertEquals(pairs.toString(), output.map(new Function<Tuple2<IntWritable, Text>, String>() {\n      @Override\n      public String call(Tuple2<IntWritable, Text> x) {\n        return x.toString();\n      }\n    }).collect().toString());\n  }\n","realPath":"core/src/test/java/test/org/apache/spark/JavaAPISuite.java","repoName":"spark","snippetEndLine":0,"snippetStartLine":0,"startLine":1081,"status":"M"}],"commitId":"1487c9af20a333ead55955acf4c0aa323bea0d07","commitMessage":"@@@[SPARK-19534][TESTS] Convert Java tests to use lambdas.  Java 8 features\n\n## What changes were proposed in this pull request?\n\nConvert tests to use Java 8 lambdas.  and modest related fixes to surrounding code.\n\n## How was this patch tested?\n\nJenkins tests\n\nAuthor: Sean Owen <sowen@cloudera.com>\n\nCloses #16964 from srowen/SPARK-19534.\n","date":"2017-02-20 01:42:50","modifiedFileCount":"45","status":"M","submitter":"Sean Owen"}]
