[{"authorTime":"2020-12-24 02:42:18","codes":[{"authorDate":"2020-11-10 01:00:52","commitOrder":2,"curCode":"  public void testFinalizeWithMultipleReducePartitions() throws IOException {\n    PushBlock[] pushBlocks = new PushBlock[] {\n      new PushBlock(0, 0, 0, ByteBuffer.wrap(new byte[2])),\n      new PushBlock(0, 1, 0, ByteBuffer.wrap(new byte[3])),\n      new PushBlock(0, 0, 1, ByteBuffer.wrap(new byte[5])),\n      new PushBlock(0, 1, 1, ByteBuffer.wrap(new byte[3]))\n    };\n    pushBlockHelper(TEST_APP, pushBlocks);\n    MergeStatuses statuses = pushResolver.finalizeShuffleMerge(\n      new FinalizeShuffleMerge(TEST_APP, 0));\n    validateMergeStatuses(statuses, new int[] {0, 1}, new long[] {5, 8});\n    MergedBlockMeta meta = pushResolver.getMergedBlockMeta(TEST_APP, 0, 0);\n    validateChunks(TEST_APP, 0, 0, meta, new int[]{5}, new int[][]{{0, 1}});\n  }\n","date":"2020-11-10 01:00:52","endLine":142,"groupId":"2427","id":1,"instanceNumber":1,"isCurCommit":0,"methodName":"testFinalizeWithMultipleReducePartitions","params":"()","path":"/mnt/clonedata/CloneManagementServer/ManagementServer/consistResult/result-spark-10-0.7/blobInfo/CC_OUT/blobs/0f/200dc721963b1871eeac1958230b6150bf5b83.src","preCode":"  public void testFinalizeWithMultipleReducePartitions() throws IOException {\n    PushBlock[] pushBlocks = new PushBlock[] {\n      new PushBlock(0, 0, 0, ByteBuffer.wrap(new byte[2])),\n      new PushBlock(0, 1, 0, ByteBuffer.wrap(new byte[3])),\n      new PushBlock(0, 0, 1, ByteBuffer.wrap(new byte[5])),\n      new PushBlock(0, 1, 1, ByteBuffer.wrap(new byte[3]))\n    };\n    pushBlockHelper(TEST_APP, pushBlocks);\n    MergeStatuses statuses = pushResolver.finalizeShuffleMerge(\n      new FinalizeShuffleMerge(TEST_APP, 0));\n    validateMergeStatuses(statuses, new int[] {0, 1}, new long[] {5, 8});\n    MergedBlockMeta meta = pushResolver.getMergedBlockMeta(TEST_APP, 0, 0);\n    validateChunks(TEST_APP, 0, 0, meta, new int[]{5}, new int[][]{{0, 1}});\n  }\n","realPath":"common/network-shuffle/src/test/java/org/apache/spark/network/shuffle/RemoteBlockPushResolverSuite.java","repoName":"spark","snippetEndLine":0,"snippetStartLine":0,"startLine":129,"status":"NB"},{"authorDate":"2020-12-24 02:42:18","commitOrder":2,"curCode":"  public void testFailureWhileTruncatingFiles() throws IOException {\n    useTestFiles(true, false);\n    PushBlock[] pushBlocks = new PushBlock[] {\n      new PushBlock(0, 0, 0, ByteBuffer.wrap(new byte[2])),\n      new PushBlock(0, 1, 0, ByteBuffer.wrap(new byte[3])),\n      new PushBlock(0, 0, 1, ByteBuffer.wrap(new byte[5])),\n      new PushBlock(0, 1, 1, ByteBuffer.wrap(new byte[3]))\n    };\n    pushBlockHelper(TEST_APP, pushBlocks);\n    RemoteBlockPushResolver.PushBlockStreamCallback callback =\n      (RemoteBlockPushResolver.PushBlockStreamCallback) pushResolver.receiveBlockDataAsStream(\n        new PushBlockStream(TEST_APP, 0, 2, 0, 0));\n    callback.onData(callback.getID(), ByteBuffer.wrap(new byte[2]));\n    callback.onComplete(callback.getID());\n    RemoteBlockPushResolver.AppShufflePartitionInfo partitionInfo = callback.getPartitionInfo();\n    TestMergeShuffleFile testIndexFile = (TestMergeShuffleFile) partitionInfo.getIndexFile();\n    \r\n    testIndexFile.close();\n    MergeStatuses statuses = pushResolver.finalizeShuffleMerge(\n      new FinalizeShuffleMerge(TEST_APP, 0));\n    validateMergeStatuses(statuses, new int[] {1}, new long[] {8});\n    MergedBlockMeta meta = pushResolver.getMergedBlockMeta(TEST_APP, 0, 1);\n    validateChunks(TEST_APP, 0, 1, meta, new int[]{5, 3}, new int[][]{{0},{1}});\n  }\n","date":"2020-12-24 02:42:18","endLine":741,"groupId":"2427","id":2,"instanceNumber":2,"isCurCommit":0,"methodName":"testFailureWhileTruncatingFiles","params":"()","path":"/mnt/clonedata/CloneManagementServer/ManagementServer/consistResult/result-spark-10-0.7/blobInfo/CC_OUT/blobs/8c/6f7434748ecc4df4f204686248deebc4c39edd.src","preCode":"  public void testFailureWhileTruncatingFiles() throws IOException {\n    useTestFiles(true, false);\n    PushBlock[] pushBlocks = new PushBlock[] {\n      new PushBlock(0, 0, 0, ByteBuffer.wrap(new byte[2])),\n      new PushBlock(0, 1, 0, ByteBuffer.wrap(new byte[3])),\n      new PushBlock(0, 0, 1, ByteBuffer.wrap(new byte[5])),\n      new PushBlock(0, 1, 1, ByteBuffer.wrap(new byte[3]))\n    };\n    pushBlockHelper(TEST_APP, pushBlocks);\n    RemoteBlockPushResolver.PushBlockStreamCallback callback =\n      (RemoteBlockPushResolver.PushBlockStreamCallback) pushResolver.receiveBlockDataAsStream(\n        new PushBlockStream(TEST_APP, 0, 2, 0, 0));\n    callback.onData(callback.getID(), ByteBuffer.wrap(new byte[2]));\n    callback.onComplete(callback.getID());\n    RemoteBlockPushResolver.AppShufflePartitionInfo partitionInfo = callback.getPartitionInfo();\n    TestMergeShuffleFile testIndexFile = (TestMergeShuffleFile) partitionInfo.getIndexFile();\n    \r\n    testIndexFile.close();\n    MergeStatuses statuses = pushResolver.finalizeShuffleMerge(\n      new FinalizeShuffleMerge(TEST_APP, 0));\n    validateMergeStatuses(statuses, new int[] {1}, new long[] {8});\n    MergedBlockMeta meta = pushResolver.getMergedBlockMeta(TEST_APP, 0, 1);\n    validateChunks(TEST_APP, 0, 1, meta, new int[]{5, 3}, new int[][]{{0},{1}});\n  }\n","realPath":"common/network-shuffle/src/test/java/org/apache/spark/network/shuffle/RemoteBlockPushResolverSuite.java","repoName":"spark","snippetEndLine":0,"snippetStartLine":0,"startLine":718,"status":"B"}],"commitId":"0677c39009de0830d995da77332f0756c76d6b56","commitMessage":"@@@[SPARK-32916][SHUFFLE][TEST-MAVEN][TEST-HADOOP2.7] Ensure the number of chunks in meta file and index file are equal\n\n\n What changes were proposed in this pull request?\n1. Fixes for bugs in `RemoteBlockPushResolver` where the number of chunks in meta file and index file are inconsistent due to exceptions while writing to either index file or meta file. This java class was introduced in https://github.com/apache/spark/pull/30062.\n - If the writing to index file fails.  the position of meta file is not reset. This means that the number of chunks in meta file is inconsistent with index file.\n- During the exception handling while writing to index/meta file.  we just set the pointer to the start position. If the files are closed just after this then it doesn't get rid of any the extra bytes written to it.\n2. Adds an IOException threshold. If the `RemoteBlockPushResolver` encounters IOExceptions greater than this threshold  while updating data/meta/index file of a shuffle partition.  then it responds to the client with  exception- `IOExceptions exceeded the threshold` so that client can stop pushing data for this shuffle partition.\n3. When the update to metadata fails.  exception is not propagated back to the client. This results in the increased size of the current chunk. However.  with (2) in place.  the current chunk will still be of a manageable size.\n\n\n Why are the changes needed?\nThis fix is needed for the bugs mentioned above.\n1. Moved writing to meta file after index file. This fixes the issue because if there is an exception writing to meta file.  then the index file position is not updated. With this change.  if there is an exception writing to index file.  then none of the files are effectively updated and the same is true vice-versa.\n2. Truncating the lengths of data/index/meta files when the partition is finalized.\n3. When the IOExceptions have reached the threshold.  it is most likely that future blocks will also face the issue. So.  it is better to let the clients know so that they can stop pushing the blocks for that partition.\n4. When just the meta update fails.  client retries pushing the block which was successfully merged to data file. This can be avoided by letting the chunk grow slightly.\n\n\n Does this PR introduce _any_ user-facing change?\nNo\n\n\n How was this patch tested?\nAdded unit tests for all the bugs and threshold.\n\nCloses #30433 from otterc/SPARK-32916-followup.\n\nAuthored-by: Chandni Singh <singh.chandni@gmail.com>\nSigned-off-by: Mridul Muralidharan <mridul<at>gmail.com>\n","date":"2020-12-24 02:42:18","modifiedFileCount":"4","status":"M","submitter":"Chandni Singh"},{"authorTime":"2021-07-20 13:03:30","codes":[{"authorDate":"2021-07-20 13:03:30","commitOrder":3,"curCode":"  public void testFinalizeWithMultipleReducePartitions() throws IOException {\n    PushBlock[] pushBlocks = new PushBlock[] {\n      new PushBlock(0, 0, 0, ByteBuffer.wrap(new byte[2])),\n      new PushBlock(0, 1, 0, ByteBuffer.wrap(new byte[3])),\n      new PushBlock(0, 0, 1, ByteBuffer.wrap(new byte[5])),\n      new PushBlock(0, 1, 1, ByteBuffer.wrap(new byte[3]))\n    };\n    pushBlockHelper(TEST_APP, NO_ATTEMPT_ID, pushBlocks);\n    MergeStatuses statuses = pushResolver.finalizeShuffleMerge(\n      new FinalizeShuffleMerge(TEST_APP, NO_ATTEMPT_ID, 0));\n    validateMergeStatuses(statuses, new int[] {0, 1}, new long[] {5, 8});\n    MergedBlockMeta meta = pushResolver.getMergedBlockMeta(TEST_APP, 0, 0);\n    validateChunks(TEST_APP, 0, 0, meta, new int[]{5}, new int[][]{{0, 1}});\n  }\n","date":"2021-07-20 13:03:30","endLine":160,"groupId":"2427","id":3,"instanceNumber":1,"isCurCommit":0,"methodName":"testFinalizeWithMultipleReducePartitions","params":"()","path":"/mnt/clonedata/CloneManagementServer/ManagementServer/consistResult/result-spark-10-0.7/blobInfo/CC_OUT/blobs/2a/73aa56b2d28be4c2d5bb4368259993d6dbd7a4.src","preCode":"  public void testFinalizeWithMultipleReducePartitions() throws IOException {\n    PushBlock[] pushBlocks = new PushBlock[] {\n      new PushBlock(0, 0, 0, ByteBuffer.wrap(new byte[2])),\n      new PushBlock(0, 1, 0, ByteBuffer.wrap(new byte[3])),\n      new PushBlock(0, 0, 1, ByteBuffer.wrap(new byte[5])),\n      new PushBlock(0, 1, 1, ByteBuffer.wrap(new byte[3]))\n    };\n    pushBlockHelper(TEST_APP, pushBlocks);\n    MergeStatuses statuses = pushResolver.finalizeShuffleMerge(\n      new FinalizeShuffleMerge(TEST_APP, 0));\n    validateMergeStatuses(statuses, new int[] {0, 1}, new long[] {5, 8});\n    MergedBlockMeta meta = pushResolver.getMergedBlockMeta(TEST_APP, 0, 0);\n    validateChunks(TEST_APP, 0, 0, meta, new int[]{5}, new int[][]{{0, 1}});\n  }\n","realPath":"common/network-shuffle/src/test/java/org/apache/spark/network/shuffle/RemoteBlockPushResolverSuite.java","repoName":"spark","snippetEndLine":0,"snippetStartLine":0,"startLine":147,"status":"M"},{"authorDate":"2021-07-20 13:03:30","commitOrder":3,"curCode":"  public void testFailureWhileTruncatingFiles() throws IOException {\n    useTestFiles(true, false);\n    PushBlock[] pushBlocks = new PushBlock[] {\n      new PushBlock(0, 0, 0, ByteBuffer.wrap(new byte[2])),\n      new PushBlock(0, 1, 0, ByteBuffer.wrap(new byte[3])),\n      new PushBlock(0, 0, 1, ByteBuffer.wrap(new byte[5])),\n      new PushBlock(0, 1, 1, ByteBuffer.wrap(new byte[3]))\n    };\n    pushBlockHelper(TEST_APP, NO_ATTEMPT_ID, pushBlocks);\n    RemoteBlockPushResolver.PushBlockStreamCallback callback =\n      (RemoteBlockPushResolver.PushBlockStreamCallback) pushResolver.receiveBlockDataAsStream(\n        new PushBlockStream(TEST_APP, NO_ATTEMPT_ID, 0, 2, 0, 0));\n    callback.onData(callback.getID(), ByteBuffer.wrap(new byte[2]));\n    callback.onComplete(callback.getID());\n    RemoteBlockPushResolver.AppShufflePartitionInfo partitionInfo = callback.getPartitionInfo();\n    TestMergeShuffleFile testIndexFile = (TestMergeShuffleFile) partitionInfo.getIndexFile();\n    \r\n    testIndexFile.close();\n    MergeStatuses statuses = pushResolver.finalizeShuffleMerge(\n      new FinalizeShuffleMerge(TEST_APP, NO_ATTEMPT_ID, 0));\n    validateMergeStatuses(statuses, new int[] {1}, new long[] {8});\n    MergedBlockMeta meta = pushResolver.getMergedBlockMeta(TEST_APP, 0, 1);\n    validateChunks(TEST_APP, 0, 1, meta, new int[]{5, 3}, new int[][]{{0},{1}});\n  }\n","date":"2021-07-20 13:03:30","endLine":843,"groupId":"2427","id":4,"instanceNumber":2,"isCurCommit":0,"methodName":"testFailureWhileTruncatingFiles","params":"()","path":"/mnt/clonedata/CloneManagementServer/ManagementServer/consistResult/result-spark-10-0.7/blobInfo/CC_OUT/blobs/2a/73aa56b2d28be4c2d5bb4368259993d6dbd7a4.src","preCode":"  public void testFailureWhileTruncatingFiles() throws IOException {\n    useTestFiles(true, false);\n    PushBlock[] pushBlocks = new PushBlock[] {\n      new PushBlock(0, 0, 0, ByteBuffer.wrap(new byte[2])),\n      new PushBlock(0, 1, 0, ByteBuffer.wrap(new byte[3])),\n      new PushBlock(0, 0, 1, ByteBuffer.wrap(new byte[5])),\n      new PushBlock(0, 1, 1, ByteBuffer.wrap(new byte[3]))\n    };\n    pushBlockHelper(TEST_APP, pushBlocks);\n    RemoteBlockPushResolver.PushBlockStreamCallback callback =\n      (RemoteBlockPushResolver.PushBlockStreamCallback) pushResolver.receiveBlockDataAsStream(\n        new PushBlockStream(TEST_APP, 0, 2, 0, 0));\n    callback.onData(callback.getID(), ByteBuffer.wrap(new byte[2]));\n    callback.onComplete(callback.getID());\n    RemoteBlockPushResolver.AppShufflePartitionInfo partitionInfo = callback.getPartitionInfo();\n    TestMergeShuffleFile testIndexFile = (TestMergeShuffleFile) partitionInfo.getIndexFile();\n    \r\n    testIndexFile.close();\n    MergeStatuses statuses = pushResolver.finalizeShuffleMerge(\n      new FinalizeShuffleMerge(TEST_APP, 0));\n    validateMergeStatuses(statuses, new int[] {1}, new long[] {8});\n    MergedBlockMeta meta = pushResolver.getMergedBlockMeta(TEST_APP, 0, 1);\n    validateChunks(TEST_APP, 0, 1, meta, new int[]{5, 3}, new int[][]{{0},{1}});\n  }\n","realPath":"common/network-shuffle/src/test/java/org/apache/spark/network/shuffle/RemoteBlockPushResolverSuite.java","repoName":"spark","snippetEndLine":0,"snippetStartLine":0,"startLine":820,"status":"M"}],"commitId":"c77acf0bbc25341de2636649fdd76f9bb4bdf4ed","commitMessage":"@@@[SPARK-35546][SHUFFLE] Enable push-based shuffle when multiple app attempts are enabled and manage concurrent access to the state in a better way\n\n\n What changes were proposed in this pull request?\nThis is one of the patches for SPIP SPARK-30602 which is needed for push-based shuffle.\n\n\n Summary of the change:\nWhen Executor registers with Shuffle Service.  it will encode the merged shuffle dir created and also the application attemptId into the ShuffleManagerMeta into Json. Then in Shuffle Service.  it will decode the Json string and get the correct merged shuffle dir and also the attemptId. If the registration comes from a newer attempt.  the merged shuffle information will be updated to store the information from the newer attempt.\n\nThis PR also refactored the management of the merged shuffle information to avoid concurrency issues.\n\n Why are the changes needed?\nRefer to the SPIP in SPARK-30602.\n\n\n Does this PR introduce _any_ user-facing change?\nNo.\n\n\n How was this patch tested?\nAdded unit tests.\nThe reference PR with the consolidated changes covering the complete implementation is also provided in SPARK-30602.\nWe have already verified the functionality and the improved performance as documented in the SPIP doc.\n\nCloses #33078 from zhouyejoe/SPARK-35546.\n\nAuthored-by: Ye Zhou <yezhou@linkedin.com>\nSigned-off-by: Mridul Muralidharan <mridul<at>gmail.com>\n","date":"2021-07-20 13:03:30","modifiedFileCount":"10","status":"M","submitter":"Ye Zhou"},{"authorTime":"2021-08-02 12:16:33","codes":[{"authorDate":"2021-08-02 12:16:33","commitOrder":4,"curCode":"  public void testFinalizeWithMultipleReducePartitions() throws IOException {\n    PushBlock[] pushBlocks = new PushBlock[] {\n      new PushBlock(0, 0, 0, 0, ByteBuffer.wrap(new byte[2])),\n      new PushBlock(0, 0, 1, 0, ByteBuffer.wrap(new byte[3])),\n      new PushBlock(0, 0, 0, 1, ByteBuffer.wrap(new byte[5])),\n      new PushBlock(0, 0, 1, 1, ByteBuffer.wrap(new byte[3]))\n    };\n    pushBlockHelper(TEST_APP, NO_ATTEMPT_ID, pushBlocks);\n    MergeStatuses statuses = pushResolver.finalizeShuffleMerge(\n      new FinalizeShuffleMerge(TEST_APP, NO_ATTEMPT_ID, 0, 0));\n    validateMergeStatuses(statuses, new int[] {0, 1}, new long[] {5, 8});\n    MergedBlockMeta meta = pushResolver.getMergedBlockMeta(TEST_APP, 0, 0, 0);\n    validateChunks(TEST_APP, 0, 0, 0, meta, new int[]{5}, new int[][]{{0, 1}});\n  }\n","date":"2021-08-02 12:16:33","endLine":160,"groupId":"10108","id":5,"instanceNumber":1,"isCurCommit":0,"methodName":"testFinalizeWithMultipleReducePartitions","params":"()","path":"/mnt/clonedata/CloneManagementServer/ManagementServer/consistResult/result-spark-10-0.7/blobInfo/CC_OUT/blobs/c6/9e57d94c3c9a3236cf68f4d94f39ae39370866.src","preCode":"  public void testFinalizeWithMultipleReducePartitions() throws IOException {\n    PushBlock[] pushBlocks = new PushBlock[] {\n      new PushBlock(0, 0, 0, ByteBuffer.wrap(new byte[2])),\n      new PushBlock(0, 1, 0, ByteBuffer.wrap(new byte[3])),\n      new PushBlock(0, 0, 1, ByteBuffer.wrap(new byte[5])),\n      new PushBlock(0, 1, 1, ByteBuffer.wrap(new byte[3]))\n    };\n    pushBlockHelper(TEST_APP, NO_ATTEMPT_ID, pushBlocks);\n    MergeStatuses statuses = pushResolver.finalizeShuffleMerge(\n      new FinalizeShuffleMerge(TEST_APP, NO_ATTEMPT_ID, 0));\n    validateMergeStatuses(statuses, new int[] {0, 1}, new long[] {5, 8});\n    MergedBlockMeta meta = pushResolver.getMergedBlockMeta(TEST_APP, 0, 0);\n    validateChunks(TEST_APP, 0, 0, meta, new int[]{5}, new int[][]{{0, 1}});\n  }\n","realPath":"common/network-shuffle/src/test/java/org/apache/spark/network/shuffle/RemoteBlockPushResolverSuite.java","repoName":"spark","snippetEndLine":0,"snippetStartLine":0,"startLine":147,"status":"M"},{"authorDate":"2021-08-02 12:16:33","commitOrder":4,"curCode":"  public void testFailureWhileTruncatingFiles() throws IOException {\n    useTestFiles(true, false);\n    PushBlock[] pushBlocks = new PushBlock[] {\n      new PushBlock(0, 0, 0, 0, ByteBuffer.wrap(new byte[2])),\n      new PushBlock(0, 0, 1, 0, ByteBuffer.wrap(new byte[3])),\n      new PushBlock(0, 0, 0, 1, ByteBuffer.wrap(new byte[5])),\n      new PushBlock(0, 0, 1, 1, ByteBuffer.wrap(new byte[3]))\n    };\n    pushBlockHelper(TEST_APP, NO_ATTEMPT_ID, pushBlocks);\n    RemoteBlockPushResolver.PushBlockStreamCallback callback =\n      (RemoteBlockPushResolver.PushBlockStreamCallback) pushResolver.receiveBlockDataAsStream(\n        new PushBlockStream(TEST_APP, NO_ATTEMPT_ID, 0, 0, 2, 0, 0));\n    callback.onData(callback.getID(), ByteBuffer.wrap(new byte[2]));\n    callback.onComplete(callback.getID());\n    RemoteBlockPushResolver.AppShufflePartitionInfo partitionInfo = callback.getPartitionInfo();\n    TestMergeShuffleFile testIndexFile = (TestMergeShuffleFile) partitionInfo.getIndexFile();\n    \r\n    testIndexFile.close();\n    MergeStatuses statuses = pushResolver.finalizeShuffleMerge(\n      new FinalizeShuffleMerge(TEST_APP, NO_ATTEMPT_ID, 0, 0));\n    validateMergeStatuses(statuses, new int[] {1}, new long[] {8});\n    MergedBlockMeta meta = pushResolver.getMergedBlockMeta(TEST_APP, 0, 0, 1);\n    validateChunks(TEST_APP, 0, 0, 1, meta, new int[]{5, 3}, new int[][]{{0},{1}});\n  }\n","date":"2021-08-02 12:16:33","endLine":844,"groupId":"10108","id":6,"instanceNumber":2,"isCurCommit":0,"methodName":"testFailureWhileTruncatingFiles","params":"()","path":"/mnt/clonedata/CloneManagementServer/ManagementServer/consistResult/result-spark-10-0.7/blobInfo/CC_OUT/blobs/c6/9e57d94c3c9a3236cf68f4d94f39ae39370866.src","preCode":"  public void testFailureWhileTruncatingFiles() throws IOException {\n    useTestFiles(true, false);\n    PushBlock[] pushBlocks = new PushBlock[] {\n      new PushBlock(0, 0, 0, ByteBuffer.wrap(new byte[2])),\n      new PushBlock(0, 1, 0, ByteBuffer.wrap(new byte[3])),\n      new PushBlock(0, 0, 1, ByteBuffer.wrap(new byte[5])),\n      new PushBlock(0, 1, 1, ByteBuffer.wrap(new byte[3]))\n    };\n    pushBlockHelper(TEST_APP, NO_ATTEMPT_ID, pushBlocks);\n    RemoteBlockPushResolver.PushBlockStreamCallback callback =\n      (RemoteBlockPushResolver.PushBlockStreamCallback) pushResolver.receiveBlockDataAsStream(\n        new PushBlockStream(TEST_APP, NO_ATTEMPT_ID, 0, 2, 0, 0));\n    callback.onData(callback.getID(), ByteBuffer.wrap(new byte[2]));\n    callback.onComplete(callback.getID());\n    RemoteBlockPushResolver.AppShufflePartitionInfo partitionInfo = callback.getPartitionInfo();\n    TestMergeShuffleFile testIndexFile = (TestMergeShuffleFile) partitionInfo.getIndexFile();\n    \r\n    testIndexFile.close();\n    MergeStatuses statuses = pushResolver.finalizeShuffleMerge(\n      new FinalizeShuffleMerge(TEST_APP, NO_ATTEMPT_ID, 0));\n    validateMergeStatuses(statuses, new int[] {1}, new long[] {8});\n    MergedBlockMeta meta = pushResolver.getMergedBlockMeta(TEST_APP, 0, 1);\n    validateChunks(TEST_APP, 0, 1, meta, new int[]{5, 3}, new int[][]{{0},{1}});\n  }\n","realPath":"common/network-shuffle/src/test/java/org/apache/spark/network/shuffle/RemoteBlockPushResolverSuite.java","repoName":"spark","snippetEndLine":0,"snippetStartLine":0,"startLine":821,"status":"M"}],"commitId":"c039d998128dd0dab27f43e7de083a71b9d1cfcf","commitMessage":"@@@[SPARK-32923][CORE][SHUFFLE] Handle indeterminate stage retries for push-based shuffle\n\n\n What changes were proposed in this pull request?\n[[SPARK-23243](https://issues.apache.org/jira/browse/SPARK-23243)] and [[SPARK-25341](https://issues.apache.org/jira/browse/SPARK-25341)] addressed cases of stage retries for indeterminate stage involving operations like repartition. This PR addresses the same issues in the context of push-based shuffle. Currently there is no way to distinguish the current execution of a stage for a shuffle ID. Therefore the changes explained below are necessary.\n\nCore changes are summarized as follows:\n\n1. Introduce a new variable `shuffleMergeId` in `ShuffleDependency` which is monotonically increasing value tracking the temporal ordering of execution of <stage-id.  stage-attempt-id> for a shuffle ID.\n2. Correspondingly make changes in the push-based shuffle protocol layer in `MergedShuffleFileManager`.  `BlockStoreClient` passing the `shuffleMergeId` in order to keep track of the shuffle output in separate files on the shuffle service side.\n3. `DAGScheduler` increments the `shuffleMergeId` tracked in `ShuffleDependency` in the cases of a indeterministic stage execution\n4. Deterministic stage will have `shuffleMergeId` set to 0 as no special handling is needed in this case and indeterminate stage will have `shuffleMergeId` starting from 1.\n\n\n Why are the changes needed?\n\nNew protocol changes are needed due to the reasons explained above.\n\n\n Does this PR introduce _any_ user-facing change?\n\nNo\n\n\n How was this patch tested?\nAdded new unit tests in `RemoteBlockPushResolverSuite.  DAGSchedulerSuite.  BlockIdSuite.  ErrorHandlerSuite`\n\nCloses #33034 from venkata91/SPARK-32923.\n\nAuthored-by: Venkata krishnan Sowrirajan <vsowrirajan@linkedin.com>\nSigned-off-by: Mridul Muralidharan <mridul<at>gmail.com>\n","date":"2021-08-02 12:16:33","modifiedFileCount":"22","status":"M","submitter":"Venkata krishnan Sowrirajan"}]
