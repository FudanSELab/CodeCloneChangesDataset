[{"authorTime":"2021-08-20 04:36:40","codes":[{"authorDate":"2021-08-20 04:36:40","commitOrder":5,"curCode":"  public void testUpgradeOneToTwo(HoodieTableType tableType) throws IOException {\n    \r\n    Map<String, String> params = new HashMap<>();\n    addNewTableParamsToProps(params);\n    if (tableType == HoodieTableType.MERGE_ON_READ) {\n      params.put(TYPE.key(), HoodieTableType.MERGE_ON_READ.name());\n      metaClient = HoodieTestUtils.init(hadoopConf, basePath, HoodieTableType.MERGE_ON_READ);\n    }\n    HoodieWriteConfig cfg = getConfigBuilder().withAutoCommit(false).withRollbackUsingMarkers(false).withProps(params).build();\n    SparkRDDWriteClient client = getHoodieWriteClient(cfg);\n    \r\n    doInsert(client);\n\n    \r\n    downgradeTableConfigsFromTwoToOne(cfg);\n\n    \r\n    new SparkUpgradeDowngrade(metaClient, cfg, context).run(metaClient, HoodieTableVersion.TWO, cfg, context, null);\n\n    \r\n    metaClient = HoodieTableMetaClient.builder().setConf(context.getHadoopConf().get()).setBasePath(cfg.getBasePath())\n        .setLayoutVersion(Option.of(new TimelineLayoutVersion(cfg.getTimelineLayoutVersion()))).build();\n    assertEquals(metaClient.getTableConfig().getTableVersion().versionCode(), HoodieTableVersion.TWO.versionCode());\n    assertTableVersionFromPropertyFile(HoodieTableVersion.TWO);\n\n    \r\n    assertTableProps(cfg);\n  }\n","date":"2021-08-20 04:36:40","endLine":231,"groupId":"6004","id":1,"instanceNumber":1,"isCurCommit":0,"methodName":"testUpgradeOneToTwo","params":"(HoodieTableTypetableType)","path":"/mnt/clonedata/CloneManagementServer/ManagementServer/consistResult/result-hudi-10-0.7/blobInfo/CC_OUT/blobs/12/3a33c3d462bbb7017e875bbc7d5571941119de.src","preCode":"  public void testUpgradeOneToTwo(HoodieTableType tableType) throws IOException {\n    \r\n    Map<String, String> params = new HashMap<>();\n    addNewTableParamsToProps(params);\n    if (tableType == HoodieTableType.MERGE_ON_READ) {\n      params.put(TYPE.key(), HoodieTableType.MERGE_ON_READ.name());\n      metaClient = HoodieTestUtils.init(hadoopConf, basePath, HoodieTableType.MERGE_ON_READ);\n    }\n    HoodieWriteConfig cfg = getConfigBuilder().withAutoCommit(false).withRollbackUsingMarkers(false).withProps(params).build();\n    SparkRDDWriteClient client = getHoodieWriteClient(cfg);\n    \r\n    doInsert(client);\n\n    \r\n    downgradeTableConfigsFromTwoToOne(cfg);\n\n    \r\n    new SparkUpgradeDowngrade(metaClient, cfg, context).run(metaClient, HoodieTableVersion.TWO, cfg, context, null);\n\n    \r\n    metaClient = HoodieTableMetaClient.builder().setConf(context.getHadoopConf().get()).setBasePath(cfg.getBasePath())\n        .setLayoutVersion(Option.of(new TimelineLayoutVersion(cfg.getTimelineLayoutVersion()))).build();\n    assertEquals(metaClient.getTableConfig().getTableVersion().versionCode(), HoodieTableVersion.TWO.versionCode());\n    assertTableVersionFromPropertyFile(HoodieTableVersion.TWO);\n\n    \r\n    assertTableProps(cfg);\n  }\n","realPath":"hudi-client/hudi-spark-client/src/test/java/org/apache/hudi/table/upgrade/TestUpgradeDowngrade.java","repoName":"hudi","snippetEndLine":0,"snippetStartLine":0,"startLine":204,"status":"MB"},{"authorDate":"2021-08-20 04:36:40","commitOrder":5,"curCode":"  public void testDowngrade(boolean deletePartialMarkerFiles, HoodieTableType tableType, HoodieTableVersion fromVersion) throws IOException {\n    MarkerType markerType = fromVersion == HoodieTableVersion.TWO ? MarkerType.TIMELINE_SERVER_BASED : MarkerType.DIRECT;\n    \r\n    Map<String, String> params = new HashMap<>();\n    if (fromVersion == HoodieTableVersion.TWO) {\n      addNewTableParamsToProps(params);\n    }\n    if (tableType == HoodieTableType.MERGE_ON_READ) {\n      params.put(TYPE.key(), HoodieTableType.MERGE_ON_READ.name());\n      metaClient = HoodieTestUtils.init(hadoopConf, basePath, HoodieTableType.MERGE_ON_READ);\n    }\n    HoodieWriteConfig cfg = getConfigBuilder().withAutoCommit(false).withRollbackUsingMarkers(true)\n        .withMarkersType(markerType.name()).withProps(params).build();\n    SparkRDDWriteClient client = getHoodieWriteClient(cfg);\n\n    if (fromVersion == HoodieTableVersion.TWO) {\n      \r\n      HoodieTableConfig tableConfig = metaClient.getTableConfig();\n      tableConfig.setValue(HoodieTableConfig.NAME, cfg.getTableName());\n      tableConfig.setValue(HoodieTableConfig.PARTITION_FIELDS, cfg.getString(KeyGeneratorOptions.PARTITIONPATH_FIELD.key()));\n      tableConfig.setValue(HoodieTableConfig.RECORDKEY_FIELDS, cfg.getString(KeyGeneratorOptions.RECORDKEY_FIELD.key()));\n      tableConfig.setValue(BASE_FILE_FORMAT, cfg.getString(BASE_FILE_FORMAT));\n    }\n\n    \r\n    List<FileSlice> firstPartitionCommit2FileSlices = new ArrayList<>();\n    List<FileSlice> secondPartitionCommit2FileSlices = new ArrayList<>();\n    Pair<List<HoodieRecord>, List<HoodieRecord>> inputRecords = twoUpsertCommitDataWithTwoPartitions(firstPartitionCommit2FileSlices, secondPartitionCommit2FileSlices, cfg, client, false);\n\n    HoodieTable table = this.getHoodieTable(metaClient, cfg);\n    HoodieInstant commitInstant = table.getPendingCommitTimeline().lastInstant().get();\n\n    \r\n    WriteMarkers writeMarkers = WriteMarkersFactory.get(markerType, table, commitInstant.getTimestamp());\n    List<String> markerPaths = new ArrayList<>(writeMarkers.allMarkerFilePaths());\n    if (deletePartialMarkerFiles) {\n      String toDeleteMarkerFile = markerPaths.get(0);\n      table.getMetaClient().getFs().delete(new Path(table.getMetaClient().getTempFolderPath() + \"/\" + commitInstant.getTimestamp() + \"/\" + toDeleteMarkerFile));\n      markerPaths.remove(toDeleteMarkerFile);\n    }\n\n    \r\n    HoodieTableVersion toVersion = HoodieTableVersion.ZERO;\n    if (fromVersion == HoodieTableVersion.TWO) {\n      prepForDowngradeFromTwoToOne();\n      toVersion = HoodieTableVersion.ONE;\n    } else {\n      prepForDowngradeFromOneToZero();\n    }\n\n    \r\n    new SparkUpgradeDowngrade(metaClient, cfg, context).run(metaClient, toVersion, cfg, context, null);\n\n    \r\n    assertMarkerFilesForDowngrade(table, commitInstant, toVersion == HoodieTableVersion.ONE);\n\n    \r\n    metaClient = HoodieTableMetaClient.builder().setConf(context.getHadoopConf().get()).setBasePath(cfg.getBasePath())\n        .setLayoutVersion(Option.of(new TimelineLayoutVersion(cfg.getTimelineLayoutVersion()))).build();\n    assertEquals(metaClient.getTableConfig().getTableVersion().versionCode(), toVersion.versionCode());\n    assertTableVersionFromPropertyFile(toVersion);\n\n    \r\n    \r\n\r\n\r\n\r\n\r\n\n  }\n","date":"2021-08-20 04:36:40","endLine":342,"groupId":"2843","id":2,"instanceNumber":2,"isCurCommit":0,"methodName":"testDowngrade","params":"(booleandeletePartialMarkerFiles@HoodieTableTypetableType@HoodieTableVersionfromVersion)","path":"/mnt/clonedata/CloneManagementServer/ManagementServer/consistResult/result-hudi-10-0.7/blobInfo/CC_OUT/blobs/12/3a33c3d462bbb7017e875bbc7d5571941119de.src","preCode":"  public void testDowngrade(boolean deletePartialMarkerFiles, HoodieTableType tableType, HoodieTableVersion fromVersion) throws IOException {\n    MarkerType markerType = fromVersion == HoodieTableVersion.TWO ? MarkerType.TIMELINE_SERVER_BASED : MarkerType.DIRECT;\n    \r\n    Map<String, String> params = new HashMap<>();\n    if (fromVersion == HoodieTableVersion.TWO) {\n      addNewTableParamsToProps(params);\n    }\n    if (tableType == HoodieTableType.MERGE_ON_READ) {\n      params.put(TYPE.key(), HoodieTableType.MERGE_ON_READ.name());\n      metaClient = HoodieTestUtils.init(hadoopConf, basePath, HoodieTableType.MERGE_ON_READ);\n    }\n    HoodieWriteConfig cfg = getConfigBuilder().withAutoCommit(false).withRollbackUsingMarkers(true)\n        .withMarkersType(markerType.name()).withProps(params).build();\n    SparkRDDWriteClient client = getHoodieWriteClient(cfg);\n\n    if (fromVersion == HoodieTableVersion.TWO) {\n      \r\n      HoodieTableConfig tableConfig = metaClient.getTableConfig();\n      tableConfig.setValue(HoodieTableConfig.NAME, cfg.getTableName());\n      tableConfig.setValue(HoodieTableConfig.PARTITION_FIELDS, cfg.getString(KeyGeneratorOptions.PARTITIONPATH_FIELD.key()));\n      tableConfig.setValue(HoodieTableConfig.RECORDKEY_FIELDS, cfg.getString(KeyGeneratorOptions.RECORDKEY_FIELD.key()));\n      tableConfig.setValue(BASE_FILE_FORMAT, cfg.getString(BASE_FILE_FORMAT));\n    }\n\n    \r\n    List<FileSlice> firstPartitionCommit2FileSlices = new ArrayList<>();\n    List<FileSlice> secondPartitionCommit2FileSlices = new ArrayList<>();\n    Pair<List<HoodieRecord>, List<HoodieRecord>> inputRecords = twoUpsertCommitDataWithTwoPartitions(firstPartitionCommit2FileSlices, secondPartitionCommit2FileSlices, cfg, client, false);\n\n    HoodieTable table = this.getHoodieTable(metaClient, cfg);\n    HoodieInstant commitInstant = table.getPendingCommitTimeline().lastInstant().get();\n\n    \r\n    WriteMarkers writeMarkers = WriteMarkersFactory.get(markerType, table, commitInstant.getTimestamp());\n    List<String> markerPaths = new ArrayList<>(writeMarkers.allMarkerFilePaths());\n    if (deletePartialMarkerFiles) {\n      String toDeleteMarkerFile = markerPaths.get(0);\n      table.getMetaClient().getFs().delete(new Path(table.getMetaClient().getTempFolderPath() + \"/\" + commitInstant.getTimestamp() + \"/\" + toDeleteMarkerFile));\n      markerPaths.remove(toDeleteMarkerFile);\n    }\n\n    \r\n    HoodieTableVersion toVersion = HoodieTableVersion.ZERO;\n    if (fromVersion == HoodieTableVersion.TWO) {\n      prepForDowngradeFromTwoToOne();\n      toVersion = HoodieTableVersion.ONE;\n    } else {\n      prepForDowngradeFromOneToZero();\n    }\n\n    \r\n    new SparkUpgradeDowngrade(metaClient, cfg, context).run(metaClient, toVersion, cfg, context, null);\n\n    \r\n    assertMarkerFilesForDowngrade(table, commitInstant, toVersion == HoodieTableVersion.ONE);\n\n    \r\n    metaClient = HoodieTableMetaClient.builder().setConf(context.getHadoopConf().get()).setBasePath(cfg.getBasePath())\n        .setLayoutVersion(Option.of(new TimelineLayoutVersion(cfg.getTimelineLayoutVersion()))).build();\n    assertEquals(metaClient.getTableConfig().getTableVersion().versionCode(), toVersion.versionCode());\n    assertTableVersionFromPropertyFile(toVersion);\n\n    \r\n    \r\n\r\n\r\n\r\n\r\n\n  }\n","realPath":"hudi-client/hudi-spark-client/src/test/java/org/apache/hudi/table/upgrade/TestUpgradeDowngrade.java","repoName":"hudi","snippetEndLine":0,"snippetStartLine":0,"startLine":273,"status":"MB"}],"commitId":"c350d05dd3301f14fa9d688746c9de2416db3f11","commitMessage":"@@@Restore 0.8.0 config keys with deprecated annotation (#3506)\n\nCo-authored-by: Sagar Sumit <sagarsumit09@gmail.com>\nCo-authored-by: Vinoth Chandar <vinoth@apache.org>","date":"2021-08-20 04:36:40","modifiedFileCount":"109","status":"M","submitter":"Udit Mehrotra"},{"authorTime":"2021-08-20 17:42:59","codes":[{"authorDate":"2021-08-20 04:36:40","commitOrder":6,"curCode":"  public void testUpgradeOneToTwo(HoodieTableType tableType) throws IOException {\n    \r\n    Map<String, String> params = new HashMap<>();\n    addNewTableParamsToProps(params);\n    if (tableType == HoodieTableType.MERGE_ON_READ) {\n      params.put(TYPE.key(), HoodieTableType.MERGE_ON_READ.name());\n      metaClient = HoodieTestUtils.init(hadoopConf, basePath, HoodieTableType.MERGE_ON_READ);\n    }\n    HoodieWriteConfig cfg = getConfigBuilder().withAutoCommit(false).withRollbackUsingMarkers(false).withProps(params).build();\n    SparkRDDWriteClient client = getHoodieWriteClient(cfg);\n    \r\n    doInsert(client);\n\n    \r\n    downgradeTableConfigsFromTwoToOne(cfg);\n\n    \r\n    new SparkUpgradeDowngrade(metaClient, cfg, context).run(metaClient, HoodieTableVersion.TWO, cfg, context, null);\n\n    \r\n    metaClient = HoodieTableMetaClient.builder().setConf(context.getHadoopConf().get()).setBasePath(cfg.getBasePath())\n        .setLayoutVersion(Option.of(new TimelineLayoutVersion(cfg.getTimelineLayoutVersion()))).build();\n    assertEquals(metaClient.getTableConfig().getTableVersion().versionCode(), HoodieTableVersion.TWO.versionCode());\n    assertTableVersionFromPropertyFile(HoodieTableVersion.TWO);\n\n    \r\n    assertTableProps(cfg);\n  }\n","date":"2021-08-20 04:36:40","endLine":231,"groupId":"10595","id":3,"instanceNumber":1,"isCurCommit":0,"methodName":"testUpgradeOneToTwo","params":"(HoodieTableTypetableType)","path":"/mnt/clonedata/CloneManagementServer/ManagementServer/consistResult/result-hudi-10-0.7/blobInfo/CC_OUT/blobs/12/3a33c3d462bbb7017e875bbc7d5571941119de.src","preCode":"  public void testUpgradeOneToTwo(HoodieTableType tableType) throws IOException {\n    \r\n    Map<String, String> params = new HashMap<>();\n    addNewTableParamsToProps(params);\n    if (tableType == HoodieTableType.MERGE_ON_READ) {\n      params.put(TYPE.key(), HoodieTableType.MERGE_ON_READ.name());\n      metaClient = HoodieTestUtils.init(hadoopConf, basePath, HoodieTableType.MERGE_ON_READ);\n    }\n    HoodieWriteConfig cfg = getConfigBuilder().withAutoCommit(false).withRollbackUsingMarkers(false).withProps(params).build();\n    SparkRDDWriteClient client = getHoodieWriteClient(cfg);\n    \r\n    doInsert(client);\n\n    \r\n    downgradeTableConfigsFromTwoToOne(cfg);\n\n    \r\n    new SparkUpgradeDowngrade(metaClient, cfg, context).run(metaClient, HoodieTableVersion.TWO, cfg, context, null);\n\n    \r\n    metaClient = HoodieTableMetaClient.builder().setConf(context.getHadoopConf().get()).setBasePath(cfg.getBasePath())\n        .setLayoutVersion(Option.of(new TimelineLayoutVersion(cfg.getTimelineLayoutVersion()))).build();\n    assertEquals(metaClient.getTableConfig().getTableVersion().versionCode(), HoodieTableVersion.TWO.versionCode());\n    assertTableVersionFromPropertyFile(HoodieTableVersion.TWO);\n\n    \r\n    assertTableProps(cfg);\n  }\n","realPath":"hudi-client/hudi-spark-client/src/test/java/org/apache/hudi/table/upgrade/TestUpgradeDowngrade.java","repoName":"hudi","snippetEndLine":0,"snippetStartLine":0,"startLine":204,"status":"N"},{"authorDate":"2021-08-20 17:42:59","commitOrder":6,"curCode":"  public void testDowngrade(boolean deletePartialMarkerFiles, HoodieTableType tableType, HoodieTableVersion fromVersion) throws IOException {\n    MarkerType markerType = fromVersion == HoodieTableVersion.TWO ? MarkerType.TIMELINE_SERVER_BASED : MarkerType.DIRECT;\n    \r\n    Map<String, String> params = new HashMap<>();\n    if (fromVersion == HoodieTableVersion.TWO) {\n      addNewTableParamsToProps(params);\n    }\n    if (tableType == HoodieTableType.MERGE_ON_READ) {\n      params.put(TYPE.key(), HoodieTableType.MERGE_ON_READ.name());\n      metaClient = HoodieTestUtils.init(hadoopConf, basePath, HoodieTableType.MERGE_ON_READ);\n    }\n    HoodieWriteConfig cfg = getConfigBuilder().withAutoCommit(false).withRollbackUsingMarkers(true)\n        .withMarkersType(markerType.name()).withProps(params).build();\n    SparkRDDWriteClient client = getHoodieWriteClient(cfg);\n\n    if (fromVersion == HoodieTableVersion.TWO) {\n      \r\n      HoodieTableConfig tableConfig = metaClient.getTableConfig();\n      tableConfig.setValue(HoodieTableConfig.NAME, cfg.getTableName());\n      tableConfig.setValue(HoodieTableConfig.PARTITION_FIELDS, cfg.getString(KeyGeneratorOptions.PARTITIONPATH_FIELD_NAME.key()));\n      tableConfig.setValue(HoodieTableConfig.RECORDKEY_FIELDS, cfg.getString(KeyGeneratorOptions.RECORDKEY_FIELD_NAME.key()));\n      tableConfig.setValue(BASE_FILE_FORMAT, cfg.getString(BASE_FILE_FORMAT));\n    }\n\n    \r\n    List<FileSlice> firstPartitionCommit2FileSlices = new ArrayList<>();\n    List<FileSlice> secondPartitionCommit2FileSlices = new ArrayList<>();\n    Pair<List<HoodieRecord>, List<HoodieRecord>> inputRecords = twoUpsertCommitDataWithTwoPartitions(firstPartitionCommit2FileSlices, secondPartitionCommit2FileSlices, cfg, client, false);\n\n    HoodieTable table = this.getHoodieTable(metaClient, cfg);\n    HoodieInstant commitInstant = table.getPendingCommitTimeline().lastInstant().get();\n\n    \r\n    WriteMarkers writeMarkers = WriteMarkersFactory.get(markerType, table, commitInstant.getTimestamp());\n    List<String> markerPaths = new ArrayList<>(writeMarkers.allMarkerFilePaths());\n    if (deletePartialMarkerFiles) {\n      String toDeleteMarkerFile = markerPaths.get(0);\n      table.getMetaClient().getFs().delete(new Path(table.getMetaClient().getTempFolderPath() + \"/\" + commitInstant.getTimestamp() + \"/\" + toDeleteMarkerFile));\n      markerPaths.remove(toDeleteMarkerFile);\n    }\n\n    \r\n    HoodieTableVersion toVersion = HoodieTableVersion.ZERO;\n    if (fromVersion == HoodieTableVersion.TWO) {\n      prepForDowngradeFromTwoToOne();\n      toVersion = HoodieTableVersion.ONE;\n    } else {\n      prepForDowngradeFromOneToZero();\n    }\n\n    \r\n    new SparkUpgradeDowngrade(metaClient, cfg, context).run(metaClient, toVersion, cfg, context, null);\n\n    \r\n    assertMarkerFilesForDowngrade(table, commitInstant, toVersion == HoodieTableVersion.ONE);\n\n    \r\n    metaClient = HoodieTableMetaClient.builder().setConf(context.getHadoopConf().get()).setBasePath(cfg.getBasePath())\n        .setLayoutVersion(Option.of(new TimelineLayoutVersion(cfg.getTimelineLayoutVersion()))).build();\n    assertEquals(metaClient.getTableConfig().getTableVersion().versionCode(), toVersion.versionCode());\n    assertTableVersionFromPropertyFile(toVersion);\n\n    \r\n    \r\n\r\n\r\n\r\n\r\n\n  }\n","date":"2021-08-20 17:42:59","endLine":342,"groupId":"10595","id":4,"instanceNumber":2,"isCurCommit":1,"methodName":"testDowngrade","params":"(booleandeletePartialMarkerFiles@HoodieTableTypetableType@HoodieTableVersionfromVersion)","path":"/mnt/clonedata/CloneManagementServer/ManagementServer/consistResult/result-hudi-10-0.7/blobInfo/CC_OUT/blobs/79/2da4e08619992a5ebf41e0f927980ce48f1876.src","preCode":"  public void testDowngrade(boolean deletePartialMarkerFiles, HoodieTableType tableType, HoodieTableVersion fromVersion) throws IOException {\n    MarkerType markerType = fromVersion == HoodieTableVersion.TWO ? MarkerType.TIMELINE_SERVER_BASED : MarkerType.DIRECT;\n    \r\n    Map<String, String> params = new HashMap<>();\n    if (fromVersion == HoodieTableVersion.TWO) {\n      addNewTableParamsToProps(params);\n    }\n    if (tableType == HoodieTableType.MERGE_ON_READ) {\n      params.put(TYPE.key(), HoodieTableType.MERGE_ON_READ.name());\n      metaClient = HoodieTestUtils.init(hadoopConf, basePath, HoodieTableType.MERGE_ON_READ);\n    }\n    HoodieWriteConfig cfg = getConfigBuilder().withAutoCommit(false).withRollbackUsingMarkers(true)\n        .withMarkersType(markerType.name()).withProps(params).build();\n    SparkRDDWriteClient client = getHoodieWriteClient(cfg);\n\n    if (fromVersion == HoodieTableVersion.TWO) {\n      \r\n      HoodieTableConfig tableConfig = metaClient.getTableConfig();\n      tableConfig.setValue(HoodieTableConfig.NAME, cfg.getTableName());\n      tableConfig.setValue(HoodieTableConfig.PARTITION_FIELDS, cfg.getString(KeyGeneratorOptions.PARTITIONPATH_FIELD.key()));\n      tableConfig.setValue(HoodieTableConfig.RECORDKEY_FIELDS, cfg.getString(KeyGeneratorOptions.RECORDKEY_FIELD.key()));\n      tableConfig.setValue(BASE_FILE_FORMAT, cfg.getString(BASE_FILE_FORMAT));\n    }\n\n    \r\n    List<FileSlice> firstPartitionCommit2FileSlices = new ArrayList<>();\n    List<FileSlice> secondPartitionCommit2FileSlices = new ArrayList<>();\n    Pair<List<HoodieRecord>, List<HoodieRecord>> inputRecords = twoUpsertCommitDataWithTwoPartitions(firstPartitionCommit2FileSlices, secondPartitionCommit2FileSlices, cfg, client, false);\n\n    HoodieTable table = this.getHoodieTable(metaClient, cfg);\n    HoodieInstant commitInstant = table.getPendingCommitTimeline().lastInstant().get();\n\n    \r\n    WriteMarkers writeMarkers = WriteMarkersFactory.get(markerType, table, commitInstant.getTimestamp());\n    List<String> markerPaths = new ArrayList<>(writeMarkers.allMarkerFilePaths());\n    if (deletePartialMarkerFiles) {\n      String toDeleteMarkerFile = markerPaths.get(0);\n      table.getMetaClient().getFs().delete(new Path(table.getMetaClient().getTempFolderPath() + \"/\" + commitInstant.getTimestamp() + \"/\" + toDeleteMarkerFile));\n      markerPaths.remove(toDeleteMarkerFile);\n    }\n\n    \r\n    HoodieTableVersion toVersion = HoodieTableVersion.ZERO;\n    if (fromVersion == HoodieTableVersion.TWO) {\n      prepForDowngradeFromTwoToOne();\n      toVersion = HoodieTableVersion.ONE;\n    } else {\n      prepForDowngradeFromOneToZero();\n    }\n\n    \r\n    new SparkUpgradeDowngrade(metaClient, cfg, context).run(metaClient, toVersion, cfg, context, null);\n\n    \r\n    assertMarkerFilesForDowngrade(table, commitInstant, toVersion == HoodieTableVersion.ONE);\n\n    \r\n    metaClient = HoodieTableMetaClient.builder().setConf(context.getHadoopConf().get()).setBasePath(cfg.getBasePath())\n        .setLayoutVersion(Option.of(new TimelineLayoutVersion(cfg.getTimelineLayoutVersion()))).build();\n    assertEquals(metaClient.getTableConfig().getTableVersion().versionCode(), toVersion.versionCode());\n    assertTableVersionFromPropertyFile(toVersion);\n\n    \r\n    \r\n\r\n\r\n\r\n\r\n\n  }\n","realPath":"hudi-client/hudi-spark-client/src/test/java/org/apache/hudi/table/upgrade/TestUpgradeDowngrade.java","repoName":"hudi","snippetEndLine":0,"snippetStartLine":0,"startLine":273,"status":"M"}],"commitId":"e39d0a2f2852ef51c524e5b16a1cecb099674eed","commitMessage":"@@@Keep non-conflicting names for common configs between DataSourceOptions and HoodieWriteConfig (#3511)\n\n","date":"2021-08-20 17:42:59","modifiedFileCount":"29","status":"M","submitter":"Udit Mehrotra"}]
