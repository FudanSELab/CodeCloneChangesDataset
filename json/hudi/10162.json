[{"authorTime":"2019-08-23 11:18:50","codes":[{"authorDate":"2019-08-23 11:18:50","commitOrder":1,"curCode":"  private void testSparkSQLAfterSecondBatch() throws Exception {\n    Pair<String, String> stdOutErrPair = executeSparkSQLCommand(SPARKSQL_BATCH2_COMMANDS, true);\n    assertStdOutContains(stdOutErrPair,\n        \"+------+-------------------+\\n\"\n        + \"|GOOG  |2018-08-31 10:59:00|\\n\"\n        + \"+------+-------------------+\", 2);\n\n    assertStdOutContains(stdOutErrPair, \"|GOOG  |2018-08-31 09:59:00|6330  |1230.5   |1230.02 |\", 3);\n    assertStdOutContains(stdOutErrPair, \"|GOOG  |2018-08-31 10:59:00|9021  |1227.1993|1227.215|\", 2);\n    assertStdOutContains(stdOutErrPair,\n        \"+------+-------------------+\\n\"\n        + \"|GOOG  |2018-08-31 10:29:00|\\n\"\n        + \"+------+-------------------+\");\n    assertStdOutContains(stdOutErrPair, \"|GOOG  |2018-08-31 10:29:00|3391  |1230.1899|1230.085|\");\n  }\n","date":"2019-08-23 11:18:50","endLine":231,"groupId":"2406","id":1,"instanceNumber":1,"isCurCommit":0,"methodName":"testSparkSQLAfterSecondBatch","params":"()","path":"/mnt/clonedata/CloneManagementServer/ManagementServer/consistResult/result-hudi-10-0.7/blobInfo/CC_OUT/blobs/ee/3855a830edf52c7aaf8e8a3844c58b47077dcb.src","preCode":"  private void testSparkSQLAfterSecondBatch() throws Exception {\n    Pair<String, String> stdOutErrPair = executeSparkSQLCommand(SPARKSQL_BATCH2_COMMANDS, true);\n    assertStdOutContains(stdOutErrPair,\n        \"+------+-------------------+\\n\"\n        + \"|GOOG  |2018-08-31 10:59:00|\\n\"\n        + \"+------+-------------------+\", 2);\n\n    assertStdOutContains(stdOutErrPair, \"|GOOG  |2018-08-31 09:59:00|6330  |1230.5   |1230.02 |\", 3);\n    assertStdOutContains(stdOutErrPair, \"|GOOG  |2018-08-31 10:59:00|9021  |1227.1993|1227.215|\", 2);\n    assertStdOutContains(stdOutErrPair,\n        \"+------+-------------------+\\n\"\n        + \"|GOOG  |2018-08-31 10:29:00|\\n\"\n        + \"+------+-------------------+\");\n    assertStdOutContains(stdOutErrPair, \"|GOOG  |2018-08-31 10:29:00|3391  |1230.1899|1230.085|\");\n  }\n","realPath":"hudi-integ-test/src/test/java/org/apache/hudi/integ/ITTestHoodieDemo.java","repoName":"hudi","snippetEndLine":0,"snippetStartLine":0,"startLine":217,"status":"B"},{"authorDate":"2019-08-23 11:18:50","commitOrder":1,"curCode":"  private void testIncrementalSparkSQLQuery() throws Exception {\n    Pair<String, String> stdOutErrPair = executeSparkSQLCommand(SPARKSQL_INCREMENTAL_COMMANDS, true);\n    assertStdOutContains(stdOutErrPair, \"|GOOG  |2018-08-31 10:59:00|9021  |1227.1993|1227.215|\");\n    assertStdOutContains(stdOutErrPair,\n        \"|default |stock_ticks_cow           |false      |\\n\"\n        + \"|default |stock_ticks_derived_mor   |false      |\\n\"\n        + \"|default |stock_ticks_derived_mor_rt|false      |\\n\"\n        + \"|default |stock_ticks_mor           |false      |\\n\"\n        + \"|default |stock_ticks_mor_rt        |false      |\\n\"\n        + \"|        |stock_ticks_cow_incr      |true       |\");\n    assertStdOutContains(stdOutErrPair,\n        \"|count(1)|\\n\"\n        + \"+--------+\\n\"\n        + \"|99     |\", 2);\n  }\n","date":"2019-08-23 11:18:50","endLine":266,"groupId":"4479","id":2,"instanceNumber":2,"isCurCommit":0,"methodName":"testIncrementalSparkSQLQuery","params":"()","path":"/mnt/clonedata/CloneManagementServer/ManagementServer/consistResult/result-hudi-10-0.7/blobInfo/CC_OUT/blobs/ee/3855a830edf52c7aaf8e8a3844c58b47077dcb.src","preCode":"  private void testIncrementalSparkSQLQuery() throws Exception {\n    Pair<String, String> stdOutErrPair = executeSparkSQLCommand(SPARKSQL_INCREMENTAL_COMMANDS, true);\n    assertStdOutContains(stdOutErrPair, \"|GOOG  |2018-08-31 10:59:00|9021  |1227.1993|1227.215|\");\n    assertStdOutContains(stdOutErrPair,\n        \"|default |stock_ticks_cow           |false      |\\n\"\n        + \"|default |stock_ticks_derived_mor   |false      |\\n\"\n        + \"|default |stock_ticks_derived_mor_rt|false      |\\n\"\n        + \"|default |stock_ticks_mor           |false      |\\n\"\n        + \"|default |stock_ticks_mor_rt        |false      |\\n\"\n        + \"|        |stock_ticks_cow_incr      |true       |\");\n    assertStdOutContains(stdOutErrPair,\n        \"|count(1)|\\n\"\n        + \"+--------+\\n\"\n        + \"|99     |\", 2);\n  }\n","realPath":"hudi-integ-test/src/test/java/org/apache/hudi/integ/ITTestHoodieDemo.java","repoName":"hudi","snippetEndLine":0,"snippetStartLine":0,"startLine":252,"status":"B"}],"commitId":"6edf0b9defe8c82b0ef0076a6300c00c1641a6e6","commitMessage":"@@@[HUDI-68] Pom cleanup & demo automation (#846)\n\n - [HUDI-172] Cleanup Maven POM/Classpath\n  - Fix ordering of dependencies in poms.  to enable better resolution\n  - Idea is to place more specific ones at the top\n  - And place dependencies which use them below them\n- [HUDI-68] : Automate demo steps on docker setup\n - Move hive queries from hive cli to beeline\n - Standardize on taking query input from text command files\n - Deltastreamer ingest.  also does hive sync in a single step\n - Spark Incremental Query materialized as a derived Hive table using datasource\n - Fix flakiness in HDFS spin up and output comparison\n - Code cleanup around streamlining and loc reduction\n - Also fixed pom to not shade some hive classs in spark.  to enable hive sync\n","date":"2019-08-23 11:18:50","modifiedFileCount":"4","status":"B","submitter":"vinoth chandar"},{"authorTime":"2019-08-23 11:18:50","codes":[{"authorDate":"2019-12-12 05:42:05","commitOrder":2,"curCode":"  private void testSparkSQLAfterSecondBatch() throws Exception {\n    Pair<String, String> stdOutErrPair = executeSparkSQLCommand(SPARKSQL_BATCH2_COMMANDS, true);\n    assertStdOutContains(stdOutErrPair,\n        \"+------+-------------------+\\n|GOOG  |2018-08-31 10:59:00|\\n+------+-------------------+\", 2);\n\n    assertStdOutContains(stdOutErrPair, \"|GOOG  |2018-08-31 09:59:00|6330  |1230.5   |1230.02 |\", 3);\n    assertStdOutContains(stdOutErrPair, \"|GOOG  |2018-08-31 10:59:00|9021  |1227.1993|1227.215|\", 2);\n    assertStdOutContains(stdOutErrPair,\n        \"+------+-------------------+\\n|GOOG  |2018-08-31 10:29:00|\\n+------+-------------------+\");\n    assertStdOutContains(stdOutErrPair, \"|GOOG  |2018-08-31 10:29:00|3391  |1230.1899|1230.085|\");\n  }\n","date":"2019-12-12 05:42:05","endLine":269,"groupId":"2546","id":3,"instanceNumber":1,"isCurCommit":0,"methodName":"testSparkSQLAfterSecondBatch","params":"()","path":"/mnt/clonedata/CloneManagementServer/ManagementServer/consistResult/result-hudi-10-0.7/blobInfo/CC_OUT/blobs/99/a8d010e9e7889bcace9ab65d680c71cb5e03d0.src","preCode":"  private void testSparkSQLAfterSecondBatch() throws Exception {\n    Pair<String, String> stdOutErrPair = executeSparkSQLCommand(SPARKSQL_BATCH2_COMMANDS, true);\n    assertStdOutContains(stdOutErrPair,\n        \"+------+-------------------+\\n\" + \"|GOOG  |2018-08-31 10:59:00|\\n\" + \"+------+-------------------+\", 2);\n\n    assertStdOutContains(stdOutErrPair, \"|GOOG  |2018-08-31 09:59:00|6330  |1230.5   |1230.02 |\", 3);\n    assertStdOutContains(stdOutErrPair, \"|GOOG  |2018-08-31 10:59:00|9021  |1227.1993|1227.215|\", 2);\n    assertStdOutContains(stdOutErrPair,\n        \"+------+-------------------+\\n\" + \"|GOOG  |2018-08-31 10:29:00|\\n\" + \"+------+-------------------+\");\n    assertStdOutContains(stdOutErrPair, \"|GOOG  |2018-08-31 10:29:00|3391  |1230.1899|1230.085|\");\n  }\n","realPath":"hudi-integ-test/src/test/java/org/apache/hudi/integ/ITTestHoodieDemo.java","repoName":"hudi","snippetEndLine":0,"snippetStartLine":0,"startLine":259,"status":"M"},{"authorDate":"2019-08-23 11:18:50","commitOrder":2,"curCode":"  private void testIncrementalSparkSQLQuery() throws Exception {\n    Pair<String, String> stdOutErrPair = executeSparkSQLCommand(SPARKSQL_INCREMENTAL_COMMANDS, true);\n    assertStdOutContains(stdOutErrPair, \"|GOOG  |2018-08-31 10:59:00|9021  |1227.1993|1227.215|\");\n    assertStdOutContains(stdOutErrPair,\n        \"|default |stock_ticks_cow           |false      |\\n\"\n        + \"|default |stock_ticks_derived_mor   |false      |\\n\"\n        + \"|default |stock_ticks_derived_mor_rt|false      |\\n\"\n        + \"|default |stock_ticks_mor           |false      |\\n\"\n        + \"|default |stock_ticks_mor_rt        |false      |\\n\"\n        + \"|        |stock_ticks_cow_incr      |true       |\");\n    assertStdOutContains(stdOutErrPair,\n        \"|count(1)|\\n\"\n        + \"+--------+\\n\"\n        + \"|99     |\", 2);\n  }\n","date":"2019-08-23 11:18:50","endLine":266,"groupId":"4479","id":4,"instanceNumber":2,"isCurCommit":0,"methodName":"testIncrementalSparkSQLQuery","params":"()","path":"/mnt/clonedata/CloneManagementServer/ManagementServer/consistResult/result-hudi-10-0.7/blobInfo/CC_OUT/blobs/ee/3855a830edf52c7aaf8e8a3844c58b47077dcb.src","preCode":"  private void testIncrementalSparkSQLQuery() throws Exception {\n    Pair<String, String> stdOutErrPair = executeSparkSQLCommand(SPARKSQL_INCREMENTAL_COMMANDS, true);\n    assertStdOutContains(stdOutErrPair, \"|GOOG  |2018-08-31 10:59:00|9021  |1227.1993|1227.215|\");\n    assertStdOutContains(stdOutErrPair,\n        \"|default |stock_ticks_cow           |false      |\\n\"\n        + \"|default |stock_ticks_derived_mor   |false      |\\n\"\n        + \"|default |stock_ticks_derived_mor_rt|false      |\\n\"\n        + \"|default |stock_ticks_mor           |false      |\\n\"\n        + \"|default |stock_ticks_mor_rt        |false      |\\n\"\n        + \"|        |stock_ticks_cow_incr      |true       |\");\n    assertStdOutContains(stdOutErrPair,\n        \"|count(1)|\\n\"\n        + \"+--------+\\n\"\n        + \"|99     |\", 2);\n  }\n","realPath":"hudi-integ-test/src/test/java/org/apache/hudi/integ/ITTestHoodieDemo.java","repoName":"hudi","snippetEndLine":0,"snippetStartLine":0,"startLine":252,"status":"N"}],"commitId":"ba514cfea0fbb3a71a93fc7b5d5fe06dc1fdfffd","commitMessage":"@@@[MINOR] Remove redundant plus operator (#1097)\n\n","date":"2019-12-12 05:42:05","modifiedFileCount":"46","status":"M","submitter":"lamber-ken"},{"authorTime":"2020-01-17 15:58:47","codes":[{"authorDate":"2019-12-12 05:42:05","commitOrder":3,"curCode":"  private void testSparkSQLAfterSecondBatch() throws Exception {\n    Pair<String, String> stdOutErrPair = executeSparkSQLCommand(SPARKSQL_BATCH2_COMMANDS, true);\n    assertStdOutContains(stdOutErrPair,\n        \"+------+-------------------+\\n|GOOG  |2018-08-31 10:59:00|\\n+------+-------------------+\", 2);\n\n    assertStdOutContains(stdOutErrPair, \"|GOOG  |2018-08-31 09:59:00|6330  |1230.5   |1230.02 |\", 3);\n    assertStdOutContains(stdOutErrPair, \"|GOOG  |2018-08-31 10:59:00|9021  |1227.1993|1227.215|\", 2);\n    assertStdOutContains(stdOutErrPair,\n        \"+------+-------------------+\\n|GOOG  |2018-08-31 10:29:00|\\n+------+-------------------+\");\n    assertStdOutContains(stdOutErrPair, \"|GOOG  |2018-08-31 10:29:00|3391  |1230.1899|1230.085|\");\n  }\n","date":"2019-12-12 05:42:05","endLine":269,"groupId":"2546","id":5,"instanceNumber":1,"isCurCommit":0,"methodName":"testSparkSQLAfterSecondBatch","params":"()","path":"/mnt/clonedata/CloneManagementServer/ManagementServer/consistResult/result-hudi-10-0.7/blobInfo/CC_OUT/blobs/99/a8d010e9e7889bcace9ab65d680c71cb5e03d0.src","preCode":"  private void testSparkSQLAfterSecondBatch() throws Exception {\n    Pair<String, String> stdOutErrPair = executeSparkSQLCommand(SPARKSQL_BATCH2_COMMANDS, true);\n    assertStdOutContains(stdOutErrPair,\n        \"+------+-------------------+\\n|GOOG  |2018-08-31 10:59:00|\\n+------+-------------------+\", 2);\n\n    assertStdOutContains(stdOutErrPair, \"|GOOG  |2018-08-31 09:59:00|6330  |1230.5   |1230.02 |\", 3);\n    assertStdOutContains(stdOutErrPair, \"|GOOG  |2018-08-31 10:59:00|9021  |1227.1993|1227.215|\", 2);\n    assertStdOutContains(stdOutErrPair,\n        \"+------+-------------------+\\n|GOOG  |2018-08-31 10:29:00|\\n+------+-------------------+\");\n    assertStdOutContains(stdOutErrPair, \"|GOOG  |2018-08-31 10:29:00|3391  |1230.1899|1230.085|\");\n  }\n","realPath":"hudi-integ-test/src/test/java/org/apache/hudi/integ/ITTestHoodieDemo.java","repoName":"hudi","snippetEndLine":0,"snippetStartLine":0,"startLine":259,"status":"N"},{"authorDate":"2020-01-17 15:58:47","commitOrder":3,"curCode":"  private void testIncrementalSparkSQLQuery() throws Exception {\n    Pair<String, String> stdOutErrPair = executeSparkSQLCommand(SPARKSQL_INCREMENTAL_COMMANDS, true);\n    assertStdOutContains(stdOutErrPair, \"|GOOG  |2018-08-31 10:59:00|9021  |1227.1993|1227.215|\");\n    assertStdOutContains(stdOutErrPair, \"|default |stock_ticks_cow           |false      |\\n\"\n        + \"|default |stock_ticks_derived_mor_ro|false      |\\n\"\n        + \"|default |stock_ticks_derived_mor_rt|false      |\\n\"\n        + \"|default |stock_ticks_mor_ro        |false      |\\n\"\n        + \"|default |stock_ticks_mor_rt        |false      |\\n\"\n        + \"|        |stock_ticks_cow_incr      |true       |\");\n    assertStdOutContains(stdOutErrPair, \"|count(1)|\\n+--------+\\n|99     |\", 2);\n  }\n","date":"2020-01-17 15:58:47","endLine":301,"groupId":"4479","id":6,"instanceNumber":2,"isCurCommit":0,"methodName":"testIncrementalSparkSQLQuery","params":"()","path":"/mnt/clonedata/CloneManagementServer/ManagementServer/consistResult/result-hudi-10-0.7/blobInfo/CC_OUT/blobs/7d/114145571c3b7c30b557ba18d213a15b00f38b.src","preCode":"  private void testIncrementalSparkSQLQuery() throws Exception {\n    Pair<String, String> stdOutErrPair = executeSparkSQLCommand(SPARKSQL_INCREMENTAL_COMMANDS, true);\n    assertStdOutContains(stdOutErrPair, \"|GOOG  |2018-08-31 10:59:00|9021  |1227.1993|1227.215|\");\n    assertStdOutContains(stdOutErrPair, \"|default |stock_ticks_cow           |false      |\\n\"\n        + \"|default |stock_ticks_derived_mor   |false      |\\n\" + \"|default |stock_ticks_derived_mor_rt|false      |\\n\"\n        + \"|default |stock_ticks_mor           |false      |\\n\" + \"|default |stock_ticks_mor_rt        |false      |\\n\"\n        + \"|        |stock_ticks_cow_incr      |true       |\");\n    assertStdOutContains(stdOutErrPair, \"|count(1)|\\n\" + \"+--------+\\n\" + \"|99     |\", 2);\n  }\n","realPath":"hudi-integ-test/src/test/java/org/apache/hudi/integ/ITTestHoodieDemo.java","repoName":"hudi","snippetEndLine":0,"snippetStartLine":0,"startLine":291,"status":"M"}],"commitId":"c2c0f6b13d5b72b3098ed1b343b0a89679f854b3","commitMessage":"@@@[HUDI-509] Renaming code in sync with cWiki restructuring (#1212)\n\n - Storage Type replaced with Table Type (remaining instances)\n - View types replaced with query types;\n - ReadOptimized view referred as Snapshot Query\n - TableFileSystemView sub interfaces renamed to BaseFileOnly and Slice Views\n - HoodieDataFile renamed to HoodieBaseFile\n - Hive Sync tool will register RO tables for MOR with a `_ro` suffix\n - Datasource/Deltastreamer options renamed accordingly\n - Support fallback to old config values as well.  so migration is painless\n - Config for controlling _ro suffix addition\n - Renaming DataFile to BaseFile across DTOs.  HoodieFileSlice and AbstractTableFileSystemView\n","date":"2020-01-17 15:58:47","modifiedFileCount":"71","status":"M","submitter":"vinoth chandar"},{"authorTime":"2020-08-04 11:19:21","codes":[{"authorDate":"2020-08-04 11:19:21","commitOrder":4,"curCode":"  private void testSparkSQLAfterSecondBatch() throws Exception {\n    Pair<String, String> stdOutErrPair = executeSparkSQLCommand(SPARKSQL_BATCH2_COMMANDS, true);\n    assertStdOutContains(stdOutErrPair,\n        \"+------+-------------------+\\n|GOOG  |2018-08-31 10:59:00|\\n+------+-------------------+\", 4);\n\n    assertStdOutContains(stdOutErrPair, \"|GOOG  |2018-08-31 09:59:00|6330  |1230.5   |1230.02 |\", 6);\n    assertStdOutContains(stdOutErrPair, \"|GOOG  |2018-08-31 10:59:00|9021  |1227.1993|1227.215|\", 4);\n    assertStdOutContains(stdOutErrPair,\n        \"+------+-------------------+\\n|GOOG  |2018-08-31 10:29:00|\\n+------+-------------------+\", 2);\n    assertStdOutContains(stdOutErrPair, \"|GOOG  |2018-08-31 10:29:00|3391  |1230.1899|1230.085|\", 2);\n  }\n","date":"2020-08-04 11:19:21","endLine":357,"groupId":"10162","id":7,"instanceNumber":1,"isCurCommit":0,"methodName":"testSparkSQLAfterSecondBatch","params":"()","path":"/mnt/clonedata/CloneManagementServer/ManagementServer/consistResult/result-hudi-10-0.7/blobInfo/CC_OUT/blobs/d2/a0841729a97760b4cbe08ed57cecf6c169d8d1.src","preCode":"  private void testSparkSQLAfterSecondBatch() throws Exception {\n    Pair<String, String> stdOutErrPair = executeSparkSQLCommand(SPARKSQL_BATCH2_COMMANDS, true);\n    assertStdOutContains(stdOutErrPair,\n        \"+------+-------------------+\\n|GOOG  |2018-08-31 10:59:00|\\n+------+-------------------+\", 2);\n\n    assertStdOutContains(stdOutErrPair, \"|GOOG  |2018-08-31 09:59:00|6330  |1230.5   |1230.02 |\", 3);\n    assertStdOutContains(stdOutErrPair, \"|GOOG  |2018-08-31 10:59:00|9021  |1227.1993|1227.215|\", 2);\n    assertStdOutContains(stdOutErrPair,\n        \"+------+-------------------+\\n|GOOG  |2018-08-31 10:29:00|\\n+------+-------------------+\");\n    assertStdOutContains(stdOutErrPair, \"|GOOG  |2018-08-31 10:29:00|3391  |1230.1899|1230.085|\");\n  }\n","realPath":"hudi-integ-test/src/test/java/org/apache/hudi/integ/ITTestHoodieDemo.java","repoName":"hudi","snippetEndLine":0,"snippetStartLine":0,"startLine":347,"status":"M"},{"authorDate":"2020-08-04 11:19:21","commitOrder":4,"curCode":"  private void testIncrementalSparkSQLQuery() throws Exception {\n    Pair<String, String> stdOutErrPair = executeSparkSQLCommand(SPARKSQL_INCREMENTAL_COMMANDS, true);\n    assertStdOutContains(stdOutErrPair, \"|GOOG  |2018-08-31 10:59:00|9021  |1227.1993|1227.215|\", 2);\n    assertStdOutContains(stdOutErrPair, \"|default |stock_ticks_cow              |false      |\\n\"\n        + \"|default |stock_ticks_cow_bs           |false      |\\n\"\n        + \"|default |stock_ticks_derived_mor_bs_ro|false      |\\n\"\n        + \"|default |stock_ticks_derived_mor_bs_rt|false      |\\n\"\n        + \"|default |stock_ticks_derived_mor_ro   |false      |\\n\"\n        + \"|default |stock_ticks_derived_mor_rt   |false      |\\n\"\n        + \"|default |stock_ticks_mor_bs_ro        |false      |\\n\"\n        + \"|default |stock_ticks_mor_bs_rt        |false      |\"\n        + \"|default |stock_ticks_mor_ro           |false      |\\n\"\n        + \"|default |stock_ticks_mor_rt           |false      |\");\n    assertStdOutContains(stdOutErrPair, \"|count(1)|\\n+--------+\\n|99     |\", 4);\n  }\n","date":"2020-08-04 11:19:21","endLine":406,"groupId":"10162","id":8,"instanceNumber":2,"isCurCommit":0,"methodName":"testIncrementalSparkSQLQuery","params":"()","path":"/mnt/clonedata/CloneManagementServer/ManagementServer/consistResult/result-hudi-10-0.7/blobInfo/CC_OUT/blobs/d2/a0841729a97760b4cbe08ed57cecf6c169d8d1.src","preCode":"  private void testIncrementalSparkSQLQuery() throws Exception {\n    Pair<String, String> stdOutErrPair = executeSparkSQLCommand(SPARKSQL_INCREMENTAL_COMMANDS, true);\n    assertStdOutContains(stdOutErrPair, \"|GOOG  |2018-08-31 10:59:00|9021  |1227.1993|1227.215|\");\n    assertStdOutContains(stdOutErrPair, \"|default |stock_ticks_cow           |false      |\\n\"\n        + \"|default |stock_ticks_derived_mor_ro|false      |\\n\"\n        + \"|default |stock_ticks_derived_mor_rt|false      |\\n\"\n        + \"|default |stock_ticks_mor_ro        |false      |\\n\"\n        + \"|default |stock_ticks_mor_rt        |false      |\\n\"\n        + \"|        |stock_ticks_cow_incr      |true       |\");\n    assertStdOutContains(stdOutErrPair, \"|count(1)|\\n+--------+\\n|99     |\", 2);\n  }\n","realPath":"hudi-integ-test/src/test/java/org/apache/hudi/integ/ITTestHoodieDemo.java","repoName":"hudi","snippetEndLine":0,"snippetStartLine":0,"startLine":392,"status":"M"}],"commitId":"539621bd33893d99a07b8f739a1e965ca72acdc9","commitMessage":"@@@[HUDI-242] Support for RFC-12/Bootstrapping of external datasets to hudi (#1876)\n\n- [HUDI-418] Bootstrap Index Implementation using HFile with unit-test\n - [HUDI-421] FileSystem View Changes to support Bootstrap with unit-tests\n - [HUDI-424] Implement Query Side Integration for querying tables containing bootstrap file slices\n - [HUDI-423] Implement upsert functionality for handling updates to these bootstrap file slices\n - [HUDI-421] Bootstrap Write Client with tests\n - [HUDI-425] Added HoodieDeltaStreamer support\n - [HUDI-899] Add a knob to change partition-path style while performing metadata bootstrap\n - [HUDI-900] Metadata Bootstrap Key Generator needs to handle complex keys correctly\n - [HUDI-424] Simplify Record reader implementation\n - [HUDI-423] Implement upsert functionality for handling updates to these bootstrap file slices\n - [HUDI-420] Hoodie Demo working with hive and sparkSQL. Also.  Hoodie CLI working with bootstrap tables\n\nCo-authored-by: Mehrotra <uditme@amazon.com>\nCo-authored-by: Vinoth Chandar <vinoth@apache.org>\nCo-authored-by: Balaji Varadarajan <varadarb@uber.com>\n","date":"2020-08-04 11:19:21","modifiedFileCount":"89","status":"M","submitter":"vinoth chandar"}]
