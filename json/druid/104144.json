[{"authorTime":"2018-08-31 00:56:26","codes":[{"authorDate":"2018-08-31 00:56:26","commitOrder":1,"curCode":"  public void testIndexTask() throws Exception\n  {\n    final Task indexTask = new IndexTask(\n        null,\n        null,\n        new IndexIngestionSpec(\n            new DataSchema(\n                \"foo\",\n                null,\n                new AggregatorFactory[]{new DoubleSumAggregatorFactory(\"met\", \"met\")},\n                new UniformGranularitySpec(\n                    Granularities.DAY,\n                    null,\n                    ImmutableList.of(Intervals.of(\"2010-01-01/P2D\"))\n                ),\n                null,\n                mapper\n            ),\n            new IndexIOConfig(new MockFirehoseFactory(false), false),\n            new IndexTuningConfig(\n                10000,\n                10,\n                null,\n                null,\n                null,\n                null,\n                indexSpec,\n                3,\n                true,\n                true,\n                false,\n                null,\n                null,\n                null,\n                null,\n                null,\n                null,\n                null\n            )\n        ),\n        null,\n        AuthTestUtils.TEST_AUTHORIZER_MAPPER,\n        null,\n        ROW_INGESTION_METERS_FACTORY\n    );\n\n    final Optional<TaskStatus> preRunTaskStatus = tsqa.getStatus(indexTask.getId());\n    Assert.assertTrue(\"pre run task status not present\", !preRunTaskStatus.isPresent());\n\n    final TaskStatus mergedStatus = runTask(indexTask);\n    final TaskStatus status = taskStorage.getStatus(indexTask.getId()).get();\n    final List<DataSegment> publishedSegments = byIntervalOrdering.sortedCopy(mdc.getPublished());\n    final List<DataSegment> loggedSegments = byIntervalOrdering.sortedCopy(tsqa.getInsertedSegments(indexTask.getId()));\n\n    Assert.assertEquals(\"statusCode\", TaskState.SUCCESS, status.getStatusCode());\n    Assert.assertEquals(\"merged statusCode\", TaskState.SUCCESS, mergedStatus.getStatusCode());\n    Assert.assertEquals(\"segments logged vs published\", loggedSegments, publishedSegments);\n    Assert.assertEquals(\"num segments published\", 2, mdc.getPublished().size());\n    Assert.assertEquals(\"num segments nuked\", 0, mdc.getNuked().size());\n\n    Assert.assertEquals(\"segment1 datasource\", \"foo\", publishedSegments.get(0).getDataSource());\n    Assert.assertEquals(\"segment1 interval\", Intervals.of(\"2010-01-01/P1D\"), publishedSegments.get(0).getInterval());\n    Assert.assertEquals(\n        \"segment1 dimensions\",\n        ImmutableList.of(\"dim1\", \"dim2\"),\n        publishedSegments.get(0).getDimensions()\n    );\n    Assert.assertEquals(\"segment1 metrics\", ImmutableList.of(\"met\"), publishedSegments.get(0).getMetrics());\n\n    Assert.assertEquals(\"segment2 datasource\", \"foo\", publishedSegments.get(1).getDataSource());\n    Assert.assertEquals(\"segment2 interval\", Intervals.of(\"2010-01-02/P1D\"), publishedSegments.get(1).getInterval());\n    Assert.assertEquals(\n        \"segment2 dimensions\",\n        ImmutableList.of(\"dim1\", \"dim2\"),\n        publishedSegments.get(1).getDimensions()\n    );\n    Assert.assertEquals(\"segment2 metrics\", ImmutableList.of(\"met\"), publishedSegments.get(1).getMetrics());\n  }\n","date":"2018-08-31 00:56:26","endLine":744,"groupId":"7933","id":1,"instanceNumber":1,"isCurCommit":0,"methodName":"testIndexTask","params":"()","path":"/mnt/clonedata/CloneManagementServer/ManagementServer/consistResult/result-druid-10-0.7/blobInfo/CC_OUT/blobs/0d/e8debf718ced71c74552e50d9d420f45cdfee4.src","preCode":"  public void testIndexTask() throws Exception\n  {\n    final Task indexTask = new IndexTask(\n        null,\n        null,\n        new IndexIngestionSpec(\n            new DataSchema(\n                \"foo\",\n                null,\n                new AggregatorFactory[]{new DoubleSumAggregatorFactory(\"met\", \"met\")},\n                new UniformGranularitySpec(\n                    Granularities.DAY,\n                    null,\n                    ImmutableList.of(Intervals.of(\"2010-01-01/P2D\"))\n                ),\n                null,\n                mapper\n            ),\n            new IndexIOConfig(new MockFirehoseFactory(false), false),\n            new IndexTuningConfig(\n                10000,\n                10,\n                null,\n                null,\n                null,\n                null,\n                indexSpec,\n                3,\n                true,\n                true,\n                false,\n                null,\n                null,\n                null,\n                null,\n                null,\n                null,\n                null\n            )\n        ),\n        null,\n        AuthTestUtils.TEST_AUTHORIZER_MAPPER,\n        null,\n        ROW_INGESTION_METERS_FACTORY\n    );\n\n    final Optional<TaskStatus> preRunTaskStatus = tsqa.getStatus(indexTask.getId());\n    Assert.assertTrue(\"pre run task status not present\", !preRunTaskStatus.isPresent());\n\n    final TaskStatus mergedStatus = runTask(indexTask);\n    final TaskStatus status = taskStorage.getStatus(indexTask.getId()).get();\n    final List<DataSegment> publishedSegments = byIntervalOrdering.sortedCopy(mdc.getPublished());\n    final List<DataSegment> loggedSegments = byIntervalOrdering.sortedCopy(tsqa.getInsertedSegments(indexTask.getId()));\n\n    Assert.assertEquals(\"statusCode\", TaskState.SUCCESS, status.getStatusCode());\n    Assert.assertEquals(\"merged statusCode\", TaskState.SUCCESS, mergedStatus.getStatusCode());\n    Assert.assertEquals(\"segments logged vs published\", loggedSegments, publishedSegments);\n    Assert.assertEquals(\"num segments published\", 2, mdc.getPublished().size());\n    Assert.assertEquals(\"num segments nuked\", 0, mdc.getNuked().size());\n\n    Assert.assertEquals(\"segment1 datasource\", \"foo\", publishedSegments.get(0).getDataSource());\n    Assert.assertEquals(\"segment1 interval\", Intervals.of(\"2010-01-01/P1D\"), publishedSegments.get(0).getInterval());\n    Assert.assertEquals(\n        \"segment1 dimensions\",\n        ImmutableList.of(\"dim1\", \"dim2\"),\n        publishedSegments.get(0).getDimensions()\n    );\n    Assert.assertEquals(\"segment1 metrics\", ImmutableList.of(\"met\"), publishedSegments.get(0).getMetrics());\n\n    Assert.assertEquals(\"segment2 datasource\", \"foo\", publishedSegments.get(1).getDataSource());\n    Assert.assertEquals(\"segment2 interval\", Intervals.of(\"2010-01-02/P1D\"), publishedSegments.get(1).getInterval());\n    Assert.assertEquals(\n        \"segment2 dimensions\",\n        ImmutableList.of(\"dim1\", \"dim2\"),\n        publishedSegments.get(1).getDimensions()\n    );\n    Assert.assertEquals(\"segment2 metrics\", ImmutableList.of(\"met\"), publishedSegments.get(1).getMetrics());\n  }\n","realPath":"indexing-service/src/test/java/org/apache/druid/indexing/overlord/TaskLifecycleTest.java","repoName":"druid","snippetEndLine":0,"snippetStartLine":0,"startLine":667,"status":"B"},{"authorDate":"2018-08-31 00:56:26","commitOrder":1,"curCode":"  public void testResumeTasks() throws Exception\n  {\n    final Task indexTask = new IndexTask(\n        null,\n        null,\n        new IndexIngestionSpec(\n            new DataSchema(\n                \"foo\",\n                null,\n                new AggregatorFactory[]{new DoubleSumAggregatorFactory(\"met\", \"met\")},\n                new UniformGranularitySpec(\n                    Granularities.DAY,\n                    null,\n                    ImmutableList.of(Intervals.of(\"2010-01-01/P2D\"))\n                ),\n                null,\n                mapper\n            ),\n            new IndexIOConfig(new MockFirehoseFactory(false), false),\n            new IndexTuningConfig(\n                10000,\n                10,\n                null,\n                null,\n                null,\n                null,\n                indexSpec,\n                null,\n                false,\n                null,\n                null,\n                null,\n                null,\n                null,\n                null,\n                null,\n                null,\n                null\n            )\n        ),\n        null,\n        AuthTestUtils.TEST_AUTHORIZER_MAPPER,\n        null,\n        ROW_INGESTION_METERS_FACTORY\n    );\n\n    final long startTime = System.currentTimeMillis();\n\n    \r\n    taskQueue.start();\n    taskStorage.insert(indexTask, TaskStatus.running(indexTask.getId()));\n\n    while (tsqa.getStatus(indexTask.getId()).get().isRunnable()) {\n      if (System.currentTimeMillis() > startTime + 10 * 1000) {\n        throw new ISE(\"Where did the task go?!: %s\", indexTask.getId());\n      }\n\n      Thread.sleep(100);\n    }\n\n    final TaskStatus status = taskStorage.getStatus(indexTask.getId()).get();\n    final List<DataSegment> publishedSegments = byIntervalOrdering.sortedCopy(mdc.getPublished());\n    final List<DataSegment> loggedSegments = byIntervalOrdering.sortedCopy(tsqa.getInsertedSegments(indexTask.getId()));\n\n    Assert.assertEquals(\"statusCode\", TaskState.SUCCESS, status.getStatusCode());\n    Assert.assertEquals(\"segments logged vs published\", loggedSegments, publishedSegments);\n    Assert.assertEquals(\"num segments published\", 2, mdc.getPublished().size());\n    Assert.assertEquals(\"num segments nuked\", 0, mdc.getNuked().size());\n\n    Assert.assertEquals(\"segment1 datasource\", \"foo\", publishedSegments.get(0).getDataSource());\n    Assert.assertEquals(\"segment1 interval\", Intervals.of(\"2010-01-01/P1D\"), publishedSegments.get(0).getInterval());\n    Assert.assertEquals(\n        \"segment1 dimensions\",\n        ImmutableList.of(\"dim1\", \"dim2\"),\n        publishedSegments.get(0).getDimensions()\n    );\n    Assert.assertEquals(\"segment1 metrics\", ImmutableList.of(\"met\"), publishedSegments.get(0).getMetrics());\n\n    Assert.assertEquals(\"segment2 datasource\", \"foo\", publishedSegments.get(1).getDataSource());\n    Assert.assertEquals(\"segment2 interval\", Intervals.of(\"2010-01-02/P1D\"), publishedSegments.get(1).getInterval());\n    Assert.assertEquals(\n        \"segment2 dimensions\",\n        ImmutableList.of(\"dim1\", \"dim2\"),\n        publishedSegments.get(1).getDimensions()\n    );\n    Assert.assertEquals(\"segment2 metrics\", ImmutableList.of(\"met\"), publishedSegments.get(1).getMetrics());\n  }\n","date":"2018-08-31 00:56:26","endLine":1220,"groupId":"14206","id":2,"instanceNumber":2,"isCurCommit":0,"methodName":"testResumeTasks","params":"()","path":"/mnt/clonedata/CloneManagementServer/ManagementServer/consistResult/result-druid-10-0.7/blobInfo/CC_OUT/blobs/0d/e8debf718ced71c74552e50d9d420f45cdfee4.src","preCode":"  public void testResumeTasks() throws Exception\n  {\n    final Task indexTask = new IndexTask(\n        null,\n        null,\n        new IndexIngestionSpec(\n            new DataSchema(\n                \"foo\",\n                null,\n                new AggregatorFactory[]{new DoubleSumAggregatorFactory(\"met\", \"met\")},\n                new UniformGranularitySpec(\n                    Granularities.DAY,\n                    null,\n                    ImmutableList.of(Intervals.of(\"2010-01-01/P2D\"))\n                ),\n                null,\n                mapper\n            ),\n            new IndexIOConfig(new MockFirehoseFactory(false), false),\n            new IndexTuningConfig(\n                10000,\n                10,\n                null,\n                null,\n                null,\n                null,\n                indexSpec,\n                null,\n                false,\n                null,\n                null,\n                null,\n                null,\n                null,\n                null,\n                null,\n                null,\n                null\n            )\n        ),\n        null,\n        AuthTestUtils.TEST_AUTHORIZER_MAPPER,\n        null,\n        ROW_INGESTION_METERS_FACTORY\n    );\n\n    final long startTime = System.currentTimeMillis();\n\n    \r\n    taskQueue.start();\n    taskStorage.insert(indexTask, TaskStatus.running(indexTask.getId()));\n\n    while (tsqa.getStatus(indexTask.getId()).get().isRunnable()) {\n      if (System.currentTimeMillis() > startTime + 10 * 1000) {\n        throw new ISE(\"Where did the task go?!: %s\", indexTask.getId());\n      }\n\n      Thread.sleep(100);\n    }\n\n    final TaskStatus status = taskStorage.getStatus(indexTask.getId()).get();\n    final List<DataSegment> publishedSegments = byIntervalOrdering.sortedCopy(mdc.getPublished());\n    final List<DataSegment> loggedSegments = byIntervalOrdering.sortedCopy(tsqa.getInsertedSegments(indexTask.getId()));\n\n    Assert.assertEquals(\"statusCode\", TaskState.SUCCESS, status.getStatusCode());\n    Assert.assertEquals(\"segments logged vs published\", loggedSegments, publishedSegments);\n    Assert.assertEquals(\"num segments published\", 2, mdc.getPublished().size());\n    Assert.assertEquals(\"num segments nuked\", 0, mdc.getNuked().size());\n\n    Assert.assertEquals(\"segment1 datasource\", \"foo\", publishedSegments.get(0).getDataSource());\n    Assert.assertEquals(\"segment1 interval\", Intervals.of(\"2010-01-01/P1D\"), publishedSegments.get(0).getInterval());\n    Assert.assertEquals(\n        \"segment1 dimensions\",\n        ImmutableList.of(\"dim1\", \"dim2\"),\n        publishedSegments.get(0).getDimensions()\n    );\n    Assert.assertEquals(\"segment1 metrics\", ImmutableList.of(\"met\"), publishedSegments.get(0).getMetrics());\n\n    Assert.assertEquals(\"segment2 datasource\", \"foo\", publishedSegments.get(1).getDataSource());\n    Assert.assertEquals(\"segment2 interval\", Intervals.of(\"2010-01-02/P1D\"), publishedSegments.get(1).getInterval());\n    Assert.assertEquals(\n        \"segment2 dimensions\",\n        ImmutableList.of(\"dim1\", \"dim2\"),\n        publishedSegments.get(1).getDimensions()\n    );\n    Assert.assertEquals(\"segment2 metrics\", ImmutableList.of(\"met\"), publishedSegments.get(1).getMetrics());\n  }\n","realPath":"indexing-service/src/test/java/org/apache/druid/indexing/overlord/TaskLifecycleTest.java","repoName":"druid","snippetEndLine":0,"snippetStartLine":0,"startLine":1134,"status":"B"}],"commitId":"431d3d8497f9079857c3baa7ae7ab4cb44a22355","commitMessage":"@@@Rename io.druid to org.apache.druid. (#6266)\n\n* Rename io.druid to org.apache.druid.\n\n* Fix META-INF files and remove some benchmark results.\n\n* MonitorsConfig update for metrics package migration.\n\n* Reorder some dimensions in inner queries for some reason.\n\n* Fix protobuf tests.\n","date":"2018-08-31 00:56:26","modifiedFileCount":"5","status":"B","submitter":"Gian Merlino"},{"authorTime":"2018-10-07 07:45:07","codes":[{"authorDate":"2018-10-07 07:45:07","commitOrder":2,"curCode":"  public void testIndexTask() throws Exception\n  {\n    final Task indexTask = new IndexTask(\n        null,\n        null,\n        new IndexIngestionSpec(\n            new DataSchema(\n                \"foo\",\n                null,\n                new AggregatorFactory[]{new DoubleSumAggregatorFactory(\"met\", \"met\")},\n                new UniformGranularitySpec(\n                    Granularities.DAY,\n                    null,\n                    ImmutableList.of(Intervals.of(\"2010-01-01/P2D\"))\n                ),\n                null,\n                mapper\n            ),\n            new IndexIOConfig(new MockFirehoseFactory(false), false),\n            new IndexTuningConfig(\n                10000,\n                10,\n                null,\n                null,\n                null,\n                null,\n                null,\n                indexSpec,\n                3,\n                true,\n                true,\n                false,\n                null,\n                null,\n                null,\n                null,\n                null,\n                null,\n                null\n            )\n        ),\n        null,\n        AuthTestUtils.TEST_AUTHORIZER_MAPPER,\n        null,\n        ROW_INGESTION_METERS_FACTORY\n    );\n\n    final Optional<TaskStatus> preRunTaskStatus = tsqa.getStatus(indexTask.getId());\n    Assert.assertTrue(\"pre run task status not present\", !preRunTaskStatus.isPresent());\n\n    final TaskStatus mergedStatus = runTask(indexTask);\n    final TaskStatus status = taskStorage.getStatus(indexTask.getId()).get();\n    final List<DataSegment> publishedSegments = byIntervalOrdering.sortedCopy(mdc.getPublished());\n    final List<DataSegment> loggedSegments = byIntervalOrdering.sortedCopy(tsqa.getInsertedSegments(indexTask.getId()));\n\n    Assert.assertEquals(\"statusCode\", TaskState.SUCCESS, status.getStatusCode());\n    Assert.assertEquals(\"merged statusCode\", TaskState.SUCCESS, mergedStatus.getStatusCode());\n    Assert.assertEquals(\"segments logged vs published\", loggedSegments, publishedSegments);\n    Assert.assertEquals(\"num segments published\", 2, mdc.getPublished().size());\n    Assert.assertEquals(\"num segments nuked\", 0, mdc.getNuked().size());\n\n    Assert.assertEquals(\"segment1 datasource\", \"foo\", publishedSegments.get(0).getDataSource());\n    Assert.assertEquals(\"segment1 interval\", Intervals.of(\"2010-01-01/P1D\"), publishedSegments.get(0).getInterval());\n    Assert.assertEquals(\n        \"segment1 dimensions\",\n        ImmutableList.of(\"dim1\", \"dim2\"),\n        publishedSegments.get(0).getDimensions()\n    );\n    Assert.assertEquals(\"segment1 metrics\", ImmutableList.of(\"met\"), publishedSegments.get(0).getMetrics());\n\n    Assert.assertEquals(\"segment2 datasource\", \"foo\", publishedSegments.get(1).getDataSource());\n    Assert.assertEquals(\"segment2 interval\", Intervals.of(\"2010-01-02/P1D\"), publishedSegments.get(1).getInterval());\n    Assert.assertEquals(\n        \"segment2 dimensions\",\n        ImmutableList.of(\"dim1\", \"dim2\"),\n        publishedSegments.get(1).getDimensions()\n    );\n    Assert.assertEquals(\"segment2 metrics\", ImmutableList.of(\"met\"), publishedSegments.get(1).getMetrics());\n  }\n","date":"2018-10-07 07:45:07","endLine":747,"groupId":"7933","id":3,"instanceNumber":1,"isCurCommit":0,"methodName":"testIndexTask","params":"()","path":"/mnt/clonedata/CloneManagementServer/ManagementServer/consistResult/result-druid-10-0.7/blobInfo/CC_OUT/blobs/af/814d9f8497924ba6a3bac7c9b37ec1f3b7fa97.src","preCode":"  public void testIndexTask() throws Exception\n  {\n    final Task indexTask = new IndexTask(\n        null,\n        null,\n        new IndexIngestionSpec(\n            new DataSchema(\n                \"foo\",\n                null,\n                new AggregatorFactory[]{new DoubleSumAggregatorFactory(\"met\", \"met\")},\n                new UniformGranularitySpec(\n                    Granularities.DAY,\n                    null,\n                    ImmutableList.of(Intervals.of(\"2010-01-01/P2D\"))\n                ),\n                null,\n                mapper\n            ),\n            new IndexIOConfig(new MockFirehoseFactory(false), false),\n            new IndexTuningConfig(\n                10000,\n                10,\n                null,\n                null,\n                null,\n                null,\n                indexSpec,\n                3,\n                true,\n                true,\n                false,\n                null,\n                null,\n                null,\n                null,\n                null,\n                null,\n                null\n            )\n        ),\n        null,\n        AuthTestUtils.TEST_AUTHORIZER_MAPPER,\n        null,\n        ROW_INGESTION_METERS_FACTORY\n    );\n\n    final Optional<TaskStatus> preRunTaskStatus = tsqa.getStatus(indexTask.getId());\n    Assert.assertTrue(\"pre run task status not present\", !preRunTaskStatus.isPresent());\n\n    final TaskStatus mergedStatus = runTask(indexTask);\n    final TaskStatus status = taskStorage.getStatus(indexTask.getId()).get();\n    final List<DataSegment> publishedSegments = byIntervalOrdering.sortedCopy(mdc.getPublished());\n    final List<DataSegment> loggedSegments = byIntervalOrdering.sortedCopy(tsqa.getInsertedSegments(indexTask.getId()));\n\n    Assert.assertEquals(\"statusCode\", TaskState.SUCCESS, status.getStatusCode());\n    Assert.assertEquals(\"merged statusCode\", TaskState.SUCCESS, mergedStatus.getStatusCode());\n    Assert.assertEquals(\"segments logged vs published\", loggedSegments, publishedSegments);\n    Assert.assertEquals(\"num segments published\", 2, mdc.getPublished().size());\n    Assert.assertEquals(\"num segments nuked\", 0, mdc.getNuked().size());\n\n    Assert.assertEquals(\"segment1 datasource\", \"foo\", publishedSegments.get(0).getDataSource());\n    Assert.assertEquals(\"segment1 interval\", Intervals.of(\"2010-01-01/P1D\"), publishedSegments.get(0).getInterval());\n    Assert.assertEquals(\n        \"segment1 dimensions\",\n        ImmutableList.of(\"dim1\", \"dim2\"),\n        publishedSegments.get(0).getDimensions()\n    );\n    Assert.assertEquals(\"segment1 metrics\", ImmutableList.of(\"met\"), publishedSegments.get(0).getMetrics());\n\n    Assert.assertEquals(\"segment2 datasource\", \"foo\", publishedSegments.get(1).getDataSource());\n    Assert.assertEquals(\"segment2 interval\", Intervals.of(\"2010-01-02/P1D\"), publishedSegments.get(1).getInterval());\n    Assert.assertEquals(\n        \"segment2 dimensions\",\n        ImmutableList.of(\"dim1\", \"dim2\"),\n        publishedSegments.get(1).getDimensions()\n    );\n    Assert.assertEquals(\"segment2 metrics\", ImmutableList.of(\"met\"), publishedSegments.get(1).getMetrics());\n  }\n","realPath":"indexing-service/src/test/java/org/apache/druid/indexing/overlord/TaskLifecycleTest.java","repoName":"druid","snippetEndLine":0,"snippetStartLine":0,"startLine":669,"status":"M"},{"authorDate":"2018-10-07 07:45:07","commitOrder":2,"curCode":"  public void testResumeTasks() throws Exception\n  {\n    final Task indexTask = new IndexTask(\n        null,\n        null,\n        new IndexIngestionSpec(\n            new DataSchema(\n                \"foo\",\n                null,\n                new AggregatorFactory[]{new DoubleSumAggregatorFactory(\"met\", \"met\")},\n                new UniformGranularitySpec(\n                    Granularities.DAY,\n                    null,\n                    ImmutableList.of(Intervals.of(\"2010-01-01/P2D\"))\n                ),\n                null,\n                mapper\n            ),\n            new IndexIOConfig(new MockFirehoseFactory(false), false),\n            new IndexTuningConfig(\n                10000,\n                10,\n                null,\n                null,\n                null,\n                null,\n                null,\n                indexSpec,\n                null,\n                false,\n                null,\n                null,\n                null,\n                null,\n                null,\n                null,\n                null,\n                null,\n                null\n            )\n        ),\n        null,\n        AuthTestUtils.TEST_AUTHORIZER_MAPPER,\n        null,\n        ROW_INGESTION_METERS_FACTORY\n    );\n\n    final long startTime = System.currentTimeMillis();\n\n    \r\n    taskQueue.start();\n    taskStorage.insert(indexTask, TaskStatus.running(indexTask.getId()));\n\n    while (tsqa.getStatus(indexTask.getId()).get().isRunnable()) {\n      if (System.currentTimeMillis() > startTime + 10 * 1000) {\n        throw new ISE(\"Where did the task go?!: %s\", indexTask.getId());\n      }\n\n      Thread.sleep(100);\n    }\n\n    final TaskStatus status = taskStorage.getStatus(indexTask.getId()).get();\n    final List<DataSegment> publishedSegments = byIntervalOrdering.sortedCopy(mdc.getPublished());\n    final List<DataSegment> loggedSegments = byIntervalOrdering.sortedCopy(tsqa.getInsertedSegments(indexTask.getId()));\n\n    Assert.assertEquals(\"statusCode\", TaskState.SUCCESS, status.getStatusCode());\n    Assert.assertEquals(\"segments logged vs published\", loggedSegments, publishedSegments);\n    Assert.assertEquals(\"num segments published\", 2, mdc.getPublished().size());\n    Assert.assertEquals(\"num segments nuked\", 0, mdc.getNuked().size());\n\n    Assert.assertEquals(\"segment1 datasource\", \"foo\", publishedSegments.get(0).getDataSource());\n    Assert.assertEquals(\"segment1 interval\", Intervals.of(\"2010-01-01/P1D\"), publishedSegments.get(0).getInterval());\n    Assert.assertEquals(\n        \"segment1 dimensions\",\n        ImmutableList.of(\"dim1\", \"dim2\"),\n        publishedSegments.get(0).getDimensions()\n    );\n    Assert.assertEquals(\"segment1 metrics\", ImmutableList.of(\"met\"), publishedSegments.get(0).getMetrics());\n\n    Assert.assertEquals(\"segment2 datasource\", \"foo\", publishedSegments.get(1).getDataSource());\n    Assert.assertEquals(\"segment2 interval\", Intervals.of(\"2010-01-02/P1D\"), publishedSegments.get(1).getInterval());\n    Assert.assertEquals(\n        \"segment2 dimensions\",\n        ImmutableList.of(\"dim1\", \"dim2\"),\n        publishedSegments.get(1).getDimensions()\n    );\n    Assert.assertEquals(\"segment2 metrics\", ImmutableList.of(\"met\"), publishedSegments.get(1).getMetrics());\n  }\n","date":"2018-10-07 07:45:07","endLine":1225,"groupId":"14206","id":4,"instanceNumber":2,"isCurCommit":0,"methodName":"testResumeTasks","params":"()","path":"/mnt/clonedata/CloneManagementServer/ManagementServer/consistResult/result-druid-10-0.7/blobInfo/CC_OUT/blobs/af/814d9f8497924ba6a3bac7c9b37ec1f3b7fa97.src","preCode":"  public void testResumeTasks() throws Exception\n  {\n    final Task indexTask = new IndexTask(\n        null,\n        null,\n        new IndexIngestionSpec(\n            new DataSchema(\n                \"foo\",\n                null,\n                new AggregatorFactory[]{new DoubleSumAggregatorFactory(\"met\", \"met\")},\n                new UniformGranularitySpec(\n                    Granularities.DAY,\n                    null,\n                    ImmutableList.of(Intervals.of(\"2010-01-01/P2D\"))\n                ),\n                null,\n                mapper\n            ),\n            new IndexIOConfig(new MockFirehoseFactory(false), false),\n            new IndexTuningConfig(\n                10000,\n                10,\n                null,\n                null,\n                null,\n                null,\n                indexSpec,\n                null,\n                false,\n                null,\n                null,\n                null,\n                null,\n                null,\n                null,\n                null,\n                null,\n                null\n            )\n        ),\n        null,\n        AuthTestUtils.TEST_AUTHORIZER_MAPPER,\n        null,\n        ROW_INGESTION_METERS_FACTORY\n    );\n\n    final long startTime = System.currentTimeMillis();\n\n    \r\n    taskQueue.start();\n    taskStorage.insert(indexTask, TaskStatus.running(indexTask.getId()));\n\n    while (tsqa.getStatus(indexTask.getId()).get().isRunnable()) {\n      if (System.currentTimeMillis() > startTime + 10 * 1000) {\n        throw new ISE(\"Where did the task go?!: %s\", indexTask.getId());\n      }\n\n      Thread.sleep(100);\n    }\n\n    final TaskStatus status = taskStorage.getStatus(indexTask.getId()).get();\n    final List<DataSegment> publishedSegments = byIntervalOrdering.sortedCopy(mdc.getPublished());\n    final List<DataSegment> loggedSegments = byIntervalOrdering.sortedCopy(tsqa.getInsertedSegments(indexTask.getId()));\n\n    Assert.assertEquals(\"statusCode\", TaskState.SUCCESS, status.getStatusCode());\n    Assert.assertEquals(\"segments logged vs published\", loggedSegments, publishedSegments);\n    Assert.assertEquals(\"num segments published\", 2, mdc.getPublished().size());\n    Assert.assertEquals(\"num segments nuked\", 0, mdc.getNuked().size());\n\n    Assert.assertEquals(\"segment1 datasource\", \"foo\", publishedSegments.get(0).getDataSource());\n    Assert.assertEquals(\"segment1 interval\", Intervals.of(\"2010-01-01/P1D\"), publishedSegments.get(0).getInterval());\n    Assert.assertEquals(\n        \"segment1 dimensions\",\n        ImmutableList.of(\"dim1\", \"dim2\"),\n        publishedSegments.get(0).getDimensions()\n    );\n    Assert.assertEquals(\"segment1 metrics\", ImmutableList.of(\"met\"), publishedSegments.get(0).getMetrics());\n\n    Assert.assertEquals(\"segment2 datasource\", \"foo\", publishedSegments.get(1).getDataSource());\n    Assert.assertEquals(\"segment2 interval\", Intervals.of(\"2010-01-02/P1D\"), publishedSegments.get(1).getInterval());\n    Assert.assertEquals(\n        \"segment2 dimensions\",\n        ImmutableList.of(\"dim1\", \"dim2\"),\n        publishedSegments.get(1).getDimensions()\n    );\n    Assert.assertEquals(\"segment2 metrics\", ImmutableList.of(\"met\"), publishedSegments.get(1).getMetrics());\n  }\n","realPath":"indexing-service/src/test/java/org/apache/druid/indexing/overlord/TaskLifecycleTest.java","repoName":"druid","snippetEndLine":0,"snippetStartLine":0,"startLine":1138,"status":"M"}],"commitId":"45aa51a00c642a501834e2dfe54d68cbab8e0464","commitMessage":"@@@Add support hash partitioning by a subset of dimensions to indexTask (#6326)\n\n* Add support hash partitioning by a subset of dimensions to indexTask\n\n* add doc\n\n* fix style\n\n* fix test\n\n* fix doc\n\n* fix build\n","date":"2018-10-07 07:45:07","modifiedFileCount":"9","status":"M","submitter":"Jihoon Son"},{"authorTime":"2019-01-11 01:50:14","codes":[{"authorDate":"2019-01-11 01:50:14","commitOrder":3,"curCode":"  public void testIndexTask() throws Exception\n  {\n    final Task indexTask = new IndexTask(\n        null,\n        null,\n        new IndexIngestionSpec(\n            new DataSchema(\n                \"foo\",\n                null,\n                new AggregatorFactory[]{new DoubleSumAggregatorFactory(\"met\", \"met\")},\n                new UniformGranularitySpec(\n                    Granularities.DAY,\n                    null,\n                    ImmutableList.of(Intervals.of(\"2010-01-01/P2D\"))\n                ),\n                null,\n                mapper\n            ),\n            new IndexIOConfig(new MockFirehoseFactory(false), false),\n            new IndexTuningConfig(\n                null,\n                10000,\n                10,\n                null,\n                null,\n                null,\n                null,\n                null,\n                indexSpec,\n                3,\n                true,\n                true,\n                false,\n                null,\n                null,\n                null,\n                null,\n                null,\n                null,\n                null\n            )\n        ),\n        null,\n        AuthTestUtils.TEST_AUTHORIZER_MAPPER,\n        null,\n        ROW_INGESTION_METERS_FACTORY\n    );\n\n    final Optional<TaskStatus> preRunTaskStatus = tsqa.getStatus(indexTask.getId());\n    Assert.assertTrue(\"pre run task status not present\", !preRunTaskStatus.isPresent());\n\n    final TaskStatus mergedStatus = runTask(indexTask);\n    final TaskStatus status = taskStorage.getStatus(indexTask.getId()).get();\n    final List<DataSegment> publishedSegments = byIntervalOrdering.sortedCopy(mdc.getPublished());\n    final List<DataSegment> loggedSegments = byIntervalOrdering.sortedCopy(tsqa.getInsertedSegments(indexTask.getId()));\n\n    Assert.assertEquals(\"statusCode\", TaskState.SUCCESS, status.getStatusCode());\n    Assert.assertEquals(\"merged statusCode\", TaskState.SUCCESS, mergedStatus.getStatusCode());\n    Assert.assertEquals(\"segments logged vs published\", loggedSegments, publishedSegments);\n    Assert.assertEquals(\"num segments published\", 2, mdc.getPublished().size());\n    Assert.assertEquals(\"num segments nuked\", 0, mdc.getNuked().size());\n\n    Assert.assertEquals(\"segment1 datasource\", \"foo\", publishedSegments.get(0).getDataSource());\n    Assert.assertEquals(\"segment1 interval\", Intervals.of(\"2010-01-01/P1D\"), publishedSegments.get(0).getInterval());\n    Assert.assertEquals(\n        \"segment1 dimensions\",\n        ImmutableList.of(\"dim1\", \"dim2\"),\n        publishedSegments.get(0).getDimensions()\n    );\n    Assert.assertEquals(\"segment1 metrics\", ImmutableList.of(\"met\"), publishedSegments.get(0).getMetrics());\n\n    Assert.assertEquals(\"segment2 datasource\", \"foo\", publishedSegments.get(1).getDataSource());\n    Assert.assertEquals(\"segment2 interval\", Intervals.of(\"2010-01-02/P1D\"), publishedSegments.get(1).getInterval());\n    Assert.assertEquals(\n        \"segment2 dimensions\",\n        ImmutableList.of(\"dim1\", \"dim2\"),\n        publishedSegments.get(1).getDimensions()\n    );\n    Assert.assertEquals(\"segment2 metrics\", ImmutableList.of(\"met\"), publishedSegments.get(1).getMetrics());\n  }\n","date":"2019-01-11 01:50:14","endLine":749,"groupId":"7933","id":5,"instanceNumber":1,"isCurCommit":0,"methodName":"testIndexTask","params":"()","path":"/mnt/clonedata/CloneManagementServer/ManagementServer/consistResult/result-druid-10-0.7/blobInfo/CC_OUT/blobs/66/f0ec9d0c2e9ac9adeaccd6fccea8a95c9fa97f.src","preCode":"  public void testIndexTask() throws Exception\n  {\n    final Task indexTask = new IndexTask(\n        null,\n        null,\n        new IndexIngestionSpec(\n            new DataSchema(\n                \"foo\",\n                null,\n                new AggregatorFactory[]{new DoubleSumAggregatorFactory(\"met\", \"met\")},\n                new UniformGranularitySpec(\n                    Granularities.DAY,\n                    null,\n                    ImmutableList.of(Intervals.of(\"2010-01-01/P2D\"))\n                ),\n                null,\n                mapper\n            ),\n            new IndexIOConfig(new MockFirehoseFactory(false), false),\n            new IndexTuningConfig(\n                10000,\n                10,\n                null,\n                null,\n                null,\n                null,\n                null,\n                indexSpec,\n                3,\n                true,\n                true,\n                false,\n                null,\n                null,\n                null,\n                null,\n                null,\n                null,\n                null\n            )\n        ),\n        null,\n        AuthTestUtils.TEST_AUTHORIZER_MAPPER,\n        null,\n        ROW_INGESTION_METERS_FACTORY\n    );\n\n    final Optional<TaskStatus> preRunTaskStatus = tsqa.getStatus(indexTask.getId());\n    Assert.assertTrue(\"pre run task status not present\", !preRunTaskStatus.isPresent());\n\n    final TaskStatus mergedStatus = runTask(indexTask);\n    final TaskStatus status = taskStorage.getStatus(indexTask.getId()).get();\n    final List<DataSegment> publishedSegments = byIntervalOrdering.sortedCopy(mdc.getPublished());\n    final List<DataSegment> loggedSegments = byIntervalOrdering.sortedCopy(tsqa.getInsertedSegments(indexTask.getId()));\n\n    Assert.assertEquals(\"statusCode\", TaskState.SUCCESS, status.getStatusCode());\n    Assert.assertEquals(\"merged statusCode\", TaskState.SUCCESS, mergedStatus.getStatusCode());\n    Assert.assertEquals(\"segments logged vs published\", loggedSegments, publishedSegments);\n    Assert.assertEquals(\"num segments published\", 2, mdc.getPublished().size());\n    Assert.assertEquals(\"num segments nuked\", 0, mdc.getNuked().size());\n\n    Assert.assertEquals(\"segment1 datasource\", \"foo\", publishedSegments.get(0).getDataSource());\n    Assert.assertEquals(\"segment1 interval\", Intervals.of(\"2010-01-01/P1D\"), publishedSegments.get(0).getInterval());\n    Assert.assertEquals(\n        \"segment1 dimensions\",\n        ImmutableList.of(\"dim1\", \"dim2\"),\n        publishedSegments.get(0).getDimensions()\n    );\n    Assert.assertEquals(\"segment1 metrics\", ImmutableList.of(\"met\"), publishedSegments.get(0).getMetrics());\n\n    Assert.assertEquals(\"segment2 datasource\", \"foo\", publishedSegments.get(1).getDataSource());\n    Assert.assertEquals(\"segment2 interval\", Intervals.of(\"2010-01-02/P1D\"), publishedSegments.get(1).getInterval());\n    Assert.assertEquals(\n        \"segment2 dimensions\",\n        ImmutableList.of(\"dim1\", \"dim2\"),\n        publishedSegments.get(1).getDimensions()\n    );\n    Assert.assertEquals(\"segment2 metrics\", ImmutableList.of(\"met\"), publishedSegments.get(1).getMetrics());\n  }\n","realPath":"indexing-service/src/test/java/org/apache/druid/indexing/overlord/TaskLifecycleTest.java","repoName":"druid","snippetEndLine":0,"snippetStartLine":0,"startLine":670,"status":"M"},{"authorDate":"2019-01-11 01:50:14","commitOrder":3,"curCode":"  public void testResumeTasks() throws Exception\n  {\n    final Task indexTask = new IndexTask(\n        null,\n        null,\n        new IndexIngestionSpec(\n            new DataSchema(\n                \"foo\",\n                null,\n                new AggregatorFactory[]{new DoubleSumAggregatorFactory(\"met\", \"met\")},\n                new UniformGranularitySpec(\n                    Granularities.DAY,\n                    null,\n                    ImmutableList.of(Intervals.of(\"2010-01-01/P2D\"))\n                ),\n                null,\n                mapper\n            ),\n            new IndexIOConfig(new MockFirehoseFactory(false), false),\n            new IndexTuningConfig(\n                null,\n                10000,\n                10,\n                null,\n                null,\n                null,\n                null,\n                null,\n                indexSpec,\n                null,\n                false,\n                null,\n                null,\n                null,\n                null,\n                null,\n                null,\n                null,\n                null,\n                null\n            )\n        ),\n        null,\n        AuthTestUtils.TEST_AUTHORIZER_MAPPER,\n        null,\n        ROW_INGESTION_METERS_FACTORY\n    );\n\n    final long startTime = System.currentTimeMillis();\n\n    \r\n    taskQueue.start();\n    taskStorage.insert(indexTask, TaskStatus.running(indexTask.getId()));\n\n    while (tsqa.getStatus(indexTask.getId()).get().isRunnable()) {\n      if (System.currentTimeMillis() > startTime + 10 * 1000) {\n        throw new ISE(\"Where did the task go?!: %s\", indexTask.getId());\n      }\n\n      Thread.sleep(100);\n    }\n\n    final TaskStatus status = taskStorage.getStatus(indexTask.getId()).get();\n    final List<DataSegment> publishedSegments = byIntervalOrdering.sortedCopy(mdc.getPublished());\n    final List<DataSegment> loggedSegments = byIntervalOrdering.sortedCopy(tsqa.getInsertedSegments(indexTask.getId()));\n\n    Assert.assertEquals(\"statusCode\", TaskState.SUCCESS, status.getStatusCode());\n    Assert.assertEquals(\"segments logged vs published\", loggedSegments, publishedSegments);\n    Assert.assertEquals(\"num segments published\", 2, mdc.getPublished().size());\n    Assert.assertEquals(\"num segments nuked\", 0, mdc.getNuked().size());\n\n    Assert.assertEquals(\"segment1 datasource\", \"foo\", publishedSegments.get(0).getDataSource());\n    Assert.assertEquals(\"segment1 interval\", Intervals.of(\"2010-01-01/P1D\"), publishedSegments.get(0).getInterval());\n    Assert.assertEquals(\n        \"segment1 dimensions\",\n        ImmutableList.of(\"dim1\", \"dim2\"),\n        publishedSegments.get(0).getDimensions()\n    );\n    Assert.assertEquals(\"segment1 metrics\", ImmutableList.of(\"met\"), publishedSegments.get(0).getMetrics());\n\n    Assert.assertEquals(\"segment2 datasource\", \"foo\", publishedSegments.get(1).getDataSource());\n    Assert.assertEquals(\"segment2 interval\", Intervals.of(\"2010-01-02/P1D\"), publishedSegments.get(1).getInterval());\n    Assert.assertEquals(\n        \"segment2 dimensions\",\n        ImmutableList.of(\"dim1\", \"dim2\"),\n        publishedSegments.get(1).getDimensions()\n    );\n    Assert.assertEquals(\"segment2 metrics\", ImmutableList.of(\"met\"), publishedSegments.get(1).getMetrics());\n  }\n","date":"2019-01-11 01:50:14","endLine":1229,"groupId":"14206","id":6,"instanceNumber":2,"isCurCommit":0,"methodName":"testResumeTasks","params":"()","path":"/mnt/clonedata/CloneManagementServer/ManagementServer/consistResult/result-druid-10-0.7/blobInfo/CC_OUT/blobs/66/f0ec9d0c2e9ac9adeaccd6fccea8a95c9fa97f.src","preCode":"  public void testResumeTasks() throws Exception\n  {\n    final Task indexTask = new IndexTask(\n        null,\n        null,\n        new IndexIngestionSpec(\n            new DataSchema(\n                \"foo\",\n                null,\n                new AggregatorFactory[]{new DoubleSumAggregatorFactory(\"met\", \"met\")},\n                new UniformGranularitySpec(\n                    Granularities.DAY,\n                    null,\n                    ImmutableList.of(Intervals.of(\"2010-01-01/P2D\"))\n                ),\n                null,\n                mapper\n            ),\n            new IndexIOConfig(new MockFirehoseFactory(false), false),\n            new IndexTuningConfig(\n                10000,\n                10,\n                null,\n                null,\n                null,\n                null,\n                null,\n                indexSpec,\n                null,\n                false,\n                null,\n                null,\n                null,\n                null,\n                null,\n                null,\n                null,\n                null,\n                null\n            )\n        ),\n        null,\n        AuthTestUtils.TEST_AUTHORIZER_MAPPER,\n        null,\n        ROW_INGESTION_METERS_FACTORY\n    );\n\n    final long startTime = System.currentTimeMillis();\n\n    \r\n    taskQueue.start();\n    taskStorage.insert(indexTask, TaskStatus.running(indexTask.getId()));\n\n    while (tsqa.getStatus(indexTask.getId()).get().isRunnable()) {\n      if (System.currentTimeMillis() > startTime + 10 * 1000) {\n        throw new ISE(\"Where did the task go?!: %s\", indexTask.getId());\n      }\n\n      Thread.sleep(100);\n    }\n\n    final TaskStatus status = taskStorage.getStatus(indexTask.getId()).get();\n    final List<DataSegment> publishedSegments = byIntervalOrdering.sortedCopy(mdc.getPublished());\n    final List<DataSegment> loggedSegments = byIntervalOrdering.sortedCopy(tsqa.getInsertedSegments(indexTask.getId()));\n\n    Assert.assertEquals(\"statusCode\", TaskState.SUCCESS, status.getStatusCode());\n    Assert.assertEquals(\"segments logged vs published\", loggedSegments, publishedSegments);\n    Assert.assertEquals(\"num segments published\", 2, mdc.getPublished().size());\n    Assert.assertEquals(\"num segments nuked\", 0, mdc.getNuked().size());\n\n    Assert.assertEquals(\"segment1 datasource\", \"foo\", publishedSegments.get(0).getDataSource());\n    Assert.assertEquals(\"segment1 interval\", Intervals.of(\"2010-01-01/P1D\"), publishedSegments.get(0).getInterval());\n    Assert.assertEquals(\n        \"segment1 dimensions\",\n        ImmutableList.of(\"dim1\", \"dim2\"),\n        publishedSegments.get(0).getDimensions()\n    );\n    Assert.assertEquals(\"segment1 metrics\", ImmutableList.of(\"met\"), publishedSegments.get(0).getMetrics());\n\n    Assert.assertEquals(\"segment2 datasource\", \"foo\", publishedSegments.get(1).getDataSource());\n    Assert.assertEquals(\"segment2 interval\", Intervals.of(\"2010-01-02/P1D\"), publishedSegments.get(1).getInterval());\n    Assert.assertEquals(\n        \"segment2 dimensions\",\n        ImmutableList.of(\"dim1\", \"dim2\"),\n        publishedSegments.get(1).getDimensions()\n    );\n    Assert.assertEquals(\"segment2 metrics\", ImmutableList.of(\"met\"), publishedSegments.get(1).getMetrics());\n  }\n","realPath":"indexing-service/src/test/java/org/apache/druid/indexing/overlord/TaskLifecycleTest.java","repoName":"druid","snippetEndLine":0,"snippetStartLine":0,"startLine":1141,"status":"M"}],"commitId":"c35a39d70bf705aa49c3a3c97bab87959bb80a4e","commitMessage":"@@@Add support maxRowsPerSegment for auto compaction (#6780)\n\n* Add support maxRowsPerSegment for auto compaction\n\n* fix build\n\n* fix build\n\n* fix teamcity\n\n* add test\n\n* fix test\n\n* address comment\n","date":"2019-01-11 01:50:14","modifiedFileCount":"33","status":"M","submitter":"Jihoon Son"},{"authorTime":"2019-03-16 14:29:25","codes":[{"authorDate":"2019-03-16 14:29:25","commitOrder":4,"curCode":"  public void testIndexTask() throws Exception\n  {\n    final Task indexTask = new IndexTask(\n        null,\n        null,\n        new IndexIngestionSpec(\n            new DataSchema(\n                \"foo\",\n                null,\n                new AggregatorFactory[]{new DoubleSumAggregatorFactory(\"met\", \"met\")},\n                new UniformGranularitySpec(\n                    Granularities.DAY,\n                    null,\n                    ImmutableList.of(Intervals.of(\"2010-01-01/P2D\"))\n                ),\n                null,\n                mapper\n            ),\n            new IndexIOConfig(new MockFirehoseFactory(false), false),\n            new IndexTuningConfig(\n                null,\n                10000,\n                10,\n                null,\n                null,\n                null,\n                null,\n                null,\n                indexSpec,\n                3,\n                true,\n                false,\n                null,\n                null,\n                null,\n                null,\n                null,\n                null,\n                null\n            )\n        ),\n        null,\n        AuthTestUtils.TEST_AUTHORIZER_MAPPER,\n        null,\n        ROW_INGESTION_METERS_FACTORY\n    );\n\n    final Optional<TaskStatus> preRunTaskStatus = tsqa.getStatus(indexTask.getId());\n    Assert.assertTrue(\"pre run task status not present\", !preRunTaskStatus.isPresent());\n\n    final TaskStatus mergedStatus = runTask(indexTask);\n    final TaskStatus status = taskStorage.getStatus(indexTask.getId()).get();\n    final List<DataSegment> publishedSegments = byIntervalOrdering.sortedCopy(mdc.getPublished());\n    final List<DataSegment> loggedSegments = byIntervalOrdering.sortedCopy(tsqa.getInsertedSegments(indexTask.getId()));\n\n    Assert.assertEquals(\"statusCode\", TaskState.SUCCESS, status.getStatusCode());\n    Assert.assertEquals(\"merged statusCode\", TaskState.SUCCESS, mergedStatus.getStatusCode());\n    Assert.assertEquals(\"segments logged vs published\", loggedSegments, publishedSegments);\n    Assert.assertEquals(\"num segments published\", 2, mdc.getPublished().size());\n    Assert.assertEquals(\"num segments nuked\", 0, mdc.getNuked().size());\n\n    Assert.assertEquals(\"segment1 datasource\", \"foo\", publishedSegments.get(0).getDataSource());\n    Assert.assertEquals(\"segment1 interval\", Intervals.of(\"2010-01-01/P1D\"), publishedSegments.get(0).getInterval());\n    Assert.assertEquals(\n        \"segment1 dimensions\",\n        ImmutableList.of(\"dim1\", \"dim2\"),\n        publishedSegments.get(0).getDimensions()\n    );\n    Assert.assertEquals(\"segment1 metrics\", ImmutableList.of(\"met\"), publishedSegments.get(0).getMetrics());\n\n    Assert.assertEquals(\"segment2 datasource\", \"foo\", publishedSegments.get(1).getDataSource());\n    Assert.assertEquals(\"segment2 interval\", Intervals.of(\"2010-01-02/P1D\"), publishedSegments.get(1).getInterval());\n    Assert.assertEquals(\n        \"segment2 dimensions\",\n        ImmutableList.of(\"dim1\", \"dim2\"),\n        publishedSegments.get(1).getDimensions()\n    );\n    Assert.assertEquals(\"segment2 metrics\", ImmutableList.of(\"met\"), publishedSegments.get(1).getMetrics());\n  }\n","date":"2019-03-16 14:29:25","endLine":742,"groupId":"7933","id":7,"instanceNumber":1,"isCurCommit":0,"methodName":"testIndexTask","params":"()","path":"/mnt/clonedata/CloneManagementServer/ManagementServer/consistResult/result-druid-10-0.7/blobInfo/CC_OUT/blobs/6f/2b99218a2649eb187fb0f28f22de2071f0c516.src","preCode":"  public void testIndexTask() throws Exception\n  {\n    final Task indexTask = new IndexTask(\n        null,\n        null,\n        new IndexIngestionSpec(\n            new DataSchema(\n                \"foo\",\n                null,\n                new AggregatorFactory[]{new DoubleSumAggregatorFactory(\"met\", \"met\")},\n                new UniformGranularitySpec(\n                    Granularities.DAY,\n                    null,\n                    ImmutableList.of(Intervals.of(\"2010-01-01/P2D\"))\n                ),\n                null,\n                mapper\n            ),\n            new IndexIOConfig(new MockFirehoseFactory(false), false),\n            new IndexTuningConfig(\n                null,\n                10000,\n                10,\n                null,\n                null,\n                null,\n                null,\n                null,\n                indexSpec,\n                3,\n                true,\n                true,\n                false,\n                null,\n                null,\n                null,\n                null,\n                null,\n                null,\n                null\n            )\n        ),\n        null,\n        AuthTestUtils.TEST_AUTHORIZER_MAPPER,\n        null,\n        ROW_INGESTION_METERS_FACTORY\n    );\n\n    final Optional<TaskStatus> preRunTaskStatus = tsqa.getStatus(indexTask.getId());\n    Assert.assertTrue(\"pre run task status not present\", !preRunTaskStatus.isPresent());\n\n    final TaskStatus mergedStatus = runTask(indexTask);\n    final TaskStatus status = taskStorage.getStatus(indexTask.getId()).get();\n    final List<DataSegment> publishedSegments = byIntervalOrdering.sortedCopy(mdc.getPublished());\n    final List<DataSegment> loggedSegments = byIntervalOrdering.sortedCopy(tsqa.getInsertedSegments(indexTask.getId()));\n\n    Assert.assertEquals(\"statusCode\", TaskState.SUCCESS, status.getStatusCode());\n    Assert.assertEquals(\"merged statusCode\", TaskState.SUCCESS, mergedStatus.getStatusCode());\n    Assert.assertEquals(\"segments logged vs published\", loggedSegments, publishedSegments);\n    Assert.assertEquals(\"num segments published\", 2, mdc.getPublished().size());\n    Assert.assertEquals(\"num segments nuked\", 0, mdc.getNuked().size());\n\n    Assert.assertEquals(\"segment1 datasource\", \"foo\", publishedSegments.get(0).getDataSource());\n    Assert.assertEquals(\"segment1 interval\", Intervals.of(\"2010-01-01/P1D\"), publishedSegments.get(0).getInterval());\n    Assert.assertEquals(\n        \"segment1 dimensions\",\n        ImmutableList.of(\"dim1\", \"dim2\"),\n        publishedSegments.get(0).getDimensions()\n    );\n    Assert.assertEquals(\"segment1 metrics\", ImmutableList.of(\"met\"), publishedSegments.get(0).getMetrics());\n\n    Assert.assertEquals(\"segment2 datasource\", \"foo\", publishedSegments.get(1).getDataSource());\n    Assert.assertEquals(\"segment2 interval\", Intervals.of(\"2010-01-02/P1D\"), publishedSegments.get(1).getInterval());\n    Assert.assertEquals(\n        \"segment2 dimensions\",\n        ImmutableList.of(\"dim1\", \"dim2\"),\n        publishedSegments.get(1).getDimensions()\n    );\n    Assert.assertEquals(\"segment2 metrics\", ImmutableList.of(\"met\"), publishedSegments.get(1).getMetrics());\n  }\n","realPath":"indexing-service/src/test/java/org/apache/druid/indexing/overlord/TaskLifecycleTest.java","repoName":"druid","snippetEndLine":0,"snippetStartLine":0,"startLine":664,"status":"M"},{"authorDate":"2019-03-16 14:29:25","commitOrder":4,"curCode":"  public void testResumeTasks() throws Exception\n  {\n    final Task indexTask = new IndexTask(\n        null,\n        null,\n        new IndexIngestionSpec(\n            new DataSchema(\n                \"foo\",\n                null,\n                new AggregatorFactory[]{new DoubleSumAggregatorFactory(\"met\", \"met\")},\n                new UniformGranularitySpec(\n                    Granularities.DAY,\n                    null,\n                    ImmutableList.of(Intervals.of(\"2010-01-01/P2D\"))\n                ),\n                null,\n                mapper\n            ),\n            new IndexIOConfig(new MockFirehoseFactory(false), false),\n            new IndexTuningConfig(\n                null,\n                10000,\n                10,\n                null,\n                null,\n                null,\n                null,\n                null,\n                indexSpec,\n                null,\n                false,\n                null,\n                null,\n                null,\n                null,\n                null,\n                null,\n                null,\n                null\n            )\n        ),\n        null,\n        AuthTestUtils.TEST_AUTHORIZER_MAPPER,\n        null,\n        ROW_INGESTION_METERS_FACTORY\n    );\n\n    final long startTime = System.currentTimeMillis();\n\n    \r\n    taskQueue.start();\n    taskStorage.insert(indexTask, TaskStatus.running(indexTask.getId()));\n\n    while (tsqa.getStatus(indexTask.getId()).get().isRunnable()) {\n      if (System.currentTimeMillis() > startTime + 10 * 1000) {\n        throw new ISE(\"Where did the task go?!: %s\", indexTask.getId());\n      }\n\n      Thread.sleep(100);\n    }\n\n    final TaskStatus status = taskStorage.getStatus(indexTask.getId()).get();\n    final List<DataSegment> publishedSegments = byIntervalOrdering.sortedCopy(mdc.getPublished());\n    final List<DataSegment> loggedSegments = byIntervalOrdering.sortedCopy(tsqa.getInsertedSegments(indexTask.getId()));\n\n    Assert.assertEquals(\"statusCode\", TaskState.SUCCESS, status.getStatusCode());\n    Assert.assertEquals(\"segments logged vs published\", loggedSegments, publishedSegments);\n    Assert.assertEquals(\"num segments published\", 2, mdc.getPublished().size());\n    Assert.assertEquals(\"num segments nuked\", 0, mdc.getNuked().size());\n\n    Assert.assertEquals(\"segment1 datasource\", \"foo\", publishedSegments.get(0).getDataSource());\n    Assert.assertEquals(\"segment1 interval\", Intervals.of(\"2010-01-01/P1D\"), publishedSegments.get(0).getInterval());\n    Assert.assertEquals(\n        \"segment1 dimensions\",\n        ImmutableList.of(\"dim1\", \"dim2\"),\n        publishedSegments.get(0).getDimensions()\n    );\n    Assert.assertEquals(\"segment1 metrics\", ImmutableList.of(\"met\"), publishedSegments.get(0).getMetrics());\n\n    Assert.assertEquals(\"segment2 datasource\", \"foo\", publishedSegments.get(1).getDataSource());\n    Assert.assertEquals(\"segment2 interval\", Intervals.of(\"2010-01-02/P1D\"), publishedSegments.get(1).getInterval());\n    Assert.assertEquals(\n        \"segment2 dimensions\",\n        ImmutableList.of(\"dim1\", \"dim2\"),\n        publishedSegments.get(1).getDimensions()\n    );\n    Assert.assertEquals(\"segment2 metrics\", ImmutableList.of(\"met\"), publishedSegments.get(1).getMetrics());\n  }\n","date":"2019-03-16 14:29:25","endLine":1220,"groupId":"14206","id":8,"instanceNumber":2,"isCurCommit":0,"methodName":"testResumeTasks","params":"()","path":"/mnt/clonedata/CloneManagementServer/ManagementServer/consistResult/result-druid-10-0.7/blobInfo/CC_OUT/blobs/6f/2b99218a2649eb187fb0f28f22de2071f0c516.src","preCode":"  public void testResumeTasks() throws Exception\n  {\n    final Task indexTask = new IndexTask(\n        null,\n        null,\n        new IndexIngestionSpec(\n            new DataSchema(\n                \"foo\",\n                null,\n                new AggregatorFactory[]{new DoubleSumAggregatorFactory(\"met\", \"met\")},\n                new UniformGranularitySpec(\n                    Granularities.DAY,\n                    null,\n                    ImmutableList.of(Intervals.of(\"2010-01-01/P2D\"))\n                ),\n                null,\n                mapper\n            ),\n            new IndexIOConfig(new MockFirehoseFactory(false), false),\n            new IndexTuningConfig(\n                null,\n                10000,\n                10,\n                null,\n                null,\n                null,\n                null,\n                null,\n                indexSpec,\n                null,\n                false,\n                null,\n                null,\n                null,\n                null,\n                null,\n                null,\n                null,\n                null,\n                null\n            )\n        ),\n        null,\n        AuthTestUtils.TEST_AUTHORIZER_MAPPER,\n        null,\n        ROW_INGESTION_METERS_FACTORY\n    );\n\n    final long startTime = System.currentTimeMillis();\n\n    \r\n    taskQueue.start();\n    taskStorage.insert(indexTask, TaskStatus.running(indexTask.getId()));\n\n    while (tsqa.getStatus(indexTask.getId()).get().isRunnable()) {\n      if (System.currentTimeMillis() > startTime + 10 * 1000) {\n        throw new ISE(\"Where did the task go?!: %s\", indexTask.getId());\n      }\n\n      Thread.sleep(100);\n    }\n\n    final TaskStatus status = taskStorage.getStatus(indexTask.getId()).get();\n    final List<DataSegment> publishedSegments = byIntervalOrdering.sortedCopy(mdc.getPublished());\n    final List<DataSegment> loggedSegments = byIntervalOrdering.sortedCopy(tsqa.getInsertedSegments(indexTask.getId()));\n\n    Assert.assertEquals(\"statusCode\", TaskState.SUCCESS, status.getStatusCode());\n    Assert.assertEquals(\"segments logged vs published\", loggedSegments, publishedSegments);\n    Assert.assertEquals(\"num segments published\", 2, mdc.getPublished().size());\n    Assert.assertEquals(\"num segments nuked\", 0, mdc.getNuked().size());\n\n    Assert.assertEquals(\"segment1 datasource\", \"foo\", publishedSegments.get(0).getDataSource());\n    Assert.assertEquals(\"segment1 interval\", Intervals.of(\"2010-01-01/P1D\"), publishedSegments.get(0).getInterval());\n    Assert.assertEquals(\n        \"segment1 dimensions\",\n        ImmutableList.of(\"dim1\", \"dim2\"),\n        publishedSegments.get(0).getDimensions()\n    );\n    Assert.assertEquals(\"segment1 metrics\", ImmutableList.of(\"met\"), publishedSegments.get(0).getMetrics());\n\n    Assert.assertEquals(\"segment2 datasource\", \"foo\", publishedSegments.get(1).getDataSource());\n    Assert.assertEquals(\"segment2 interval\", Intervals.of(\"2010-01-02/P1D\"), publishedSegments.get(1).getInterval());\n    Assert.assertEquals(\n        \"segment2 dimensions\",\n        ImmutableList.of(\"dim1\", \"dim2\"),\n        publishedSegments.get(1).getDimensions()\n    );\n    Assert.assertEquals(\"segment2 metrics\", ImmutableList.of(\"met\"), publishedSegments.get(1).getMetrics());\n  }\n","realPath":"indexing-service/src/test/java/org/apache/druid/indexing/overlord/TaskLifecycleTest.java","repoName":"druid","snippetEndLine":0,"snippetStartLine":0,"startLine":1133,"status":"M"}],"commitId":"892d1d35d6cc00487d583f05f1cc138782180ef9","commitMessage":"@@@Deprecate NoneShardSpec and drop support for automatic segment merge (#6883)\n\n* Deprecate noneShardSpec\n\n* clean up noneShardSpec constructor\n\n* revert unnecessary change\n\n* Deprecate mergeTask\n\n* add more doc\n\n* remove convert from indexMerger\n\n* Remove mergeTask\n\n* remove HadoopDruidConverterConfig\n\n* fix build\n\n* fix build\n\n* fix teamcity\n\n* fix teamcity\n\n* fix ServerModule\n\n* fix compilation\n\n* fix compilation\n","date":"2019-03-16 14:29:25","modifiedFileCount":"40","status":"M","submitter":"Jihoon Son"},{"authorTime":"2019-07-11 03:22:24","codes":[{"authorDate":"2019-07-11 03:22:24","commitOrder":5,"curCode":"  public void testIndexTask() throws Exception\n  {\n    final Task indexTask = new IndexTask(\n        null,\n        null,\n        new IndexIngestionSpec(\n            new DataSchema(\n                \"foo\",\n                null,\n                new AggregatorFactory[]{new DoubleSumAggregatorFactory(\"met\", \"met\")},\n                new UniformGranularitySpec(\n                    Granularities.DAY,\n                    null,\n                    ImmutableList.of(Intervals.of(\"2010-01-01/P2D\"))\n                ),\n                null,\n                mapper\n            ),\n            new IndexIOConfig(new MockFirehoseFactory(false), false),\n            new IndexTuningConfig(\n                null,\n                10000,\n                10,\n                null,\n                null,\n                null,\n                null,\n                null,\n                indexSpec,\n                null,\n                3,\n                true,\n                false,\n                null,\n                null,\n                null,\n                null,\n                null,\n                null,\n                null\n            )\n        ),\n        null,\n        AuthTestUtils.TEST_AUTHORIZER_MAPPER,\n        null,\n        ROW_INGESTION_METERS_FACTORY\n    );\n\n    final Optional<TaskStatus> preRunTaskStatus = tsqa.getStatus(indexTask.getId());\n    Assert.assertTrue(\"pre run task status not present\", !preRunTaskStatus.isPresent());\n\n    final TaskStatus mergedStatus = runTask(indexTask);\n    final TaskStatus status = taskStorage.getStatus(indexTask.getId()).get();\n    final List<DataSegment> publishedSegments = byIntervalOrdering.sortedCopy(mdc.getPublished());\n    final List<DataSegment> loggedSegments = byIntervalOrdering.sortedCopy(tsqa.getInsertedSegments(indexTask.getId()));\n\n    Assert.assertEquals(\"statusCode\", TaskState.SUCCESS, status.getStatusCode());\n    Assert.assertEquals(\"merged statusCode\", TaskState.SUCCESS, mergedStatus.getStatusCode());\n    Assert.assertEquals(\"segments logged vs published\", loggedSegments, publishedSegments);\n    Assert.assertEquals(\"num segments published\", 2, mdc.getPublished().size());\n    Assert.assertEquals(\"num segments nuked\", 0, mdc.getNuked().size());\n\n    Assert.assertEquals(\"segment1 datasource\", \"foo\", publishedSegments.get(0).getDataSource());\n    Assert.assertEquals(\"segment1 interval\", Intervals.of(\"2010-01-01/P1D\"), publishedSegments.get(0).getInterval());\n    Assert.assertEquals(\n        \"segment1 dimensions\",\n        ImmutableList.of(\"dim1\", \"dim2\"),\n        publishedSegments.get(0).getDimensions()\n    );\n    Assert.assertEquals(\"segment1 metrics\", ImmutableList.of(\"met\"), publishedSegments.get(0).getMetrics());\n\n    Assert.assertEquals(\"segment2 datasource\", \"foo\", publishedSegments.get(1).getDataSource());\n    Assert.assertEquals(\"segment2 interval\", Intervals.of(\"2010-01-02/P1D\"), publishedSegments.get(1).getInterval());\n    Assert.assertEquals(\n        \"segment2 dimensions\",\n        ImmutableList.of(\"dim1\", \"dim2\"),\n        publishedSegments.get(1).getDimensions()\n    );\n    Assert.assertEquals(\"segment2 metrics\", ImmutableList.of(\"met\"), publishedSegments.get(1).getMetrics());\n  }\n","date":"2019-07-11 03:22:24","endLine":743,"groupId":"7933","id":9,"instanceNumber":1,"isCurCommit":0,"methodName":"testIndexTask","params":"()","path":"/mnt/clonedata/CloneManagementServer/ManagementServer/consistResult/result-druid-10-0.7/blobInfo/CC_OUT/blobs/d8/c0af320a9e289662a7bca7b017f48d54db856c.src","preCode":"  public void testIndexTask() throws Exception\n  {\n    final Task indexTask = new IndexTask(\n        null,\n        null,\n        new IndexIngestionSpec(\n            new DataSchema(\n                \"foo\",\n                null,\n                new AggregatorFactory[]{new DoubleSumAggregatorFactory(\"met\", \"met\")},\n                new UniformGranularitySpec(\n                    Granularities.DAY,\n                    null,\n                    ImmutableList.of(Intervals.of(\"2010-01-01/P2D\"))\n                ),\n                null,\n                mapper\n            ),\n            new IndexIOConfig(new MockFirehoseFactory(false), false),\n            new IndexTuningConfig(\n                null,\n                10000,\n                10,\n                null,\n                null,\n                null,\n                null,\n                null,\n                indexSpec,\n                3,\n                true,\n                false,\n                null,\n                null,\n                null,\n                null,\n                null,\n                null,\n                null\n            )\n        ),\n        null,\n        AuthTestUtils.TEST_AUTHORIZER_MAPPER,\n        null,\n        ROW_INGESTION_METERS_FACTORY\n    );\n\n    final Optional<TaskStatus> preRunTaskStatus = tsqa.getStatus(indexTask.getId());\n    Assert.assertTrue(\"pre run task status not present\", !preRunTaskStatus.isPresent());\n\n    final TaskStatus mergedStatus = runTask(indexTask);\n    final TaskStatus status = taskStorage.getStatus(indexTask.getId()).get();\n    final List<DataSegment> publishedSegments = byIntervalOrdering.sortedCopy(mdc.getPublished());\n    final List<DataSegment> loggedSegments = byIntervalOrdering.sortedCopy(tsqa.getInsertedSegments(indexTask.getId()));\n\n    Assert.assertEquals(\"statusCode\", TaskState.SUCCESS, status.getStatusCode());\n    Assert.assertEquals(\"merged statusCode\", TaskState.SUCCESS, mergedStatus.getStatusCode());\n    Assert.assertEquals(\"segments logged vs published\", loggedSegments, publishedSegments);\n    Assert.assertEquals(\"num segments published\", 2, mdc.getPublished().size());\n    Assert.assertEquals(\"num segments nuked\", 0, mdc.getNuked().size());\n\n    Assert.assertEquals(\"segment1 datasource\", \"foo\", publishedSegments.get(0).getDataSource());\n    Assert.assertEquals(\"segment1 interval\", Intervals.of(\"2010-01-01/P1D\"), publishedSegments.get(0).getInterval());\n    Assert.assertEquals(\n        \"segment1 dimensions\",\n        ImmutableList.of(\"dim1\", \"dim2\"),\n        publishedSegments.get(0).getDimensions()\n    );\n    Assert.assertEquals(\"segment1 metrics\", ImmutableList.of(\"met\"), publishedSegments.get(0).getMetrics());\n\n    Assert.assertEquals(\"segment2 datasource\", \"foo\", publishedSegments.get(1).getDataSource());\n    Assert.assertEquals(\"segment2 interval\", Intervals.of(\"2010-01-02/P1D\"), publishedSegments.get(1).getInterval());\n    Assert.assertEquals(\n        \"segment2 dimensions\",\n        ImmutableList.of(\"dim1\", \"dim2\"),\n        publishedSegments.get(1).getDimensions()\n    );\n    Assert.assertEquals(\"segment2 metrics\", ImmutableList.of(\"met\"), publishedSegments.get(1).getMetrics());\n  }\n","realPath":"indexing-service/src/test/java/org/apache/druid/indexing/overlord/TaskLifecycleTest.java","repoName":"druid","snippetEndLine":0,"snippetStartLine":0,"startLine":664,"status":"M"},{"authorDate":"2019-07-11 03:22:24","commitOrder":5,"curCode":"  public void testResumeTasks() throws Exception\n  {\n    final Task indexTask = new IndexTask(\n        null,\n        null,\n        new IndexIngestionSpec(\n            new DataSchema(\n                \"foo\",\n                null,\n                new AggregatorFactory[]{new DoubleSumAggregatorFactory(\"met\", \"met\")},\n                new UniformGranularitySpec(\n                    Granularities.DAY,\n                    null,\n                    ImmutableList.of(Intervals.of(\"2010-01-01/P2D\"))\n                ),\n                null,\n                mapper\n            ),\n            new IndexIOConfig(new MockFirehoseFactory(false), false),\n            new IndexTuningConfig(\n                null,\n                10000,\n                10,\n                null,\n                null,\n                null,\n                null,\n                null,\n                indexSpec,\n                null,\n                null,\n                false,\n                null,\n                null,\n                null,\n                null,\n                null,\n                null,\n                null,\n                null\n            )\n        ),\n        null,\n        AuthTestUtils.TEST_AUTHORIZER_MAPPER,\n        null,\n        ROW_INGESTION_METERS_FACTORY\n    );\n\n    final long startTime = System.currentTimeMillis();\n\n    \r\n    taskQueue.start();\n    taskStorage.insert(indexTask, TaskStatus.running(indexTask.getId()));\n\n    while (tsqa.getStatus(indexTask.getId()).get().isRunnable()) {\n      if (System.currentTimeMillis() > startTime + 10 * 1000) {\n        throw new ISE(\"Where did the task go?!: %s\", indexTask.getId());\n      }\n\n      Thread.sleep(100);\n    }\n\n    final TaskStatus status = taskStorage.getStatus(indexTask.getId()).get();\n    final List<DataSegment> publishedSegments = byIntervalOrdering.sortedCopy(mdc.getPublished());\n    final List<DataSegment> loggedSegments = byIntervalOrdering.sortedCopy(tsqa.getInsertedSegments(indexTask.getId()));\n\n    Assert.assertEquals(\"statusCode\", TaskState.SUCCESS, status.getStatusCode());\n    Assert.assertEquals(\"segments logged vs published\", loggedSegments, publishedSegments);\n    Assert.assertEquals(\"num segments published\", 2, mdc.getPublished().size());\n    Assert.assertEquals(\"num segments nuked\", 0, mdc.getNuked().size());\n\n    Assert.assertEquals(\"segment1 datasource\", \"foo\", publishedSegments.get(0).getDataSource());\n    Assert.assertEquals(\"segment1 interval\", Intervals.of(\"2010-01-01/P1D\"), publishedSegments.get(0).getInterval());\n    Assert.assertEquals(\n        \"segment1 dimensions\",\n        ImmutableList.of(\"dim1\", \"dim2\"),\n        publishedSegments.get(0).getDimensions()\n    );\n    Assert.assertEquals(\"segment1 metrics\", ImmutableList.of(\"met\"), publishedSegments.get(0).getMetrics());\n\n    Assert.assertEquals(\"segment2 datasource\", \"foo\", publishedSegments.get(1).getDataSource());\n    Assert.assertEquals(\"segment2 interval\", Intervals.of(\"2010-01-02/P1D\"), publishedSegments.get(1).getInterval());\n    Assert.assertEquals(\n        \"segment2 dimensions\",\n        ImmutableList.of(\"dim1\", \"dim2\"),\n        publishedSegments.get(1).getDimensions()\n    );\n    Assert.assertEquals(\"segment2 metrics\", ImmutableList.of(\"met\"), publishedSegments.get(1).getMetrics());\n  }\n","date":"2019-07-11 03:22:24","endLine":1223,"groupId":"14206","id":10,"instanceNumber":2,"isCurCommit":0,"methodName":"testResumeTasks","params":"()","path":"/mnt/clonedata/CloneManagementServer/ManagementServer/consistResult/result-druid-10-0.7/blobInfo/CC_OUT/blobs/d8/c0af320a9e289662a7bca7b017f48d54db856c.src","preCode":"  public void testResumeTasks() throws Exception\n  {\n    final Task indexTask = new IndexTask(\n        null,\n        null,\n        new IndexIngestionSpec(\n            new DataSchema(\n                \"foo\",\n                null,\n                new AggregatorFactory[]{new DoubleSumAggregatorFactory(\"met\", \"met\")},\n                new UniformGranularitySpec(\n                    Granularities.DAY,\n                    null,\n                    ImmutableList.of(Intervals.of(\"2010-01-01/P2D\"))\n                ),\n                null,\n                mapper\n            ),\n            new IndexIOConfig(new MockFirehoseFactory(false), false),\n            new IndexTuningConfig(\n                null,\n                10000,\n                10,\n                null,\n                null,\n                null,\n                null,\n                null,\n                indexSpec,\n                null,\n                false,\n                null,\n                null,\n                null,\n                null,\n                null,\n                null,\n                null,\n                null\n            )\n        ),\n        null,\n        AuthTestUtils.TEST_AUTHORIZER_MAPPER,\n        null,\n        ROW_INGESTION_METERS_FACTORY\n    );\n\n    final long startTime = System.currentTimeMillis();\n\n    \r\n    taskQueue.start();\n    taskStorage.insert(indexTask, TaskStatus.running(indexTask.getId()));\n\n    while (tsqa.getStatus(indexTask.getId()).get().isRunnable()) {\n      if (System.currentTimeMillis() > startTime + 10 * 1000) {\n        throw new ISE(\"Where did the task go?!: %s\", indexTask.getId());\n      }\n\n      Thread.sleep(100);\n    }\n\n    final TaskStatus status = taskStorage.getStatus(indexTask.getId()).get();\n    final List<DataSegment> publishedSegments = byIntervalOrdering.sortedCopy(mdc.getPublished());\n    final List<DataSegment> loggedSegments = byIntervalOrdering.sortedCopy(tsqa.getInsertedSegments(indexTask.getId()));\n\n    Assert.assertEquals(\"statusCode\", TaskState.SUCCESS, status.getStatusCode());\n    Assert.assertEquals(\"segments logged vs published\", loggedSegments, publishedSegments);\n    Assert.assertEquals(\"num segments published\", 2, mdc.getPublished().size());\n    Assert.assertEquals(\"num segments nuked\", 0, mdc.getNuked().size());\n\n    Assert.assertEquals(\"segment1 datasource\", \"foo\", publishedSegments.get(0).getDataSource());\n    Assert.assertEquals(\"segment1 interval\", Intervals.of(\"2010-01-01/P1D\"), publishedSegments.get(0).getInterval());\n    Assert.assertEquals(\n        \"segment1 dimensions\",\n        ImmutableList.of(\"dim1\", \"dim2\"),\n        publishedSegments.get(0).getDimensions()\n    );\n    Assert.assertEquals(\"segment1 metrics\", ImmutableList.of(\"met\"), publishedSegments.get(0).getMetrics());\n\n    Assert.assertEquals(\"segment2 datasource\", \"foo\", publishedSegments.get(1).getDataSource());\n    Assert.assertEquals(\"segment2 interval\", Intervals.of(\"2010-01-02/P1D\"), publishedSegments.get(1).getInterval());\n    Assert.assertEquals(\n        \"segment2 dimensions\",\n        ImmutableList.of(\"dim1\", \"dim2\"),\n        publishedSegments.get(1).getDimensions()\n    );\n    Assert.assertEquals(\"segment2 metrics\", ImmutableList.of(\"met\"), publishedSegments.get(1).getMetrics());\n  }\n","realPath":"indexing-service/src/test/java/org/apache/druid/indexing/overlord/TaskLifecycleTest.java","repoName":"druid","snippetEndLine":0,"snippetStartLine":0,"startLine":1135,"status":"M"}],"commitId":"14aec7fceca90dfaf9b2ce4dae68186d04ffcc47","commitMessage":"@@@add config to optionally disable all compression  in intermediate segment persists while ingestion (#7919)\n\n* disable all compression in intermediate segment persists while ingestion\n\n* more changes and build fix\n\n* by default retain existing indexingSpec for intermediate persisted segments\n\n* document indexSpecForIntermediatePersists index tuning config\n\n* fix build issues\n\n* update serde tests\n","date":"2019-07-11 03:22:24","modifiedFileCount":"56","status":"M","submitter":"Himanshu"},{"authorTime":"2019-07-30 08:06:33","codes":[{"authorDate":"2019-07-30 08:06:33","commitOrder":6,"curCode":"  public void testIndexTask() throws Exception\n  {\n    final Task indexTask = new IndexTask(\n        null,\n        null,\n        new IndexIngestionSpec(\n            new DataSchema(\n                \"foo\",\n                null,\n                new AggregatorFactory[]{new DoubleSumAggregatorFactory(\"met\", \"met\")},\n                new UniformGranularitySpec(\n                    Granularities.DAY,\n                    null,\n                    ImmutableList.of(Intervals.of(\"2010-01-01/P2D\"))\n                ),\n                null,\n                mapper\n            ),\n            new IndexIOConfig(new MockFirehoseFactory(false), false),\n            new IndexTuningConfig(\n                null,\n                10000,\n                10,\n                null,\n                null,\n                null,\n                null,\n                null,\n                indexSpec,\n                null,\n                3,\n                true,\n                false,\n                null,\n                null,\n                null,\n                null,\n                null,\n                null,\n                null\n            )\n        ),\n        null,\n        AuthTestUtils.TEST_AUTHORIZER_MAPPER,\n        null,\n        ROW_INGESTION_METERS_FACTORY,\n        appenderatorsManager\n    );\n\n    final Optional<TaskStatus> preRunTaskStatus = tsqa.getStatus(indexTask.getId());\n    Assert.assertTrue(\"pre run task status not present\", !preRunTaskStatus.isPresent());\n\n    final TaskStatus mergedStatus = runTask(indexTask);\n    final TaskStatus status = taskStorage.getStatus(indexTask.getId()).get();\n    final List<DataSegment> publishedSegments = byIntervalOrdering.sortedCopy(mdc.getPublished());\n    final List<DataSegment> loggedSegments = byIntervalOrdering.sortedCopy(tsqa.getInsertedSegments(indexTask.getId()));\n\n    Assert.assertEquals(\"statusCode\", TaskState.SUCCESS, status.getStatusCode());\n    Assert.assertEquals(\"merged statusCode\", TaskState.SUCCESS, mergedStatus.getStatusCode());\n    Assert.assertEquals(\"segments logged vs published\", loggedSegments, publishedSegments);\n    Assert.assertEquals(\"num segments published\", 2, mdc.getPublished().size());\n    Assert.assertEquals(\"num segments nuked\", 0, mdc.getNuked().size());\n\n    Assert.assertEquals(\"segment1 datasource\", \"foo\", publishedSegments.get(0).getDataSource());\n    Assert.assertEquals(\"segment1 interval\", Intervals.of(\"2010-01-01/P1D\"), publishedSegments.get(0).getInterval());\n    Assert.assertEquals(\n        \"segment1 dimensions\",\n        ImmutableList.of(\"dim1\", \"dim2\"),\n        publishedSegments.get(0).getDimensions()\n    );\n    Assert.assertEquals(\"segment1 metrics\", ImmutableList.of(\"met\"), publishedSegments.get(0).getMetrics());\n\n    Assert.assertEquals(\"segment2 datasource\", \"foo\", publishedSegments.get(1).getDataSource());\n    Assert.assertEquals(\"segment2 interval\", Intervals.of(\"2010-01-02/P1D\"), publishedSegments.get(1).getInterval());\n    Assert.assertEquals(\n        \"segment2 dimensions\",\n        ImmutableList.of(\"dim1\", \"dim2\"),\n        publishedSegments.get(1).getDimensions()\n    );\n    Assert.assertEquals(\"segment2 metrics\", ImmutableList.of(\"met\"), publishedSegments.get(1).getMetrics());\n  }\n","date":"2019-07-30 08:06:33","endLine":752,"groupId":"7933","id":11,"instanceNumber":1,"isCurCommit":0,"methodName":"testIndexTask","params":"()","path":"/mnt/clonedata/CloneManagementServer/ManagementServer/consistResult/result-druid-10-0.7/blobInfo/CC_OUT/blobs/cb/320e8bcfb6ec45c7fa15f70b09b359ec8b92f9.src","preCode":"  public void testIndexTask() throws Exception\n  {\n    final Task indexTask = new IndexTask(\n        null,\n        null,\n        new IndexIngestionSpec(\n            new DataSchema(\n                \"foo\",\n                null,\n                new AggregatorFactory[]{new DoubleSumAggregatorFactory(\"met\", \"met\")},\n                new UniformGranularitySpec(\n                    Granularities.DAY,\n                    null,\n                    ImmutableList.of(Intervals.of(\"2010-01-01/P2D\"))\n                ),\n                null,\n                mapper\n            ),\n            new IndexIOConfig(new MockFirehoseFactory(false), false),\n            new IndexTuningConfig(\n                null,\n                10000,\n                10,\n                null,\n                null,\n                null,\n                null,\n                null,\n                indexSpec,\n                null,\n                3,\n                true,\n                false,\n                null,\n                null,\n                null,\n                null,\n                null,\n                null,\n                null\n            )\n        ),\n        null,\n        AuthTestUtils.TEST_AUTHORIZER_MAPPER,\n        null,\n        ROW_INGESTION_METERS_FACTORY\n    );\n\n    final Optional<TaskStatus> preRunTaskStatus = tsqa.getStatus(indexTask.getId());\n    Assert.assertTrue(\"pre run task status not present\", !preRunTaskStatus.isPresent());\n\n    final TaskStatus mergedStatus = runTask(indexTask);\n    final TaskStatus status = taskStorage.getStatus(indexTask.getId()).get();\n    final List<DataSegment> publishedSegments = byIntervalOrdering.sortedCopy(mdc.getPublished());\n    final List<DataSegment> loggedSegments = byIntervalOrdering.sortedCopy(tsqa.getInsertedSegments(indexTask.getId()));\n\n    Assert.assertEquals(\"statusCode\", TaskState.SUCCESS, status.getStatusCode());\n    Assert.assertEquals(\"merged statusCode\", TaskState.SUCCESS, mergedStatus.getStatusCode());\n    Assert.assertEquals(\"segments logged vs published\", loggedSegments, publishedSegments);\n    Assert.assertEquals(\"num segments published\", 2, mdc.getPublished().size());\n    Assert.assertEquals(\"num segments nuked\", 0, mdc.getNuked().size());\n\n    Assert.assertEquals(\"segment1 datasource\", \"foo\", publishedSegments.get(0).getDataSource());\n    Assert.assertEquals(\"segment1 interval\", Intervals.of(\"2010-01-01/P1D\"), publishedSegments.get(0).getInterval());\n    Assert.assertEquals(\n        \"segment1 dimensions\",\n        ImmutableList.of(\"dim1\", \"dim2\"),\n        publishedSegments.get(0).getDimensions()\n    );\n    Assert.assertEquals(\"segment1 metrics\", ImmutableList.of(\"met\"), publishedSegments.get(0).getMetrics());\n\n    Assert.assertEquals(\"segment2 datasource\", \"foo\", publishedSegments.get(1).getDataSource());\n    Assert.assertEquals(\"segment2 interval\", Intervals.of(\"2010-01-02/P1D\"), publishedSegments.get(1).getInterval());\n    Assert.assertEquals(\n        \"segment2 dimensions\",\n        ImmutableList.of(\"dim1\", \"dim2\"),\n        publishedSegments.get(1).getDimensions()\n    );\n    Assert.assertEquals(\"segment2 metrics\", ImmutableList.of(\"met\"), publishedSegments.get(1).getMetrics());\n  }\n","realPath":"indexing-service/src/test/java/org/apache/druid/indexing/overlord/TaskLifecycleTest.java","repoName":"druid","snippetEndLine":0,"snippetStartLine":0,"startLine":672,"status":"M"},{"authorDate":"2019-07-30 08:06:33","commitOrder":6,"curCode":"  public void testResumeTasks() throws Exception\n  {\n    final Task indexTask = new IndexTask(\n        null,\n        null,\n        new IndexIngestionSpec(\n            new DataSchema(\n                \"foo\",\n                null,\n                new AggregatorFactory[]{new DoubleSumAggregatorFactory(\"met\", \"met\")},\n                new UniformGranularitySpec(\n                    Granularities.DAY,\n                    null,\n                    ImmutableList.of(Intervals.of(\"2010-01-01/P2D\"))\n                ),\n                null,\n                mapper\n            ),\n            new IndexIOConfig(new MockFirehoseFactory(false), false),\n            new IndexTuningConfig(\n                null,\n                10000,\n                10,\n                null,\n                null,\n                null,\n                null,\n                null,\n                indexSpec,\n                null,\n                null,\n                false,\n                null,\n                null,\n                null,\n                null,\n                null,\n                null,\n                null,\n                null\n            )\n        ),\n        null,\n        AuthTestUtils.TEST_AUTHORIZER_MAPPER,\n        null,\n        ROW_INGESTION_METERS_FACTORY,\n        appenderatorsManager\n    );\n\n    final long startTime = System.currentTimeMillis();\n\n    \r\n    taskQueue.start();\n    taskStorage.insert(indexTask, TaskStatus.running(indexTask.getId()));\n\n    while (tsqa.getStatus(indexTask.getId()).get().isRunnable()) {\n      if (System.currentTimeMillis() > startTime + 10 * 1000) {\n        throw new ISE(\"Where did the task go?!: %s\", indexTask.getId());\n      }\n\n      Thread.sleep(100);\n    }\n\n    final TaskStatus status = taskStorage.getStatus(indexTask.getId()).get();\n    final List<DataSegment> publishedSegments = byIntervalOrdering.sortedCopy(mdc.getPublished());\n    final List<DataSegment> loggedSegments = byIntervalOrdering.sortedCopy(tsqa.getInsertedSegments(indexTask.getId()));\n\n    Assert.assertEquals(\"statusCode\", TaskState.SUCCESS, status.getStatusCode());\n    Assert.assertEquals(\"segments logged vs published\", loggedSegments, publishedSegments);\n    Assert.assertEquals(\"num segments published\", 2, mdc.getPublished().size());\n    Assert.assertEquals(\"num segments nuked\", 0, mdc.getNuked().size());\n\n    Assert.assertEquals(\"segment1 datasource\", \"foo\", publishedSegments.get(0).getDataSource());\n    Assert.assertEquals(\"segment1 interval\", Intervals.of(\"2010-01-01/P1D\"), publishedSegments.get(0).getInterval());\n    Assert.assertEquals(\n        \"segment1 dimensions\",\n        ImmutableList.of(\"dim1\", \"dim2\"),\n        publishedSegments.get(0).getDimensions()\n    );\n    Assert.assertEquals(\"segment1 metrics\", ImmutableList.of(\"met\"), publishedSegments.get(0).getMetrics());\n\n    Assert.assertEquals(\"segment2 datasource\", \"foo\", publishedSegments.get(1).getDataSource());\n    Assert.assertEquals(\"segment2 interval\", Intervals.of(\"2010-01-02/P1D\"), publishedSegments.get(1).getInterval());\n    Assert.assertEquals(\n        \"segment2 dimensions\",\n        ImmutableList.of(\"dim1\", \"dim2\"),\n        publishedSegments.get(1).getDimensions()\n    );\n    Assert.assertEquals(\"segment2 metrics\", ImmutableList.of(\"met\"), publishedSegments.get(1).getMetrics());\n  }\n","date":"2019-07-30 08:06:33","endLine":1240,"groupId":"14206","id":12,"instanceNumber":2,"isCurCommit":0,"methodName":"testResumeTasks","params":"()","path":"/mnt/clonedata/CloneManagementServer/ManagementServer/consistResult/result-druid-10-0.7/blobInfo/CC_OUT/blobs/cb/320e8bcfb6ec45c7fa15f70b09b359ec8b92f9.src","preCode":"  public void testResumeTasks() throws Exception\n  {\n    final Task indexTask = new IndexTask(\n        null,\n        null,\n        new IndexIngestionSpec(\n            new DataSchema(\n                \"foo\",\n                null,\n                new AggregatorFactory[]{new DoubleSumAggregatorFactory(\"met\", \"met\")},\n                new UniformGranularitySpec(\n                    Granularities.DAY,\n                    null,\n                    ImmutableList.of(Intervals.of(\"2010-01-01/P2D\"))\n                ),\n                null,\n                mapper\n            ),\n            new IndexIOConfig(new MockFirehoseFactory(false), false),\n            new IndexTuningConfig(\n                null,\n                10000,\n                10,\n                null,\n                null,\n                null,\n                null,\n                null,\n                indexSpec,\n                null,\n                null,\n                false,\n                null,\n                null,\n                null,\n                null,\n                null,\n                null,\n                null,\n                null\n            )\n        ),\n        null,\n        AuthTestUtils.TEST_AUTHORIZER_MAPPER,\n        null,\n        ROW_INGESTION_METERS_FACTORY\n    );\n\n    final long startTime = System.currentTimeMillis();\n\n    \r\n    taskQueue.start();\n    taskStorage.insert(indexTask, TaskStatus.running(indexTask.getId()));\n\n    while (tsqa.getStatus(indexTask.getId()).get().isRunnable()) {\n      if (System.currentTimeMillis() > startTime + 10 * 1000) {\n        throw new ISE(\"Where did the task go?!: %s\", indexTask.getId());\n      }\n\n      Thread.sleep(100);\n    }\n\n    final TaskStatus status = taskStorage.getStatus(indexTask.getId()).get();\n    final List<DataSegment> publishedSegments = byIntervalOrdering.sortedCopy(mdc.getPublished());\n    final List<DataSegment> loggedSegments = byIntervalOrdering.sortedCopy(tsqa.getInsertedSegments(indexTask.getId()));\n\n    Assert.assertEquals(\"statusCode\", TaskState.SUCCESS, status.getStatusCode());\n    Assert.assertEquals(\"segments logged vs published\", loggedSegments, publishedSegments);\n    Assert.assertEquals(\"num segments published\", 2, mdc.getPublished().size());\n    Assert.assertEquals(\"num segments nuked\", 0, mdc.getNuked().size());\n\n    Assert.assertEquals(\"segment1 datasource\", \"foo\", publishedSegments.get(0).getDataSource());\n    Assert.assertEquals(\"segment1 interval\", Intervals.of(\"2010-01-01/P1D\"), publishedSegments.get(0).getInterval());\n    Assert.assertEquals(\n        \"segment1 dimensions\",\n        ImmutableList.of(\"dim1\", \"dim2\"),\n        publishedSegments.get(0).getDimensions()\n    );\n    Assert.assertEquals(\"segment1 metrics\", ImmutableList.of(\"met\"), publishedSegments.get(0).getMetrics());\n\n    Assert.assertEquals(\"segment2 datasource\", \"foo\", publishedSegments.get(1).getDataSource());\n    Assert.assertEquals(\"segment2 interval\", Intervals.of(\"2010-01-02/P1D\"), publishedSegments.get(1).getInterval());\n    Assert.assertEquals(\n        \"segment2 dimensions\",\n        ImmutableList.of(\"dim1\", \"dim2\"),\n        publishedSegments.get(1).getDimensions()\n    );\n    Assert.assertEquals(\"segment2 metrics\", ImmutableList.of(\"met\"), publishedSegments.get(1).getMetrics());\n  }\n","realPath":"indexing-service/src/test/java/org/apache/druid/indexing/overlord/TaskLifecycleTest.java","repoName":"druid","snippetEndLine":0,"snippetStartLine":0,"startLine":1151,"status":"M"}],"commitId":"640b7afc1cee911a27de7bf938dda24a85ba1510","commitMessage":"@@@Add CliIndexer process type and initial task runner implementation (#8107)\n\n* Add CliIndexer process type and initial task runner implementation\n\n* Fix HttpRemoteTaskRunnerTest\n\n* Remove batch sanity check on PeonAppenderatorsManager\n\n* Fix paralle index tests\n\n* PR comments\n\n* Adjust Jersey resource logging\n\n* Additional cleanup\n\n* Fix SystemSchemaTest\n\n* Add comment to LocalDataSegmentPusherTest absolute path test\n\n* More PR comments\n\n* Use Server annotated with RemoteChatHandler\n\n* More PR comments\n\n* Checkstyle\n\n* PR comments\n\n* Add task shutdown to stopGracefully\n\n* Small cleanup\n\n* Compile fix\n\n* Address PR comments\n\n* Adjust TaskReportFileWriter and fix nits\n\n* Remove unnecessary closer\n\n* More PR comments\n\n* Minor adjustments\n\n* PR comments\n\n* ThreadingTaskRunner: cancel  task run future not shutdownFuture and remove thread from workitem\n","date":"2019-07-30 08:06:33","modifiedFileCount":"64","status":"M","submitter":"Jonathan Wei"},{"authorTime":"2019-07-31 08:24:39","codes":[{"authorDate":"2019-07-31 08:24:39","commitOrder":7,"curCode":"  public void testIndexTask() throws Exception\n  {\n    final Task indexTask = new IndexTask(\n        null,\n        null,\n        new IndexIngestionSpec(\n            new DataSchema(\n                \"foo\",\n                null,\n                new AggregatorFactory[]{new DoubleSumAggregatorFactory(\"met\", \"met\")},\n                new UniformGranularitySpec(\n                    Granularities.DAY,\n                    null,\n                    ImmutableList.of(Intervals.of(\"2010-01-01/P2D\"))\n                ),\n                null,\n                mapper\n            ),\n            new IndexIOConfig(new MockFirehoseFactory(false), false),\n            new IndexTuningConfig(\n                null,\n                10000,\n                10,\n                null,\n                null,\n                null,\n                null,\n                null,\n                null,\n                indexSpec,\n                null,\n                3,\n                false,\n                null,\n                null,\n                null,\n                null,\n                null,\n                null,\n                null\n            )\n        ),\n        null,\n        AuthTestUtils.TEST_AUTHORIZER_MAPPER,\n        null,\n        ROW_INGESTION_METERS_FACTORY,\n        appenderatorsManager\n    );\n\n    final Optional<TaskStatus> preRunTaskStatus = tsqa.getStatus(indexTask.getId());\n    Assert.assertTrue(\"pre run task status not present\", !preRunTaskStatus.isPresent());\n\n    final TaskStatus mergedStatus = runTask(indexTask);\n    final TaskStatus status = taskStorage.getStatus(indexTask.getId()).get();\n    final List<DataSegment> publishedSegments = byIntervalOrdering.sortedCopy(mdc.getPublished());\n    final List<DataSegment> loggedSegments = byIntervalOrdering.sortedCopy(tsqa.getInsertedSegments(indexTask.getId()));\n\n    Assert.assertEquals(\"statusCode\", TaskState.SUCCESS, status.getStatusCode());\n    Assert.assertEquals(\"merged statusCode\", TaskState.SUCCESS, mergedStatus.getStatusCode());\n    Assert.assertEquals(\"segments logged vs published\", loggedSegments, publishedSegments);\n    Assert.assertEquals(\"num segments published\", 2, mdc.getPublished().size());\n    Assert.assertEquals(\"num segments nuked\", 0, mdc.getNuked().size());\n\n    Assert.assertEquals(\"segment1 datasource\", \"foo\", publishedSegments.get(0).getDataSource());\n    Assert.assertEquals(\"segment1 interval\", Intervals.of(\"2010-01-01/P1D\"), publishedSegments.get(0).getInterval());\n    Assert.assertEquals(\n        \"segment1 dimensions\",\n        ImmutableList.of(\"dim1\", \"dim2\"),\n        publishedSegments.get(0).getDimensions()\n    );\n    Assert.assertEquals(\"segment1 metrics\", ImmutableList.of(\"met\"), publishedSegments.get(0).getMetrics());\n\n    Assert.assertEquals(\"segment2 datasource\", \"foo\", publishedSegments.get(1).getDataSource());\n    Assert.assertEquals(\"segment2 interval\", Intervals.of(\"2010-01-02/P1D\"), publishedSegments.get(1).getInterval());\n    Assert.assertEquals(\n        \"segment2 dimensions\",\n        ImmutableList.of(\"dim1\", \"dim2\"),\n        publishedSegments.get(1).getDimensions()\n    );\n    Assert.assertEquals(\"segment2 metrics\", ImmutableList.of(\"met\"), publishedSegments.get(1).getMetrics());\n  }\n","date":"2019-07-31 08:24:39","endLine":752,"groupId":"7933","id":13,"instanceNumber":1,"isCurCommit":0,"methodName":"testIndexTask","params":"()","path":"/mnt/clonedata/CloneManagementServer/ManagementServer/consistResult/result-druid-10-0.7/blobInfo/CC_OUT/blobs/8d/b01cbab5f776813e3581be3ed2cf1751999b87.src","preCode":"  public void testIndexTask() throws Exception\n  {\n    final Task indexTask = new IndexTask(\n        null,\n        null,\n        new IndexIngestionSpec(\n            new DataSchema(\n                \"foo\",\n                null,\n                new AggregatorFactory[]{new DoubleSumAggregatorFactory(\"met\", \"met\")},\n                new UniformGranularitySpec(\n                    Granularities.DAY,\n                    null,\n                    ImmutableList.of(Intervals.of(\"2010-01-01/P2D\"))\n                ),\n                null,\n                mapper\n            ),\n            new IndexIOConfig(new MockFirehoseFactory(false), false),\n            new IndexTuningConfig(\n                null,\n                10000,\n                10,\n                null,\n                null,\n                null,\n                null,\n                null,\n                indexSpec,\n                null,\n                3,\n                true,\n                false,\n                null,\n                null,\n                null,\n                null,\n                null,\n                null,\n                null\n            )\n        ),\n        null,\n        AuthTestUtils.TEST_AUTHORIZER_MAPPER,\n        null,\n        ROW_INGESTION_METERS_FACTORY,\n        appenderatorsManager\n    );\n\n    final Optional<TaskStatus> preRunTaskStatus = tsqa.getStatus(indexTask.getId());\n    Assert.assertTrue(\"pre run task status not present\", !preRunTaskStatus.isPresent());\n\n    final TaskStatus mergedStatus = runTask(indexTask);\n    final TaskStatus status = taskStorage.getStatus(indexTask.getId()).get();\n    final List<DataSegment> publishedSegments = byIntervalOrdering.sortedCopy(mdc.getPublished());\n    final List<DataSegment> loggedSegments = byIntervalOrdering.sortedCopy(tsqa.getInsertedSegments(indexTask.getId()));\n\n    Assert.assertEquals(\"statusCode\", TaskState.SUCCESS, status.getStatusCode());\n    Assert.assertEquals(\"merged statusCode\", TaskState.SUCCESS, mergedStatus.getStatusCode());\n    Assert.assertEquals(\"segments logged vs published\", loggedSegments, publishedSegments);\n    Assert.assertEquals(\"num segments published\", 2, mdc.getPublished().size());\n    Assert.assertEquals(\"num segments nuked\", 0, mdc.getNuked().size());\n\n    Assert.assertEquals(\"segment1 datasource\", \"foo\", publishedSegments.get(0).getDataSource());\n    Assert.assertEquals(\"segment1 interval\", Intervals.of(\"2010-01-01/P1D\"), publishedSegments.get(0).getInterval());\n    Assert.assertEquals(\n        \"segment1 dimensions\",\n        ImmutableList.of(\"dim1\", \"dim2\"),\n        publishedSegments.get(0).getDimensions()\n    );\n    Assert.assertEquals(\"segment1 metrics\", ImmutableList.of(\"met\"), publishedSegments.get(0).getMetrics());\n\n    Assert.assertEquals(\"segment2 datasource\", \"foo\", publishedSegments.get(1).getDataSource());\n    Assert.assertEquals(\"segment2 interval\", Intervals.of(\"2010-01-02/P1D\"), publishedSegments.get(1).getInterval());\n    Assert.assertEquals(\n        \"segment2 dimensions\",\n        ImmutableList.of(\"dim1\", \"dim2\"),\n        publishedSegments.get(1).getDimensions()\n    );\n    Assert.assertEquals(\"segment2 metrics\", ImmutableList.of(\"met\"), publishedSegments.get(1).getMetrics());\n  }\n","realPath":"indexing-service/src/test/java/org/apache/druid/indexing/overlord/TaskLifecycleTest.java","repoName":"druid","snippetEndLine":0,"snippetStartLine":0,"startLine":672,"status":"M"},{"authorDate":"2019-07-31 08:24:39","commitOrder":7,"curCode":"  public void testResumeTasks() throws Exception\n  {\n    final Task indexTask = new IndexTask(\n        null,\n        null,\n        new IndexIngestionSpec(\n            new DataSchema(\n                \"foo\",\n                null,\n                new AggregatorFactory[]{new DoubleSumAggregatorFactory(\"met\", \"met\")},\n                new UniformGranularitySpec(\n                    Granularities.DAY,\n                    null,\n                    ImmutableList.of(Intervals.of(\"2010-01-01/P2D\"))\n                ),\n                null,\n                mapper\n            ),\n            new IndexIOConfig(new MockFirehoseFactory(false), false),\n            new IndexTuningConfig(\n                null,\n                10000,\n                10,\n                null,\n                null,\n                null,\n                null,\n                null,\n                null,\n                indexSpec,\n                null,\n                null,\n                null,\n                null,\n                null,\n                null,\n                null,\n                null,\n                null,\n                null\n            )\n        ),\n        null,\n        AuthTestUtils.TEST_AUTHORIZER_MAPPER,\n        null,\n        ROW_INGESTION_METERS_FACTORY,\n        appenderatorsManager\n    );\n\n    final long startTime = System.currentTimeMillis();\n\n    \r\n    taskQueue.start();\n    taskStorage.insert(indexTask, TaskStatus.running(indexTask.getId()));\n\n    while (tsqa.getStatus(indexTask.getId()).get().isRunnable()) {\n      if (System.currentTimeMillis() > startTime + 10 * 1000) {\n        throw new ISE(\"Where did the task go?!: %s\", indexTask.getId());\n      }\n\n      Thread.sleep(100);\n    }\n\n    final TaskStatus status = taskStorage.getStatus(indexTask.getId()).get();\n    final List<DataSegment> publishedSegments = byIntervalOrdering.sortedCopy(mdc.getPublished());\n    final List<DataSegment> loggedSegments = byIntervalOrdering.sortedCopy(tsqa.getInsertedSegments(indexTask.getId()));\n\n    Assert.assertEquals(\"statusCode\", TaskState.SUCCESS, status.getStatusCode());\n    Assert.assertEquals(\"segments logged vs published\", loggedSegments, publishedSegments);\n    Assert.assertEquals(\"num segments published\", 2, mdc.getPublished().size());\n    Assert.assertEquals(\"num segments nuked\", 0, mdc.getNuked().size());\n\n    Assert.assertEquals(\"segment1 datasource\", \"foo\", publishedSegments.get(0).getDataSource());\n    Assert.assertEquals(\"segment1 interval\", Intervals.of(\"2010-01-01/P1D\"), publishedSegments.get(0).getInterval());\n    Assert.assertEquals(\n        \"segment1 dimensions\",\n        ImmutableList.of(\"dim1\", \"dim2\"),\n        publishedSegments.get(0).getDimensions()\n    );\n    Assert.assertEquals(\"segment1 metrics\", ImmutableList.of(\"met\"), publishedSegments.get(0).getMetrics());\n\n    Assert.assertEquals(\"segment2 datasource\", \"foo\", publishedSegments.get(1).getDataSource());\n    Assert.assertEquals(\"segment2 interval\", Intervals.of(\"2010-01-02/P1D\"), publishedSegments.get(1).getInterval());\n    Assert.assertEquals(\n        \"segment2 dimensions\",\n        ImmutableList.of(\"dim1\", \"dim2\"),\n        publishedSegments.get(1).getDimensions()\n    );\n    Assert.assertEquals(\"segment2 metrics\", ImmutableList.of(\"met\"), publishedSegments.get(1).getMetrics());\n  }\n","date":"2019-07-31 08:24:39","endLine":1240,"groupId":"14206","id":14,"instanceNumber":2,"isCurCommit":0,"methodName":"testResumeTasks","params":"()","path":"/mnt/clonedata/CloneManagementServer/ManagementServer/consistResult/result-druid-10-0.7/blobInfo/CC_OUT/blobs/8d/b01cbab5f776813e3581be3ed2cf1751999b87.src","preCode":"  public void testResumeTasks() throws Exception\n  {\n    final Task indexTask = new IndexTask(\n        null,\n        null,\n        new IndexIngestionSpec(\n            new DataSchema(\n                \"foo\",\n                null,\n                new AggregatorFactory[]{new DoubleSumAggregatorFactory(\"met\", \"met\")},\n                new UniformGranularitySpec(\n                    Granularities.DAY,\n                    null,\n                    ImmutableList.of(Intervals.of(\"2010-01-01/P2D\"))\n                ),\n                null,\n                mapper\n            ),\n            new IndexIOConfig(new MockFirehoseFactory(false), false),\n            new IndexTuningConfig(\n                null,\n                10000,\n                10,\n                null,\n                null,\n                null,\n                null,\n                null,\n                indexSpec,\n                null,\n                null,\n                false,\n                null,\n                null,\n                null,\n                null,\n                null,\n                null,\n                null,\n                null\n            )\n        ),\n        null,\n        AuthTestUtils.TEST_AUTHORIZER_MAPPER,\n        null,\n        ROW_INGESTION_METERS_FACTORY,\n        appenderatorsManager\n    );\n\n    final long startTime = System.currentTimeMillis();\n\n    \r\n    taskQueue.start();\n    taskStorage.insert(indexTask, TaskStatus.running(indexTask.getId()));\n\n    while (tsqa.getStatus(indexTask.getId()).get().isRunnable()) {\n      if (System.currentTimeMillis() > startTime + 10 * 1000) {\n        throw new ISE(\"Where did the task go?!: %s\", indexTask.getId());\n      }\n\n      Thread.sleep(100);\n    }\n\n    final TaskStatus status = taskStorage.getStatus(indexTask.getId()).get();\n    final List<DataSegment> publishedSegments = byIntervalOrdering.sortedCopy(mdc.getPublished());\n    final List<DataSegment> loggedSegments = byIntervalOrdering.sortedCopy(tsqa.getInsertedSegments(indexTask.getId()));\n\n    Assert.assertEquals(\"statusCode\", TaskState.SUCCESS, status.getStatusCode());\n    Assert.assertEquals(\"segments logged vs published\", loggedSegments, publishedSegments);\n    Assert.assertEquals(\"num segments published\", 2, mdc.getPublished().size());\n    Assert.assertEquals(\"num segments nuked\", 0, mdc.getNuked().size());\n\n    Assert.assertEquals(\"segment1 datasource\", \"foo\", publishedSegments.get(0).getDataSource());\n    Assert.assertEquals(\"segment1 interval\", Intervals.of(\"2010-01-01/P1D\"), publishedSegments.get(0).getInterval());\n    Assert.assertEquals(\n        \"segment1 dimensions\",\n        ImmutableList.of(\"dim1\", \"dim2\"),\n        publishedSegments.get(0).getDimensions()\n    );\n    Assert.assertEquals(\"segment1 metrics\", ImmutableList.of(\"met\"), publishedSegments.get(0).getMetrics());\n\n    Assert.assertEquals(\"segment2 datasource\", \"foo\", publishedSegments.get(1).getDataSource());\n    Assert.assertEquals(\"segment2 interval\", Intervals.of(\"2010-01-02/P1D\"), publishedSegments.get(1).getInterval());\n    Assert.assertEquals(\n        \"segment2 dimensions\",\n        ImmutableList.of(\"dim1\", \"dim2\"),\n        publishedSegments.get(1).getDimensions()\n    );\n    Assert.assertEquals(\"segment2 metrics\", ImmutableList.of(\"met\"), publishedSegments.get(1).getMetrics());\n  }\n","realPath":"indexing-service/src/test/java/org/apache/druid/indexing/overlord/TaskLifecycleTest.java","repoName":"druid","snippetEndLine":0,"snippetStartLine":0,"startLine":1151,"status":"M"}],"commitId":"385f492a555add279a8c6dd368954fde18c41dcb","commitMessage":"@@@Use PartitionsSpec for all task types (#8141)\n\n* Use partitionsSpec for all task types\n\n* fix doc\n\n* fix typos and revert to use isPushRequired\n\n* address comments\n\n* move partitionsSpec to core\n\n* remove hadoopPartitionsSpec\n","date":"2019-07-31 08:24:39","modifiedFileCount":"29","status":"M","submitter":"Jihoon Son"},{"authorTime":"2019-08-16 05:57:02","codes":[{"authorDate":"2019-08-16 05:57:02","commitOrder":8,"curCode":"  public void testIndexTask() throws Exception\n  {\n    final Task indexTask = new IndexTask(\n        null,\n        null,\n        new IndexIngestionSpec(\n            new DataSchema(\n                \"foo\",\n                null,\n                new AggregatorFactory[]{new DoubleSumAggregatorFactory(\"met\", \"met\")},\n                new UniformGranularitySpec(\n                    Granularities.DAY,\n                    null,\n                    ImmutableList.of(Intervals.of(\"2010-01-01/P2D\"))\n                ),\n                null,\n                mapper\n            ),\n            new IndexIOConfig(new MockFirehoseFactory(false), false),\n            new IndexTuningConfig(\n                null,\n                10000,\n                10,\n                null,\n                null,\n                null,\n                null,\n                null,\n                null,\n                indexSpec,\n                null,\n                3,\n                false,\n                null,\n                null,\n                null,\n                null,\n                null,\n                null,\n                null\n            )\n        ),\n        null,\n        AuthTestUtils.TEST_AUTHORIZER_MAPPER,\n        null,\n        ROW_INGESTION_METERS_FACTORY,\n        appenderatorsManager\n    );\n\n    final Optional<TaskStatus> preRunTaskStatus = tsqa.getStatus(indexTask.getId());\n    Assert.assertTrue(\"pre run task status not present\", !preRunTaskStatus.isPresent());\n\n    final TaskStatus mergedStatus = runTask(indexTask);\n    final TaskStatus status = taskStorage.getStatus(indexTask.getId()).get();\n    final List<DataSegment> publishedSegments = byIntervalOrdering.sortedCopy(mdc.getPublished());\n    final List<DataSegment> loggedSegments = byIntervalOrdering.sortedCopy(tsqa.getInsertedSegments(indexTask.getId()));\n\n    Assert.assertEquals(\"statusCode\", TaskState.SUCCESS, status.getStatusCode());\n    Assert.assertEquals(taskLocation, status.getLocation());\n    Assert.assertEquals(\"merged statusCode\", TaskState.SUCCESS, mergedStatus.getStatusCode());\n    Assert.assertEquals(\"segments logged vs published\", loggedSegments, publishedSegments);\n    Assert.assertEquals(\"num segments published\", 2, mdc.getPublished().size());\n    Assert.assertEquals(\"num segments nuked\", 0, mdc.getNuked().size());\n\n    Assert.assertEquals(\"segment1 datasource\", \"foo\", publishedSegments.get(0).getDataSource());\n    Assert.assertEquals(\"segment1 interval\", Intervals.of(\"2010-01-01/P1D\"), publishedSegments.get(0).getInterval());\n    Assert.assertEquals(\n        \"segment1 dimensions\",\n        ImmutableList.of(\"dim1\", \"dim2\"),\n        publishedSegments.get(0).getDimensions()\n    );\n    Assert.assertEquals(\"segment1 metrics\", ImmutableList.of(\"met\"), publishedSegments.get(0).getMetrics());\n\n    Assert.assertEquals(\"segment2 datasource\", \"foo\", publishedSegments.get(1).getDataSource());\n    Assert.assertEquals(\"segment2 interval\", Intervals.of(\"2010-01-02/P1D\"), publishedSegments.get(1).getInterval());\n    Assert.assertEquals(\n        \"segment2 dimensions\",\n        ImmutableList.of(\"dim1\", \"dim2\"),\n        publishedSegments.get(1).getDimensions()\n    );\n    Assert.assertEquals(\"segment2 metrics\", ImmutableList.of(\"met\"), publishedSegments.get(1).getMetrics());\n  }\n","date":"2019-08-16 05:57:02","endLine":763,"groupId":"7933","id":15,"instanceNumber":1,"isCurCommit":0,"methodName":"testIndexTask","params":"()","path":"/mnt/clonedata/CloneManagementServer/ManagementServer/consistResult/result-druid-10-0.7/blobInfo/CC_OUT/blobs/1a/e89f7a0a35e2067543a7012d74e3a171e672e6.src","preCode":"  public void testIndexTask() throws Exception\n  {\n    final Task indexTask = new IndexTask(\n        null,\n        null,\n        new IndexIngestionSpec(\n            new DataSchema(\n                \"foo\",\n                null,\n                new AggregatorFactory[]{new DoubleSumAggregatorFactory(\"met\", \"met\")},\n                new UniformGranularitySpec(\n                    Granularities.DAY,\n                    null,\n                    ImmutableList.of(Intervals.of(\"2010-01-01/P2D\"))\n                ),\n                null,\n                mapper\n            ),\n            new IndexIOConfig(new MockFirehoseFactory(false), false),\n            new IndexTuningConfig(\n                null,\n                10000,\n                10,\n                null,\n                null,\n                null,\n                null,\n                null,\n                null,\n                indexSpec,\n                null,\n                3,\n                false,\n                null,\n                null,\n                null,\n                null,\n                null,\n                null,\n                null\n            )\n        ),\n        null,\n        AuthTestUtils.TEST_AUTHORIZER_MAPPER,\n        null,\n        ROW_INGESTION_METERS_FACTORY,\n        appenderatorsManager\n    );\n\n    final Optional<TaskStatus> preRunTaskStatus = tsqa.getStatus(indexTask.getId());\n    Assert.assertTrue(\"pre run task status not present\", !preRunTaskStatus.isPresent());\n\n    final TaskStatus mergedStatus = runTask(indexTask);\n    final TaskStatus status = taskStorage.getStatus(indexTask.getId()).get();\n    final List<DataSegment> publishedSegments = byIntervalOrdering.sortedCopy(mdc.getPublished());\n    final List<DataSegment> loggedSegments = byIntervalOrdering.sortedCopy(tsqa.getInsertedSegments(indexTask.getId()));\n\n    Assert.assertEquals(\"statusCode\", TaskState.SUCCESS, status.getStatusCode());\n    Assert.assertEquals(\"merged statusCode\", TaskState.SUCCESS, mergedStatus.getStatusCode());\n    Assert.assertEquals(\"segments logged vs published\", loggedSegments, publishedSegments);\n    Assert.assertEquals(\"num segments published\", 2, mdc.getPublished().size());\n    Assert.assertEquals(\"num segments nuked\", 0, mdc.getNuked().size());\n\n    Assert.assertEquals(\"segment1 datasource\", \"foo\", publishedSegments.get(0).getDataSource());\n    Assert.assertEquals(\"segment1 interval\", Intervals.of(\"2010-01-01/P1D\"), publishedSegments.get(0).getInterval());\n    Assert.assertEquals(\n        \"segment1 dimensions\",\n        ImmutableList.of(\"dim1\", \"dim2\"),\n        publishedSegments.get(0).getDimensions()\n    );\n    Assert.assertEquals(\"segment1 metrics\", ImmutableList.of(\"met\"), publishedSegments.get(0).getMetrics());\n\n    Assert.assertEquals(\"segment2 datasource\", \"foo\", publishedSegments.get(1).getDataSource());\n    Assert.assertEquals(\"segment2 interval\", Intervals.of(\"2010-01-02/P1D\"), publishedSegments.get(1).getInterval());\n    Assert.assertEquals(\n        \"segment2 dimensions\",\n        ImmutableList.of(\"dim1\", \"dim2\"),\n        publishedSegments.get(1).getDimensions()\n    );\n    Assert.assertEquals(\"segment2 metrics\", ImmutableList.of(\"met\"), publishedSegments.get(1).getMetrics());\n  }\n","realPath":"indexing-service/src/test/java/org/apache/druid/indexing/overlord/TaskLifecycleTest.java","repoName":"druid","snippetEndLine":0,"snippetStartLine":0,"startLine":682,"status":"M"},{"authorDate":"2019-08-16 05:57:02","commitOrder":8,"curCode":"  public void testResumeTasks() throws Exception\n  {\n    final Task indexTask = new IndexTask(\n        null,\n        null,\n        new IndexIngestionSpec(\n            new DataSchema(\n                \"foo\",\n                null,\n                new AggregatorFactory[]{new DoubleSumAggregatorFactory(\"met\", \"met\")},\n                new UniformGranularitySpec(\n                    Granularities.DAY,\n                    null,\n                    ImmutableList.of(Intervals.of(\"2010-01-01/P2D\"))\n                ),\n                null,\n                mapper\n            ),\n            new IndexIOConfig(new MockFirehoseFactory(false), false),\n            new IndexTuningConfig(\n                null,\n                10000,\n                10,\n                null,\n                null,\n                null,\n                null,\n                null,\n                null,\n                indexSpec,\n                null,\n                null,\n                null,\n                null,\n                null,\n                null,\n                null,\n                null,\n                null,\n                null\n            )\n        ),\n        null,\n        AuthTestUtils.TEST_AUTHORIZER_MAPPER,\n        null,\n        ROW_INGESTION_METERS_FACTORY,\n        appenderatorsManager\n    );\n\n    final long startTime = System.currentTimeMillis();\n\n    \r\n    taskQueue.start();\n    taskStorage.insert(indexTask, TaskStatus.running(indexTask.getId()));\n\n    while (tsqa.getStatus(indexTask.getId()).get().isRunnable()) {\n      if (System.currentTimeMillis() > startTime + 10 * 1000) {\n        throw new ISE(\"Where did the task go?!: %s\", indexTask.getId());\n      }\n\n      Thread.sleep(100);\n    }\n\n    final TaskStatus status = taskStorage.getStatus(indexTask.getId()).get();\n    final List<DataSegment> publishedSegments = byIntervalOrdering.sortedCopy(mdc.getPublished());\n    final List<DataSegment> loggedSegments = byIntervalOrdering.sortedCopy(tsqa.getInsertedSegments(indexTask.getId()));\n\n    Assert.assertEquals(\"statusCode\", TaskState.SUCCESS, status.getStatusCode());\n    Assert.assertEquals(taskLocation, status.getLocation());\n    Assert.assertEquals(\"segments logged vs published\", loggedSegments, publishedSegments);\n    Assert.assertEquals(\"num segments published\", 2, mdc.getPublished().size());\n    Assert.assertEquals(\"num segments nuked\", 0, mdc.getNuked().size());\n\n    Assert.assertEquals(\"segment1 datasource\", \"foo\", publishedSegments.get(0).getDataSource());\n    Assert.assertEquals(\"segment1 interval\", Intervals.of(\"2010-01-01/P1D\"), publishedSegments.get(0).getInterval());\n    Assert.assertEquals(\n        \"segment1 dimensions\",\n        ImmutableList.of(\"dim1\", \"dim2\"),\n        publishedSegments.get(0).getDimensions()\n    );\n    Assert.assertEquals(\"segment1 metrics\", ImmutableList.of(\"met\"), publishedSegments.get(0).getMetrics());\n\n    Assert.assertEquals(\"segment2 datasource\", \"foo\", publishedSegments.get(1).getDataSource());\n    Assert.assertEquals(\"segment2 interval\", Intervals.of(\"2010-01-02/P1D\"), publishedSegments.get(1).getInterval());\n    Assert.assertEquals(\n        \"segment2 dimensions\",\n        ImmutableList.of(\"dim1\", \"dim2\"),\n        publishedSegments.get(1).getDimensions()\n    );\n    Assert.assertEquals(\"segment2 metrics\", ImmutableList.of(\"met\"), publishedSegments.get(1).getMetrics());\n  }\n","date":"2019-08-16 05:57:02","endLine":1278,"groupId":"12487","id":16,"instanceNumber":2,"isCurCommit":0,"methodName":"testResumeTasks","params":"()","path":"/mnt/clonedata/CloneManagementServer/ManagementServer/consistResult/result-druid-10-0.7/blobInfo/CC_OUT/blobs/1a/e89f7a0a35e2067543a7012d74e3a171e672e6.src","preCode":"  public void testResumeTasks() throws Exception\n  {\n    final Task indexTask = new IndexTask(\n        null,\n        null,\n        new IndexIngestionSpec(\n            new DataSchema(\n                \"foo\",\n                null,\n                new AggregatorFactory[]{new DoubleSumAggregatorFactory(\"met\", \"met\")},\n                new UniformGranularitySpec(\n                    Granularities.DAY,\n                    null,\n                    ImmutableList.of(Intervals.of(\"2010-01-01/P2D\"))\n                ),\n                null,\n                mapper\n            ),\n            new IndexIOConfig(new MockFirehoseFactory(false), false),\n            new IndexTuningConfig(\n                null,\n                10000,\n                10,\n                null,\n                null,\n                null,\n                null,\n                null,\n                null,\n                indexSpec,\n                null,\n                null,\n                null,\n                null,\n                null,\n                null,\n                null,\n                null,\n                null,\n                null\n            )\n        ),\n        null,\n        AuthTestUtils.TEST_AUTHORIZER_MAPPER,\n        null,\n        ROW_INGESTION_METERS_FACTORY,\n        appenderatorsManager\n    );\n\n    final long startTime = System.currentTimeMillis();\n\n    \r\n    taskQueue.start();\n    taskStorage.insert(indexTask, TaskStatus.running(indexTask.getId()));\n\n    while (tsqa.getStatus(indexTask.getId()).get().isRunnable()) {\n      if (System.currentTimeMillis() > startTime + 10 * 1000) {\n        throw new ISE(\"Where did the task go?!: %s\", indexTask.getId());\n      }\n\n      Thread.sleep(100);\n    }\n\n    final TaskStatus status = taskStorage.getStatus(indexTask.getId()).get();\n    final List<DataSegment> publishedSegments = byIntervalOrdering.sortedCopy(mdc.getPublished());\n    final List<DataSegment> loggedSegments = byIntervalOrdering.sortedCopy(tsqa.getInsertedSegments(indexTask.getId()));\n\n    Assert.assertEquals(\"statusCode\", TaskState.SUCCESS, status.getStatusCode());\n    Assert.assertEquals(\"segments logged vs published\", loggedSegments, publishedSegments);\n    Assert.assertEquals(\"num segments published\", 2, mdc.getPublished().size());\n    Assert.assertEquals(\"num segments nuked\", 0, mdc.getNuked().size());\n\n    Assert.assertEquals(\"segment1 datasource\", \"foo\", publishedSegments.get(0).getDataSource());\n    Assert.assertEquals(\"segment1 interval\", Intervals.of(\"2010-01-01/P1D\"), publishedSegments.get(0).getInterval());\n    Assert.assertEquals(\n        \"segment1 dimensions\",\n        ImmutableList.of(\"dim1\", \"dim2\"),\n        publishedSegments.get(0).getDimensions()\n    );\n    Assert.assertEquals(\"segment1 metrics\", ImmutableList.of(\"met\"), publishedSegments.get(0).getMetrics());\n\n    Assert.assertEquals(\"segment2 datasource\", \"foo\", publishedSegments.get(1).getDataSource());\n    Assert.assertEquals(\"segment2 interval\", Intervals.of(\"2010-01-02/P1D\"), publishedSegments.get(1).getInterval());\n    Assert.assertEquals(\n        \"segment2 dimensions\",\n        ImmutableList.of(\"dim1\", \"dim2\"),\n        publishedSegments.get(1).getDimensions()\n    );\n    Assert.assertEquals(\"segment2 metrics\", ImmutableList.of(\"met\"), publishedSegments.get(1).getMetrics());\n  }\n","realPath":"indexing-service/src/test/java/org/apache/druid/indexing/overlord/TaskLifecycleTest.java","repoName":"druid","snippetEndLine":0,"snippetStartLine":0,"startLine":1188,"status":"M"}],"commitId":"ef7b9606f2137a7a724e65c52c28375ce0dff427","commitMessage":"@@@Keep track of task location for completed tasks (#8286)\n\n* Keep track of task location for completed tasks\n\n* Add TaskLifecycleTest location checks\n","date":"2019-08-16 05:57:02","modifiedFileCount":"8","status":"M","submitter":"Jonathan Wei"},{"authorTime":"2019-08-23 18:13:54","codes":[{"authorDate":"2019-08-23 18:13:54","commitOrder":9,"curCode":"  public void testIndexTask() throws Exception\n  {\n    final Task indexTask = new IndexTask(\n        null,\n        null,\n        new IndexIngestionSpec(\n            new DataSchema(\n                \"foo\",\n                null,\n                new AggregatorFactory[]{new DoubleSumAggregatorFactory(\"met\", \"met\")},\n                new UniformGranularitySpec(\n                    Granularities.DAY,\n                    null,\n                    ImmutableList.of(Intervals.of(\"2010-01-01/P2D\"))\n                ),\n                null,\n                mapper\n            ),\n            new IndexIOConfig(new MockFirehoseFactory(false), false),\n            new IndexTuningConfig(\n                null,\n                10000,\n                10,\n                null,\n                null,\n                null,\n                null,\n                null,\n                null,\n                indexSpec,\n                null,\n                3,\n                false,\n                null,\n                null,\n                null,\n                null,\n                null,\n                null,\n                null\n            )\n        ),\n        null,\n        AuthTestUtils.TEST_AUTHORIZER_MAPPER,\n        null,\n        ROW_INGESTION_METERS_FACTORY,\n        appenderatorsManager\n    );\n\n    final Optional<TaskStatus> preRunTaskStatus = tsqa.getStatus(indexTask.getId());\n    Assert.assertTrue(\"pre run task status not present\", !preRunTaskStatus.isPresent());\n\n    final TaskStatus mergedStatus = runTask(indexTask);\n    final TaskStatus status = taskStorage.getStatus(indexTask.getId()).get();\n    final List<DataSegment> publishedSegments = BY_INTERVAL_ORDERING.sortedCopy(mdc.getPublished());\n    final List<DataSegment> loggedSegments = BY_INTERVAL_ORDERING.sortedCopy(tsqa.getInsertedSegments(indexTask.getId()));\n\n    Assert.assertEquals(\"statusCode\", TaskState.SUCCESS, status.getStatusCode());\n    Assert.assertEquals(taskLocation, status.getLocation());\n    Assert.assertEquals(\"merged statusCode\", TaskState.SUCCESS, mergedStatus.getStatusCode());\n    Assert.assertEquals(\"segments logged vs published\", loggedSegments, publishedSegments);\n    Assert.assertEquals(\"num segments published\", 2, mdc.getPublished().size());\n    Assert.assertEquals(\"num segments nuked\", 0, mdc.getNuked().size());\n\n    Assert.assertEquals(\"segment1 datasource\", \"foo\", publishedSegments.get(0).getDataSource());\n    Assert.assertEquals(\"segment1 interval\", Intervals.of(\"2010-01-01/P1D\"), publishedSegments.get(0).getInterval());\n    Assert.assertEquals(\n        \"segment1 dimensions\",\n        ImmutableList.of(\"dim1\", \"dim2\"),\n        publishedSegments.get(0).getDimensions()\n    );\n    Assert.assertEquals(\"segment1 metrics\", ImmutableList.of(\"met\"), publishedSegments.get(0).getMetrics());\n\n    Assert.assertEquals(\"segment2 datasource\", \"foo\", publishedSegments.get(1).getDataSource());\n    Assert.assertEquals(\"segment2 interval\", Intervals.of(\"2010-01-02/P1D\"), publishedSegments.get(1).getInterval());\n    Assert.assertEquals(\n        \"segment2 dimensions\",\n        ImmutableList.of(\"dim1\", \"dim2\"),\n        publishedSegments.get(1).getDimensions()\n    );\n    Assert.assertEquals(\"segment2 metrics\", ImmutableList.of(\"met\"), publishedSegments.get(1).getMetrics());\n  }\n","date":"2019-08-23 18:13:54","endLine":769,"groupId":"7933","id":17,"instanceNumber":1,"isCurCommit":0,"methodName":"testIndexTask","params":"()","path":"/mnt/clonedata/CloneManagementServer/ManagementServer/consistResult/result-druid-10-0.7/blobInfo/CC_OUT/blobs/e8/b0222ba650aa0809cac6d35d4712920c6cb087.src","preCode":"  public void testIndexTask() throws Exception\n  {\n    final Task indexTask = new IndexTask(\n        null,\n        null,\n        new IndexIngestionSpec(\n            new DataSchema(\n                \"foo\",\n                null,\n                new AggregatorFactory[]{new DoubleSumAggregatorFactory(\"met\", \"met\")},\n                new UniformGranularitySpec(\n                    Granularities.DAY,\n                    null,\n                    ImmutableList.of(Intervals.of(\"2010-01-01/P2D\"))\n                ),\n                null,\n                mapper\n            ),\n            new IndexIOConfig(new MockFirehoseFactory(false), false),\n            new IndexTuningConfig(\n                null,\n                10000,\n                10,\n                null,\n                null,\n                null,\n                null,\n                null,\n                null,\n                indexSpec,\n                null,\n                3,\n                false,\n                null,\n                null,\n                null,\n                null,\n                null,\n                null,\n                null\n            )\n        ),\n        null,\n        AuthTestUtils.TEST_AUTHORIZER_MAPPER,\n        null,\n        ROW_INGESTION_METERS_FACTORY,\n        appenderatorsManager\n    );\n\n    final Optional<TaskStatus> preRunTaskStatus = tsqa.getStatus(indexTask.getId());\n    Assert.assertTrue(\"pre run task status not present\", !preRunTaskStatus.isPresent());\n\n    final TaskStatus mergedStatus = runTask(indexTask);\n    final TaskStatus status = taskStorage.getStatus(indexTask.getId()).get();\n    final List<DataSegment> publishedSegments = byIntervalOrdering.sortedCopy(mdc.getPublished());\n    final List<DataSegment> loggedSegments = byIntervalOrdering.sortedCopy(tsqa.getInsertedSegments(indexTask.getId()));\n\n    Assert.assertEquals(\"statusCode\", TaskState.SUCCESS, status.getStatusCode());\n    Assert.assertEquals(taskLocation, status.getLocation());\n    Assert.assertEquals(\"merged statusCode\", TaskState.SUCCESS, mergedStatus.getStatusCode());\n    Assert.assertEquals(\"segments logged vs published\", loggedSegments, publishedSegments);\n    Assert.assertEquals(\"num segments published\", 2, mdc.getPublished().size());\n    Assert.assertEquals(\"num segments nuked\", 0, mdc.getNuked().size());\n\n    Assert.assertEquals(\"segment1 datasource\", \"foo\", publishedSegments.get(0).getDataSource());\n    Assert.assertEquals(\"segment1 interval\", Intervals.of(\"2010-01-01/P1D\"), publishedSegments.get(0).getInterval());\n    Assert.assertEquals(\n        \"segment1 dimensions\",\n        ImmutableList.of(\"dim1\", \"dim2\"),\n        publishedSegments.get(0).getDimensions()\n    );\n    Assert.assertEquals(\"segment1 metrics\", ImmutableList.of(\"met\"), publishedSegments.get(0).getMetrics());\n\n    Assert.assertEquals(\"segment2 datasource\", \"foo\", publishedSegments.get(1).getDataSource());\n    Assert.assertEquals(\"segment2 interval\", Intervals.of(\"2010-01-02/P1D\"), publishedSegments.get(1).getInterval());\n    Assert.assertEquals(\n        \"segment2 dimensions\",\n        ImmutableList.of(\"dim1\", \"dim2\"),\n        publishedSegments.get(1).getDimensions()\n    );\n    Assert.assertEquals(\"segment2 metrics\", ImmutableList.of(\"met\"), publishedSegments.get(1).getMetrics());\n  }\n","realPath":"indexing-service/src/test/java/org/apache/druid/indexing/overlord/TaskLifecycleTest.java","repoName":"druid","snippetEndLine":0,"snippetStartLine":0,"startLine":688,"status":"M"},{"authorDate":"2019-08-23 18:13:54","commitOrder":9,"curCode":"  public void testResumeTasks() throws Exception\n  {\n    final Task indexTask = new IndexTask(\n        null,\n        null,\n        new IndexIngestionSpec(\n            new DataSchema(\n                \"foo\",\n                null,\n                new AggregatorFactory[]{new DoubleSumAggregatorFactory(\"met\", \"met\")},\n                new UniformGranularitySpec(\n                    Granularities.DAY,\n                    null,\n                    ImmutableList.of(Intervals.of(\"2010-01-01/P2D\"))\n                ),\n                null,\n                mapper\n            ),\n            new IndexIOConfig(new MockFirehoseFactory(false), false),\n            new IndexTuningConfig(\n                null,\n                10000,\n                10,\n                null,\n                null,\n                null,\n                null,\n                null,\n                null,\n                indexSpec,\n                null,\n                null,\n                null,\n                null,\n                null,\n                null,\n                null,\n                null,\n                null,\n                null\n            )\n        ),\n        null,\n        AuthTestUtils.TEST_AUTHORIZER_MAPPER,\n        null,\n        ROW_INGESTION_METERS_FACTORY,\n        appenderatorsManager\n    );\n\n    final long startTime = System.currentTimeMillis();\n\n    \r\n    taskQueue.start();\n    taskStorage.insert(indexTask, TaskStatus.running(indexTask.getId()));\n\n    while (tsqa.getStatus(indexTask.getId()).get().isRunnable()) {\n      if (System.currentTimeMillis() > startTime + 10 * 1000) {\n        throw new ISE(\"Where did the task go?!: %s\", indexTask.getId());\n      }\n\n      Thread.sleep(100);\n    }\n\n    final TaskStatus status = taskStorage.getStatus(indexTask.getId()).get();\n    final List<DataSegment> publishedSegments = BY_INTERVAL_ORDERING.sortedCopy(mdc.getPublished());\n    final List<DataSegment> loggedSegments = BY_INTERVAL_ORDERING.sortedCopy(tsqa.getInsertedSegments(indexTask.getId()));\n\n    Assert.assertEquals(\"statusCode\", TaskState.SUCCESS, status.getStatusCode());\n    Assert.assertEquals(taskLocation, status.getLocation());\n    Assert.assertEquals(\"segments logged vs published\", loggedSegments, publishedSegments);\n    Assert.assertEquals(\"num segments published\", 2, mdc.getPublished().size());\n    Assert.assertEquals(\"num segments nuked\", 0, mdc.getNuked().size());\n\n    Assert.assertEquals(\"segment1 datasource\", \"foo\", publishedSegments.get(0).getDataSource());\n    Assert.assertEquals(\"segment1 interval\", Intervals.of(\"2010-01-01/P1D\"), publishedSegments.get(0).getInterval());\n    Assert.assertEquals(\n        \"segment1 dimensions\",\n        ImmutableList.of(\"dim1\", \"dim2\"),\n        publishedSegments.get(0).getDimensions()\n    );\n    Assert.assertEquals(\"segment1 metrics\", ImmutableList.of(\"met\"), publishedSegments.get(0).getMetrics());\n\n    Assert.assertEquals(\"segment2 datasource\", \"foo\", publishedSegments.get(1).getDataSource());\n    Assert.assertEquals(\"segment2 interval\", Intervals.of(\"2010-01-02/P1D\"), publishedSegments.get(1).getInterval());\n    Assert.assertEquals(\n        \"segment2 dimensions\",\n        ImmutableList.of(\"dim1\", \"dim2\"),\n        publishedSegments.get(1).getDimensions()\n    );\n    Assert.assertEquals(\"segment2 metrics\", ImmutableList.of(\"met\"), publishedSegments.get(1).getMetrics());\n  }\n","date":"2019-08-23 18:13:54","endLine":1284,"groupId":"12487","id":18,"instanceNumber":2,"isCurCommit":0,"methodName":"testResumeTasks","params":"()","path":"/mnt/clonedata/CloneManagementServer/ManagementServer/consistResult/result-druid-10-0.7/blobInfo/CC_OUT/blobs/e8/b0222ba650aa0809cac6d35d4712920c6cb087.src","preCode":"  public void testResumeTasks() throws Exception\n  {\n    final Task indexTask = new IndexTask(\n        null,\n        null,\n        new IndexIngestionSpec(\n            new DataSchema(\n                \"foo\",\n                null,\n                new AggregatorFactory[]{new DoubleSumAggregatorFactory(\"met\", \"met\")},\n                new UniformGranularitySpec(\n                    Granularities.DAY,\n                    null,\n                    ImmutableList.of(Intervals.of(\"2010-01-01/P2D\"))\n                ),\n                null,\n                mapper\n            ),\n            new IndexIOConfig(new MockFirehoseFactory(false), false),\n            new IndexTuningConfig(\n                null,\n                10000,\n                10,\n                null,\n                null,\n                null,\n                null,\n                null,\n                null,\n                indexSpec,\n                null,\n                null,\n                null,\n                null,\n                null,\n                null,\n                null,\n                null,\n                null,\n                null\n            )\n        ),\n        null,\n        AuthTestUtils.TEST_AUTHORIZER_MAPPER,\n        null,\n        ROW_INGESTION_METERS_FACTORY,\n        appenderatorsManager\n    );\n\n    final long startTime = System.currentTimeMillis();\n\n    \r\n    taskQueue.start();\n    taskStorage.insert(indexTask, TaskStatus.running(indexTask.getId()));\n\n    while (tsqa.getStatus(indexTask.getId()).get().isRunnable()) {\n      if (System.currentTimeMillis() > startTime + 10 * 1000) {\n        throw new ISE(\"Where did the task go?!: %s\", indexTask.getId());\n      }\n\n      Thread.sleep(100);\n    }\n\n    final TaskStatus status = taskStorage.getStatus(indexTask.getId()).get();\n    final List<DataSegment> publishedSegments = byIntervalOrdering.sortedCopy(mdc.getPublished());\n    final List<DataSegment> loggedSegments = byIntervalOrdering.sortedCopy(tsqa.getInsertedSegments(indexTask.getId()));\n\n    Assert.assertEquals(\"statusCode\", TaskState.SUCCESS, status.getStatusCode());\n    Assert.assertEquals(taskLocation, status.getLocation());\n    Assert.assertEquals(\"segments logged vs published\", loggedSegments, publishedSegments);\n    Assert.assertEquals(\"num segments published\", 2, mdc.getPublished().size());\n    Assert.assertEquals(\"num segments nuked\", 0, mdc.getNuked().size());\n\n    Assert.assertEquals(\"segment1 datasource\", \"foo\", publishedSegments.get(0).getDataSource());\n    Assert.assertEquals(\"segment1 interval\", Intervals.of(\"2010-01-01/P1D\"), publishedSegments.get(0).getInterval());\n    Assert.assertEquals(\n        \"segment1 dimensions\",\n        ImmutableList.of(\"dim1\", \"dim2\"),\n        publishedSegments.get(0).getDimensions()\n    );\n    Assert.assertEquals(\"segment1 metrics\", ImmutableList.of(\"met\"), publishedSegments.get(0).getMetrics());\n\n    Assert.assertEquals(\"segment2 datasource\", \"foo\", publishedSegments.get(1).getDataSource());\n    Assert.assertEquals(\"segment2 interval\", Intervals.of(\"2010-01-02/P1D\"), publishedSegments.get(1).getInterval());\n    Assert.assertEquals(\n        \"segment2 dimensions\",\n        ImmutableList.of(\"dim1\", \"dim2\"),\n        publishedSegments.get(1).getDimensions()\n    );\n    Assert.assertEquals(\"segment2 metrics\", ImmutableList.of(\"met\"), publishedSegments.get(1).getMetrics());\n  }\n","realPath":"indexing-service/src/test/java/org/apache/druid/indexing/overlord/TaskLifecycleTest.java","repoName":"druid","snippetEndLine":0,"snippetStartLine":0,"startLine":1194,"status":"M"}],"commitId":"33f0753a70361e7d345a488034f76a889f7c3682","commitMessage":"@@@Add Checkstyle for constant name static final (#8060)\n\n* check ctyle for constant field name\n\n* check ctyle for constant field name\n\n* check ctyle for constant field name\n\n* check ctyle for constant field name\n\n* check ctyle for constant field name\n\n* check ctyle for constant field name\n\n* check ctyle for constant field name\n\n* check ctyle for constant field name\n\n* check ctyle for constant field name\n\n* merging with upstream\n\n* review-1\n\n* unknow changes\n\n* unknow changes\n\n* review-2\n\n* merging with master\n\n* review-2 1 changes\n\n* review changes-2 2\n\n* bug fix\n","date":"2019-08-23 18:13:54","modifiedFileCount":"298","status":"M","submitter":"SandishKumarHN"},{"authorTime":"2019-11-16 01:22:09","codes":[{"authorDate":"2019-11-16 01:22:09","commitOrder":10,"curCode":"  public void testIndexTask() throws Exception\n  {\n    final Task indexTask = new IndexTask(\n        null,\n        null,\n        new IndexIngestionSpec(\n            new DataSchema(\n                \"foo\",\n                new TimestampSpec(null, null, null),\n                DimensionsSpec.EMPTY,\n                new AggregatorFactory[]{new DoubleSumAggregatorFactory(\"met\", \"met\")},\n                new UniformGranularitySpec(\n                    Granularities.DAY,\n                    null,\n                    ImmutableList.of(Intervals.of(\"2010-01-01/P2D\"))\n                ),\n                null\n            ),\n            new IndexIOConfig(null, new MockInputSource(), new NoopInputFormat(), false),\n            new IndexTuningConfig(\n                null,\n                10000,\n                10,\n                null,\n                null,\n                null,\n                null,\n                null,\n                null,\n                indexSpec,\n                null,\n                3,\n                false,\n                null,\n                null,\n                null,\n                null,\n                null,\n                null,\n                null\n            )\n        ),\n        null,\n        AuthTestUtils.TEST_AUTHORIZER_MAPPER,\n        null,\n        ROW_INGESTION_METERS_FACTORY,\n        appenderatorsManager\n    );\n\n    final Optional<TaskStatus> preRunTaskStatus = tsqa.getStatus(indexTask.getId());\n    Assert.assertTrue(\"pre run task status not present\", !preRunTaskStatus.isPresent());\n\n    final TaskStatus mergedStatus = runTask(indexTask);\n    final TaskStatus status = taskStorage.getStatus(indexTask.getId()).get();\n    final List<DataSegment> publishedSegments = BY_INTERVAL_ORDERING.sortedCopy(mdc.getPublished());\n    final List<DataSegment> loggedSegments = BY_INTERVAL_ORDERING.sortedCopy(tsqa.getInsertedSegments(indexTask.getId()));\n\n    Assert.assertEquals(\"statusCode\", TaskState.SUCCESS, status.getStatusCode());\n    Assert.assertEquals(taskLocation, status.getLocation());\n    Assert.assertEquals(\"merged statusCode\", TaskState.SUCCESS, mergedStatus.getStatusCode());\n    Assert.assertEquals(\"segments logged vs published\", loggedSegments, publishedSegments);\n    Assert.assertEquals(\"num segments published\", 2, mdc.getPublished().size());\n    Assert.assertEquals(\"num segments nuked\", 0, mdc.getNuked().size());\n\n    Assert.assertEquals(\"segment1 datasource\", \"foo\", publishedSegments.get(0).getDataSource());\n    Assert.assertEquals(\"segment1 interval\", Intervals.of(\"2010-01-01/P1D\"), publishedSegments.get(0).getInterval());\n    Assert.assertEquals(\n        \"segment1 dimensions\",\n        ImmutableList.of(\"dim1\", \"dim2\"),\n        publishedSegments.get(0).getDimensions()\n    );\n    Assert.assertEquals(\"segment1 metrics\", ImmutableList.of(\"met\"), publishedSegments.get(0).getMetrics());\n\n    Assert.assertEquals(\"segment2 datasource\", \"foo\", publishedSegments.get(1).getDataSource());\n    Assert.assertEquals(\"segment2 interval\", Intervals.of(\"2010-01-02/P1D\"), publishedSegments.get(1).getInterval());\n    Assert.assertEquals(\n        \"segment2 dimensions\",\n        ImmutableList.of(\"dim1\", \"dim2\"),\n        publishedSegments.get(1).getDimensions()\n    );\n    Assert.assertEquals(\"segment2 metrics\", ImmutableList.of(\"met\"), publishedSegments.get(1).getMetrics());\n  }\n","date":"2019-11-16 01:22:09","endLine":802,"groupId":"7933","id":19,"instanceNumber":1,"isCurCommit":0,"methodName":"testIndexTask","params":"()","path":"/mnt/clonedata/CloneManagementServer/ManagementServer/consistResult/result-druid-10-0.7/blobInfo/CC_OUT/blobs/38/d25dd2b1dada3146b84dddcaed36090f8dbf1c.src","preCode":"  public void testIndexTask() throws Exception\n  {\n    final Task indexTask = new IndexTask(\n        null,\n        null,\n        new IndexIngestionSpec(\n            new DataSchema(\n                \"foo\",\n                null,\n                new AggregatorFactory[]{new DoubleSumAggregatorFactory(\"met\", \"met\")},\n                new UniformGranularitySpec(\n                    Granularities.DAY,\n                    null,\n                    ImmutableList.of(Intervals.of(\"2010-01-01/P2D\"))\n                ),\n                null,\n                mapper\n            ),\n            new IndexIOConfig(new MockFirehoseFactory(false), false),\n            new IndexTuningConfig(\n                null,\n                10000,\n                10,\n                null,\n                null,\n                null,\n                null,\n                null,\n                null,\n                indexSpec,\n                null,\n                3,\n                false,\n                null,\n                null,\n                null,\n                null,\n                null,\n                null,\n                null\n            )\n        ),\n        null,\n        AuthTestUtils.TEST_AUTHORIZER_MAPPER,\n        null,\n        ROW_INGESTION_METERS_FACTORY,\n        appenderatorsManager\n    );\n\n    final Optional<TaskStatus> preRunTaskStatus = tsqa.getStatus(indexTask.getId());\n    Assert.assertTrue(\"pre run task status not present\", !preRunTaskStatus.isPresent());\n\n    final TaskStatus mergedStatus = runTask(indexTask);\n    final TaskStatus status = taskStorage.getStatus(indexTask.getId()).get();\n    final List<DataSegment> publishedSegments = BY_INTERVAL_ORDERING.sortedCopy(mdc.getPublished());\n    final List<DataSegment> loggedSegments = BY_INTERVAL_ORDERING.sortedCopy(tsqa.getInsertedSegments(indexTask.getId()));\n\n    Assert.assertEquals(\"statusCode\", TaskState.SUCCESS, status.getStatusCode());\n    Assert.assertEquals(taskLocation, status.getLocation());\n    Assert.assertEquals(\"merged statusCode\", TaskState.SUCCESS, mergedStatus.getStatusCode());\n    Assert.assertEquals(\"segments logged vs published\", loggedSegments, publishedSegments);\n    Assert.assertEquals(\"num segments published\", 2, mdc.getPublished().size());\n    Assert.assertEquals(\"num segments nuked\", 0, mdc.getNuked().size());\n\n    Assert.assertEquals(\"segment1 datasource\", \"foo\", publishedSegments.get(0).getDataSource());\n    Assert.assertEquals(\"segment1 interval\", Intervals.of(\"2010-01-01/P1D\"), publishedSegments.get(0).getInterval());\n    Assert.assertEquals(\n        \"segment1 dimensions\",\n        ImmutableList.of(\"dim1\", \"dim2\"),\n        publishedSegments.get(0).getDimensions()\n    );\n    Assert.assertEquals(\"segment1 metrics\", ImmutableList.of(\"met\"), publishedSegments.get(0).getMetrics());\n\n    Assert.assertEquals(\"segment2 datasource\", \"foo\", publishedSegments.get(1).getDataSource());\n    Assert.assertEquals(\"segment2 interval\", Intervals.of(\"2010-01-02/P1D\"), publishedSegments.get(1).getInterval());\n    Assert.assertEquals(\n        \"segment2 dimensions\",\n        ImmutableList.of(\"dim1\", \"dim2\"),\n        publishedSegments.get(1).getDimensions()\n    );\n    Assert.assertEquals(\"segment2 metrics\", ImmutableList.of(\"met\"), publishedSegments.get(1).getMetrics());\n  }\n","realPath":"indexing-service/src/test/java/org/apache/druid/indexing/overlord/TaskLifecycleTest.java","repoName":"druid","snippetEndLine":0,"snippetStartLine":0,"startLine":721,"status":"M"},{"authorDate":"2019-11-16 01:22:09","commitOrder":10,"curCode":"  public void testResumeTasks() throws Exception\n  {\n    final Task indexTask = new IndexTask(\n        null,\n        null,\n        new IndexIngestionSpec(\n            new DataSchema(\n                \"foo\",\n                new TimestampSpec(null, null, null),\n                DimensionsSpec.EMPTY,\n                new AggregatorFactory[]{new DoubleSumAggregatorFactory(\"met\", \"met\")},\n                new UniformGranularitySpec(\n                    Granularities.DAY,\n                    null,\n                    ImmutableList.of(Intervals.of(\"2010-01-01/P2D\"))\n                ),\n                null\n            ),\n            new IndexIOConfig(null, new MockInputSource(), new NoopInputFormat(), false),\n            new IndexTuningConfig(\n                null,\n                10000,\n                10,\n                null,\n                null,\n                null,\n                null,\n                null,\n                null,\n                indexSpec,\n                null,\n                null,\n                null,\n                null,\n                null,\n                null,\n                null,\n                null,\n                null,\n                null\n            )\n        ),\n        null,\n        AuthTestUtils.TEST_AUTHORIZER_MAPPER,\n        null,\n        ROW_INGESTION_METERS_FACTORY,\n        appenderatorsManager\n    );\n\n    final long startTime = System.currentTimeMillis();\n\n    \r\n    taskQueue.start();\n    taskStorage.insert(indexTask, TaskStatus.running(indexTask.getId()));\n\n    while (tsqa.getStatus(indexTask.getId()).get().isRunnable()) {\n      if (System.currentTimeMillis() > startTime + 10 * 1000) {\n        throw new ISE(\"Where did the task go?!: %s\", indexTask.getId());\n      }\n\n      Thread.sleep(100);\n    }\n\n    final TaskStatus status = taskStorage.getStatus(indexTask.getId()).get();\n    final List<DataSegment> publishedSegments = BY_INTERVAL_ORDERING.sortedCopy(mdc.getPublished());\n    final List<DataSegment> loggedSegments = BY_INTERVAL_ORDERING.sortedCopy(tsqa.getInsertedSegments(indexTask.getId()));\n\n    Assert.assertEquals(\"statusCode\", TaskState.SUCCESS, status.getStatusCode());\n    Assert.assertEquals(taskLocation, status.getLocation());\n    Assert.assertEquals(\"segments logged vs published\", loggedSegments, publishedSegments);\n    Assert.assertEquals(\"num segments published\", 2, mdc.getPublished().size());\n    Assert.assertEquals(\"num segments nuked\", 0, mdc.getNuked().size());\n\n    Assert.assertEquals(\"segment1 datasource\", \"foo\", publishedSegments.get(0).getDataSource());\n    Assert.assertEquals(\"segment1 interval\", Intervals.of(\"2010-01-01/P1D\"), publishedSegments.get(0).getInterval());\n    Assert.assertEquals(\n        \"segment1 dimensions\",\n        ImmutableList.of(\"dim1\", \"dim2\"),\n        publishedSegments.get(0).getDimensions()\n    );\n    Assert.assertEquals(\"segment1 metrics\", ImmutableList.of(\"met\"), publishedSegments.get(0).getMetrics());\n\n    Assert.assertEquals(\"segment2 datasource\", \"foo\", publishedSegments.get(1).getDataSource());\n    Assert.assertEquals(\"segment2 interval\", Intervals.of(\"2010-01-02/P1D\"), publishedSegments.get(1).getInterval());\n    Assert.assertEquals(\n        \"segment2 dimensions\",\n        ImmutableList.of(\"dim1\", \"dim2\"),\n        publishedSegments.get(1).getDimensions()\n    );\n    Assert.assertEquals(\"segment2 metrics\", ImmutableList.of(\"met\"), publishedSegments.get(1).getMetrics());\n  }\n","date":"2019-11-16 01:22:09","endLine":1323,"groupId":"12487","id":20,"instanceNumber":2,"isCurCommit":0,"methodName":"testResumeTasks","params":"()","path":"/mnt/clonedata/CloneManagementServer/ManagementServer/consistResult/result-druid-10-0.7/blobInfo/CC_OUT/blobs/38/d25dd2b1dada3146b84dddcaed36090f8dbf1c.src","preCode":"  public void testResumeTasks() throws Exception\n  {\n    final Task indexTask = new IndexTask(\n        null,\n        null,\n        new IndexIngestionSpec(\n            new DataSchema(\n                \"foo\",\n                null,\n                new AggregatorFactory[]{new DoubleSumAggregatorFactory(\"met\", \"met\")},\n                new UniformGranularitySpec(\n                    Granularities.DAY,\n                    null,\n                    ImmutableList.of(Intervals.of(\"2010-01-01/P2D\"))\n                ),\n                null,\n                mapper\n            ),\n            new IndexIOConfig(new MockFirehoseFactory(false), false),\n            new IndexTuningConfig(\n                null,\n                10000,\n                10,\n                null,\n                null,\n                null,\n                null,\n                null,\n                null,\n                indexSpec,\n                null,\n                null,\n                null,\n                null,\n                null,\n                null,\n                null,\n                null,\n                null,\n                null\n            )\n        ),\n        null,\n        AuthTestUtils.TEST_AUTHORIZER_MAPPER,\n        null,\n        ROW_INGESTION_METERS_FACTORY,\n        appenderatorsManager\n    );\n\n    final long startTime = System.currentTimeMillis();\n\n    \r\n    taskQueue.start();\n    taskStorage.insert(indexTask, TaskStatus.running(indexTask.getId()));\n\n    while (tsqa.getStatus(indexTask.getId()).get().isRunnable()) {\n      if (System.currentTimeMillis() > startTime + 10 * 1000) {\n        throw new ISE(\"Where did the task go?!: %s\", indexTask.getId());\n      }\n\n      Thread.sleep(100);\n    }\n\n    final TaskStatus status = taskStorage.getStatus(indexTask.getId()).get();\n    final List<DataSegment> publishedSegments = BY_INTERVAL_ORDERING.sortedCopy(mdc.getPublished());\n    final List<DataSegment> loggedSegments = BY_INTERVAL_ORDERING.sortedCopy(tsqa.getInsertedSegments(indexTask.getId()));\n\n    Assert.assertEquals(\"statusCode\", TaskState.SUCCESS, status.getStatusCode());\n    Assert.assertEquals(taskLocation, status.getLocation());\n    Assert.assertEquals(\"segments logged vs published\", loggedSegments, publishedSegments);\n    Assert.assertEquals(\"num segments published\", 2, mdc.getPublished().size());\n    Assert.assertEquals(\"num segments nuked\", 0, mdc.getNuked().size());\n\n    Assert.assertEquals(\"segment1 datasource\", \"foo\", publishedSegments.get(0).getDataSource());\n    Assert.assertEquals(\"segment1 interval\", Intervals.of(\"2010-01-01/P1D\"), publishedSegments.get(0).getInterval());\n    Assert.assertEquals(\n        \"segment1 dimensions\",\n        ImmutableList.of(\"dim1\", \"dim2\"),\n        publishedSegments.get(0).getDimensions()\n    );\n    Assert.assertEquals(\"segment1 metrics\", ImmutableList.of(\"met\"), publishedSegments.get(0).getMetrics());\n\n    Assert.assertEquals(\"segment2 datasource\", \"foo\", publishedSegments.get(1).getDataSource());\n    Assert.assertEquals(\"segment2 interval\", Intervals.of(\"2010-01-02/P1D\"), publishedSegments.get(1).getInterval());\n    Assert.assertEquals(\n        \"segment2 dimensions\",\n        ImmutableList.of(\"dim1\", \"dim2\"),\n        publishedSegments.get(1).getDimensions()\n    );\n    Assert.assertEquals(\"segment2 metrics\", ImmutableList.of(\"met\"), publishedSegments.get(1).getMetrics());\n  }\n","realPath":"indexing-service/src/test/java/org/apache/druid/indexing/overlord/TaskLifecycleTest.java","repoName":"druid","snippetEndLine":0,"snippetStartLine":0,"startLine":1233,"status":"M"}],"commitId":"1611792855ad9def8b6f5b1375862d05d1acca0a","commitMessage":"@@@Add InputSource and InputFormat interfaces (#8823)\n\n* Add InputSource and InputFormat interfaces\n\n* revert orc dependency\n\n* fix dimension exclusions and failing unit tests\n\n* fix tests\n\n* fix test\n\n* fix test\n\n* fix firehose and inputSource for parallel indexing task\n\n* fix tc\n\n* fix tc: remove unused method\n\n* Formattable\n\n* add needsFormat(); renamed to ObjectSource; pass metricsName for reader\n\n* address comments\n\n* fix closing resource\n\n* fix checkstyle\n\n* fix tests\n\n* remove verify from csv\n\n* Revert \"remove verify from csv\"\n\nThis reverts commit 1ea7758489cc8c9d708bd691fd48e62085fd9455.\n\n* address comments\n\n* fix import order and javadoc\n\n* flatMap\n\n* sampleLine\n\n* Add IntermediateRowParsingReader\n\n* Address comments\n\n* move csv reader test\n\n* remove test for verify\n\n* adjust comments\n\n* Fix InputEntityIteratingReader\n\n* rename source -> entity\n\n* address comments\n","date":"2019-11-16 01:22:09","modifiedFileCount":"72","status":"M","submitter":"Jihoon Son"},{"authorTime":"2020-08-27 08:08:12","codes":[{"authorDate":"2020-08-27 08:08:12","commitOrder":11,"curCode":"  public void testIndexTask() throws Exception\n  {\n    final Task indexTask = new IndexTask(\n        null,\n        null,\n        new IndexIngestionSpec(\n            new DataSchema(\n                \"foo\",\n                new TimestampSpec(null, null, null),\n                DimensionsSpec.EMPTY,\n                new AggregatorFactory[]{new DoubleSumAggregatorFactory(\"met\", \"met\")},\n                new UniformGranularitySpec(\n                    Granularities.DAY,\n                    null,\n                    ImmutableList.of(Intervals.of(\"2010-01-01/P2D\"))\n                ),\n                null\n            ),\n            new IndexIOConfig(null, new MockInputSource(), new NoopInputFormat(), false),\n            new IndexTuningConfig(\n                null,\n                10000,\n                10,\n                null,\n                null,\n                null,\n                null,\n                null,\n                null,\n                indexSpec,\n                null,\n                3,\n                false,\n                null,\n                null,\n                null,\n                null,\n                null,\n                null,\n                null\n            )\n        ),\n        null\n    );\n\n    final Optional<TaskStatus> preRunTaskStatus = tsqa.getStatus(indexTask.getId());\n    Assert.assertTrue(\"pre run task status not present\", !preRunTaskStatus.isPresent());\n\n    final TaskStatus mergedStatus = runTask(indexTask);\n    final TaskStatus status = taskStorage.getStatus(indexTask.getId()).get();\n    final List<DataSegment> publishedSegments = BY_INTERVAL_ORDERING.sortedCopy(mdc.getPublished());\n    final List<DataSegment> loggedSegments = BY_INTERVAL_ORDERING.sortedCopy(tsqa.getInsertedSegments(indexTask.getId()));\n\n    Assert.assertEquals(\"statusCode\", TaskState.SUCCESS, status.getStatusCode());\n    Assert.assertEquals(taskLocation, status.getLocation());\n    Assert.assertEquals(\"merged statusCode\", TaskState.SUCCESS, mergedStatus.getStatusCode());\n    Assert.assertEquals(\"segments logged vs published\", loggedSegments, publishedSegments);\n    Assert.assertEquals(\"num segments published\", 2, mdc.getPublished().size());\n    Assert.assertEquals(\"num segments nuked\", 0, mdc.getNuked().size());\n\n    Assert.assertEquals(\"segment1 datasource\", \"foo\", publishedSegments.get(0).getDataSource());\n    Assert.assertEquals(\"segment1 interval\", Intervals.of(\"2010-01-01/P1D\"), publishedSegments.get(0).getInterval());\n    Assert.assertEquals(\n        \"segment1 dimensions\",\n        ImmutableList.of(\"dim1\", \"dim2\"),\n        publishedSegments.get(0).getDimensions()\n    );\n    Assert.assertEquals(\"segment1 metrics\", ImmutableList.of(\"met\"), publishedSegments.get(0).getMetrics());\n\n    Assert.assertEquals(\"segment2 datasource\", \"foo\", publishedSegments.get(1).getDataSource());\n    Assert.assertEquals(\"segment2 interval\", Intervals.of(\"2010-01-02/P1D\"), publishedSegments.get(1).getInterval());\n    Assert.assertEquals(\n        \"segment2 dimensions\",\n        ImmutableList.of(\"dim1\", \"dim2\"),\n        publishedSegments.get(1).getDimensions()\n    );\n    Assert.assertEquals(\"segment2 metrics\", ImmutableList.of(\"met\"), publishedSegments.get(1).getMetrics());\n  }\n","date":"2020-08-27 08:08:12","endLine":804,"groupId":"7933","id":21,"instanceNumber":1,"isCurCommit":0,"methodName":"testIndexTask","params":"()","path":"/mnt/clonedata/CloneManagementServer/ManagementServer/consistResult/result-druid-10-0.7/blobInfo/CC_OUT/blobs/99/ab9a480e9e22e0ff66f204d5b99f7fa005fd6d.src","preCode":"  public void testIndexTask() throws Exception\n  {\n    final Task indexTask = new IndexTask(\n        null,\n        null,\n        new IndexIngestionSpec(\n            new DataSchema(\n                \"foo\",\n                new TimestampSpec(null, null, null),\n                DimensionsSpec.EMPTY,\n                new AggregatorFactory[]{new DoubleSumAggregatorFactory(\"met\", \"met\")},\n                new UniformGranularitySpec(\n                    Granularities.DAY,\n                    null,\n                    ImmutableList.of(Intervals.of(\"2010-01-01/P2D\"))\n                ),\n                null\n            ),\n            new IndexIOConfig(null, new MockInputSource(), new NoopInputFormat(), false),\n            new IndexTuningConfig(\n                null,\n                10000,\n                10,\n                null,\n                null,\n                null,\n                null,\n                null,\n                null,\n                indexSpec,\n                null,\n                3,\n                false,\n                null,\n                null,\n                null,\n                null,\n                null,\n                null,\n                null\n            )\n        ),\n        null,\n        AuthTestUtils.TEST_AUTHORIZER_MAPPER,\n        null,\n        ROW_INGESTION_METERS_FACTORY,\n        appenderatorsManager\n    );\n\n    final Optional<TaskStatus> preRunTaskStatus = tsqa.getStatus(indexTask.getId());\n    Assert.assertTrue(\"pre run task status not present\", !preRunTaskStatus.isPresent());\n\n    final TaskStatus mergedStatus = runTask(indexTask);\n    final TaskStatus status = taskStorage.getStatus(indexTask.getId()).get();\n    final List<DataSegment> publishedSegments = BY_INTERVAL_ORDERING.sortedCopy(mdc.getPublished());\n    final List<DataSegment> loggedSegments = BY_INTERVAL_ORDERING.sortedCopy(tsqa.getInsertedSegments(indexTask.getId()));\n\n    Assert.assertEquals(\"statusCode\", TaskState.SUCCESS, status.getStatusCode());\n    Assert.assertEquals(taskLocation, status.getLocation());\n    Assert.assertEquals(\"merged statusCode\", TaskState.SUCCESS, mergedStatus.getStatusCode());\n    Assert.assertEquals(\"segments logged vs published\", loggedSegments, publishedSegments);\n    Assert.assertEquals(\"num segments published\", 2, mdc.getPublished().size());\n    Assert.assertEquals(\"num segments nuked\", 0, mdc.getNuked().size());\n\n    Assert.assertEquals(\"segment1 datasource\", \"foo\", publishedSegments.get(0).getDataSource());\n    Assert.assertEquals(\"segment1 interval\", Intervals.of(\"2010-01-01/P1D\"), publishedSegments.get(0).getInterval());\n    Assert.assertEquals(\n        \"segment1 dimensions\",\n        ImmutableList.of(\"dim1\", \"dim2\"),\n        publishedSegments.get(0).getDimensions()\n    );\n    Assert.assertEquals(\"segment1 metrics\", ImmutableList.of(\"met\"), publishedSegments.get(0).getMetrics());\n\n    Assert.assertEquals(\"segment2 datasource\", \"foo\", publishedSegments.get(1).getDataSource());\n    Assert.assertEquals(\"segment2 interval\", Intervals.of(\"2010-01-02/P1D\"), publishedSegments.get(1).getInterval());\n    Assert.assertEquals(\n        \"segment2 dimensions\",\n        ImmutableList.of(\"dim1\", \"dim2\"),\n        publishedSegments.get(1).getDimensions()\n    );\n    Assert.assertEquals(\"segment2 metrics\", ImmutableList.of(\"met\"), publishedSegments.get(1).getMetrics());\n  }\n","realPath":"indexing-service/src/test/java/org/apache/druid/indexing/overlord/TaskLifecycleTest.java","repoName":"druid","snippetEndLine":0,"snippetStartLine":0,"startLine":727,"status":"M"},{"authorDate":"2020-08-27 08:08:12","commitOrder":11,"curCode":"  public void testResumeTasks() throws Exception\n  {\n    final Task indexTask = new IndexTask(\n        null,\n        null,\n        new IndexIngestionSpec(\n            new DataSchema(\n                \"foo\",\n                new TimestampSpec(null, null, null),\n                DimensionsSpec.EMPTY,\n                new AggregatorFactory[]{new DoubleSumAggregatorFactory(\"met\", \"met\")},\n                new UniformGranularitySpec(\n                    Granularities.DAY,\n                    null,\n                    ImmutableList.of(Intervals.of(\"2010-01-01/P2D\"))\n                ),\n                null\n            ),\n            new IndexIOConfig(null, new MockInputSource(), new NoopInputFormat(), false),\n            new IndexTuningConfig(\n                null,\n                10000,\n                10,\n                null,\n                null,\n                null,\n                null,\n                null,\n                null,\n                indexSpec,\n                null,\n                null,\n                null,\n                null,\n                null,\n                null,\n                null,\n                null,\n                null,\n                null\n            )\n        ),\n        null\n    );\n\n    final long startTime = System.currentTimeMillis();\n\n    \r\n    taskQueue.start();\n    taskStorage.insert(indexTask, TaskStatus.running(indexTask.getId()));\n\n    while (tsqa.getStatus(indexTask.getId()).get().isRunnable()) {\n      if (System.currentTimeMillis() > startTime + 10 * 1000) {\n        throw new ISE(\"Where did the task go?!: %s\", indexTask.getId());\n      }\n\n      Thread.sleep(100);\n    }\n\n    final TaskStatus status = taskStorage.getStatus(indexTask.getId()).get();\n    final List<DataSegment> publishedSegments = BY_INTERVAL_ORDERING.sortedCopy(mdc.getPublished());\n    final List<DataSegment> loggedSegments = BY_INTERVAL_ORDERING.sortedCopy(tsqa.getInsertedSegments(indexTask.getId()));\n\n    Assert.assertEquals(\"statusCode\", TaskState.SUCCESS, status.getStatusCode());\n    Assert.assertEquals(taskLocation, status.getLocation());\n    Assert.assertEquals(\"segments logged vs published\", loggedSegments, publishedSegments);\n    Assert.assertEquals(\"num segments published\", 2, mdc.getPublished().size());\n    Assert.assertEquals(\"num segments nuked\", 0, mdc.getNuked().size());\n\n    Assert.assertEquals(\"segment1 datasource\", \"foo\", publishedSegments.get(0).getDataSource());\n    Assert.assertEquals(\"segment1 interval\", Intervals.of(\"2010-01-01/P1D\"), publishedSegments.get(0).getInterval());\n    Assert.assertEquals(\n        \"segment1 dimensions\",\n        ImmutableList.of(\"dim1\", \"dim2\"),\n        publishedSegments.get(0).getDimensions()\n    );\n    Assert.assertEquals(\"segment1 metrics\", ImmutableList.of(\"met\"), publishedSegments.get(0).getMetrics());\n\n    Assert.assertEquals(\"segment2 datasource\", \"foo\", publishedSegments.get(1).getDataSource());\n    Assert.assertEquals(\"segment2 interval\", Intervals.of(\"2010-01-02/P1D\"), publishedSegments.get(1).getInterval());\n    Assert.assertEquals(\n        \"segment2 dimensions\",\n        ImmutableList.of(\"dim1\", \"dim2\"),\n        publishedSegments.get(1).getDimensions()\n    );\n    Assert.assertEquals(\"segment2 metrics\", ImmutableList.of(\"met\"), publishedSegments.get(1).getMetrics());\n  }\n","date":"2020-08-27 08:08:12","endLine":1318,"groupId":"12487","id":22,"instanceNumber":2,"isCurCommit":0,"methodName":"testResumeTasks","params":"()","path":"/mnt/clonedata/CloneManagementServer/ManagementServer/consistResult/result-druid-10-0.7/blobInfo/CC_OUT/blobs/99/ab9a480e9e22e0ff66f204d5b99f7fa005fd6d.src","preCode":"  public void testResumeTasks() throws Exception\n  {\n    final Task indexTask = new IndexTask(\n        null,\n        null,\n        new IndexIngestionSpec(\n            new DataSchema(\n                \"foo\",\n                new TimestampSpec(null, null, null),\n                DimensionsSpec.EMPTY,\n                new AggregatorFactory[]{new DoubleSumAggregatorFactory(\"met\", \"met\")},\n                new UniformGranularitySpec(\n                    Granularities.DAY,\n                    null,\n                    ImmutableList.of(Intervals.of(\"2010-01-01/P2D\"))\n                ),\n                null\n            ),\n            new IndexIOConfig(null, new MockInputSource(), new NoopInputFormat(), false),\n            new IndexTuningConfig(\n                null,\n                10000,\n                10,\n                null,\n                null,\n                null,\n                null,\n                null,\n                null,\n                indexSpec,\n                null,\n                null,\n                null,\n                null,\n                null,\n                null,\n                null,\n                null,\n                null,\n                null\n            )\n        ),\n        null,\n        AuthTestUtils.TEST_AUTHORIZER_MAPPER,\n        null,\n        ROW_INGESTION_METERS_FACTORY,\n        appenderatorsManager\n    );\n\n    final long startTime = System.currentTimeMillis();\n\n    \r\n    taskQueue.start();\n    taskStorage.insert(indexTask, TaskStatus.running(indexTask.getId()));\n\n    while (tsqa.getStatus(indexTask.getId()).get().isRunnable()) {\n      if (System.currentTimeMillis() > startTime + 10 * 1000) {\n        throw new ISE(\"Where did the task go?!: %s\", indexTask.getId());\n      }\n\n      Thread.sleep(100);\n    }\n\n    final TaskStatus status = taskStorage.getStatus(indexTask.getId()).get();\n    final List<DataSegment> publishedSegments = BY_INTERVAL_ORDERING.sortedCopy(mdc.getPublished());\n    final List<DataSegment> loggedSegments = BY_INTERVAL_ORDERING.sortedCopy(tsqa.getInsertedSegments(indexTask.getId()));\n\n    Assert.assertEquals(\"statusCode\", TaskState.SUCCESS, status.getStatusCode());\n    Assert.assertEquals(taskLocation, status.getLocation());\n    Assert.assertEquals(\"segments logged vs published\", loggedSegments, publishedSegments);\n    Assert.assertEquals(\"num segments published\", 2, mdc.getPublished().size());\n    Assert.assertEquals(\"num segments nuked\", 0, mdc.getNuked().size());\n\n    Assert.assertEquals(\"segment1 datasource\", \"foo\", publishedSegments.get(0).getDataSource());\n    Assert.assertEquals(\"segment1 interval\", Intervals.of(\"2010-01-01/P1D\"), publishedSegments.get(0).getInterval());\n    Assert.assertEquals(\n        \"segment1 dimensions\",\n        ImmutableList.of(\"dim1\", \"dim2\"),\n        publishedSegments.get(0).getDimensions()\n    );\n    Assert.assertEquals(\"segment1 metrics\", ImmutableList.of(\"met\"), publishedSegments.get(0).getMetrics());\n\n    Assert.assertEquals(\"segment2 datasource\", \"foo\", publishedSegments.get(1).getDataSource());\n    Assert.assertEquals(\"segment2 interval\", Intervals.of(\"2010-01-02/P1D\"), publishedSegments.get(1).getInterval());\n    Assert.assertEquals(\n        \"segment2 dimensions\",\n        ImmutableList.of(\"dim1\", \"dim2\"),\n        publishedSegments.get(1).getDimensions()\n    );\n    Assert.assertEquals(\"segment2 metrics\", ImmutableList.of(\"met\"), publishedSegments.get(1).getMetrics());\n  }\n","realPath":"indexing-service/src/test/java/org/apache/druid/indexing/overlord/TaskLifecycleTest.java","repoName":"druid","snippetEndLine":0,"snippetStartLine":0,"startLine":1232,"status":"M"}],"commitId":"f82fd22fa7de175200b7127c34c2eb2900bf7317","commitMessage":"@@@Move tools for indexing to TaskToolbox instead of injecting them in constructor (#10308)\n\n* Move tools for indexing to TaskToolbox instead of injecting them in constructor\n\n* oops.  other changes\n\n* fix test\n\n* unnecessary new file\n\n* fix test\n\n* fix build","date":"2020-08-27 08:08:12","modifiedFileCount":"67","status":"M","submitter":"Jihoon Son"},{"authorTime":"2020-10-24 09:34:26","codes":[{"authorDate":"2020-10-24 09:34:26","commitOrder":12,"curCode":"  public void testIndexTask() throws Exception\n  {\n    final Task indexTask = new IndexTask(\n        null,\n        null,\n        new IndexIngestionSpec(\n            new DataSchema(\n                \"foo\",\n                new TimestampSpec(null, null, null),\n                DimensionsSpec.EMPTY,\n                new AggregatorFactory[]{new DoubleSumAggregatorFactory(\"met\", \"met\")},\n                new UniformGranularitySpec(\n                    Granularities.DAY,\n                    null,\n                    ImmutableList.of(Intervals.of(\"2010-01-01/P2D\"))\n                ),\n                null\n            ),\n            new IndexIOConfig(null, new MockInputSource(), new NoopInputFormat(), false),\n            new IndexTuningConfig(\n                null,\n                10000,\n                null,\n                10,\n                null,\n                null,\n                null,\n                null,\n                null,\n                null,\n                indexSpec,\n                null,\n                3,\n                false,\n                null,\n                null,\n                null,\n                null,\n                null,\n                null,\n                null\n            )\n        ),\n        null\n    );\n\n    final Optional<TaskStatus> preRunTaskStatus = tsqa.getStatus(indexTask.getId());\n    Assert.assertTrue(\"pre run task status not present\", !preRunTaskStatus.isPresent());\n\n    final TaskStatus mergedStatus = runTask(indexTask);\n    final TaskStatus status = taskStorage.getStatus(indexTask.getId()).get();\n    final List<DataSegment> publishedSegments = BY_INTERVAL_ORDERING.sortedCopy(mdc.getPublished());\n    final List<DataSegment> loggedSegments = BY_INTERVAL_ORDERING.sortedCopy(tsqa.getInsertedSegments(indexTask.getId()));\n\n    Assert.assertEquals(\"statusCode\", TaskState.SUCCESS, status.getStatusCode());\n    Assert.assertEquals(taskLocation, status.getLocation());\n    Assert.assertEquals(\"merged statusCode\", TaskState.SUCCESS, mergedStatus.getStatusCode());\n    Assert.assertEquals(\"segments logged vs published\", loggedSegments, publishedSegments);\n    Assert.assertEquals(\"num segments published\", 2, mdc.getPublished().size());\n    Assert.assertEquals(\"num segments nuked\", 0, mdc.getNuked().size());\n\n    Assert.assertEquals(\"segment1 datasource\", \"foo\", publishedSegments.get(0).getDataSource());\n    Assert.assertEquals(\"segment1 interval\", Intervals.of(\"2010-01-01/P1D\"), publishedSegments.get(0).getInterval());\n    Assert.assertEquals(\n        \"segment1 dimensions\",\n        ImmutableList.of(\"dim1\", \"dim2\"),\n        publishedSegments.get(0).getDimensions()\n    );\n    Assert.assertEquals(\"segment1 metrics\", ImmutableList.of(\"met\"), publishedSegments.get(0).getMetrics());\n\n    Assert.assertEquals(\"segment2 datasource\", \"foo\", publishedSegments.get(1).getDataSource());\n    Assert.assertEquals(\"segment2 interval\", Intervals.of(\"2010-01-02/P1D\"), publishedSegments.get(1).getInterval());\n    Assert.assertEquals(\n        \"segment2 dimensions\",\n        ImmutableList.of(\"dim1\", \"dim2\"),\n        publishedSegments.get(1).getDimensions()\n    );\n    Assert.assertEquals(\"segment2 metrics\", ImmutableList.of(\"met\"), publishedSegments.get(1).getMetrics());\n  }\n","date":"2020-10-24 09:34:26","endLine":805,"groupId":"7933","id":23,"instanceNumber":1,"isCurCommit":0,"methodName":"testIndexTask","params":"()","path":"/mnt/clonedata/CloneManagementServer/ManagementServer/consistResult/result-druid-10-0.7/blobInfo/CC_OUT/blobs/48/f75d7a82c32a4206662d6bdb878c3089b73bb3.src","preCode":"  public void testIndexTask() throws Exception\n  {\n    final Task indexTask = new IndexTask(\n        null,\n        null,\n        new IndexIngestionSpec(\n            new DataSchema(\n                \"foo\",\n                new TimestampSpec(null, null, null),\n                DimensionsSpec.EMPTY,\n                new AggregatorFactory[]{new DoubleSumAggregatorFactory(\"met\", \"met\")},\n                new UniformGranularitySpec(\n                    Granularities.DAY,\n                    null,\n                    ImmutableList.of(Intervals.of(\"2010-01-01/P2D\"))\n                ),\n                null\n            ),\n            new IndexIOConfig(null, new MockInputSource(), new NoopInputFormat(), false),\n            new IndexTuningConfig(\n                null,\n                10000,\n                10,\n                null,\n                null,\n                null,\n                null,\n                null,\n                null,\n                indexSpec,\n                null,\n                3,\n                false,\n                null,\n                null,\n                null,\n                null,\n                null,\n                null,\n                null\n            )\n        ),\n        null\n    );\n\n    final Optional<TaskStatus> preRunTaskStatus = tsqa.getStatus(indexTask.getId());\n    Assert.assertTrue(\"pre run task status not present\", !preRunTaskStatus.isPresent());\n\n    final TaskStatus mergedStatus = runTask(indexTask);\n    final TaskStatus status = taskStorage.getStatus(indexTask.getId()).get();\n    final List<DataSegment> publishedSegments = BY_INTERVAL_ORDERING.sortedCopy(mdc.getPublished());\n    final List<DataSegment> loggedSegments = BY_INTERVAL_ORDERING.sortedCopy(tsqa.getInsertedSegments(indexTask.getId()));\n\n    Assert.assertEquals(\"statusCode\", TaskState.SUCCESS, status.getStatusCode());\n    Assert.assertEquals(taskLocation, status.getLocation());\n    Assert.assertEquals(\"merged statusCode\", TaskState.SUCCESS, mergedStatus.getStatusCode());\n    Assert.assertEquals(\"segments logged vs published\", loggedSegments, publishedSegments);\n    Assert.assertEquals(\"num segments published\", 2, mdc.getPublished().size());\n    Assert.assertEquals(\"num segments nuked\", 0, mdc.getNuked().size());\n\n    Assert.assertEquals(\"segment1 datasource\", \"foo\", publishedSegments.get(0).getDataSource());\n    Assert.assertEquals(\"segment1 interval\", Intervals.of(\"2010-01-01/P1D\"), publishedSegments.get(0).getInterval());\n    Assert.assertEquals(\n        \"segment1 dimensions\",\n        ImmutableList.of(\"dim1\", \"dim2\"),\n        publishedSegments.get(0).getDimensions()\n    );\n    Assert.assertEquals(\"segment1 metrics\", ImmutableList.of(\"met\"), publishedSegments.get(0).getMetrics());\n\n    Assert.assertEquals(\"segment2 datasource\", \"foo\", publishedSegments.get(1).getDataSource());\n    Assert.assertEquals(\"segment2 interval\", Intervals.of(\"2010-01-02/P1D\"), publishedSegments.get(1).getInterval());\n    Assert.assertEquals(\n        \"segment2 dimensions\",\n        ImmutableList.of(\"dim1\", \"dim2\"),\n        publishedSegments.get(1).getDimensions()\n    );\n    Assert.assertEquals(\"segment2 metrics\", ImmutableList.of(\"met\"), publishedSegments.get(1).getMetrics());\n  }\n","realPath":"indexing-service/src/test/java/org/apache/druid/indexing/overlord/TaskLifecycleTest.java","repoName":"druid","snippetEndLine":0,"snippetStartLine":0,"startLine":727,"status":"M"},{"authorDate":"2020-10-24 09:34:26","commitOrder":12,"curCode":"  public void testResumeTasks() throws Exception\n  {\n    final Task indexTask = new IndexTask(\n        null,\n        null,\n        new IndexIngestionSpec(\n            new DataSchema(\n                \"foo\",\n                new TimestampSpec(null, null, null),\n                DimensionsSpec.EMPTY,\n                new AggregatorFactory[]{new DoubleSumAggregatorFactory(\"met\", \"met\")},\n                new UniformGranularitySpec(\n                    Granularities.DAY,\n                    null,\n                    ImmutableList.of(Intervals.of(\"2010-01-01/P2D\"))\n                ),\n                null\n            ),\n            new IndexIOConfig(null, new MockInputSource(), new NoopInputFormat(), false),\n            new IndexTuningConfig(\n                null,\n                10000,\n                null,\n                10,\n                null,\n                null,\n                null,\n                null,\n                null,\n                null,\n                indexSpec,\n                null,\n                null,\n                null,\n                null,\n                null,\n                null,\n                null,\n                null,\n                null,\n                null\n            )\n        ),\n        null\n    );\n\n    final long startTime = System.currentTimeMillis();\n\n    \r\n    taskQueue.start();\n    taskStorage.insert(indexTask, TaskStatus.running(indexTask.getId()));\n\n    while (tsqa.getStatus(indexTask.getId()).get().isRunnable()) {\n      if (System.currentTimeMillis() > startTime + 10 * 1000) {\n        throw new ISE(\"Where did the task go?!: %s\", indexTask.getId());\n      }\n\n      Thread.sleep(100);\n    }\n\n    final TaskStatus status = taskStorage.getStatus(indexTask.getId()).get();\n    final List<DataSegment> publishedSegments = BY_INTERVAL_ORDERING.sortedCopy(mdc.getPublished());\n    final List<DataSegment> loggedSegments = BY_INTERVAL_ORDERING.sortedCopy(tsqa.getInsertedSegments(indexTask.getId()));\n\n    Assert.assertEquals(\"statusCode\", TaskState.SUCCESS, status.getStatusCode());\n    Assert.assertEquals(taskLocation, status.getLocation());\n    Assert.assertEquals(\"segments logged vs published\", loggedSegments, publishedSegments);\n    Assert.assertEquals(\"num segments published\", 2, mdc.getPublished().size());\n    Assert.assertEquals(\"num segments nuked\", 0, mdc.getNuked().size());\n\n    Assert.assertEquals(\"segment1 datasource\", \"foo\", publishedSegments.get(0).getDataSource());\n    Assert.assertEquals(\"segment1 interval\", Intervals.of(\"2010-01-01/P1D\"), publishedSegments.get(0).getInterval());\n    Assert.assertEquals(\n        \"segment1 dimensions\",\n        ImmutableList.of(\"dim1\", \"dim2\"),\n        publishedSegments.get(0).getDimensions()\n    );\n    Assert.assertEquals(\"segment1 metrics\", ImmutableList.of(\"met\"), publishedSegments.get(0).getMetrics());\n\n    Assert.assertEquals(\"segment2 datasource\", \"foo\", publishedSegments.get(1).getDataSource());\n    Assert.assertEquals(\"segment2 interval\", Intervals.of(\"2010-01-02/P1D\"), publishedSegments.get(1).getInterval());\n    Assert.assertEquals(\n        \"segment2 dimensions\",\n        ImmutableList.of(\"dim1\", \"dim2\"),\n        publishedSegments.get(1).getDimensions()\n    );\n    Assert.assertEquals(\"segment2 metrics\", ImmutableList.of(\"met\"), publishedSegments.get(1).getMetrics());\n  }\n","date":"2020-10-24 09:34:26","endLine":1321,"groupId":"12487","id":24,"instanceNumber":2,"isCurCommit":0,"methodName":"testResumeTasks","params":"()","path":"/mnt/clonedata/CloneManagementServer/ManagementServer/consistResult/result-druid-10-0.7/blobInfo/CC_OUT/blobs/48/f75d7a82c32a4206662d6bdb878c3089b73bb3.src","preCode":"  public void testResumeTasks() throws Exception\n  {\n    final Task indexTask = new IndexTask(\n        null,\n        null,\n        new IndexIngestionSpec(\n            new DataSchema(\n                \"foo\",\n                new TimestampSpec(null, null, null),\n                DimensionsSpec.EMPTY,\n                new AggregatorFactory[]{new DoubleSumAggregatorFactory(\"met\", \"met\")},\n                new UniformGranularitySpec(\n                    Granularities.DAY,\n                    null,\n                    ImmutableList.of(Intervals.of(\"2010-01-01/P2D\"))\n                ),\n                null\n            ),\n            new IndexIOConfig(null, new MockInputSource(), new NoopInputFormat(), false),\n            new IndexTuningConfig(\n                null,\n                10000,\n                10,\n                null,\n                null,\n                null,\n                null,\n                null,\n                null,\n                indexSpec,\n                null,\n                null,\n                null,\n                null,\n                null,\n                null,\n                null,\n                null,\n                null,\n                null\n            )\n        ),\n        null\n    );\n\n    final long startTime = System.currentTimeMillis();\n\n    \r\n    taskQueue.start();\n    taskStorage.insert(indexTask, TaskStatus.running(indexTask.getId()));\n\n    while (tsqa.getStatus(indexTask.getId()).get().isRunnable()) {\n      if (System.currentTimeMillis() > startTime + 10 * 1000) {\n        throw new ISE(\"Where did the task go?!: %s\", indexTask.getId());\n      }\n\n      Thread.sleep(100);\n    }\n\n    final TaskStatus status = taskStorage.getStatus(indexTask.getId()).get();\n    final List<DataSegment> publishedSegments = BY_INTERVAL_ORDERING.sortedCopy(mdc.getPublished());\n    final List<DataSegment> loggedSegments = BY_INTERVAL_ORDERING.sortedCopy(tsqa.getInsertedSegments(indexTask.getId()));\n\n    Assert.assertEquals(\"statusCode\", TaskState.SUCCESS, status.getStatusCode());\n    Assert.assertEquals(taskLocation, status.getLocation());\n    Assert.assertEquals(\"segments logged vs published\", loggedSegments, publishedSegments);\n    Assert.assertEquals(\"num segments published\", 2, mdc.getPublished().size());\n    Assert.assertEquals(\"num segments nuked\", 0, mdc.getNuked().size());\n\n    Assert.assertEquals(\"segment1 datasource\", \"foo\", publishedSegments.get(0).getDataSource());\n    Assert.assertEquals(\"segment1 interval\", Intervals.of(\"2010-01-01/P1D\"), publishedSegments.get(0).getInterval());\n    Assert.assertEquals(\n        \"segment1 dimensions\",\n        ImmutableList.of(\"dim1\", \"dim2\"),\n        publishedSegments.get(0).getDimensions()\n    );\n    Assert.assertEquals(\"segment1 metrics\", ImmutableList.of(\"met\"), publishedSegments.get(0).getMetrics());\n\n    Assert.assertEquals(\"segment2 datasource\", \"foo\", publishedSegments.get(1).getDataSource());\n    Assert.assertEquals(\"segment2 interval\", Intervals.of(\"2010-01-02/P1D\"), publishedSegments.get(1).getInterval());\n    Assert.assertEquals(\n        \"segment2 dimensions\",\n        ImmutableList.of(\"dim1\", \"dim2\"),\n        publishedSegments.get(1).getDimensions()\n    );\n    Assert.assertEquals(\"segment2 metrics\", ImmutableList.of(\"met\"), publishedSegments.get(1).getMetrics());\n  }\n","realPath":"indexing-service/src/test/java/org/apache/druid/indexing/overlord/TaskLifecycleTest.java","repoName":"druid","snippetEndLine":0,"snippetStartLine":0,"startLine":1234,"status":"M"}],"commitId":"f3a2903218573f5d336b082b1c9b8a60a19e8c54","commitMessage":"@@@Configurable Index Type (#10335)\n\n* Introduce a Configurable Index Type\n\n* Change to @UnstableApi\n\n* Add AppendableIndexSpecTest\n\n* Update doc\n\n* Add spelling exception\n\n* Add tests coverage\n\n* Revert some of the changes to reduce diff\n\n* Minor fixes\n\n* Update getMaxBytesInMemoryOrDefault() comment\n\n* Fix typo.  remove redundant interface\n\n* Remove off-heap spec (postponed to a later PR)\n\n* Add javadocs to AppendableIndexSpec\n\n* Describe testCreateTask()\n\n* Add tests for AppendableIndexSpec within TuningConfig\n\n* Modify hashCode() to conform with equals()\n\n* Add comment where building incremental-index\n\n* Add \"EqualsVerifier\" tests\n\n* Revert some of the API back to AppenderatorConfig\n\n* Don't use multi-line comments\n\n* Remove knob documentation (deferred)","date":"2020-10-24 09:34:26","modifiedFileCount":"72","status":"M","submitter":"Liran Funaro"},{"authorTime":"2021-01-06 14:19:09","codes":[{"authorDate":"2021-01-06 14:19:09","commitOrder":13,"curCode":"  public void testIndexTask() throws Exception\n  {\n    final Task indexTask = new IndexTask(\n        null,\n        null,\n        new IndexIngestionSpec(\n            new DataSchema(\n                \"foo\",\n                new TimestampSpec(null, null, null),\n                DimensionsSpec.EMPTY,\n                new AggregatorFactory[]{new DoubleSumAggregatorFactory(\"met\", \"met\")},\n                new UniformGranularitySpec(\n                    Granularities.DAY,\n                    null,\n                    ImmutableList.of(Intervals.of(\"2010-01-01/P2D\"))\n                ),\n                null\n            ),\n            new IndexIOConfig(null, new MockInputSource(), new NoopInputFormat(), false),\n            new IndexTuningConfig(\n                null,\n                10000,\n                null,\n                10,\n                null,\n                null,\n                null,\n                null,\n                null,\n                null,\n                indexSpec,\n                null,\n                3,\n                false,\n                null,\n                null,\n                null,\n                null,\n                null,\n                null,\n                null,\n                null\n            )\n        ),\n        null\n    );\n\n    final Optional<TaskStatus> preRunTaskStatus = tsqa.getStatus(indexTask.getId());\n    Assert.assertTrue(\"pre run task status not present\", !preRunTaskStatus.isPresent());\n\n    final TaskStatus mergedStatus = runTask(indexTask);\n    final TaskStatus status = taskStorage.getStatus(indexTask.getId()).get();\n    final List<DataSegment> publishedSegments = BY_INTERVAL_ORDERING.sortedCopy(mdc.getPublished());\n    final List<DataSegment> loggedSegments = BY_INTERVAL_ORDERING.sortedCopy(tsqa.getInsertedSegments(indexTask.getId()));\n\n    Assert.assertEquals(\"statusCode\", TaskState.SUCCESS, status.getStatusCode());\n    Assert.assertEquals(taskLocation, status.getLocation());\n    Assert.assertEquals(\"merged statusCode\", TaskState.SUCCESS, mergedStatus.getStatusCode());\n    Assert.assertEquals(\"segments logged vs published\", loggedSegments, publishedSegments);\n    Assert.assertEquals(\"num segments published\", 2, mdc.getPublished().size());\n    Assert.assertEquals(\"num segments nuked\", 0, mdc.getNuked().size());\n\n    Assert.assertEquals(\"segment1 datasource\", \"foo\", publishedSegments.get(0).getDataSource());\n    Assert.assertEquals(\"segment1 interval\", Intervals.of(\"2010-01-01/P1D\"), publishedSegments.get(0).getInterval());\n    Assert.assertEquals(\n        \"segment1 dimensions\",\n        ImmutableList.of(\"dim1\", \"dim2\"),\n        publishedSegments.get(0).getDimensions()\n    );\n    Assert.assertEquals(\"segment1 metrics\", ImmutableList.of(\"met\"), publishedSegments.get(0).getMetrics());\n\n    Assert.assertEquals(\"segment2 datasource\", \"foo\", publishedSegments.get(1).getDataSource());\n    Assert.assertEquals(\"segment2 interval\", Intervals.of(\"2010-01-02/P1D\"), publishedSegments.get(1).getInterval());\n    Assert.assertEquals(\n        \"segment2 dimensions\",\n        ImmutableList.of(\"dim1\", \"dim2\"),\n        publishedSegments.get(1).getDimensions()\n    );\n    Assert.assertEquals(\"segment2 metrics\", ImmutableList.of(\"met\"), publishedSegments.get(1).getMetrics());\n  }\n","date":"2021-01-06 14:19:09","endLine":806,"groupId":"7933","id":25,"instanceNumber":1,"isCurCommit":0,"methodName":"testIndexTask","params":"()","path":"/mnt/clonedata/CloneManagementServer/ManagementServer/consistResult/result-druid-10-0.7/blobInfo/CC_OUT/blobs/d4/8520ad3bb8fe7e4f786a3c46982530b004adcb.src","preCode":"  public void testIndexTask() throws Exception\n  {\n    final Task indexTask = new IndexTask(\n        null,\n        null,\n        new IndexIngestionSpec(\n            new DataSchema(\n                \"foo\",\n                new TimestampSpec(null, null, null),\n                DimensionsSpec.EMPTY,\n                new AggregatorFactory[]{new DoubleSumAggregatorFactory(\"met\", \"met\")},\n                new UniformGranularitySpec(\n                    Granularities.DAY,\n                    null,\n                    ImmutableList.of(Intervals.of(\"2010-01-01/P2D\"))\n                ),\n                null\n            ),\n            new IndexIOConfig(null, new MockInputSource(), new NoopInputFormat(), false),\n            new IndexTuningConfig(\n                null,\n                10000,\n                null,\n                10,\n                null,\n                null,\n                null,\n                null,\n                null,\n                null,\n                indexSpec,\n                null,\n                3,\n                false,\n                null,\n                null,\n                null,\n                null,\n                null,\n                null,\n                null\n            )\n        ),\n        null\n    );\n\n    final Optional<TaskStatus> preRunTaskStatus = tsqa.getStatus(indexTask.getId());\n    Assert.assertTrue(\"pre run task status not present\", !preRunTaskStatus.isPresent());\n\n    final TaskStatus mergedStatus = runTask(indexTask);\n    final TaskStatus status = taskStorage.getStatus(indexTask.getId()).get();\n    final List<DataSegment> publishedSegments = BY_INTERVAL_ORDERING.sortedCopy(mdc.getPublished());\n    final List<DataSegment> loggedSegments = BY_INTERVAL_ORDERING.sortedCopy(tsqa.getInsertedSegments(indexTask.getId()));\n\n    Assert.assertEquals(\"statusCode\", TaskState.SUCCESS, status.getStatusCode());\n    Assert.assertEquals(taskLocation, status.getLocation());\n    Assert.assertEquals(\"merged statusCode\", TaskState.SUCCESS, mergedStatus.getStatusCode());\n    Assert.assertEquals(\"segments logged vs published\", loggedSegments, publishedSegments);\n    Assert.assertEquals(\"num segments published\", 2, mdc.getPublished().size());\n    Assert.assertEquals(\"num segments nuked\", 0, mdc.getNuked().size());\n\n    Assert.assertEquals(\"segment1 datasource\", \"foo\", publishedSegments.get(0).getDataSource());\n    Assert.assertEquals(\"segment1 interval\", Intervals.of(\"2010-01-01/P1D\"), publishedSegments.get(0).getInterval());\n    Assert.assertEquals(\n        \"segment1 dimensions\",\n        ImmutableList.of(\"dim1\", \"dim2\"),\n        publishedSegments.get(0).getDimensions()\n    );\n    Assert.assertEquals(\"segment1 metrics\", ImmutableList.of(\"met\"), publishedSegments.get(0).getMetrics());\n\n    Assert.assertEquals(\"segment2 datasource\", \"foo\", publishedSegments.get(1).getDataSource());\n    Assert.assertEquals(\"segment2 interval\", Intervals.of(\"2010-01-02/P1D\"), publishedSegments.get(1).getInterval());\n    Assert.assertEquals(\n        \"segment2 dimensions\",\n        ImmutableList.of(\"dim1\", \"dim2\"),\n        publishedSegments.get(1).getDimensions()\n    );\n    Assert.assertEquals(\"segment2 metrics\", ImmutableList.of(\"met\"), publishedSegments.get(1).getMetrics());\n  }\n","realPath":"indexing-service/src/test/java/org/apache/druid/indexing/overlord/TaskLifecycleTest.java","repoName":"druid","snippetEndLine":0,"snippetStartLine":0,"startLine":727,"status":"M"},{"authorDate":"2021-01-06 14:19:09","commitOrder":13,"curCode":"  public void testResumeTasks() throws Exception\n  {\n    final Task indexTask = new IndexTask(\n        null,\n        null,\n        new IndexIngestionSpec(\n            new DataSchema(\n                \"foo\",\n                new TimestampSpec(null, null, null),\n                DimensionsSpec.EMPTY,\n                new AggregatorFactory[]{new DoubleSumAggregatorFactory(\"met\", \"met\")},\n                new UniformGranularitySpec(\n                    Granularities.DAY,\n                    null,\n                    ImmutableList.of(Intervals.of(\"2010-01-01/P2D\"))\n                ),\n                null\n            ),\n            new IndexIOConfig(null, new MockInputSource(), new NoopInputFormat(), false),\n            new IndexTuningConfig(\n                null,\n                10000,\n                null,\n                10,\n                null,\n                null,\n                null,\n                null,\n                null,\n                null,\n                indexSpec,\n                null,\n                null,\n                null,\n                null,\n                null,\n                null,\n                null,\n                null,\n                null,\n                null,\n                null\n            )\n        ),\n        null\n    );\n\n    final long startTime = System.currentTimeMillis();\n\n    \r\n    taskQueue.start();\n    taskStorage.insert(indexTask, TaskStatus.running(indexTask.getId()));\n\n    while (tsqa.getStatus(indexTask.getId()).get().isRunnable()) {\n      if (System.currentTimeMillis() > startTime + 10 * 1000) {\n        throw new ISE(\"Where did the task go?!: %s\", indexTask.getId());\n      }\n\n      Thread.sleep(100);\n    }\n\n    final TaskStatus status = taskStorage.getStatus(indexTask.getId()).get();\n    final List<DataSegment> publishedSegments = BY_INTERVAL_ORDERING.sortedCopy(mdc.getPublished());\n    final List<DataSegment> loggedSegments = BY_INTERVAL_ORDERING.sortedCopy(tsqa.getInsertedSegments(indexTask.getId()));\n\n    Assert.assertEquals(\"statusCode\", TaskState.SUCCESS, status.getStatusCode());\n    Assert.assertEquals(taskLocation, status.getLocation());\n    Assert.assertEquals(\"segments logged vs published\", loggedSegments, publishedSegments);\n    Assert.assertEquals(\"num segments published\", 2, mdc.getPublished().size());\n    Assert.assertEquals(\"num segments nuked\", 0, mdc.getNuked().size());\n\n    Assert.assertEquals(\"segment1 datasource\", \"foo\", publishedSegments.get(0).getDataSource());\n    Assert.assertEquals(\"segment1 interval\", Intervals.of(\"2010-01-01/P1D\"), publishedSegments.get(0).getInterval());\n    Assert.assertEquals(\n        \"segment1 dimensions\",\n        ImmutableList.of(\"dim1\", \"dim2\"),\n        publishedSegments.get(0).getDimensions()\n    );\n    Assert.assertEquals(\"segment1 metrics\", ImmutableList.of(\"met\"), publishedSegments.get(0).getMetrics());\n\n    Assert.assertEquals(\"segment2 datasource\", \"foo\", publishedSegments.get(1).getDataSource());\n    Assert.assertEquals(\"segment2 interval\", Intervals.of(\"2010-01-02/P1D\"), publishedSegments.get(1).getInterval());\n    Assert.assertEquals(\n        \"segment2 dimensions\",\n        ImmutableList.of(\"dim1\", \"dim2\"),\n        publishedSegments.get(1).getDimensions()\n    );\n    Assert.assertEquals(\"segment2 metrics\", ImmutableList.of(\"met\"), publishedSegments.get(1).getMetrics());\n  }\n","date":"2021-01-06 14:19:09","endLine":1324,"groupId":"12487","id":26,"instanceNumber":2,"isCurCommit":0,"methodName":"testResumeTasks","params":"()","path":"/mnt/clonedata/CloneManagementServer/ManagementServer/consistResult/result-druid-10-0.7/blobInfo/CC_OUT/blobs/d4/8520ad3bb8fe7e4f786a3c46982530b004adcb.src","preCode":"  public void testResumeTasks() throws Exception\n  {\n    final Task indexTask = new IndexTask(\n        null,\n        null,\n        new IndexIngestionSpec(\n            new DataSchema(\n                \"foo\",\n                new TimestampSpec(null, null, null),\n                DimensionsSpec.EMPTY,\n                new AggregatorFactory[]{new DoubleSumAggregatorFactory(\"met\", \"met\")},\n                new UniformGranularitySpec(\n                    Granularities.DAY,\n                    null,\n                    ImmutableList.of(Intervals.of(\"2010-01-01/P2D\"))\n                ),\n                null\n            ),\n            new IndexIOConfig(null, new MockInputSource(), new NoopInputFormat(), false),\n            new IndexTuningConfig(\n                null,\n                10000,\n                null,\n                10,\n                null,\n                null,\n                null,\n                null,\n                null,\n                null,\n                indexSpec,\n                null,\n                null,\n                null,\n                null,\n                null,\n                null,\n                null,\n                null,\n                null,\n                null\n            )\n        ),\n        null\n    );\n\n    final long startTime = System.currentTimeMillis();\n\n    \r\n    taskQueue.start();\n    taskStorage.insert(indexTask, TaskStatus.running(indexTask.getId()));\n\n    while (tsqa.getStatus(indexTask.getId()).get().isRunnable()) {\n      if (System.currentTimeMillis() > startTime + 10 * 1000) {\n        throw new ISE(\"Where did the task go?!: %s\", indexTask.getId());\n      }\n\n      Thread.sleep(100);\n    }\n\n    final TaskStatus status = taskStorage.getStatus(indexTask.getId()).get();\n    final List<DataSegment> publishedSegments = BY_INTERVAL_ORDERING.sortedCopy(mdc.getPublished());\n    final List<DataSegment> loggedSegments = BY_INTERVAL_ORDERING.sortedCopy(tsqa.getInsertedSegments(indexTask.getId()));\n\n    Assert.assertEquals(\"statusCode\", TaskState.SUCCESS, status.getStatusCode());\n    Assert.assertEquals(taskLocation, status.getLocation());\n    Assert.assertEquals(\"segments logged vs published\", loggedSegments, publishedSegments);\n    Assert.assertEquals(\"num segments published\", 2, mdc.getPublished().size());\n    Assert.assertEquals(\"num segments nuked\", 0, mdc.getNuked().size());\n\n    Assert.assertEquals(\"segment1 datasource\", \"foo\", publishedSegments.get(0).getDataSource());\n    Assert.assertEquals(\"segment1 interval\", Intervals.of(\"2010-01-01/P1D\"), publishedSegments.get(0).getInterval());\n    Assert.assertEquals(\n        \"segment1 dimensions\",\n        ImmutableList.of(\"dim1\", \"dim2\"),\n        publishedSegments.get(0).getDimensions()\n    );\n    Assert.assertEquals(\"segment1 metrics\", ImmutableList.of(\"met\"), publishedSegments.get(0).getMetrics());\n\n    Assert.assertEquals(\"segment2 datasource\", \"foo\", publishedSegments.get(1).getDataSource());\n    Assert.assertEquals(\"segment2 interval\", Intervals.of(\"2010-01-02/P1D\"), publishedSegments.get(1).getInterval());\n    Assert.assertEquals(\n        \"segment2 dimensions\",\n        ImmutableList.of(\"dim1\", \"dim2\"),\n        publishedSegments.get(1).getDimensions()\n    );\n    Assert.assertEquals(\"segment2 metrics\", ImmutableList.of(\"met\"), publishedSegments.get(1).getMetrics());\n  }\n","realPath":"indexing-service/src/test/java/org/apache/druid/indexing/overlord/TaskLifecycleTest.java","repoName":"druid","snippetEndLine":0,"snippetStartLine":0,"startLine":1236,"status":"M"}],"commitId":"68bb038b314c26bcc57aa96e1078c22d2f24fd35","commitMessage":"@@@Multiphase segment merge for IndexMergerV9 (#10689)\n\n* Multiphase merge for IndexMergerV9\n\n* JSON fix\n\n* Cleanup temp files\n\n* Docs\n\n* Address logging and add IT\n\n* Fix spelling and test unloader datasource name","date":"2021-01-06 14:19:09","modifiedFileCount":"40","status":"M","submitter":"Jonathan Wei"},{"authorTime":"2021-01-27 16:34:56","codes":[{"authorDate":"2021-01-27 16:34:56","commitOrder":14,"curCode":"  public void testIndexTask() throws Exception\n  {\n    final Task indexTask = new IndexTask(\n        null,\n        null,\n        new IndexIngestionSpec(\n            new DataSchema(\n                \"foo\",\n                new TimestampSpec(null, null, null),\n                DimensionsSpec.EMPTY,\n                new AggregatorFactory[]{new DoubleSumAggregatorFactory(\"met\", \"met\")},\n                new UniformGranularitySpec(\n                    Granularities.DAY,\n                    null,\n                    ImmutableList.of(Intervals.of(\"2010-01-01/P2D\"))\n                ),\n                null\n            ),\n            new IndexIOConfig(null, new MockInputSource(), new NoopInputFormat(), false),\n            new IndexTuningConfig(\n                null,\n                10000,\n                null,\n                10,\n                null,\n                null,\n                null,\n                null,\n                null,\n                null,\n                null,\n                indexSpec,\n                null,\n                3,\n                false,\n                null,\n                null,\n                null,\n                null,\n                null,\n                null,\n                null,\n                null\n            )\n        ),\n        null\n    );\n\n    final Optional<TaskStatus> preRunTaskStatus = tsqa.getStatus(indexTask.getId());\n    Assert.assertTrue(\"pre run task status not present\", !preRunTaskStatus.isPresent());\n\n    final TaskStatus mergedStatus = runTask(indexTask);\n    final TaskStatus status = taskStorage.getStatus(indexTask.getId()).get();\n    final List<DataSegment> publishedSegments = BY_INTERVAL_ORDERING.sortedCopy(mdc.getPublished());\n    final List<DataSegment> loggedSegments = BY_INTERVAL_ORDERING.sortedCopy(tsqa.getInsertedSegments(indexTask.getId()));\n\n    Assert.assertEquals(\"statusCode\", TaskState.SUCCESS, status.getStatusCode());\n    Assert.assertEquals(taskLocation, status.getLocation());\n    Assert.assertEquals(\"merged statusCode\", TaskState.SUCCESS, mergedStatus.getStatusCode());\n    Assert.assertEquals(\"segments logged vs published\", loggedSegments, publishedSegments);\n    Assert.assertEquals(\"num segments published\", 2, mdc.getPublished().size());\n    Assert.assertEquals(\"num segments nuked\", 0, mdc.getNuked().size());\n\n    Assert.assertEquals(\"segment1 datasource\", \"foo\", publishedSegments.get(0).getDataSource());\n    Assert.assertEquals(\"segment1 interval\", Intervals.of(\"2010-01-01/P1D\"), publishedSegments.get(0).getInterval());\n    Assert.assertEquals(\n        \"segment1 dimensions\",\n        ImmutableList.of(\"dim1\", \"dim2\"),\n        publishedSegments.get(0).getDimensions()\n    );\n    Assert.assertEquals(\"segment1 metrics\", ImmutableList.of(\"met\"), publishedSegments.get(0).getMetrics());\n\n    Assert.assertEquals(\"segment2 datasource\", \"foo\", publishedSegments.get(1).getDataSource());\n    Assert.assertEquals(\"segment2 interval\", Intervals.of(\"2010-01-02/P1D\"), publishedSegments.get(1).getInterval());\n    Assert.assertEquals(\n        \"segment2 dimensions\",\n        ImmutableList.of(\"dim1\", \"dim2\"),\n        publishedSegments.get(1).getDimensions()\n    );\n    Assert.assertEquals(\"segment2 metrics\", ImmutableList.of(\"met\"), publishedSegments.get(1).getMetrics());\n  }\n","date":"2021-01-27 16:34:56","endLine":807,"groupId":"7933","id":27,"instanceNumber":1,"isCurCommit":0,"methodName":"testIndexTask","params":"()","path":"/mnt/clonedata/CloneManagementServer/ManagementServer/consistResult/result-druid-10-0.7/blobInfo/CC_OUT/blobs/8b/b8e3ef6274c2f6211a805c23cf31ce2e38f8bc.src","preCode":"  public void testIndexTask() throws Exception\n  {\n    final Task indexTask = new IndexTask(\n        null,\n        null,\n        new IndexIngestionSpec(\n            new DataSchema(\n                \"foo\",\n                new TimestampSpec(null, null, null),\n                DimensionsSpec.EMPTY,\n                new AggregatorFactory[]{new DoubleSumAggregatorFactory(\"met\", \"met\")},\n                new UniformGranularitySpec(\n                    Granularities.DAY,\n                    null,\n                    ImmutableList.of(Intervals.of(\"2010-01-01/P2D\"))\n                ),\n                null\n            ),\n            new IndexIOConfig(null, new MockInputSource(), new NoopInputFormat(), false),\n            new IndexTuningConfig(\n                null,\n                10000,\n                null,\n                10,\n                null,\n                null,\n                null,\n                null,\n                null,\n                null,\n                indexSpec,\n                null,\n                3,\n                false,\n                null,\n                null,\n                null,\n                null,\n                null,\n                null,\n                null,\n                null\n            )\n        ),\n        null\n    );\n\n    final Optional<TaskStatus> preRunTaskStatus = tsqa.getStatus(indexTask.getId());\n    Assert.assertTrue(\"pre run task status not present\", !preRunTaskStatus.isPresent());\n\n    final TaskStatus mergedStatus = runTask(indexTask);\n    final TaskStatus status = taskStorage.getStatus(indexTask.getId()).get();\n    final List<DataSegment> publishedSegments = BY_INTERVAL_ORDERING.sortedCopy(mdc.getPublished());\n    final List<DataSegment> loggedSegments = BY_INTERVAL_ORDERING.sortedCopy(tsqa.getInsertedSegments(indexTask.getId()));\n\n    Assert.assertEquals(\"statusCode\", TaskState.SUCCESS, status.getStatusCode());\n    Assert.assertEquals(taskLocation, status.getLocation());\n    Assert.assertEquals(\"merged statusCode\", TaskState.SUCCESS, mergedStatus.getStatusCode());\n    Assert.assertEquals(\"segments logged vs published\", loggedSegments, publishedSegments);\n    Assert.assertEquals(\"num segments published\", 2, mdc.getPublished().size());\n    Assert.assertEquals(\"num segments nuked\", 0, mdc.getNuked().size());\n\n    Assert.assertEquals(\"segment1 datasource\", \"foo\", publishedSegments.get(0).getDataSource());\n    Assert.assertEquals(\"segment1 interval\", Intervals.of(\"2010-01-01/P1D\"), publishedSegments.get(0).getInterval());\n    Assert.assertEquals(\n        \"segment1 dimensions\",\n        ImmutableList.of(\"dim1\", \"dim2\"),\n        publishedSegments.get(0).getDimensions()\n    );\n    Assert.assertEquals(\"segment1 metrics\", ImmutableList.of(\"met\"), publishedSegments.get(0).getMetrics());\n\n    Assert.assertEquals(\"segment2 datasource\", \"foo\", publishedSegments.get(1).getDataSource());\n    Assert.assertEquals(\"segment2 interval\", Intervals.of(\"2010-01-02/P1D\"), publishedSegments.get(1).getInterval());\n    Assert.assertEquals(\n        \"segment2 dimensions\",\n        ImmutableList.of(\"dim1\", \"dim2\"),\n        publishedSegments.get(1).getDimensions()\n    );\n    Assert.assertEquals(\"segment2 metrics\", ImmutableList.of(\"met\"), publishedSegments.get(1).getMetrics());\n  }\n","realPath":"indexing-service/src/test/java/org/apache/druid/indexing/overlord/TaskLifecycleTest.java","repoName":"druid","snippetEndLine":0,"snippetStartLine":0,"startLine":727,"status":"M"},{"authorDate":"2021-01-27 16:34:56","commitOrder":14,"curCode":"  public void testResumeTasks() throws Exception\n  {\n    final Task indexTask = new IndexTask(\n        null,\n        null,\n        new IndexIngestionSpec(\n            new DataSchema(\n                \"foo\",\n                new TimestampSpec(null, null, null),\n                DimensionsSpec.EMPTY,\n                new AggregatorFactory[]{new DoubleSumAggregatorFactory(\"met\", \"met\")},\n                new UniformGranularitySpec(\n                    Granularities.DAY,\n                    null,\n                    ImmutableList.of(Intervals.of(\"2010-01-01/P2D\"))\n                ),\n                null\n            ),\n            new IndexIOConfig(null, new MockInputSource(), new NoopInputFormat(), false),\n            new IndexTuningConfig(\n                null,\n                10000,\n                null,\n                10,\n                null,\n                null,\n                null,\n                null,\n                null,\n                null,\n                null,\n                indexSpec,\n                null,\n                null,\n                null,\n                null,\n                null,\n                null,\n                null,\n                null,\n                null,\n                null,\n                null\n            )\n        ),\n        null\n    );\n\n    final long startTime = System.currentTimeMillis();\n\n    \r\n    taskQueue.start();\n    taskStorage.insert(indexTask, TaskStatus.running(indexTask.getId()));\n\n    while (tsqa.getStatus(indexTask.getId()).get().isRunnable()) {\n      if (System.currentTimeMillis() > startTime + 10 * 1000) {\n        throw new ISE(\"Where did the task go?!: %s\", indexTask.getId());\n      }\n\n      Thread.sleep(100);\n    }\n\n    final TaskStatus status = taskStorage.getStatus(indexTask.getId()).get();\n    final List<DataSegment> publishedSegments = BY_INTERVAL_ORDERING.sortedCopy(mdc.getPublished());\n    final List<DataSegment> loggedSegments = BY_INTERVAL_ORDERING.sortedCopy(tsqa.getInsertedSegments(indexTask.getId()));\n\n    Assert.assertEquals(\"statusCode\", TaskState.SUCCESS, status.getStatusCode());\n    Assert.assertEquals(taskLocation, status.getLocation());\n    Assert.assertEquals(\"segments logged vs published\", loggedSegments, publishedSegments);\n    Assert.assertEquals(\"num segments published\", 2, mdc.getPublished().size());\n    Assert.assertEquals(\"num segments nuked\", 0, mdc.getNuked().size());\n\n    Assert.assertEquals(\"segment1 datasource\", \"foo\", publishedSegments.get(0).getDataSource());\n    Assert.assertEquals(\"segment1 interval\", Intervals.of(\"2010-01-01/P1D\"), publishedSegments.get(0).getInterval());\n    Assert.assertEquals(\n        \"segment1 dimensions\",\n        ImmutableList.of(\"dim1\", \"dim2\"),\n        publishedSegments.get(0).getDimensions()\n    );\n    Assert.assertEquals(\"segment1 metrics\", ImmutableList.of(\"met\"), publishedSegments.get(0).getMetrics());\n\n    Assert.assertEquals(\"segment2 datasource\", \"foo\", publishedSegments.get(1).getDataSource());\n    Assert.assertEquals(\"segment2 interval\", Intervals.of(\"2010-01-02/P1D\"), publishedSegments.get(1).getInterval());\n    Assert.assertEquals(\n        \"segment2 dimensions\",\n        ImmutableList.of(\"dim1\", \"dim2\"),\n        publishedSegments.get(1).getDimensions()\n    );\n    Assert.assertEquals(\"segment2 metrics\", ImmutableList.of(\"met\"), publishedSegments.get(1).getMetrics());\n  }\n","date":"2021-01-27 16:34:56","endLine":1327,"groupId":"12487","id":28,"instanceNumber":2,"isCurCommit":0,"methodName":"testResumeTasks","params":"()","path":"/mnt/clonedata/CloneManagementServer/ManagementServer/consistResult/result-druid-10-0.7/blobInfo/CC_OUT/blobs/8b/b8e3ef6274c2f6211a805c23cf31ce2e38f8bc.src","preCode":"  public void testResumeTasks() throws Exception\n  {\n    final Task indexTask = new IndexTask(\n        null,\n        null,\n        new IndexIngestionSpec(\n            new DataSchema(\n                \"foo\",\n                new TimestampSpec(null, null, null),\n                DimensionsSpec.EMPTY,\n                new AggregatorFactory[]{new DoubleSumAggregatorFactory(\"met\", \"met\")},\n                new UniformGranularitySpec(\n                    Granularities.DAY,\n                    null,\n                    ImmutableList.of(Intervals.of(\"2010-01-01/P2D\"))\n                ),\n                null\n            ),\n            new IndexIOConfig(null, new MockInputSource(), new NoopInputFormat(), false),\n            new IndexTuningConfig(\n                null,\n                10000,\n                null,\n                10,\n                null,\n                null,\n                null,\n                null,\n                null,\n                null,\n                indexSpec,\n                null,\n                null,\n                null,\n                null,\n                null,\n                null,\n                null,\n                null,\n                null,\n                null,\n                null\n            )\n        ),\n        null\n    );\n\n    final long startTime = System.currentTimeMillis();\n\n    \r\n    taskQueue.start();\n    taskStorage.insert(indexTask, TaskStatus.running(indexTask.getId()));\n\n    while (tsqa.getStatus(indexTask.getId()).get().isRunnable()) {\n      if (System.currentTimeMillis() > startTime + 10 * 1000) {\n        throw new ISE(\"Where did the task go?!: %s\", indexTask.getId());\n      }\n\n      Thread.sleep(100);\n    }\n\n    final TaskStatus status = taskStorage.getStatus(indexTask.getId()).get();\n    final List<DataSegment> publishedSegments = BY_INTERVAL_ORDERING.sortedCopy(mdc.getPublished());\n    final List<DataSegment> loggedSegments = BY_INTERVAL_ORDERING.sortedCopy(tsqa.getInsertedSegments(indexTask.getId()));\n\n    Assert.assertEquals(\"statusCode\", TaskState.SUCCESS, status.getStatusCode());\n    Assert.assertEquals(taskLocation, status.getLocation());\n    Assert.assertEquals(\"segments logged vs published\", loggedSegments, publishedSegments);\n    Assert.assertEquals(\"num segments published\", 2, mdc.getPublished().size());\n    Assert.assertEquals(\"num segments nuked\", 0, mdc.getNuked().size());\n\n    Assert.assertEquals(\"segment1 datasource\", \"foo\", publishedSegments.get(0).getDataSource());\n    Assert.assertEquals(\"segment1 interval\", Intervals.of(\"2010-01-01/P1D\"), publishedSegments.get(0).getInterval());\n    Assert.assertEquals(\n        \"segment1 dimensions\",\n        ImmutableList.of(\"dim1\", \"dim2\"),\n        publishedSegments.get(0).getDimensions()\n    );\n    Assert.assertEquals(\"segment1 metrics\", ImmutableList.of(\"met\"), publishedSegments.get(0).getMetrics());\n\n    Assert.assertEquals(\"segment2 datasource\", \"foo\", publishedSegments.get(1).getDataSource());\n    Assert.assertEquals(\"segment2 interval\", Intervals.of(\"2010-01-02/P1D\"), publishedSegments.get(1).getInterval());\n    Assert.assertEquals(\n        \"segment2 dimensions\",\n        ImmutableList.of(\"dim1\", \"dim2\"),\n        publishedSegments.get(1).getDimensions()\n    );\n    Assert.assertEquals(\"segment2 metrics\", ImmutableList.of(\"met\"), publishedSegments.get(1).getMetrics());\n  }\n","realPath":"indexing-service/src/test/java/org/apache/druid/indexing/overlord/TaskLifecycleTest.java","repoName":"druid","snippetEndLine":0,"snippetStartLine":0,"startLine":1238,"status":"M"}],"commitId":"a46d561bd7e2b045a08a2e475847d4a7505a1c93","commitMessage":"@@@Fix byte calculation for maxBytesInMemory to take into account of Sink/Hydrant Object overhead (#10740)\n\n* Fix byte calculation for maxBytesInMemory to take into account of Sink/Hydrant Object overhead\n\n* Fix byte calculation for maxBytesInMemory to take into account of Sink/Hydrant Object overhead\n\n* Fix byte calculation for maxBytesInMemory to take into account of Sink/Hydrant Object overhead\n\n* Fix byte calculation for maxBytesInMemory to take into account of Sink/Hydrant Object overhead\n\n* fix checkstyle\n\n* Fix byte calculation for maxBytesInMemory to take into account of Sink/Hydrant Object overhead\n\n* Fix byte calculation for maxBytesInMemory to take into account of Sink/Hydrant Object overhead\n\n* fix test\n\n* fix test\n\n* add log\n\n* Fix byte calculation for maxBytesInMemory to take into account of Sink/Hydrant Object overhead\n\n* address comments\n\n* fix checkstyle\n\n* fix checkstyle\n\n* add config to skip overhead memory calculation\n\n* add test for the skipBytesInMemoryOverheadCheck config\n\n* add docs\n\n* fix checkstyle\n\n* fix checkstyle\n\n* fix spelling\n\n* address comments\n\n* fix travis\n\n* address comments","date":"2021-01-27 16:34:56","modifiedFileCount":"50","status":"M","submitter":"Maytas Monsereenusorn"},{"authorTime":"2021-04-02 03:29:36","codes":[{"authorDate":"2021-04-02 03:29:36","commitOrder":15,"curCode":"  public void testIndexTask() throws Exception\n  {\n    final Task indexTask = new IndexTask(\n        null,\n        null,\n        new IndexIngestionSpec(\n            new DataSchema(\n                \"foo\",\n                new TimestampSpec(null, null, null),\n                DimensionsSpec.EMPTY,\n                new AggregatorFactory[]{new DoubleSumAggregatorFactory(\"met\", \"met\")},\n                new UniformGranularitySpec(\n                    Granularities.DAY,\n                    null,\n                    ImmutableList.of(Intervals.of(\"2010-01-01/P2D\"))\n                ),\n                null\n            ),\n            new IndexIOConfig(null, new MockInputSource(), new NoopInputFormat(), false, false),\n            new IndexTuningConfig(\n                null,\n                10000,\n                null,\n                10,\n                null,\n                null,\n                null,\n                null,\n                null,\n                null,\n                null,\n                indexSpec,\n                null,\n                3,\n                false,\n                null,\n                null,\n                null,\n                null,\n                null,\n                null,\n                null,\n                null\n            )\n        ),\n        null\n    );\n\n    final Optional<TaskStatus> preRunTaskStatus = tsqa.getStatus(indexTask.getId());\n    Assert.assertTrue(\"pre run task status not present\", !preRunTaskStatus.isPresent());\n\n    final TaskStatus mergedStatus = runTask(indexTask);\n    final TaskStatus status = taskStorage.getStatus(indexTask.getId()).get();\n    final List<DataSegment> publishedSegments = BY_INTERVAL_ORDERING.sortedCopy(mdc.getPublished());\n    final List<DataSegment> loggedSegments = BY_INTERVAL_ORDERING.sortedCopy(tsqa.getInsertedSegments(indexTask.getId()));\n\n    Assert.assertEquals(\"statusCode\", TaskState.SUCCESS, status.getStatusCode());\n    Assert.assertEquals(taskLocation, status.getLocation());\n    Assert.assertEquals(\"merged statusCode\", TaskState.SUCCESS, mergedStatus.getStatusCode());\n    Assert.assertEquals(\"segments logged vs published\", loggedSegments, publishedSegments);\n    Assert.assertEquals(\"num segments published\", 2, mdc.getPublished().size());\n    Assert.assertEquals(\"num segments nuked\", 0, mdc.getNuked().size());\n\n    Assert.assertEquals(\"segment1 datasource\", \"foo\", publishedSegments.get(0).getDataSource());\n    Assert.assertEquals(\"segment1 interval\", Intervals.of(\"2010-01-01/P1D\"), publishedSegments.get(0).getInterval());\n    Assert.assertEquals(\n        \"segment1 dimensions\",\n        ImmutableList.of(\"dim1\", \"dim2\"),\n        publishedSegments.get(0).getDimensions()\n    );\n    Assert.assertEquals(\"segment1 metrics\", ImmutableList.of(\"met\"), publishedSegments.get(0).getMetrics());\n\n    Assert.assertEquals(\"segment2 datasource\", \"foo\", publishedSegments.get(1).getDataSource());\n    Assert.assertEquals(\"segment2 interval\", Intervals.of(\"2010-01-02/P1D\"), publishedSegments.get(1).getInterval());\n    Assert.assertEquals(\n        \"segment2 dimensions\",\n        ImmutableList.of(\"dim1\", \"dim2\"),\n        publishedSegments.get(1).getDimensions()\n    );\n    Assert.assertEquals(\"segment2 metrics\", ImmutableList.of(\"met\"), publishedSegments.get(1).getMetrics());\n  }\n","date":"2021-04-02 03:29:36","endLine":807,"groupId":"7933","id":29,"instanceNumber":1,"isCurCommit":0,"methodName":"testIndexTask","params":"()","path":"/mnt/clonedata/CloneManagementServer/ManagementServer/consistResult/result-druid-10-0.7/blobInfo/CC_OUT/blobs/8f/220870c47400a785d224037a706510fad9493d.src","preCode":"  public void testIndexTask() throws Exception\n  {\n    final Task indexTask = new IndexTask(\n        null,\n        null,\n        new IndexIngestionSpec(\n            new DataSchema(\n                \"foo\",\n                new TimestampSpec(null, null, null),\n                DimensionsSpec.EMPTY,\n                new AggregatorFactory[]{new DoubleSumAggregatorFactory(\"met\", \"met\")},\n                new UniformGranularitySpec(\n                    Granularities.DAY,\n                    null,\n                    ImmutableList.of(Intervals.of(\"2010-01-01/P2D\"))\n                ),\n                null\n            ),\n            new IndexIOConfig(null, new MockInputSource(), new NoopInputFormat(), false),\n            new IndexTuningConfig(\n                null,\n                10000,\n                null,\n                10,\n                null,\n                null,\n                null,\n                null,\n                null,\n                null,\n                null,\n                indexSpec,\n                null,\n                3,\n                false,\n                null,\n                null,\n                null,\n                null,\n                null,\n                null,\n                null,\n                null\n            )\n        ),\n        null\n    );\n\n    final Optional<TaskStatus> preRunTaskStatus = tsqa.getStatus(indexTask.getId());\n    Assert.assertTrue(\"pre run task status not present\", !preRunTaskStatus.isPresent());\n\n    final TaskStatus mergedStatus = runTask(indexTask);\n    final TaskStatus status = taskStorage.getStatus(indexTask.getId()).get();\n    final List<DataSegment> publishedSegments = BY_INTERVAL_ORDERING.sortedCopy(mdc.getPublished());\n    final List<DataSegment> loggedSegments = BY_INTERVAL_ORDERING.sortedCopy(tsqa.getInsertedSegments(indexTask.getId()));\n\n    Assert.assertEquals(\"statusCode\", TaskState.SUCCESS, status.getStatusCode());\n    Assert.assertEquals(taskLocation, status.getLocation());\n    Assert.assertEquals(\"merged statusCode\", TaskState.SUCCESS, mergedStatus.getStatusCode());\n    Assert.assertEquals(\"segments logged vs published\", loggedSegments, publishedSegments);\n    Assert.assertEquals(\"num segments published\", 2, mdc.getPublished().size());\n    Assert.assertEquals(\"num segments nuked\", 0, mdc.getNuked().size());\n\n    Assert.assertEquals(\"segment1 datasource\", \"foo\", publishedSegments.get(0).getDataSource());\n    Assert.assertEquals(\"segment1 interval\", Intervals.of(\"2010-01-01/P1D\"), publishedSegments.get(0).getInterval());\n    Assert.assertEquals(\n        \"segment1 dimensions\",\n        ImmutableList.of(\"dim1\", \"dim2\"),\n        publishedSegments.get(0).getDimensions()\n    );\n    Assert.assertEquals(\"segment1 metrics\", ImmutableList.of(\"met\"), publishedSegments.get(0).getMetrics());\n\n    Assert.assertEquals(\"segment2 datasource\", \"foo\", publishedSegments.get(1).getDataSource());\n    Assert.assertEquals(\"segment2 interval\", Intervals.of(\"2010-01-02/P1D\"), publishedSegments.get(1).getInterval());\n    Assert.assertEquals(\n        \"segment2 dimensions\",\n        ImmutableList.of(\"dim1\", \"dim2\"),\n        publishedSegments.get(1).getDimensions()\n    );\n    Assert.assertEquals(\"segment2 metrics\", ImmutableList.of(\"met\"), publishedSegments.get(1).getMetrics());\n  }\n","realPath":"indexing-service/src/test/java/org/apache/druid/indexing/overlord/TaskLifecycleTest.java","repoName":"druid","snippetEndLine":0,"snippetStartLine":0,"startLine":727,"status":"M"},{"authorDate":"2021-04-02 03:29:36","commitOrder":15,"curCode":"  public void testResumeTasks() throws Exception\n  {\n    final Task indexTask = new IndexTask(\n        null,\n        null,\n        new IndexIngestionSpec(\n            new DataSchema(\n                \"foo\",\n                new TimestampSpec(null, null, null),\n                DimensionsSpec.EMPTY,\n                new AggregatorFactory[]{new DoubleSumAggregatorFactory(\"met\", \"met\")},\n                new UniformGranularitySpec(\n                    Granularities.DAY,\n                    null,\n                    ImmutableList.of(Intervals.of(\"2010-01-01/P2D\"))\n                ),\n                null\n            ),\n            new IndexIOConfig(null, new MockInputSource(), new NoopInputFormat(), false, false),\n            new IndexTuningConfig(\n                null,\n                10000,\n                null,\n                10,\n                null,\n                null,\n                null,\n                null,\n                null,\n                null,\n                null,\n                indexSpec,\n                null,\n                null,\n                null,\n                null,\n                null,\n                null,\n                null,\n                null,\n                null,\n                null,\n                null\n            )\n        ),\n        null\n    );\n\n    final long startTime = System.currentTimeMillis();\n\n    \r\n    taskQueue.start();\n    taskStorage.insert(indexTask, TaskStatus.running(indexTask.getId()));\n\n    while (tsqa.getStatus(indexTask.getId()).get().isRunnable()) {\n      if (System.currentTimeMillis() > startTime + 10 * 1000) {\n        throw new ISE(\"Where did the task go?!: %s\", indexTask.getId());\n      }\n\n      Thread.sleep(100);\n    }\n\n    final TaskStatus status = taskStorage.getStatus(indexTask.getId()).get();\n    final List<DataSegment> publishedSegments = BY_INTERVAL_ORDERING.sortedCopy(mdc.getPublished());\n    final List<DataSegment> loggedSegments = BY_INTERVAL_ORDERING.sortedCopy(tsqa.getInsertedSegments(indexTask.getId()));\n\n    Assert.assertEquals(\"statusCode\", TaskState.SUCCESS, status.getStatusCode());\n    Assert.assertEquals(taskLocation, status.getLocation());\n    Assert.assertEquals(\"segments logged vs published\", loggedSegments, publishedSegments);\n    Assert.assertEquals(\"num segments published\", 2, mdc.getPublished().size());\n    Assert.assertEquals(\"num segments nuked\", 0, mdc.getNuked().size());\n\n    Assert.assertEquals(\"segment1 datasource\", \"foo\", publishedSegments.get(0).getDataSource());\n    Assert.assertEquals(\"segment1 interval\", Intervals.of(\"2010-01-01/P1D\"), publishedSegments.get(0).getInterval());\n    Assert.assertEquals(\n        \"segment1 dimensions\",\n        ImmutableList.of(\"dim1\", \"dim2\"),\n        publishedSegments.get(0).getDimensions()\n    );\n    Assert.assertEquals(\"segment1 metrics\", ImmutableList.of(\"met\"), publishedSegments.get(0).getMetrics());\n\n    Assert.assertEquals(\"segment2 datasource\", \"foo\", publishedSegments.get(1).getDataSource());\n    Assert.assertEquals(\"segment2 interval\", Intervals.of(\"2010-01-02/P1D\"), publishedSegments.get(1).getInterval());\n    Assert.assertEquals(\n        \"segment2 dimensions\",\n        ImmutableList.of(\"dim1\", \"dim2\"),\n        publishedSegments.get(1).getDimensions()\n    );\n    Assert.assertEquals(\"segment2 metrics\", ImmutableList.of(\"met\"), publishedSegments.get(1).getMetrics());\n  }\n","date":"2021-04-02 03:29:36","endLine":1327,"groupId":"12487","id":30,"instanceNumber":2,"isCurCommit":0,"methodName":"testResumeTasks","params":"()","path":"/mnt/clonedata/CloneManagementServer/ManagementServer/consistResult/result-druid-10-0.7/blobInfo/CC_OUT/blobs/8f/220870c47400a785d224037a706510fad9493d.src","preCode":"  public void testResumeTasks() throws Exception\n  {\n    final Task indexTask = new IndexTask(\n        null,\n        null,\n        new IndexIngestionSpec(\n            new DataSchema(\n                \"foo\",\n                new TimestampSpec(null, null, null),\n                DimensionsSpec.EMPTY,\n                new AggregatorFactory[]{new DoubleSumAggregatorFactory(\"met\", \"met\")},\n                new UniformGranularitySpec(\n                    Granularities.DAY,\n                    null,\n                    ImmutableList.of(Intervals.of(\"2010-01-01/P2D\"))\n                ),\n                null\n            ),\n            new IndexIOConfig(null, new MockInputSource(), new NoopInputFormat(), false),\n            new IndexTuningConfig(\n                null,\n                10000,\n                null,\n                10,\n                null,\n                null,\n                null,\n                null,\n                null,\n                null,\n                null,\n                indexSpec,\n                null,\n                null,\n                null,\n                null,\n                null,\n                null,\n                null,\n                null,\n                null,\n                null,\n                null\n            )\n        ),\n        null\n    );\n\n    final long startTime = System.currentTimeMillis();\n\n    \r\n    taskQueue.start();\n    taskStorage.insert(indexTask, TaskStatus.running(indexTask.getId()));\n\n    while (tsqa.getStatus(indexTask.getId()).get().isRunnable()) {\n      if (System.currentTimeMillis() > startTime + 10 * 1000) {\n        throw new ISE(\"Where did the task go?!: %s\", indexTask.getId());\n      }\n\n      Thread.sleep(100);\n    }\n\n    final TaskStatus status = taskStorage.getStatus(indexTask.getId()).get();\n    final List<DataSegment> publishedSegments = BY_INTERVAL_ORDERING.sortedCopy(mdc.getPublished());\n    final List<DataSegment> loggedSegments = BY_INTERVAL_ORDERING.sortedCopy(tsqa.getInsertedSegments(indexTask.getId()));\n\n    Assert.assertEquals(\"statusCode\", TaskState.SUCCESS, status.getStatusCode());\n    Assert.assertEquals(taskLocation, status.getLocation());\n    Assert.assertEquals(\"segments logged vs published\", loggedSegments, publishedSegments);\n    Assert.assertEquals(\"num segments published\", 2, mdc.getPublished().size());\n    Assert.assertEquals(\"num segments nuked\", 0, mdc.getNuked().size());\n\n    Assert.assertEquals(\"segment1 datasource\", \"foo\", publishedSegments.get(0).getDataSource());\n    Assert.assertEquals(\"segment1 interval\", Intervals.of(\"2010-01-01/P1D\"), publishedSegments.get(0).getInterval());\n    Assert.assertEquals(\n        \"segment1 dimensions\",\n        ImmutableList.of(\"dim1\", \"dim2\"),\n        publishedSegments.get(0).getDimensions()\n    );\n    Assert.assertEquals(\"segment1 metrics\", ImmutableList.of(\"met\"), publishedSegments.get(0).getMetrics());\n\n    Assert.assertEquals(\"segment2 datasource\", \"foo\", publishedSegments.get(1).getDataSource());\n    Assert.assertEquals(\"segment2 interval\", Intervals.of(\"2010-01-02/P1D\"), publishedSegments.get(1).getInterval());\n    Assert.assertEquals(\n        \"segment2 dimensions\",\n        ImmutableList.of(\"dim1\", \"dim2\"),\n        publishedSegments.get(1).getDimensions()\n    );\n    Assert.assertEquals(\"segment2 metrics\", ImmutableList.of(\"met\"), publishedSegments.get(1).getMetrics());\n  }\n","realPath":"indexing-service/src/test/java/org/apache/druid/indexing/overlord/TaskLifecycleTest.java","repoName":"druid","snippetEndLine":0,"snippetStartLine":0,"startLine":1238,"status":"M"}],"commitId":"d7f529336463dad273a742808b49d524bdc4ae11","commitMessage":"@@@Add an option for ingestion task to drop (mark unused) all existing segments that are contained by interval in the ingestionSpec (#11025)\n\n* Auto-Compaction can run indefinitely when segmentGranularity is changed from coarser to finer.\n\n* Add option to drop segments after ingestion\n\n* fix checkstyle\n\n* add tests\n\n* add tests\n\n* add tests\n\n* fix test\n\n* add tests\n\n* fix checkstyle\n\n* fix checkstyle\n\n* add docs\n\n* fix docs\n\n* address comments\n\n* address comments\n\n* fix spelling","date":"2021-04-02 03:29:36","modifiedFileCount":"44","status":"M","submitter":"Maytas Monsereenusorn"},{"authorTime":"2021-04-09 12:03:00","codes":[{"authorDate":"2021-04-09 12:03:00","commitOrder":16,"curCode":"  public void testIndexTask() throws Exception\n  {\n    final Task indexTask = new IndexTask(\n        null,\n        null,\n        new IndexIngestionSpec(\n            new DataSchema(\n                \"foo\",\n                new TimestampSpec(null, null, null),\n                DimensionsSpec.EMPTY,\n                new AggregatorFactory[]{new DoubleSumAggregatorFactory(\"met\", \"met\")},\n                new UniformGranularitySpec(\n                    Granularities.DAY,\n                    null,\n                    ImmutableList.of(Intervals.of(\"2010-01-01/P2D\"))\n                ),\n                null\n            ),\n            new IndexIOConfig(null, new MockInputSource(), new NoopInputFormat(), false, false),\n            new IndexTuningConfig(\n                null,\n                10000,\n                null,\n                10,\n                null,\n                null,\n                null,\n                null,\n                null,\n                null,\n                null,\n                indexSpec,\n                null,\n                3,\n                false,\n                null,\n                null,\n                null,\n                null,\n                null,\n                null,\n                null,\n                null,\n                null\n            )\n        ),\n        null\n    );\n\n    final Optional<TaskStatus> preRunTaskStatus = tsqa.getStatus(indexTask.getId());\n    Assert.assertTrue(\"pre run task status not present\", !preRunTaskStatus.isPresent());\n\n    final TaskStatus mergedStatus = runTask(indexTask);\n    final TaskStatus status = taskStorage.getStatus(indexTask.getId()).get();\n    final List<DataSegment> publishedSegments = BY_INTERVAL_ORDERING.sortedCopy(mdc.getPublished());\n    final List<DataSegment> loggedSegments = BY_INTERVAL_ORDERING.sortedCopy(tsqa.getInsertedSegments(indexTask.getId()));\n\n    Assert.assertEquals(\"statusCode\", TaskState.SUCCESS, status.getStatusCode());\n    Assert.assertEquals(taskLocation, status.getLocation());\n    Assert.assertEquals(\"merged statusCode\", TaskState.SUCCESS, mergedStatus.getStatusCode());\n    Assert.assertEquals(\"segments logged vs published\", loggedSegments, publishedSegments);\n    Assert.assertEquals(\"num segments published\", 2, mdc.getPublished().size());\n    Assert.assertEquals(\"num segments nuked\", 0, mdc.getNuked().size());\n\n    Assert.assertEquals(\"segment1 datasource\", \"foo\", publishedSegments.get(0).getDataSource());\n    Assert.assertEquals(\"segment1 interval\", Intervals.of(\"2010-01-01/P1D\"), publishedSegments.get(0).getInterval());\n    Assert.assertEquals(\n        \"segment1 dimensions\",\n        ImmutableList.of(\"dim1\", \"dim2\"),\n        publishedSegments.get(0).getDimensions()\n    );\n    Assert.assertEquals(\"segment1 metrics\", ImmutableList.of(\"met\"), publishedSegments.get(0).getMetrics());\n\n    Assert.assertEquals(\"segment2 datasource\", \"foo\", publishedSegments.get(1).getDataSource());\n    Assert.assertEquals(\"segment2 interval\", Intervals.of(\"2010-01-02/P1D\"), publishedSegments.get(1).getInterval());\n    Assert.assertEquals(\n        \"segment2 dimensions\",\n        ImmutableList.of(\"dim1\", \"dim2\"),\n        publishedSegments.get(1).getDimensions()\n    );\n    Assert.assertEquals(\"segment2 metrics\", ImmutableList.of(\"met\"), publishedSegments.get(1).getMetrics());\n  }\n","date":"2021-04-09 12:03:00","endLine":808,"groupId":"104144","id":31,"instanceNumber":1,"isCurCommit":0,"methodName":"testIndexTask","params":"()","path":"/mnt/clonedata/CloneManagementServer/ManagementServer/consistResult/result-druid-10-0.7/blobInfo/CC_OUT/blobs/e3/ba195fce1ad2a303473425f3b232e42be82210.src","preCode":"  public void testIndexTask() throws Exception\n  {\n    final Task indexTask = new IndexTask(\n        null,\n        null,\n        new IndexIngestionSpec(\n            new DataSchema(\n                \"foo\",\n                new TimestampSpec(null, null, null),\n                DimensionsSpec.EMPTY,\n                new AggregatorFactory[]{new DoubleSumAggregatorFactory(\"met\", \"met\")},\n                new UniformGranularitySpec(\n                    Granularities.DAY,\n                    null,\n                    ImmutableList.of(Intervals.of(\"2010-01-01/P2D\"))\n                ),\n                null\n            ),\n            new IndexIOConfig(null, new MockInputSource(), new NoopInputFormat(), false, false),\n            new IndexTuningConfig(\n                null,\n                10000,\n                null,\n                10,\n                null,\n                null,\n                null,\n                null,\n                null,\n                null,\n                null,\n                indexSpec,\n                null,\n                3,\n                false,\n                null,\n                null,\n                null,\n                null,\n                null,\n                null,\n                null,\n                null\n            )\n        ),\n        null\n    );\n\n    final Optional<TaskStatus> preRunTaskStatus = tsqa.getStatus(indexTask.getId());\n    Assert.assertTrue(\"pre run task status not present\", !preRunTaskStatus.isPresent());\n\n    final TaskStatus mergedStatus = runTask(indexTask);\n    final TaskStatus status = taskStorage.getStatus(indexTask.getId()).get();\n    final List<DataSegment> publishedSegments = BY_INTERVAL_ORDERING.sortedCopy(mdc.getPublished());\n    final List<DataSegment> loggedSegments = BY_INTERVAL_ORDERING.sortedCopy(tsqa.getInsertedSegments(indexTask.getId()));\n\n    Assert.assertEquals(\"statusCode\", TaskState.SUCCESS, status.getStatusCode());\n    Assert.assertEquals(taskLocation, status.getLocation());\n    Assert.assertEquals(\"merged statusCode\", TaskState.SUCCESS, mergedStatus.getStatusCode());\n    Assert.assertEquals(\"segments logged vs published\", loggedSegments, publishedSegments);\n    Assert.assertEquals(\"num segments published\", 2, mdc.getPublished().size());\n    Assert.assertEquals(\"num segments nuked\", 0, mdc.getNuked().size());\n\n    Assert.assertEquals(\"segment1 datasource\", \"foo\", publishedSegments.get(0).getDataSource());\n    Assert.assertEquals(\"segment1 interval\", Intervals.of(\"2010-01-01/P1D\"), publishedSegments.get(0).getInterval());\n    Assert.assertEquals(\n        \"segment1 dimensions\",\n        ImmutableList.of(\"dim1\", \"dim2\"),\n        publishedSegments.get(0).getDimensions()\n    );\n    Assert.assertEquals(\"segment1 metrics\", ImmutableList.of(\"met\"), publishedSegments.get(0).getMetrics());\n\n    Assert.assertEquals(\"segment2 datasource\", \"foo\", publishedSegments.get(1).getDataSource());\n    Assert.assertEquals(\"segment2 interval\", Intervals.of(\"2010-01-02/P1D\"), publishedSegments.get(1).getInterval());\n    Assert.assertEquals(\n        \"segment2 dimensions\",\n        ImmutableList.of(\"dim1\", \"dim2\"),\n        publishedSegments.get(1).getDimensions()\n    );\n    Assert.assertEquals(\"segment2 metrics\", ImmutableList.of(\"met\"), publishedSegments.get(1).getMetrics());\n  }\n","realPath":"indexing-service/src/test/java/org/apache/druid/indexing/overlord/TaskLifecycleTest.java","repoName":"druid","snippetEndLine":0,"snippetStartLine":0,"startLine":727,"status":"M"},{"authorDate":"2021-04-09 12:03:00","commitOrder":16,"curCode":"  public void testResumeTasks() throws Exception\n  {\n    final Task indexTask = new IndexTask(\n        null,\n        null,\n        new IndexIngestionSpec(\n            new DataSchema(\n                \"foo\",\n                new TimestampSpec(null, null, null),\n                DimensionsSpec.EMPTY,\n                new AggregatorFactory[]{new DoubleSumAggregatorFactory(\"met\", \"met\")},\n                new UniformGranularitySpec(\n                    Granularities.DAY,\n                    null,\n                    ImmutableList.of(Intervals.of(\"2010-01-01/P2D\"))\n                ),\n                null\n            ),\n            new IndexIOConfig(null, new MockInputSource(), new NoopInputFormat(), false, false),\n            new IndexTuningConfig(\n                null,\n                10000,\n                null,\n                10,\n                null,\n                null,\n                null,\n                null,\n                null,\n                null,\n                null,\n                indexSpec,\n                null,\n                null,\n                null,\n                null,\n                null,\n                null,\n                null,\n                null,\n                null,\n                null,\n                null,\n                null\n            )\n        ),\n        null\n    );\n\n    final long startTime = System.currentTimeMillis();\n\n    \r\n    taskQueue.start();\n    taskStorage.insert(indexTask, TaskStatus.running(indexTask.getId()));\n\n    while (tsqa.getStatus(indexTask.getId()).get().isRunnable()) {\n      if (System.currentTimeMillis() > startTime + 10 * 1000) {\n        throw new ISE(\"Where did the task go?!: %s\", indexTask.getId());\n      }\n\n      Thread.sleep(100);\n    }\n\n    final TaskStatus status = taskStorage.getStatus(indexTask.getId()).get();\n    final List<DataSegment> publishedSegments = BY_INTERVAL_ORDERING.sortedCopy(mdc.getPublished());\n    final List<DataSegment> loggedSegments = BY_INTERVAL_ORDERING.sortedCopy(tsqa.getInsertedSegments(indexTask.getId()));\n\n    Assert.assertEquals(\"statusCode\", TaskState.SUCCESS, status.getStatusCode());\n    Assert.assertEquals(taskLocation, status.getLocation());\n    Assert.assertEquals(\"segments logged vs published\", loggedSegments, publishedSegments);\n    Assert.assertEquals(\"num segments published\", 2, mdc.getPublished().size());\n    Assert.assertEquals(\"num segments nuked\", 0, mdc.getNuked().size());\n\n    Assert.assertEquals(\"segment1 datasource\", \"foo\", publishedSegments.get(0).getDataSource());\n    Assert.assertEquals(\"segment1 interval\", Intervals.of(\"2010-01-01/P1D\"), publishedSegments.get(0).getInterval());\n    Assert.assertEquals(\n        \"segment1 dimensions\",\n        ImmutableList.of(\"dim1\", \"dim2\"),\n        publishedSegments.get(0).getDimensions()\n    );\n    Assert.assertEquals(\"segment1 metrics\", ImmutableList.of(\"met\"), publishedSegments.get(0).getMetrics());\n\n    Assert.assertEquals(\"segment2 datasource\", \"foo\", publishedSegments.get(1).getDataSource());\n    Assert.assertEquals(\"segment2 interval\", Intervals.of(\"2010-01-02/P1D\"), publishedSegments.get(1).getInterval());\n    Assert.assertEquals(\n        \"segment2 dimensions\",\n        ImmutableList.of(\"dim1\", \"dim2\"),\n        publishedSegments.get(1).getDimensions()\n    );\n    Assert.assertEquals(\"segment2 metrics\", ImmutableList.of(\"met\"), publishedSegments.get(1).getMetrics());\n  }\n","date":"2021-04-09 12:03:00","endLine":1330,"groupId":"104144","id":32,"instanceNumber":2,"isCurCommit":0,"methodName":"testResumeTasks","params":"()","path":"/mnt/clonedata/CloneManagementServer/ManagementServer/consistResult/result-druid-10-0.7/blobInfo/CC_OUT/blobs/e3/ba195fce1ad2a303473425f3b232e42be82210.src","preCode":"  public void testResumeTasks() throws Exception\n  {\n    final Task indexTask = new IndexTask(\n        null,\n        null,\n        new IndexIngestionSpec(\n            new DataSchema(\n                \"foo\",\n                new TimestampSpec(null, null, null),\n                DimensionsSpec.EMPTY,\n                new AggregatorFactory[]{new DoubleSumAggregatorFactory(\"met\", \"met\")},\n                new UniformGranularitySpec(\n                    Granularities.DAY,\n                    null,\n                    ImmutableList.of(Intervals.of(\"2010-01-01/P2D\"))\n                ),\n                null\n            ),\n            new IndexIOConfig(null, new MockInputSource(), new NoopInputFormat(), false, false),\n            new IndexTuningConfig(\n                null,\n                10000,\n                null,\n                10,\n                null,\n                null,\n                null,\n                null,\n                null,\n                null,\n                null,\n                indexSpec,\n                null,\n                null,\n                null,\n                null,\n                null,\n                null,\n                null,\n                null,\n                null,\n                null,\n                null\n            )\n        ),\n        null\n    );\n\n    final long startTime = System.currentTimeMillis();\n\n    \r\n    taskQueue.start();\n    taskStorage.insert(indexTask, TaskStatus.running(indexTask.getId()));\n\n    while (tsqa.getStatus(indexTask.getId()).get().isRunnable()) {\n      if (System.currentTimeMillis() > startTime + 10 * 1000) {\n        throw new ISE(\"Where did the task go?!: %s\", indexTask.getId());\n      }\n\n      Thread.sleep(100);\n    }\n\n    final TaskStatus status = taskStorage.getStatus(indexTask.getId()).get();\n    final List<DataSegment> publishedSegments = BY_INTERVAL_ORDERING.sortedCopy(mdc.getPublished());\n    final List<DataSegment> loggedSegments = BY_INTERVAL_ORDERING.sortedCopy(tsqa.getInsertedSegments(indexTask.getId()));\n\n    Assert.assertEquals(\"statusCode\", TaskState.SUCCESS, status.getStatusCode());\n    Assert.assertEquals(taskLocation, status.getLocation());\n    Assert.assertEquals(\"segments logged vs published\", loggedSegments, publishedSegments);\n    Assert.assertEquals(\"num segments published\", 2, mdc.getPublished().size());\n    Assert.assertEquals(\"num segments nuked\", 0, mdc.getNuked().size());\n\n    Assert.assertEquals(\"segment1 datasource\", \"foo\", publishedSegments.get(0).getDataSource());\n    Assert.assertEquals(\"segment1 interval\", Intervals.of(\"2010-01-01/P1D\"), publishedSegments.get(0).getInterval());\n    Assert.assertEquals(\n        \"segment1 dimensions\",\n        ImmutableList.of(\"dim1\", \"dim2\"),\n        publishedSegments.get(0).getDimensions()\n    );\n    Assert.assertEquals(\"segment1 metrics\", ImmutableList.of(\"met\"), publishedSegments.get(0).getMetrics());\n\n    Assert.assertEquals(\"segment2 datasource\", \"foo\", publishedSegments.get(1).getDataSource());\n    Assert.assertEquals(\"segment2 interval\", Intervals.of(\"2010-01-02/P1D\"), publishedSegments.get(1).getInterval());\n    Assert.assertEquals(\n        \"segment2 dimensions\",\n        ImmutableList.of(\"dim1\", \"dim2\"),\n        publishedSegments.get(1).getDimensions()\n    );\n    Assert.assertEquals(\"segment2 metrics\", ImmutableList.of(\"met\"), publishedSegments.get(1).getMetrics());\n  }\n","realPath":"indexing-service/src/test/java/org/apache/druid/indexing/overlord/TaskLifecycleTest.java","repoName":"druid","snippetEndLine":0,"snippetStartLine":0,"startLine":1240,"status":"M"}],"commitId":"8264203cee688607091232897749e959e7706010","commitMessage":"@@@Allow client to configure batch ingestion task to wait to complete until segments are confirmed to be available by other (#10676)\n\n* Add ability to wait for segment availability for batch jobs\n\n* IT updates\n\n* fix queries in legacy hadoop IT\n\n* Fix broken indexing integration tests\n\n* address an lgtm flag\n\n* spell checker still flagging for hadoop doc. adding under that file header too\n\n* fix compaction IT\n\n* Updates to wait for availability method\n\n* improve unit testing for patch\n\n* fix bad indentation\n\n* refactor waitForSegmentAvailability\n\n* Fixes based off of review comments\n\n* cleanup to get compile after merging with master\n\n* fix failing test after previous logic update\n\n* add back code that must have gotten deleted during conflict resolution\n\n* update some logging code\n\n* fixes to get compilation working after merge with master\n\n* reset interrupt flag in catch block after code review pointed it out\n\n* small changes following self-review\n\n* fixup some issues brought on by merge with master\n\n* small changes after review\n\n* cleanup a little bit after merge with master\n\n* Fix potential resource leak in AbstractBatchIndexTask\n\n* syntax fix\n\n* Add a Compcation TuningConfig type\n\n* add docs stipulating the lack of support by Compaction tasks for the new config\n\n* Fixup compilation errors after merge with master\n\n* Remove erreneous newline","date":"2021-04-09 12:03:00","modifiedFileCount":"106","status":"M","submitter":"Lucas Capistrant"}]
