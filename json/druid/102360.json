[{"authorTime":"2018-12-22 03:49:24","codes":[{"authorDate":"2018-12-22 03:49:24","commitOrder":2,"curCode":"  public void testMultipleParseExceptionsFailure() throws Exception\n  {\n    reportParseExceptions = false;\n    maxParseExceptions = 2;\n    maxSavedParseExceptions = 2;\n\n    recordSupplier.assign(anyObject());\n    expectLastCall().anyTimes();\n\n    expect(recordSupplier.getEarliestSequenceNumber(anyObject())).andReturn(\"0\").anyTimes();\n\n    recordSupplier.seek(anyObject(), anyString());\n    expectLastCall().anyTimes();\n\n    expect(recordSupplier.poll(anyLong())).andReturn(records.subList(2, 13)).once();\n\n\n    replayAll();\n\n    final KinesisIndexTask task = createTask(\n        null,\n        new KinesisIndexTaskIOConfig(\n            null,\n            \"sequence0\",\n            new SeekableStreamPartitions<>(stream, ImmutableMap.of(\n                shardId1,\n                \"2\"\n            )),\n            new SeekableStreamPartitions<>(stream, ImmutableMap.of(\n                shardId1,\n                \"9\"\n            )),\n            true,\n            null,\n            null,\n            \"awsEndpoint\",\n            null,\n            null,\n            null,\n            null,\n            null,\n            false\n        )\n    );\n\n    final ListenableFuture<TaskStatus> future = runTask(task);\n\n    TaskStatus status = future.get();\n\n    \r\n    Assert.assertEquals(TaskState.FAILED, status.getStatusCode());\n    verifyAll();\n    IndexTaskTest.checkTaskStatusErrorMsgForParseExceptionsExceeded(status);\n\n    \r\n    Assert.assertEquals(3, task.getRunner().getRowIngestionMeters().getProcessed());\n    Assert.assertEquals(0, task.getRunner().getRowIngestionMeters().getProcessedWithError());\n    Assert.assertEquals(3, task.getRunner().getRowIngestionMeters().getUnparseable());\n    Assert.assertEquals(0, task.getRunner().getRowIngestionMeters().getThrownAway());\n\n    \r\n    Assert.assertEquals(ImmutableSet.of(), publishedDescriptors());\n    Assert.assertNull(metadataStorageCoordinator.getDataSourceMetadata(DATA_SCHEMA.getDataSource()));\n\n    IngestionStatsAndErrorsTaskReportData reportData = getTaskReportData();\n\n    Map<String, Object> expectedMetrics = ImmutableMap.of(\n        RowIngestionMeters.BUILD_SEGMENTS,\n        ImmutableMap.of(\n            RowIngestionMeters.PROCESSED, 3,\n            RowIngestionMeters.PROCESSED_WITH_ERROR, 0,\n            RowIngestionMeters.UNPARSEABLE, 3,\n            RowIngestionMeters.THROWN_AWAY, 0\n        )\n    );\n    Assert.assertEquals(expectedMetrics, reportData.getRowStats());\n\n    Map<String, Object> unparseableEvents = ImmutableMap.of(\n        RowIngestionMeters.BUILD_SEGMENTS,\n        Arrays.asList(\n            \"Unable to parse row [unparseable2]\",\n            \"Unable to parse row [unparseable]\"\n        )\n    );\n\n    Assert.assertEquals(unparseableEvents, reportData.getUnparseableEvents());\n  }\n","date":"2018-12-22 03:49:24","endLine":1534,"groupId":"14312","id":1,"instanceNumber":1,"isCurCommit":0,"methodName":"testMultipleParseExceptionsFailure","params":"()","path":"/mnt/clonedata/CloneManagementServer/ManagementServer/consistResult/result-druid-10-0.7/blobInfo/CC_OUT/blobs/ce/f97955db2e8ceaae814ce65540385404536749.src","preCode":"  public void testMultipleParseExceptionsFailure() throws Exception\n  {\n    reportParseExceptions = false;\n    maxParseExceptions = 2;\n    maxSavedParseExceptions = 2;\n\n    recordSupplier.assign(anyObject());\n    expectLastCall().anyTimes();\n\n    expect(recordSupplier.getEarliestSequenceNumber(anyObject())).andReturn(\"0\").anyTimes();\n\n    recordSupplier.seek(anyObject(), anyString());\n    expectLastCall().anyTimes();\n\n    expect(recordSupplier.poll(anyLong())).andReturn(records.subList(2, 13)).once();\n\n\n    replayAll();\n\n    final KinesisIndexTask task = createTask(\n        null,\n        new KinesisIndexTaskIOConfig(\n            null,\n            \"sequence0\",\n            new SeekableStreamPartitions<>(stream, ImmutableMap.of(\n                shardId1,\n                \"2\"\n            )),\n            new SeekableStreamPartitions<>(stream, ImmutableMap.of(\n                shardId1,\n                \"9\"\n            )),\n            true,\n            null,\n            null,\n            \"awsEndpoint\",\n            null,\n            null,\n            null,\n            null,\n            null,\n            false\n        )\n    );\n\n    final ListenableFuture<TaskStatus> future = runTask(task);\n\n    TaskStatus status = future.get();\n\n    \r\n    Assert.assertEquals(TaskState.FAILED, status.getStatusCode());\n    verifyAll();\n    IndexTaskTest.checkTaskStatusErrorMsgForParseExceptionsExceeded(status);\n\n    \r\n    Assert.assertEquals(3, task.getRunner().getRowIngestionMeters().getProcessed());\n    Assert.assertEquals(0, task.getRunner().getRowIngestionMeters().getProcessedWithError());\n    Assert.assertEquals(3, task.getRunner().getRowIngestionMeters().getUnparseable());\n    Assert.assertEquals(0, task.getRunner().getRowIngestionMeters().getThrownAway());\n\n    \r\n    Assert.assertEquals(ImmutableSet.of(), publishedDescriptors());\n    Assert.assertNull(metadataStorageCoordinator.getDataSourceMetadata(DATA_SCHEMA.getDataSource()));\n\n    IngestionStatsAndErrorsTaskReportData reportData = getTaskReportData();\n\n    Map<String, Object> expectedMetrics = ImmutableMap.of(\n        RowIngestionMeters.BUILD_SEGMENTS,\n        ImmutableMap.of(\n            RowIngestionMeters.PROCESSED, 3,\n            RowIngestionMeters.PROCESSED_WITH_ERROR, 0,\n            RowIngestionMeters.UNPARSEABLE, 3,\n            RowIngestionMeters.THROWN_AWAY, 0\n        )\n    );\n    Assert.assertEquals(expectedMetrics, reportData.getRowStats());\n\n    Map<String, Object> unparseableEvents = ImmutableMap.of(\n        RowIngestionMeters.BUILD_SEGMENTS,\n        Arrays.asList(\n            \"Unable to parse row [unparseable2]\",\n            \"Unable to parse row [unparseable]\"\n        )\n    );\n\n    Assert.assertEquals(unparseableEvents, reportData.getUnparseableEvents());\n  }\n","realPath":"extensions-core/kinesis-indexing-service/src/test/java/org/apache/druid/indexing/kinesis/KinesisIndexTaskTest.java","repoName":"druid","snippetEndLine":0,"snippetStartLine":0,"startLine":1448,"status":"B"},{"authorDate":"2018-12-22 03:49:24","commitOrder":2,"curCode":"  public void testMultipleParseExceptionsFailure() throws Exception\n  {\n    reportParseExceptions = false;\n    maxParseExceptions = 2;\n    maxSavedParseExceptions = 2;\n\n    \r\n    try (final KafkaProducer<byte[], byte[]> kafkaProducer = kafkaServer.newProducer()) {\n      for (ProducerRecord<byte[], byte[]> record : records) {\n        kafkaProducer.send(record).get();\n      }\n    }\n\n    final KafkaIndexTask task = createTask(\n        null,\n        new KafkaIndexTaskIOConfig(\n            0,\n            \"sequence0\",\n            new SeekableStreamPartitions<>(topic, ImmutableMap.of(0, 2L)),\n            new SeekableStreamPartitions<>(topic, ImmutableMap.of(0, 10L)),\n            kafkaServer.consumerProperties(),\n            true,\n            null,\n            null,\n            false\n        )\n    );\n\n    final ListenableFuture<TaskStatus> future = runTask(task);\n\n    TaskStatus status = future.get();\n\n    \r\n    Assert.assertEquals(TaskState.FAILED, status.getStatusCode());\n    IndexTaskTest.checkTaskStatusErrorMsgForParseExceptionsExceeded(status);\n\n    \r\n    Assert.assertEquals(3, task.getRunner().getRowIngestionMeters().getProcessed());\n    Assert.assertEquals(0, task.getRunner().getRowIngestionMeters().getProcessedWithError());\n    Assert.assertEquals(3, task.getRunner().getRowIngestionMeters().getUnparseable());\n    Assert.assertEquals(0, task.getRunner().getRowIngestionMeters().getThrownAway());\n\n    \r\n    Assert.assertEquals(ImmutableSet.of(), publishedDescriptors());\n    Assert.assertNull(metadataStorageCoordinator.getDataSourceMetadata(DATA_SCHEMA.getDataSource()));\n\n    IngestionStatsAndErrorsTaskReportData reportData = getTaskReportData();\n\n    Map<String, Object> expectedMetrics = ImmutableMap.of(\n        RowIngestionMeters.BUILD_SEGMENTS,\n        ImmutableMap.of(\n            RowIngestionMeters.PROCESSED, 3,\n            RowIngestionMeters.PROCESSED_WITH_ERROR, 0,\n            RowIngestionMeters.UNPARSEABLE, 3,\n            RowIngestionMeters.THROWN_AWAY, 0\n        )\n    );\n    Assert.assertEquals(expectedMetrics, reportData.getRowStats());\n\n    Map<String, Object> unparseableEvents = ImmutableMap.of(\n        RowIngestionMeters.BUILD_SEGMENTS,\n        Arrays.asList(\n            \"Unable to parse row [unparseable2]\",\n            \"Unable to parse row [unparseable]\"\n        )\n    );\n\n    Assert.assertEquals(unparseableEvents, reportData.getUnparseableEvents());\n  }\n","date":"2018-12-22 03:49:24","endLine":1443,"groupId":"12458","id":2,"instanceNumber":2,"isCurCommit":0,"methodName":"testMultipleParseExceptionsFailure","params":"()","path":"/mnt/clonedata/CloneManagementServer/ManagementServer/consistResult/result-druid-10-0.7/blobInfo/CC_OUT/blobs/72/041eb947d5fcc0208ff2a5427dfc03aa78f820.src","preCode":"  public void testMultipleParseExceptionsFailure() throws Exception\n  {\n    reportParseExceptions = false;\n    maxParseExceptions = 2;\n    maxSavedParseExceptions = 2;\n\n    \r\n    try (final KafkaProducer<byte[], byte[]> kafkaProducer = kafkaServer.newProducer()) {\n      for (ProducerRecord<byte[], byte[]> record : records) {\n        kafkaProducer.send(record).get();\n      }\n    }\n\n    final KafkaIndexTask task = createTask(\n        null,\n        new KafkaIndexTaskIOConfig(\n            0,\n            \"sequence0\",\n            new SeekableStreamPartitions<>(topic, ImmutableMap.of(0, 2L)),\n            new SeekableStreamPartitions<>(topic, ImmutableMap.of(0, 10L)),\n            kafkaServer.consumerProperties(),\n            true,\n            null,\n            null,\n            false\n        )\n    );\n\n    final ListenableFuture<TaskStatus> future = runTask(task);\n\n    TaskStatus status = future.get();\n\n    \r\n    Assert.assertEquals(TaskState.FAILED, status.getStatusCode());\n    IndexTaskTest.checkTaskStatusErrorMsgForParseExceptionsExceeded(status);\n\n    \r\n    Assert.assertEquals(3, task.getRunner().getRowIngestionMeters().getProcessed());\n    Assert.assertEquals(0, task.getRunner().getRowIngestionMeters().getProcessedWithError());\n    Assert.assertEquals(3, task.getRunner().getRowIngestionMeters().getUnparseable());\n    Assert.assertEquals(0, task.getRunner().getRowIngestionMeters().getThrownAway());\n\n    \r\n    Assert.assertEquals(ImmutableSet.of(), publishedDescriptors());\n    Assert.assertNull(metadataStorageCoordinator.getDataSourceMetadata(DATA_SCHEMA.getDataSource()));\n\n    IngestionStatsAndErrorsTaskReportData reportData = getTaskReportData();\n\n    Map<String, Object> expectedMetrics = ImmutableMap.of(\n        RowIngestionMeters.BUILD_SEGMENTS,\n        ImmutableMap.of(\n            RowIngestionMeters.PROCESSED, 3,\n            RowIngestionMeters.PROCESSED_WITH_ERROR, 0,\n            RowIngestionMeters.UNPARSEABLE, 3,\n            RowIngestionMeters.THROWN_AWAY, 0\n        )\n    );\n    Assert.assertEquals(expectedMetrics, reportData.getRowStats());\n\n    Map<String, Object> unparseableEvents = ImmutableMap.of(\n        RowIngestionMeters.BUILD_SEGMENTS,\n        Arrays.asList(\n            \"Unable to parse row [unparseable2]\",\n            \"Unable to parse row [unparseable]\"\n        )\n    );\n\n    Assert.assertEquals(unparseableEvents, reportData.getUnparseableEvents());\n  }\n","realPath":"extensions-core/kafka-indexing-service/src/test/java/org/apache/druid/indexing/kafka/KafkaIndexTaskTest.java","repoName":"druid","snippetEndLine":0,"snippetStartLine":0,"startLine":1375,"status":"MB"}],"commitId":"7c7997e8a1183a7bffad731ca94e8b4c381e8665","commitMessage":"@@@Add Kinesis Indexing Service to core Druid (#6431)\n\n* created seekablestream classes\n\n* created seekablestreamsupervisor class\n\n* first attempt to integrate kafa indexing service to use SeekableStream\n\n* seekablestream bug fixes\n\n* kafkarecordsupplier\n\n* integrated kafka indexing service with seekablestream\n\n* implemented resume/suspend and refactored some package names\n\n* moved kinesis indexing service into core druid extensions\n\n* merged some changes from kafka supervisor race condition\n\n* integrated kinesis-indexing-service with seekablestream\n\n* unite tests for kinesis-indexing-service\n\n* various bug fixes for kinesis-indexing-service\n\n* refactored kinesisindexingtask\n\n* finished up more kinesis unit tests\n\n* more bug fixes for kinesis-indexing-service\n\n* finsihed refactoring kinesis unit tests\n\n* removed KinesisParititons and KafkaPartitions to use SeekableStreamPartitions\n\n* kinesis-indexing-service code cleanup and docs\n\n* merge #6291\n\nmerge #6337\n\nmerge #6383\n\n* added more docs and reordered methods\n\n* fixd kinesis tests after merging master and added docs in seekablestream\n\n* fix various things from pr comment\n\n* improve recordsupplier and add unit tests\n\n* migrated to aws-java-sdk-kinesis\n\n* merge changes from master\n\n* fix pom files and forbiddenapi checks\n\n* checkpoint JavaType bug fix\n\n* fix pom and stuff\n\n* disable checkpointing in kinesis\n\n* fix kinesis sequence number null in closed shard\n\n* merge changes from master\n\n* fixes for kinesis tasks\n\n* capitalized <partitionType.  sequenceType>\n\n* removed abstract class loggers\n\n* conform to guava api restrictions\n\n* add docker for travis other modules test\n\n* address comments\n\n* improve RecordSupplier to supply records in batch\n\n* fix strict compile issue\n\n* add test scope for localstack dependency\n\n* kinesis indexing task refactoring\n\n* comments\n\n* github comments\n\n* minor fix\n\n* removed unneeded readme\n\n* fix deserialization bug\n\n* fix various bugs\n\n* KinesisRecordSupplier unable to catch up to earliest position in stream bug fix\n\n* minor changes to kinesis\n\n* implement deaggregate for kinesis\n\n* Merge remote-tracking branch 'upstream/master' into seekablestream\n\n* fix kinesis offset discrepancy with kafka\n\n* kinesis record supplier disable getPosition\n\n* pr comments\n\n* mock for kinesis tests and remove docker dependency for unit tests\n\n* PR comments\n\n* avg lag in kafkasupervisor #6587\n\n* refacotred SequenceMetadata in taskRunners\n\n* small fix\n\n* more small fix\n\n* recordsupplier resource leak\n\n* revert .travis.yml formatting\n\n* fix style\n\n* kinesis docs\n\n* doc part2\n\n* more docs\n\n* comments\n\n* comments*2\n\n* revert string replace changes\n\n* comments\n\n* teamcity\n\n* comments part 1\n\n* comments part 2\n\n* comments part 3\n\n* merge #6754\n\n* fix injection binding\n\n* comments\n\n* KinesisRegion refactor\n\n* comments part idk lol\n\n* can't think of a commit msg anymore\n\n* remove possiblyResetDataSourceMetadata() for IncrementalPublishingTaskRunner\n\n* commmmmmmmmmments\n\n* extra error handling in KinesisRecordSupplier getRecords\n\n* comments\n\n* quickfix\n\n* typo\n\n* oof\n","date":"2018-12-22 03:49:24","modifiedFileCount":"22","status":"M","submitter":"Joshua Sun"},{"authorTime":"2019-01-03 12:16:02","codes":[{"authorDate":"2018-12-22 03:49:24","commitOrder":3,"curCode":"  public void testMultipleParseExceptionsFailure() throws Exception\n  {\n    reportParseExceptions = false;\n    maxParseExceptions = 2;\n    maxSavedParseExceptions = 2;\n\n    recordSupplier.assign(anyObject());\n    expectLastCall().anyTimes();\n\n    expect(recordSupplier.getEarliestSequenceNumber(anyObject())).andReturn(\"0\").anyTimes();\n\n    recordSupplier.seek(anyObject(), anyString());\n    expectLastCall().anyTimes();\n\n    expect(recordSupplier.poll(anyLong())).andReturn(records.subList(2, 13)).once();\n\n\n    replayAll();\n\n    final KinesisIndexTask task = createTask(\n        null,\n        new KinesisIndexTaskIOConfig(\n            null,\n            \"sequence0\",\n            new SeekableStreamPartitions<>(stream, ImmutableMap.of(\n                shardId1,\n                \"2\"\n            )),\n            new SeekableStreamPartitions<>(stream, ImmutableMap.of(\n                shardId1,\n                \"9\"\n            )),\n            true,\n            null,\n            null,\n            \"awsEndpoint\",\n            null,\n            null,\n            null,\n            null,\n            null,\n            false\n        )\n    );\n\n    final ListenableFuture<TaskStatus> future = runTask(task);\n\n    TaskStatus status = future.get();\n\n    \r\n    Assert.assertEquals(TaskState.FAILED, status.getStatusCode());\n    verifyAll();\n    IndexTaskTest.checkTaskStatusErrorMsgForParseExceptionsExceeded(status);\n\n    \r\n    Assert.assertEquals(3, task.getRunner().getRowIngestionMeters().getProcessed());\n    Assert.assertEquals(0, task.getRunner().getRowIngestionMeters().getProcessedWithError());\n    Assert.assertEquals(3, task.getRunner().getRowIngestionMeters().getUnparseable());\n    Assert.assertEquals(0, task.getRunner().getRowIngestionMeters().getThrownAway());\n\n    \r\n    Assert.assertEquals(ImmutableSet.of(), publishedDescriptors());\n    Assert.assertNull(metadataStorageCoordinator.getDataSourceMetadata(DATA_SCHEMA.getDataSource()));\n\n    IngestionStatsAndErrorsTaskReportData reportData = getTaskReportData();\n\n    Map<String, Object> expectedMetrics = ImmutableMap.of(\n        RowIngestionMeters.BUILD_SEGMENTS,\n        ImmutableMap.of(\n            RowIngestionMeters.PROCESSED, 3,\n            RowIngestionMeters.PROCESSED_WITH_ERROR, 0,\n            RowIngestionMeters.UNPARSEABLE, 3,\n            RowIngestionMeters.THROWN_AWAY, 0\n        )\n    );\n    Assert.assertEquals(expectedMetrics, reportData.getRowStats());\n\n    Map<String, Object> unparseableEvents = ImmutableMap.of(\n        RowIngestionMeters.BUILD_SEGMENTS,\n        Arrays.asList(\n            \"Unable to parse row [unparseable2]\",\n            \"Unable to parse row [unparseable]\"\n        )\n    );\n\n    Assert.assertEquals(unparseableEvents, reportData.getUnparseableEvents());\n  }\n","date":"2018-12-22 03:49:24","endLine":1534,"groupId":"14312","id":3,"instanceNumber":1,"isCurCommit":0,"methodName":"testMultipleParseExceptionsFailure","params":"()","path":"/mnt/clonedata/CloneManagementServer/ManagementServer/consistResult/result-druid-10-0.7/blobInfo/CC_OUT/blobs/ce/f97955db2e8ceaae814ce65540385404536749.src","preCode":"  public void testMultipleParseExceptionsFailure() throws Exception\n  {\n    reportParseExceptions = false;\n    maxParseExceptions = 2;\n    maxSavedParseExceptions = 2;\n\n    recordSupplier.assign(anyObject());\n    expectLastCall().anyTimes();\n\n    expect(recordSupplier.getEarliestSequenceNumber(anyObject())).andReturn(\"0\").anyTimes();\n\n    recordSupplier.seek(anyObject(), anyString());\n    expectLastCall().anyTimes();\n\n    expect(recordSupplier.poll(anyLong())).andReturn(records.subList(2, 13)).once();\n\n\n    replayAll();\n\n    final KinesisIndexTask task = createTask(\n        null,\n        new KinesisIndexTaskIOConfig(\n            null,\n            \"sequence0\",\n            new SeekableStreamPartitions<>(stream, ImmutableMap.of(\n                shardId1,\n                \"2\"\n            )),\n            new SeekableStreamPartitions<>(stream, ImmutableMap.of(\n                shardId1,\n                \"9\"\n            )),\n            true,\n            null,\n            null,\n            \"awsEndpoint\",\n            null,\n            null,\n            null,\n            null,\n            null,\n            false\n        )\n    );\n\n    final ListenableFuture<TaskStatus> future = runTask(task);\n\n    TaskStatus status = future.get();\n\n    \r\n    Assert.assertEquals(TaskState.FAILED, status.getStatusCode());\n    verifyAll();\n    IndexTaskTest.checkTaskStatusErrorMsgForParseExceptionsExceeded(status);\n\n    \r\n    Assert.assertEquals(3, task.getRunner().getRowIngestionMeters().getProcessed());\n    Assert.assertEquals(0, task.getRunner().getRowIngestionMeters().getProcessedWithError());\n    Assert.assertEquals(3, task.getRunner().getRowIngestionMeters().getUnparseable());\n    Assert.assertEquals(0, task.getRunner().getRowIngestionMeters().getThrownAway());\n\n    \r\n    Assert.assertEquals(ImmutableSet.of(), publishedDescriptors());\n    Assert.assertNull(metadataStorageCoordinator.getDataSourceMetadata(DATA_SCHEMA.getDataSource()));\n\n    IngestionStatsAndErrorsTaskReportData reportData = getTaskReportData();\n\n    Map<String, Object> expectedMetrics = ImmutableMap.of(\n        RowIngestionMeters.BUILD_SEGMENTS,\n        ImmutableMap.of(\n            RowIngestionMeters.PROCESSED, 3,\n            RowIngestionMeters.PROCESSED_WITH_ERROR, 0,\n            RowIngestionMeters.UNPARSEABLE, 3,\n            RowIngestionMeters.THROWN_AWAY, 0\n        )\n    );\n    Assert.assertEquals(expectedMetrics, reportData.getRowStats());\n\n    Map<String, Object> unparseableEvents = ImmutableMap.of(\n        RowIngestionMeters.BUILD_SEGMENTS,\n        Arrays.asList(\n            \"Unable to parse row [unparseable2]\",\n            \"Unable to parse row [unparseable]\"\n        )\n    );\n\n    Assert.assertEquals(unparseableEvents, reportData.getUnparseableEvents());\n  }\n","realPath":"extensions-core/kinesis-indexing-service/src/test/java/org/apache/druid/indexing/kinesis/KinesisIndexTaskTest.java","repoName":"druid","snippetEndLine":0,"snippetStartLine":0,"startLine":1448,"status":"N"},{"authorDate":"2019-01-03 12:16:02","commitOrder":3,"curCode":"  public void testMultipleParseExceptionsFailure() throws Exception\n  {\n    reportParseExceptions = false;\n    maxParseExceptions = 2;\n    maxSavedParseExceptions = 2;\n\n    \r\n    try (final KafkaProducer<byte[], byte[]> kafkaProducer = kafkaServer.newProducer()) {\n      for (ProducerRecord<byte[], byte[]> record : records) {\n        kafkaProducer.send(record).get();\n      }\n    }\n\n    final KafkaIndexTask task = createTask(\n        null,\n        new KafkaIndexTaskIOConfig(\n            0,\n            \"sequence0\",\n            new SeekableStreamPartitions<>(topic, ImmutableMap.of(0, 2L)),\n            new SeekableStreamPartitions<>(topic, ImmutableMap.of(0, 10L)),\n            kafkaServer.consumerProperties(),\n            KafkaSupervisorIOConfig.DEFAULT_POLL_TIMEOUT_MILLIS,\n            true,\n            null,\n            null,\n            false\n        )\n    );\n\n    final ListenableFuture<TaskStatus> future = runTask(task);\n\n    TaskStatus status = future.get();\n\n    \r\n    Assert.assertEquals(TaskState.FAILED, status.getStatusCode());\n    IndexTaskTest.checkTaskStatusErrorMsgForParseExceptionsExceeded(status);\n\n    \r\n    Assert.assertEquals(3, task.getRunner().getRowIngestionMeters().getProcessed());\n    Assert.assertEquals(0, task.getRunner().getRowIngestionMeters().getProcessedWithError());\n    Assert.assertEquals(3, task.getRunner().getRowIngestionMeters().getUnparseable());\n    Assert.assertEquals(0, task.getRunner().getRowIngestionMeters().getThrownAway());\n\n    \r\n    Assert.assertEquals(ImmutableSet.of(), publishedDescriptors());\n    Assert.assertNull(metadataStorageCoordinator.getDataSourceMetadata(DATA_SCHEMA.getDataSource()));\n\n    IngestionStatsAndErrorsTaskReportData reportData = getTaskReportData();\n\n    Map<String, Object> expectedMetrics = ImmutableMap.of(\n        RowIngestionMeters.BUILD_SEGMENTS,\n        ImmutableMap.of(\n            RowIngestionMeters.PROCESSED, 3,\n            RowIngestionMeters.PROCESSED_WITH_ERROR, 0,\n            RowIngestionMeters.UNPARSEABLE, 3,\n            RowIngestionMeters.THROWN_AWAY, 0\n        )\n    );\n    Assert.assertEquals(expectedMetrics, reportData.getRowStats());\n\n    Map<String, Object> unparseableEvents = ImmutableMap.of(\n        RowIngestionMeters.BUILD_SEGMENTS,\n        Arrays.asList(\n            \"Unable to parse row [unparseable2]\",\n            \"Unable to parse row [unparseable]\"\n        )\n    );\n\n    Assert.assertEquals(unparseableEvents, reportData.getUnparseableEvents());\n  }\n","date":"2019-01-03 12:16:02","endLine":1459,"groupId":"12458","id":4,"instanceNumber":2,"isCurCommit":0,"methodName":"testMultipleParseExceptionsFailure","params":"()","path":"/mnt/clonedata/CloneManagementServer/ManagementServer/consistResult/result-druid-10-0.7/blobInfo/CC_OUT/blobs/f4/4877771fbd76d2e48f167fa6e25fc70e074abc.src","preCode":"  public void testMultipleParseExceptionsFailure() throws Exception\n  {\n    reportParseExceptions = false;\n    maxParseExceptions = 2;\n    maxSavedParseExceptions = 2;\n\n    \r\n    try (final KafkaProducer<byte[], byte[]> kafkaProducer = kafkaServer.newProducer()) {\n      for (ProducerRecord<byte[], byte[]> record : records) {\n        kafkaProducer.send(record).get();\n      }\n    }\n\n    final KafkaIndexTask task = createTask(\n        null,\n        new KafkaIndexTaskIOConfig(\n            0,\n            \"sequence0\",\n            new SeekableStreamPartitions<>(topic, ImmutableMap.of(0, 2L)),\n            new SeekableStreamPartitions<>(topic, ImmutableMap.of(0, 10L)),\n            kafkaServer.consumerProperties(),\n            true,\n            null,\n            null,\n            false\n        )\n    );\n\n    final ListenableFuture<TaskStatus> future = runTask(task);\n\n    TaskStatus status = future.get();\n\n    \r\n    Assert.assertEquals(TaskState.FAILED, status.getStatusCode());\n    IndexTaskTest.checkTaskStatusErrorMsgForParseExceptionsExceeded(status);\n\n    \r\n    Assert.assertEquals(3, task.getRunner().getRowIngestionMeters().getProcessed());\n    Assert.assertEquals(0, task.getRunner().getRowIngestionMeters().getProcessedWithError());\n    Assert.assertEquals(3, task.getRunner().getRowIngestionMeters().getUnparseable());\n    Assert.assertEquals(0, task.getRunner().getRowIngestionMeters().getThrownAway());\n\n    \r\n    Assert.assertEquals(ImmutableSet.of(), publishedDescriptors());\n    Assert.assertNull(metadataStorageCoordinator.getDataSourceMetadata(DATA_SCHEMA.getDataSource()));\n\n    IngestionStatsAndErrorsTaskReportData reportData = getTaskReportData();\n\n    Map<String, Object> expectedMetrics = ImmutableMap.of(\n        RowIngestionMeters.BUILD_SEGMENTS,\n        ImmutableMap.of(\n            RowIngestionMeters.PROCESSED, 3,\n            RowIngestionMeters.PROCESSED_WITH_ERROR, 0,\n            RowIngestionMeters.UNPARSEABLE, 3,\n            RowIngestionMeters.THROWN_AWAY, 0\n        )\n    );\n    Assert.assertEquals(expectedMetrics, reportData.getRowStats());\n\n    Map<String, Object> unparseableEvents = ImmutableMap.of(\n        RowIngestionMeters.BUILD_SEGMENTS,\n        Arrays.asList(\n            \"Unable to parse row [unparseable2]\",\n            \"Unable to parse row [unparseable]\"\n        )\n    );\n\n    Assert.assertEquals(unparseableEvents, reportData.getUnparseableEvents());\n  }\n","realPath":"extensions-core/kafka-indexing-service/src/test/java/org/apache/druid/indexing/kafka/KafkaIndexTaskTest.java","repoName":"druid","snippetEndLine":0,"snippetStartLine":0,"startLine":1390,"status":"M"}],"commitId":"6761663509025c5c88275a9752c96d417f879abe","commitMessage":"@@@make kafka poll timeout can be configured (#6773)\n\n* make kafka poll timeout can be configured\n\n* add doc\n\n* rename DEFAULT_POLL_TIMEOUT to DEFAULT_POLL_TIMEOUT_MILLIS\n","date":"2019-01-03 12:16:02","modifiedFileCount":"9","status":"M","submitter":"Mingming Qiu"},{"authorTime":"2019-02-19 03:50:08","codes":[{"authorDate":"2018-12-22 03:49:24","commitOrder":4,"curCode":"  public void testMultipleParseExceptionsFailure() throws Exception\n  {\n    reportParseExceptions = false;\n    maxParseExceptions = 2;\n    maxSavedParseExceptions = 2;\n\n    recordSupplier.assign(anyObject());\n    expectLastCall().anyTimes();\n\n    expect(recordSupplier.getEarliestSequenceNumber(anyObject())).andReturn(\"0\").anyTimes();\n\n    recordSupplier.seek(anyObject(), anyString());\n    expectLastCall().anyTimes();\n\n    expect(recordSupplier.poll(anyLong())).andReturn(records.subList(2, 13)).once();\n\n\n    replayAll();\n\n    final KinesisIndexTask task = createTask(\n        null,\n        new KinesisIndexTaskIOConfig(\n            null,\n            \"sequence0\",\n            new SeekableStreamPartitions<>(stream, ImmutableMap.of(\n                shardId1,\n                \"2\"\n            )),\n            new SeekableStreamPartitions<>(stream, ImmutableMap.of(\n                shardId1,\n                \"9\"\n            )),\n            true,\n            null,\n            null,\n            \"awsEndpoint\",\n            null,\n            null,\n            null,\n            null,\n            null,\n            false\n        )\n    );\n\n    final ListenableFuture<TaskStatus> future = runTask(task);\n\n    TaskStatus status = future.get();\n\n    \r\n    Assert.assertEquals(TaskState.FAILED, status.getStatusCode());\n    verifyAll();\n    IndexTaskTest.checkTaskStatusErrorMsgForParseExceptionsExceeded(status);\n\n    \r\n    Assert.assertEquals(3, task.getRunner().getRowIngestionMeters().getProcessed());\n    Assert.assertEquals(0, task.getRunner().getRowIngestionMeters().getProcessedWithError());\n    Assert.assertEquals(3, task.getRunner().getRowIngestionMeters().getUnparseable());\n    Assert.assertEquals(0, task.getRunner().getRowIngestionMeters().getThrownAway());\n\n    \r\n    Assert.assertEquals(ImmutableSet.of(), publishedDescriptors());\n    Assert.assertNull(metadataStorageCoordinator.getDataSourceMetadata(DATA_SCHEMA.getDataSource()));\n\n    IngestionStatsAndErrorsTaskReportData reportData = getTaskReportData();\n\n    Map<String, Object> expectedMetrics = ImmutableMap.of(\n        RowIngestionMeters.BUILD_SEGMENTS,\n        ImmutableMap.of(\n            RowIngestionMeters.PROCESSED, 3,\n            RowIngestionMeters.PROCESSED_WITH_ERROR, 0,\n            RowIngestionMeters.UNPARSEABLE, 3,\n            RowIngestionMeters.THROWN_AWAY, 0\n        )\n    );\n    Assert.assertEquals(expectedMetrics, reportData.getRowStats());\n\n    Map<String, Object> unparseableEvents = ImmutableMap.of(\n        RowIngestionMeters.BUILD_SEGMENTS,\n        Arrays.asList(\n            \"Unable to parse row [unparseable2]\",\n            \"Unable to parse row [unparseable]\"\n        )\n    );\n\n    Assert.assertEquals(unparseableEvents, reportData.getUnparseableEvents());\n  }\n","date":"2018-12-22 03:49:24","endLine":1534,"groupId":"14312","id":5,"instanceNumber":1,"isCurCommit":0,"methodName":"testMultipleParseExceptionsFailure","params":"()","path":"/mnt/clonedata/CloneManagementServer/ManagementServer/consistResult/result-druid-10-0.7/blobInfo/CC_OUT/blobs/ce/f97955db2e8ceaae814ce65540385404536749.src","preCode":"  public void testMultipleParseExceptionsFailure() throws Exception\n  {\n    reportParseExceptions = false;\n    maxParseExceptions = 2;\n    maxSavedParseExceptions = 2;\n\n    recordSupplier.assign(anyObject());\n    expectLastCall().anyTimes();\n\n    expect(recordSupplier.getEarliestSequenceNumber(anyObject())).andReturn(\"0\").anyTimes();\n\n    recordSupplier.seek(anyObject(), anyString());\n    expectLastCall().anyTimes();\n\n    expect(recordSupplier.poll(anyLong())).andReturn(records.subList(2, 13)).once();\n\n\n    replayAll();\n\n    final KinesisIndexTask task = createTask(\n        null,\n        new KinesisIndexTaskIOConfig(\n            null,\n            \"sequence0\",\n            new SeekableStreamPartitions<>(stream, ImmutableMap.of(\n                shardId1,\n                \"2\"\n            )),\n            new SeekableStreamPartitions<>(stream, ImmutableMap.of(\n                shardId1,\n                \"9\"\n            )),\n            true,\n            null,\n            null,\n            \"awsEndpoint\",\n            null,\n            null,\n            null,\n            null,\n            null,\n            false\n        )\n    );\n\n    final ListenableFuture<TaskStatus> future = runTask(task);\n\n    TaskStatus status = future.get();\n\n    \r\n    Assert.assertEquals(TaskState.FAILED, status.getStatusCode());\n    verifyAll();\n    IndexTaskTest.checkTaskStatusErrorMsgForParseExceptionsExceeded(status);\n\n    \r\n    Assert.assertEquals(3, task.getRunner().getRowIngestionMeters().getProcessed());\n    Assert.assertEquals(0, task.getRunner().getRowIngestionMeters().getProcessedWithError());\n    Assert.assertEquals(3, task.getRunner().getRowIngestionMeters().getUnparseable());\n    Assert.assertEquals(0, task.getRunner().getRowIngestionMeters().getThrownAway());\n\n    \r\n    Assert.assertEquals(ImmutableSet.of(), publishedDescriptors());\n    Assert.assertNull(metadataStorageCoordinator.getDataSourceMetadata(DATA_SCHEMA.getDataSource()));\n\n    IngestionStatsAndErrorsTaskReportData reportData = getTaskReportData();\n\n    Map<String, Object> expectedMetrics = ImmutableMap.of(\n        RowIngestionMeters.BUILD_SEGMENTS,\n        ImmutableMap.of(\n            RowIngestionMeters.PROCESSED, 3,\n            RowIngestionMeters.PROCESSED_WITH_ERROR, 0,\n            RowIngestionMeters.UNPARSEABLE, 3,\n            RowIngestionMeters.THROWN_AWAY, 0\n        )\n    );\n    Assert.assertEquals(expectedMetrics, reportData.getRowStats());\n\n    Map<String, Object> unparseableEvents = ImmutableMap.of(\n        RowIngestionMeters.BUILD_SEGMENTS,\n        Arrays.asList(\n            \"Unable to parse row [unparseable2]\",\n            \"Unable to parse row [unparseable]\"\n        )\n    );\n\n    Assert.assertEquals(unparseableEvents, reportData.getUnparseableEvents());\n  }\n","realPath":"extensions-core/kinesis-indexing-service/src/test/java/org/apache/druid/indexing/kinesis/KinesisIndexTaskTest.java","repoName":"druid","snippetEndLine":0,"snippetStartLine":0,"startLine":1448,"status":"N"},{"authorDate":"2019-02-19 03:50:08","commitOrder":4,"curCode":"  public void testMultipleParseExceptionsFailure() throws Exception\n  {\n    reportParseExceptions = false;\n    maxParseExceptions = 2;\n    maxSavedParseExceptions = 2;\n\n    \r\n    insertData();\n\n    final KafkaIndexTask task = createTask(\n        null,\n        new KafkaIndexTaskIOConfig(\n            0,\n            \"sequence0\",\n            new SeekableStreamPartitions<>(topic, ImmutableMap.of(0, 2L)),\n            new SeekableStreamPartitions<>(topic, ImmutableMap.of(0, 10L)),\n            kafkaServer.consumerProperties(),\n            KafkaSupervisorIOConfig.DEFAULT_POLL_TIMEOUT_MILLIS,\n            true,\n            null,\n            null\n        )\n    );\n\n    final ListenableFuture<TaskStatus> future = runTask(task);\n\n    TaskStatus status = future.get();\n\n    \r\n    Assert.assertEquals(TaskState.FAILED, status.getStatusCode());\n    IndexTaskTest.checkTaskStatusErrorMsgForParseExceptionsExceeded(status);\n\n    \r\n    Assert.assertEquals(3, task.getRunner().getRowIngestionMeters().getProcessed());\n    Assert.assertEquals(0, task.getRunner().getRowIngestionMeters().getProcessedWithError());\n    Assert.assertEquals(3, task.getRunner().getRowIngestionMeters().getUnparseable());\n    Assert.assertEquals(0, task.getRunner().getRowIngestionMeters().getThrownAway());\n\n    \r\n    Assert.assertEquals(ImmutableSet.of(), publishedDescriptors());\n    Assert.assertNull(metadataStorageCoordinator.getDataSourceMetadata(DATA_SCHEMA.getDataSource()));\n\n    IngestionStatsAndErrorsTaskReportData reportData = getTaskReportData();\n\n    Map<String, Object> expectedMetrics = ImmutableMap.of(\n        RowIngestionMeters.BUILD_SEGMENTS,\n        ImmutableMap.of(\n            RowIngestionMeters.PROCESSED, 3,\n            RowIngestionMeters.PROCESSED_WITH_ERROR, 0,\n            RowIngestionMeters.UNPARSEABLE, 3,\n            RowIngestionMeters.THROWN_AWAY, 0\n        )\n    );\n    Assert.assertEquals(expectedMetrics, reportData.getRowStats());\n\n    Map<String, Object> unparseableEvents = ImmutableMap.of(\n        RowIngestionMeters.BUILD_SEGMENTS,\n        Arrays.asList(\n            \"Unable to parse row [unparseable2]\",\n            \"Unable to parse row [unparseable]\"\n        )\n    );\n\n    Assert.assertEquals(unparseableEvents, reportData.getUnparseableEvents());\n  }\n","date":"2019-02-19 03:50:08","endLine":1414,"groupId":"12458","id":6,"instanceNumber":2,"isCurCommit":0,"methodName":"testMultipleParseExceptionsFailure","params":"()","path":"/mnt/clonedata/CloneManagementServer/ManagementServer/consistResult/result-druid-10-0.7/blobInfo/CC_OUT/blobs/3d/6308cb0a96b01947db7e17b51cab010dd61129.src","preCode":"  public void testMultipleParseExceptionsFailure() throws Exception\n  {\n    reportParseExceptions = false;\n    maxParseExceptions = 2;\n    maxSavedParseExceptions = 2;\n\n    \r\n    try (final KafkaProducer<byte[], byte[]> kafkaProducer = kafkaServer.newProducer()) {\n      for (ProducerRecord<byte[], byte[]> record : records) {\n        kafkaProducer.send(record).get();\n      }\n    }\n\n    final KafkaIndexTask task = createTask(\n        null,\n        new KafkaIndexTaskIOConfig(\n            0,\n            \"sequence0\",\n            new SeekableStreamPartitions<>(topic, ImmutableMap.of(0, 2L)),\n            new SeekableStreamPartitions<>(topic, ImmutableMap.of(0, 10L)),\n            kafkaServer.consumerProperties(),\n            KafkaSupervisorIOConfig.DEFAULT_POLL_TIMEOUT_MILLIS,\n            true,\n            null,\n            null,\n            false\n        )\n    );\n\n    final ListenableFuture<TaskStatus> future = runTask(task);\n\n    TaskStatus status = future.get();\n\n    \r\n    Assert.assertEquals(TaskState.FAILED, status.getStatusCode());\n    IndexTaskTest.checkTaskStatusErrorMsgForParseExceptionsExceeded(status);\n\n    \r\n    Assert.assertEquals(3, task.getRunner().getRowIngestionMeters().getProcessed());\n    Assert.assertEquals(0, task.getRunner().getRowIngestionMeters().getProcessedWithError());\n    Assert.assertEquals(3, task.getRunner().getRowIngestionMeters().getUnparseable());\n    Assert.assertEquals(0, task.getRunner().getRowIngestionMeters().getThrownAway());\n\n    \r\n    Assert.assertEquals(ImmutableSet.of(), publishedDescriptors());\n    Assert.assertNull(metadataStorageCoordinator.getDataSourceMetadata(DATA_SCHEMA.getDataSource()));\n\n    IngestionStatsAndErrorsTaskReportData reportData = getTaskReportData();\n\n    Map<String, Object> expectedMetrics = ImmutableMap.of(\n        RowIngestionMeters.BUILD_SEGMENTS,\n        ImmutableMap.of(\n            RowIngestionMeters.PROCESSED, 3,\n            RowIngestionMeters.PROCESSED_WITH_ERROR, 0,\n            RowIngestionMeters.UNPARSEABLE, 3,\n            RowIngestionMeters.THROWN_AWAY, 0\n        )\n    );\n    Assert.assertEquals(expectedMetrics, reportData.getRowStats());\n\n    Map<String, Object> unparseableEvents = ImmutableMap.of(\n        RowIngestionMeters.BUILD_SEGMENTS,\n        Arrays.asList(\n            \"Unable to parse row [unparseable2]\",\n            \"Unable to parse row [unparseable]\"\n        )\n    );\n\n    Assert.assertEquals(unparseableEvents, reportData.getUnparseableEvents());\n  }\n","realPath":"extensions-core/kafka-indexing-service/src/test/java/org/apache/druid/indexing/kafka/KafkaIndexTaskTest.java","repoName":"druid","snippetEndLine":0,"snippetStartLine":0,"startLine":1350,"status":"M"}],"commitId":"80a2ef7be46c4fc807ac2a74507b8ba8b6a44049","commitMessage":"@@@Support kafka transactional topics (#5404) (#6496)\n\n* Support kafka transactional topics\n\n* update kafka to version 2.0.0\n* Remove the skipOffsetGaps option since it's not used anymore\n* Adjust kafka consumer to use transactional semantics\n* Update tests\n\n* Remove unused import from test\n\n* Fix compilation\n\n* Invoke transaction api to fix a unit test\n\n* temporary modification of travis.yml for debugging\n\n* another attempt to get travis tasklogs\n\n* update kafka to 2.0.1 at all places\n\n* Remove druid-kafka-eight dependency from integration-tests.  remove the kafka firehose test and deprecate kafka-eight classes\n\n* Add deprecated in docs for kafka-eight and kafka-simple extensions\n\n* Remove skipOffsetGaps and code changes for transaction support\n\n* Fix indentation\n\n* remove skipOffsetGaps from kinesis\n\n* Add transaction api to KafkaRecordSupplierTest\n\n* Fix indent\n\n* Fix test\n\n* update kafka version to 2.1.0\n","date":"2019-02-19 03:50:08","modifiedFileCount":"25","status":"M","submitter":"Surekha"},{"authorTime":"2019-03-22 04:12:22","codes":[{"authorDate":"2019-03-22 04:12:22","commitOrder":5,"curCode":"  public void testMultipleParseExceptionsFailure() throws Exception\n  {\n    reportParseExceptions = false;\n    maxParseExceptions = 2;\n    maxSavedParseExceptions = 2;\n\n    recordSupplier.assign(anyObject());\n    expectLastCall().anyTimes();\n\n    expect(recordSupplier.getEarliestSequenceNumber(anyObject())).andReturn(\"0\").anyTimes();\n\n    recordSupplier.seek(anyObject(), anyString());\n    expectLastCall().anyTimes();\n\n    expect(recordSupplier.poll(anyLong())).andReturn(records.subList(2, 13)).once();\n\n\n    replayAll();\n\n    final KinesisIndexTask task = createTask(\n        null,\n        new KinesisIndexTaskIOConfig(\n            null,\n            \"sequence0\",\n            new SeekableStreamStartSequenceNumbers<>(stream, ImmutableMap.of(shardId1, \"2\"), ImmutableSet.of()),\n            new SeekableStreamEndSequenceNumbers<>(stream, ImmutableMap.of(shardId1, \"9\")),\n            true,\n            null,\n            null,\n            \"awsEndpoint\",\n            null,\n            null,\n            null,\n            null,\n            false\n        )\n    );\n\n    final ListenableFuture<TaskStatus> future = runTask(task);\n\n    TaskStatus status = future.get();\n\n    \r\n    Assert.assertEquals(TaskState.FAILED, status.getStatusCode());\n    verifyAll();\n    IndexTaskTest.checkTaskStatusErrorMsgForParseExceptionsExceeded(status);\n\n    \r\n    Assert.assertEquals(3, task.getRunner().getRowIngestionMeters().getProcessed());\n    Assert.assertEquals(0, task.getRunner().getRowIngestionMeters().getProcessedWithError());\n    Assert.assertEquals(3, task.getRunner().getRowIngestionMeters().getUnparseable());\n    Assert.assertEquals(0, task.getRunner().getRowIngestionMeters().getThrownAway());\n\n    \r\n    Assert.assertEquals(ImmutableSet.of(), publishedDescriptors());\n    Assert.assertNull(metadataStorageCoordinator.getDataSourceMetadata(DATA_SCHEMA.getDataSource()));\n\n    IngestionStatsAndErrorsTaskReportData reportData = getTaskReportData();\n\n    Map<String, Object> expectedMetrics = ImmutableMap.of(\n        RowIngestionMeters.BUILD_SEGMENTS,\n        ImmutableMap.of(\n            RowIngestionMeters.PROCESSED, 3,\n            RowIngestionMeters.PROCESSED_WITH_ERROR, 0,\n            RowIngestionMeters.UNPARSEABLE, 3,\n            RowIngestionMeters.THROWN_AWAY, 0\n        )\n    );\n    Assert.assertEquals(expectedMetrics, reportData.getRowStats());\n\n    Map<String, Object> unparseableEvents = ImmutableMap.of(\n        RowIngestionMeters.BUILD_SEGMENTS,\n        Arrays.asList(\n            \"Unable to parse row [unparseable2]\",\n            \"Unable to parse row [unparseable]\"\n        )\n    );\n\n    Assert.assertEquals(unparseableEvents, reportData.getUnparseableEvents());\n  }\n","date":"2019-03-22 04:12:22","endLine":1423,"groupId":"14312","id":7,"instanceNumber":1,"isCurCommit":0,"methodName":"testMultipleParseExceptionsFailure","params":"()","path":"/mnt/clonedata/CloneManagementServer/ManagementServer/consistResult/result-druid-10-0.7/blobInfo/CC_OUT/blobs/43/5bb2abf8e4aa9a5b48367bb0a0c33d66fdd44b.src","preCode":"  public void testMultipleParseExceptionsFailure() throws Exception\n  {\n    reportParseExceptions = false;\n    maxParseExceptions = 2;\n    maxSavedParseExceptions = 2;\n\n    recordSupplier.assign(anyObject());\n    expectLastCall().anyTimes();\n\n    expect(recordSupplier.getEarliestSequenceNumber(anyObject())).andReturn(\"0\").anyTimes();\n\n    recordSupplier.seek(anyObject(), anyString());\n    expectLastCall().anyTimes();\n\n    expect(recordSupplier.poll(anyLong())).andReturn(records.subList(2, 13)).once();\n\n\n    replayAll();\n\n    final KinesisIndexTask task = createTask(\n        null,\n        new KinesisIndexTaskIOConfig(\n            null,\n            \"sequence0\",\n            new SeekableStreamPartitions<>(stream, ImmutableMap.of(\n                shardId1,\n                \"2\"\n            )),\n            new SeekableStreamPartitions<>(stream, ImmutableMap.of(\n                shardId1,\n                \"9\"\n            )),\n            true,\n            null,\n            null,\n            \"awsEndpoint\",\n            null,\n            null,\n            null,\n            null,\n            null,\n            false\n        )\n    );\n\n    final ListenableFuture<TaskStatus> future = runTask(task);\n\n    TaskStatus status = future.get();\n\n    \r\n    Assert.assertEquals(TaskState.FAILED, status.getStatusCode());\n    verifyAll();\n    IndexTaskTest.checkTaskStatusErrorMsgForParseExceptionsExceeded(status);\n\n    \r\n    Assert.assertEquals(3, task.getRunner().getRowIngestionMeters().getProcessed());\n    Assert.assertEquals(0, task.getRunner().getRowIngestionMeters().getProcessedWithError());\n    Assert.assertEquals(3, task.getRunner().getRowIngestionMeters().getUnparseable());\n    Assert.assertEquals(0, task.getRunner().getRowIngestionMeters().getThrownAway());\n\n    \r\n    Assert.assertEquals(ImmutableSet.of(), publishedDescriptors());\n    Assert.assertNull(metadataStorageCoordinator.getDataSourceMetadata(DATA_SCHEMA.getDataSource()));\n\n    IngestionStatsAndErrorsTaskReportData reportData = getTaskReportData();\n\n    Map<String, Object> expectedMetrics = ImmutableMap.of(\n        RowIngestionMeters.BUILD_SEGMENTS,\n        ImmutableMap.of(\n            RowIngestionMeters.PROCESSED, 3,\n            RowIngestionMeters.PROCESSED_WITH_ERROR, 0,\n            RowIngestionMeters.UNPARSEABLE, 3,\n            RowIngestionMeters.THROWN_AWAY, 0\n        )\n    );\n    Assert.assertEquals(expectedMetrics, reportData.getRowStats());\n\n    Map<String, Object> unparseableEvents = ImmutableMap.of(\n        RowIngestionMeters.BUILD_SEGMENTS,\n        Arrays.asList(\n            \"Unable to parse row [unparseable2]\",\n            \"Unable to parse row [unparseable]\"\n        )\n    );\n\n    Assert.assertEquals(unparseableEvents, reportData.getUnparseableEvents());\n  }\n","realPath":"extensions-core/kinesis-indexing-service/src/test/java/org/apache/druid/indexing/kinesis/KinesisIndexTaskTest.java","repoName":"druid","snippetEndLine":0,"snippetStartLine":0,"startLine":1344,"status":"M"},{"authorDate":"2019-03-22 04:12:22","commitOrder":5,"curCode":"  public void testMultipleParseExceptionsFailure() throws Exception\n  {\n    reportParseExceptions = false;\n    maxParseExceptions = 2;\n    maxSavedParseExceptions = 2;\n\n    \r\n    insertData();\n\n    final KafkaIndexTask task = createTask(\n        null,\n        new KafkaIndexTaskIOConfig(\n            0,\n            \"sequence0\",\n            new SeekableStreamStartSequenceNumbers<>(topic, ImmutableMap.of(0, 2L), ImmutableSet.of()),\n            new SeekableStreamEndSequenceNumbers<>(topic, ImmutableMap.of(0, 10L)),\n            kafkaServer.consumerProperties(),\n            KafkaSupervisorIOConfig.DEFAULT_POLL_TIMEOUT_MILLIS,\n            true,\n            null,\n            null\n        )\n    );\n\n    final ListenableFuture<TaskStatus> future = runTask(task);\n\n    TaskStatus status = future.get();\n\n    \r\n    Assert.assertEquals(TaskState.FAILED, status.getStatusCode());\n    IndexTaskTest.checkTaskStatusErrorMsgForParseExceptionsExceeded(status);\n\n    \r\n    Assert.assertEquals(3, task.getRunner().getRowIngestionMeters().getProcessed());\n    Assert.assertEquals(0, task.getRunner().getRowIngestionMeters().getProcessedWithError());\n    Assert.assertEquals(3, task.getRunner().getRowIngestionMeters().getUnparseable());\n    Assert.assertEquals(0, task.getRunner().getRowIngestionMeters().getThrownAway());\n\n    \r\n    Assert.assertEquals(ImmutableSet.of(), publishedDescriptors());\n    Assert.assertNull(metadataStorageCoordinator.getDataSourceMetadata(DATA_SCHEMA.getDataSource()));\n\n    IngestionStatsAndErrorsTaskReportData reportData = getTaskReportData();\n\n    Map<String, Object> expectedMetrics = ImmutableMap.of(\n        RowIngestionMeters.BUILD_SEGMENTS,\n        ImmutableMap.of(\n            RowIngestionMeters.PROCESSED, 3,\n            RowIngestionMeters.PROCESSED_WITH_ERROR, 0,\n            RowIngestionMeters.UNPARSEABLE, 3,\n            RowIngestionMeters.THROWN_AWAY, 0\n        )\n    );\n    Assert.assertEquals(expectedMetrics, reportData.getRowStats());\n\n    Map<String, Object> unparseableEvents = ImmutableMap.of(\n        RowIngestionMeters.BUILD_SEGMENTS,\n        Arrays.asList(\n            \"Unable to parse row [unparseable2]\",\n            \"Unable to parse row [unparseable]\"\n        )\n    );\n\n    Assert.assertEquals(unparseableEvents, reportData.getUnparseableEvents());\n  }\n","date":"2019-03-22 04:12:22","endLine":1433,"groupId":"12458","id":8,"instanceNumber":2,"isCurCommit":0,"methodName":"testMultipleParseExceptionsFailure","params":"()","path":"/mnt/clonedata/CloneManagementServer/ManagementServer/consistResult/result-druid-10-0.7/blobInfo/CC_OUT/blobs/eb/ef3fc4596a43b59efefcaf094105a8ae34ab5f.src","preCode":"  public void testMultipleParseExceptionsFailure() throws Exception\n  {\n    reportParseExceptions = false;\n    maxParseExceptions = 2;\n    maxSavedParseExceptions = 2;\n\n    \r\n    insertData();\n\n    final KafkaIndexTask task = createTask(\n        null,\n        new KafkaIndexTaskIOConfig(\n            0,\n            \"sequence0\",\n            new SeekableStreamPartitions<>(topic, ImmutableMap.of(0, 2L)),\n            new SeekableStreamPartitions<>(topic, ImmutableMap.of(0, 10L)),\n            kafkaServer.consumerProperties(),\n            KafkaSupervisorIOConfig.DEFAULT_POLL_TIMEOUT_MILLIS,\n            true,\n            null,\n            null\n        )\n    );\n\n    final ListenableFuture<TaskStatus> future = runTask(task);\n\n    TaskStatus status = future.get();\n\n    \r\n    Assert.assertEquals(TaskState.FAILED, status.getStatusCode());\n    IndexTaskTest.checkTaskStatusErrorMsgForParseExceptionsExceeded(status);\n\n    \r\n    Assert.assertEquals(3, task.getRunner().getRowIngestionMeters().getProcessed());\n    Assert.assertEquals(0, task.getRunner().getRowIngestionMeters().getProcessedWithError());\n    Assert.assertEquals(3, task.getRunner().getRowIngestionMeters().getUnparseable());\n    Assert.assertEquals(0, task.getRunner().getRowIngestionMeters().getThrownAway());\n\n    \r\n    Assert.assertEquals(ImmutableSet.of(), publishedDescriptors());\n    Assert.assertNull(metadataStorageCoordinator.getDataSourceMetadata(DATA_SCHEMA.getDataSource()));\n\n    IngestionStatsAndErrorsTaskReportData reportData = getTaskReportData();\n\n    Map<String, Object> expectedMetrics = ImmutableMap.of(\n        RowIngestionMeters.BUILD_SEGMENTS,\n        ImmutableMap.of(\n            RowIngestionMeters.PROCESSED, 3,\n            RowIngestionMeters.PROCESSED_WITH_ERROR, 0,\n            RowIngestionMeters.UNPARSEABLE, 3,\n            RowIngestionMeters.THROWN_AWAY, 0\n        )\n    );\n    Assert.assertEquals(expectedMetrics, reportData.getRowStats());\n\n    Map<String, Object> unparseableEvents = ImmutableMap.of(\n        RowIngestionMeters.BUILD_SEGMENTS,\n        Arrays.asList(\n            \"Unable to parse row [unparseable2]\",\n            \"Unable to parse row [unparseable]\"\n        )\n    );\n\n    Assert.assertEquals(unparseableEvents, reportData.getUnparseableEvents());\n  }\n","realPath":"extensions-core/kafka-indexing-service/src/test/java/org/apache/druid/indexing/kafka/KafkaIndexTaskTest.java","repoName":"druid","snippetEndLine":0,"snippetStartLine":0,"startLine":1369,"status":"M"}],"commitId":"0c5dcf5586e33607849e397209f3eb0b10661f1e","commitMessage":"@@@Fix exclusivity for start offset in kinesis indexing service & check exclusivity properly in IndexerSQLMetadataStorageCoordinator (#7291)\n\n* Fix exclusivity for start offset in kinesis indexing service\n\n* some adjustment\n\n* Fix SeekableStreamDataSourceMetadata\n\n* Add missing javadocs\n\n* Add missing comments and unit test\n\n* fix SeekableStreamStartSequenceNumbers.plus and add comments\n\n* remove extra exclusivePartitions in KafkaIOConfig and fix downgrade issue\n\n* Add javadocs\n\n* fix compilation\n\n* fix test\n\n* remove unused variable\n","date":"2019-03-22 04:12:22","modifiedFileCount":"30","status":"M","submitter":"Jihoon Son"},{"authorTime":"2019-03-22 04:12:22","codes":[{"authorDate":"2019-04-09 10:19:34","commitOrder":6,"curCode":"  public void testMultipleParseExceptionsFailure() throws Exception\n  {\n    reportParseExceptions = false;\n    maxParseExceptions = 2;\n    maxSavedParseExceptions = 2;\n\n    recordSupplier.assign(anyObject());\n    expectLastCall().anyTimes();\n\n    expect(recordSupplier.getEarliestSequenceNumber(anyObject())).andReturn(\"0\").anyTimes();\n\n    recordSupplier.seek(anyObject(), anyString());\n    expectLastCall().anyTimes();\n\n    expect(recordSupplier.poll(anyLong())).andReturn(records.subList(2, 13)).once();\n\n\n    replayAll();\n\n    final KinesisIndexTask task = createTask(\n        null,\n        new KinesisIndexTaskIOConfig(\n            0,\n            \"sequence0\",\n            new SeekableStreamStartSequenceNumbers<>(stream, ImmutableMap.of(shardId1, \"2\"), ImmutableSet.of()),\n            new SeekableStreamEndSequenceNumbers<>(stream, ImmutableMap.of(shardId1, \"9\")),\n            true,\n            null,\n            null,\n            \"awsEndpoint\",\n            null,\n            null,\n            null,\n            null,\n            false\n        )\n    );\n\n    final ListenableFuture<TaskStatus> future = runTask(task);\n\n    TaskStatus status = future.get();\n\n    \r\n    Assert.assertEquals(TaskState.FAILED, status.getStatusCode());\n    verifyAll();\n    IndexTaskTest.checkTaskStatusErrorMsgForParseExceptionsExceeded(status);\n\n    \r\n    Assert.assertEquals(3, task.getRunner().getRowIngestionMeters().getProcessed());\n    Assert.assertEquals(0, task.getRunner().getRowIngestionMeters().getProcessedWithError());\n    Assert.assertEquals(3, task.getRunner().getRowIngestionMeters().getUnparseable());\n    Assert.assertEquals(0, task.getRunner().getRowIngestionMeters().getThrownAway());\n\n    \r\n    Assert.assertEquals(ImmutableSet.of(), publishedDescriptors());\n    Assert.assertNull(metadataStorageCoordinator.getDataSourceMetadata(DATA_SCHEMA.getDataSource()));\n\n    IngestionStatsAndErrorsTaskReportData reportData = getTaskReportData();\n\n    Map<String, Object> expectedMetrics = ImmutableMap.of(\n        RowIngestionMeters.BUILD_SEGMENTS,\n        ImmutableMap.of(\n            RowIngestionMeters.PROCESSED, 3,\n            RowIngestionMeters.PROCESSED_WITH_ERROR, 0,\n            RowIngestionMeters.UNPARSEABLE, 3,\n            RowIngestionMeters.THROWN_AWAY, 0\n        )\n    );\n    Assert.assertEquals(expectedMetrics, reportData.getRowStats());\n\n    Map<String, Object> unparseableEvents = ImmutableMap.of(\n        RowIngestionMeters.BUILD_SEGMENTS,\n        Arrays.asList(\n            \"Unable to parse row [unparseable2]\",\n            \"Unable to parse row [unparseable]\"\n        )\n    );\n\n    Assert.assertEquals(unparseableEvents, reportData.getUnparseableEvents());\n  }\n","date":"2019-04-09 10:19:34","endLine":1426,"groupId":"14312","id":9,"instanceNumber":1,"isCurCommit":0,"methodName":"testMultipleParseExceptionsFailure","params":"()","path":"/mnt/clonedata/CloneManagementServer/ManagementServer/consistResult/result-druid-10-0.7/blobInfo/CC_OUT/blobs/96/9cc399636844b4dc771e9dcb171336b8b71c50.src","preCode":"  public void testMultipleParseExceptionsFailure() throws Exception\n  {\n    reportParseExceptions = false;\n    maxParseExceptions = 2;\n    maxSavedParseExceptions = 2;\n\n    recordSupplier.assign(anyObject());\n    expectLastCall().anyTimes();\n\n    expect(recordSupplier.getEarliestSequenceNumber(anyObject())).andReturn(\"0\").anyTimes();\n\n    recordSupplier.seek(anyObject(), anyString());\n    expectLastCall().anyTimes();\n\n    expect(recordSupplier.poll(anyLong())).andReturn(records.subList(2, 13)).once();\n\n\n    replayAll();\n\n    final KinesisIndexTask task = createTask(\n        null,\n        new KinesisIndexTaskIOConfig(\n            null,\n            \"sequence0\",\n            new SeekableStreamStartSequenceNumbers<>(stream, ImmutableMap.of(shardId1, \"2\"), ImmutableSet.of()),\n            new SeekableStreamEndSequenceNumbers<>(stream, ImmutableMap.of(shardId1, \"9\")),\n            true,\n            null,\n            null,\n            \"awsEndpoint\",\n            null,\n            null,\n            null,\n            null,\n            false\n        )\n    );\n\n    final ListenableFuture<TaskStatus> future = runTask(task);\n\n    TaskStatus status = future.get();\n\n    \r\n    Assert.assertEquals(TaskState.FAILED, status.getStatusCode());\n    verifyAll();\n    IndexTaskTest.checkTaskStatusErrorMsgForParseExceptionsExceeded(status);\n\n    \r\n    Assert.assertEquals(3, task.getRunner().getRowIngestionMeters().getProcessed());\n    Assert.assertEquals(0, task.getRunner().getRowIngestionMeters().getProcessedWithError());\n    Assert.assertEquals(3, task.getRunner().getRowIngestionMeters().getUnparseable());\n    Assert.assertEquals(0, task.getRunner().getRowIngestionMeters().getThrownAway());\n\n    \r\n    Assert.assertEquals(ImmutableSet.of(), publishedDescriptors());\n    Assert.assertNull(metadataStorageCoordinator.getDataSourceMetadata(DATA_SCHEMA.getDataSource()));\n\n    IngestionStatsAndErrorsTaskReportData reportData = getTaskReportData();\n\n    Map<String, Object> expectedMetrics = ImmutableMap.of(\n        RowIngestionMeters.BUILD_SEGMENTS,\n        ImmutableMap.of(\n            RowIngestionMeters.PROCESSED, 3,\n            RowIngestionMeters.PROCESSED_WITH_ERROR, 0,\n            RowIngestionMeters.UNPARSEABLE, 3,\n            RowIngestionMeters.THROWN_AWAY, 0\n        )\n    );\n    Assert.assertEquals(expectedMetrics, reportData.getRowStats());\n\n    Map<String, Object> unparseableEvents = ImmutableMap.of(\n        RowIngestionMeters.BUILD_SEGMENTS,\n        Arrays.asList(\n            \"Unable to parse row [unparseable2]\",\n            \"Unable to parse row [unparseable]\"\n        )\n    );\n\n    Assert.assertEquals(unparseableEvents, reportData.getUnparseableEvents());\n  }\n","realPath":"extensions-core/kinesis-indexing-service/src/test/java/org/apache/druid/indexing/kinesis/KinesisIndexTaskTest.java","repoName":"druid","snippetEndLine":0,"snippetStartLine":0,"startLine":1347,"status":"M"},{"authorDate":"2019-03-22 04:12:22","commitOrder":6,"curCode":"  public void testMultipleParseExceptionsFailure() throws Exception\n  {\n    reportParseExceptions = false;\n    maxParseExceptions = 2;\n    maxSavedParseExceptions = 2;\n\n    \r\n    insertData();\n\n    final KafkaIndexTask task = createTask(\n        null,\n        new KafkaIndexTaskIOConfig(\n            0,\n            \"sequence0\",\n            new SeekableStreamStartSequenceNumbers<>(topic, ImmutableMap.of(0, 2L), ImmutableSet.of()),\n            new SeekableStreamEndSequenceNumbers<>(topic, ImmutableMap.of(0, 10L)),\n            kafkaServer.consumerProperties(),\n            KafkaSupervisorIOConfig.DEFAULT_POLL_TIMEOUT_MILLIS,\n            true,\n            null,\n            null\n        )\n    );\n\n    final ListenableFuture<TaskStatus> future = runTask(task);\n\n    TaskStatus status = future.get();\n\n    \r\n    Assert.assertEquals(TaskState.FAILED, status.getStatusCode());\n    IndexTaskTest.checkTaskStatusErrorMsgForParseExceptionsExceeded(status);\n\n    \r\n    Assert.assertEquals(3, task.getRunner().getRowIngestionMeters().getProcessed());\n    Assert.assertEquals(0, task.getRunner().getRowIngestionMeters().getProcessedWithError());\n    Assert.assertEquals(3, task.getRunner().getRowIngestionMeters().getUnparseable());\n    Assert.assertEquals(0, task.getRunner().getRowIngestionMeters().getThrownAway());\n\n    \r\n    Assert.assertEquals(ImmutableSet.of(), publishedDescriptors());\n    Assert.assertNull(metadataStorageCoordinator.getDataSourceMetadata(DATA_SCHEMA.getDataSource()));\n\n    IngestionStatsAndErrorsTaskReportData reportData = getTaskReportData();\n\n    Map<String, Object> expectedMetrics = ImmutableMap.of(\n        RowIngestionMeters.BUILD_SEGMENTS,\n        ImmutableMap.of(\n            RowIngestionMeters.PROCESSED, 3,\n            RowIngestionMeters.PROCESSED_WITH_ERROR, 0,\n            RowIngestionMeters.UNPARSEABLE, 3,\n            RowIngestionMeters.THROWN_AWAY, 0\n        )\n    );\n    Assert.assertEquals(expectedMetrics, reportData.getRowStats());\n\n    Map<String, Object> unparseableEvents = ImmutableMap.of(\n        RowIngestionMeters.BUILD_SEGMENTS,\n        Arrays.asList(\n            \"Unable to parse row [unparseable2]\",\n            \"Unable to parse row [unparseable]\"\n        )\n    );\n\n    Assert.assertEquals(unparseableEvents, reportData.getUnparseableEvents());\n  }\n","date":"2019-03-22 04:12:22","endLine":1433,"groupId":"12458","id":10,"instanceNumber":2,"isCurCommit":0,"methodName":"testMultipleParseExceptionsFailure","params":"()","path":"/mnt/clonedata/CloneManagementServer/ManagementServer/consistResult/result-druid-10-0.7/blobInfo/CC_OUT/blobs/eb/ef3fc4596a43b59efefcaf094105a8ae34ab5f.src","preCode":"  public void testMultipleParseExceptionsFailure() throws Exception\n  {\n    reportParseExceptions = false;\n    maxParseExceptions = 2;\n    maxSavedParseExceptions = 2;\n\n    \r\n    insertData();\n\n    final KafkaIndexTask task = createTask(\n        null,\n        new KafkaIndexTaskIOConfig(\n            0,\n            \"sequence0\",\n            new SeekableStreamStartSequenceNumbers<>(topic, ImmutableMap.of(0, 2L), ImmutableSet.of()),\n            new SeekableStreamEndSequenceNumbers<>(topic, ImmutableMap.of(0, 10L)),\n            kafkaServer.consumerProperties(),\n            KafkaSupervisorIOConfig.DEFAULT_POLL_TIMEOUT_MILLIS,\n            true,\n            null,\n            null\n        )\n    );\n\n    final ListenableFuture<TaskStatus> future = runTask(task);\n\n    TaskStatus status = future.get();\n\n    \r\n    Assert.assertEquals(TaskState.FAILED, status.getStatusCode());\n    IndexTaskTest.checkTaskStatusErrorMsgForParseExceptionsExceeded(status);\n\n    \r\n    Assert.assertEquals(3, task.getRunner().getRowIngestionMeters().getProcessed());\n    Assert.assertEquals(0, task.getRunner().getRowIngestionMeters().getProcessedWithError());\n    Assert.assertEquals(3, task.getRunner().getRowIngestionMeters().getUnparseable());\n    Assert.assertEquals(0, task.getRunner().getRowIngestionMeters().getThrownAway());\n\n    \r\n    Assert.assertEquals(ImmutableSet.of(), publishedDescriptors());\n    Assert.assertNull(metadataStorageCoordinator.getDataSourceMetadata(DATA_SCHEMA.getDataSource()));\n\n    IngestionStatsAndErrorsTaskReportData reportData = getTaskReportData();\n\n    Map<String, Object> expectedMetrics = ImmutableMap.of(\n        RowIngestionMeters.BUILD_SEGMENTS,\n        ImmutableMap.of(\n            RowIngestionMeters.PROCESSED, 3,\n            RowIngestionMeters.PROCESSED_WITH_ERROR, 0,\n            RowIngestionMeters.UNPARSEABLE, 3,\n            RowIngestionMeters.THROWN_AWAY, 0\n        )\n    );\n    Assert.assertEquals(expectedMetrics, reportData.getRowStats());\n\n    Map<String, Object> unparseableEvents = ImmutableMap.of(\n        RowIngestionMeters.BUILD_SEGMENTS,\n        Arrays.asList(\n            \"Unable to parse row [unparseable2]\",\n            \"Unable to parse row [unparseable]\"\n        )\n    );\n\n    Assert.assertEquals(unparseableEvents, reportData.getUnparseableEvents());\n  }\n","realPath":"extensions-core/kafka-indexing-service/src/test/java/org/apache/druid/indexing/kafka/KafkaIndexTaskTest.java","repoName":"druid","snippetEndLine":0,"snippetStartLine":0,"startLine":1369,"status":"N"}],"commitId":"e87d6e32b3b3f56441a6bdef4974e9c24ffcbf47","commitMessage":"@@@Support kinesis compatibility (#7351)\n\n","date":"2019-04-09 10:19:34","modifiedFileCount":"5","status":"M","submitter":"Jihoon Son"},{"authorTime":"2019-03-22 04:12:22","codes":[{"authorDate":"2019-05-07 03:28:56","commitOrder":7,"curCode":"  public void testMultipleParseExceptionsFailure() throws Exception\n  {\n    reportParseExceptions = false;\n    maxParseExceptions = 2;\n    maxSavedParseExceptions = 2;\n\n    recordSupplier.assign(anyObject());\n    expectLastCall().anyTimes();\n\n    expect(recordSupplier.getEarliestSequenceNumber(anyObject())).andReturn(\"0\").anyTimes();\n\n    recordSupplier.seek(anyObject(), anyString());\n    expectLastCall().anyTimes();\n\n    expect(recordSupplier.poll(anyLong())).andReturn(records.subList(2, 13)).once();\n\n    recordSupplier.close();\n    expectLastCall().once();\n\n    replayAll();\n\n    final KinesisIndexTask task = createTask(\n        null,\n        new KinesisIndexTaskIOConfig(\n            0,\n            \"sequence0\",\n            new SeekableStreamStartSequenceNumbers<>(stream, ImmutableMap.of(shardId1, \"2\"), ImmutableSet.of()),\n            new SeekableStreamEndSequenceNumbers<>(stream, ImmutableMap.of(shardId1, \"9\")),\n            true,\n            null,\n            null,\n            \"awsEndpoint\",\n            null,\n            null,\n            null,\n            null,\n            false\n        )\n    );\n\n    final ListenableFuture<TaskStatus> future = runTask(task);\n\n    TaskStatus status = future.get();\n\n    \r\n    Assert.assertEquals(TaskState.FAILED, status.getStatusCode());\n    verifyAll();\n    IndexTaskTest.checkTaskStatusErrorMsgForParseExceptionsExceeded(status);\n\n    \r\n    Assert.assertEquals(3, task.getRunner().getRowIngestionMeters().getProcessed());\n    Assert.assertEquals(0, task.getRunner().getRowIngestionMeters().getProcessedWithError());\n    Assert.assertEquals(3, task.getRunner().getRowIngestionMeters().getUnparseable());\n    Assert.assertEquals(0, task.getRunner().getRowIngestionMeters().getThrownAway());\n\n    \r\n    Assert.assertEquals(ImmutableSet.of(), publishedDescriptors());\n    Assert.assertNull(metadataStorageCoordinator.getDataSourceMetadata(DATA_SCHEMA.getDataSource()));\n\n    IngestionStatsAndErrorsTaskReportData reportData = getTaskReportData();\n\n    Map<String, Object> expectedMetrics = ImmutableMap.of(\n        RowIngestionMeters.BUILD_SEGMENTS,\n        ImmutableMap.of(\n            RowIngestionMeters.PROCESSED, 3,\n            RowIngestionMeters.PROCESSED_WITH_ERROR, 0,\n            RowIngestionMeters.UNPARSEABLE, 3,\n            RowIngestionMeters.THROWN_AWAY, 0\n        )\n    );\n    Assert.assertEquals(expectedMetrics, reportData.getRowStats());\n\n    Map<String, Object> unparseableEvents = ImmutableMap.of(\n        RowIngestionMeters.BUILD_SEGMENTS,\n        Arrays.asList(\n            \"Unable to parse row [unparseable2]\",\n            \"Unable to parse row [unparseable]\"\n        )\n    );\n\n    Assert.assertEquals(unparseableEvents, reportData.getUnparseableEvents());\n  }\n","date":"2019-05-07 03:28:56","endLine":1431,"groupId":"14312","id":11,"instanceNumber":1,"isCurCommit":0,"methodName":"testMultipleParseExceptionsFailure","params":"()","path":"/mnt/clonedata/CloneManagementServer/ManagementServer/consistResult/result-druid-10-0.7/blobInfo/CC_OUT/blobs/a0/4556d6abf014cb8df7c63deac4cc88526b481d.src","preCode":"  public void testMultipleParseExceptionsFailure() throws Exception\n  {\n    reportParseExceptions = false;\n    maxParseExceptions = 2;\n    maxSavedParseExceptions = 2;\n\n    recordSupplier.assign(anyObject());\n    expectLastCall().anyTimes();\n\n    expect(recordSupplier.getEarliestSequenceNumber(anyObject())).andReturn(\"0\").anyTimes();\n\n    recordSupplier.seek(anyObject(), anyString());\n    expectLastCall().anyTimes();\n\n    expect(recordSupplier.poll(anyLong())).andReturn(records.subList(2, 13)).once();\n\n\n    replayAll();\n\n    final KinesisIndexTask task = createTask(\n        null,\n        new KinesisIndexTaskIOConfig(\n            0,\n            \"sequence0\",\n            new SeekableStreamStartSequenceNumbers<>(stream, ImmutableMap.of(shardId1, \"2\"), ImmutableSet.of()),\n            new SeekableStreamEndSequenceNumbers<>(stream, ImmutableMap.of(shardId1, \"9\")),\n            true,\n            null,\n            null,\n            \"awsEndpoint\",\n            null,\n            null,\n            null,\n            null,\n            false\n        )\n    );\n\n    final ListenableFuture<TaskStatus> future = runTask(task);\n\n    TaskStatus status = future.get();\n\n    \r\n    Assert.assertEquals(TaskState.FAILED, status.getStatusCode());\n    verifyAll();\n    IndexTaskTest.checkTaskStatusErrorMsgForParseExceptionsExceeded(status);\n\n    \r\n    Assert.assertEquals(3, task.getRunner().getRowIngestionMeters().getProcessed());\n    Assert.assertEquals(0, task.getRunner().getRowIngestionMeters().getProcessedWithError());\n    Assert.assertEquals(3, task.getRunner().getRowIngestionMeters().getUnparseable());\n    Assert.assertEquals(0, task.getRunner().getRowIngestionMeters().getThrownAway());\n\n    \r\n    Assert.assertEquals(ImmutableSet.of(), publishedDescriptors());\n    Assert.assertNull(metadataStorageCoordinator.getDataSourceMetadata(DATA_SCHEMA.getDataSource()));\n\n    IngestionStatsAndErrorsTaskReportData reportData = getTaskReportData();\n\n    Map<String, Object> expectedMetrics = ImmutableMap.of(\n        RowIngestionMeters.BUILD_SEGMENTS,\n        ImmutableMap.of(\n            RowIngestionMeters.PROCESSED, 3,\n            RowIngestionMeters.PROCESSED_WITH_ERROR, 0,\n            RowIngestionMeters.UNPARSEABLE, 3,\n            RowIngestionMeters.THROWN_AWAY, 0\n        )\n    );\n    Assert.assertEquals(expectedMetrics, reportData.getRowStats());\n\n    Map<String, Object> unparseableEvents = ImmutableMap.of(\n        RowIngestionMeters.BUILD_SEGMENTS,\n        Arrays.asList(\n            \"Unable to parse row [unparseable2]\",\n            \"Unable to parse row [unparseable]\"\n        )\n    );\n\n    Assert.assertEquals(unparseableEvents, reportData.getUnparseableEvents());\n  }\n","realPath":"extensions-core/kinesis-indexing-service/src/test/java/org/apache/druid/indexing/kinesis/KinesisIndexTaskTest.java","repoName":"druid","snippetEndLine":0,"snippetStartLine":0,"startLine":1350,"status":"M"},{"authorDate":"2019-03-22 04:12:22","commitOrder":7,"curCode":"  public void testMultipleParseExceptionsFailure() throws Exception\n  {\n    reportParseExceptions = false;\n    maxParseExceptions = 2;\n    maxSavedParseExceptions = 2;\n\n    \r\n    insertData();\n\n    final KafkaIndexTask task = createTask(\n        null,\n        new KafkaIndexTaskIOConfig(\n            0,\n            \"sequence0\",\n            new SeekableStreamStartSequenceNumbers<>(topic, ImmutableMap.of(0, 2L), ImmutableSet.of()),\n            new SeekableStreamEndSequenceNumbers<>(topic, ImmutableMap.of(0, 10L)),\n            kafkaServer.consumerProperties(),\n            KafkaSupervisorIOConfig.DEFAULT_POLL_TIMEOUT_MILLIS,\n            true,\n            null,\n            null\n        )\n    );\n\n    final ListenableFuture<TaskStatus> future = runTask(task);\n\n    TaskStatus status = future.get();\n\n    \r\n    Assert.assertEquals(TaskState.FAILED, status.getStatusCode());\n    IndexTaskTest.checkTaskStatusErrorMsgForParseExceptionsExceeded(status);\n\n    \r\n    Assert.assertEquals(3, task.getRunner().getRowIngestionMeters().getProcessed());\n    Assert.assertEquals(0, task.getRunner().getRowIngestionMeters().getProcessedWithError());\n    Assert.assertEquals(3, task.getRunner().getRowIngestionMeters().getUnparseable());\n    Assert.assertEquals(0, task.getRunner().getRowIngestionMeters().getThrownAway());\n\n    \r\n    Assert.assertEquals(ImmutableSet.of(), publishedDescriptors());\n    Assert.assertNull(metadataStorageCoordinator.getDataSourceMetadata(DATA_SCHEMA.getDataSource()));\n\n    IngestionStatsAndErrorsTaskReportData reportData = getTaskReportData();\n\n    Map<String, Object> expectedMetrics = ImmutableMap.of(\n        RowIngestionMeters.BUILD_SEGMENTS,\n        ImmutableMap.of(\n            RowIngestionMeters.PROCESSED, 3,\n            RowIngestionMeters.PROCESSED_WITH_ERROR, 0,\n            RowIngestionMeters.UNPARSEABLE, 3,\n            RowIngestionMeters.THROWN_AWAY, 0\n        )\n    );\n    Assert.assertEquals(expectedMetrics, reportData.getRowStats());\n\n    Map<String, Object> unparseableEvents = ImmutableMap.of(\n        RowIngestionMeters.BUILD_SEGMENTS,\n        Arrays.asList(\n            \"Unable to parse row [unparseable2]\",\n            \"Unable to parse row [unparseable]\"\n        )\n    );\n\n    Assert.assertEquals(unparseableEvents, reportData.getUnparseableEvents());\n  }\n","date":"2019-03-22 04:12:22","endLine":1433,"groupId":"12458","id":12,"instanceNumber":2,"isCurCommit":0,"methodName":"testMultipleParseExceptionsFailure","params":"()","path":"/mnt/clonedata/CloneManagementServer/ManagementServer/consistResult/result-druid-10-0.7/blobInfo/CC_OUT/blobs/eb/ef3fc4596a43b59efefcaf094105a8ae34ab5f.src","preCode":"  public void testMultipleParseExceptionsFailure() throws Exception\n  {\n    reportParseExceptions = false;\n    maxParseExceptions = 2;\n    maxSavedParseExceptions = 2;\n\n    \r\n    insertData();\n\n    final KafkaIndexTask task = createTask(\n        null,\n        new KafkaIndexTaskIOConfig(\n            0,\n            \"sequence0\",\n            new SeekableStreamStartSequenceNumbers<>(topic, ImmutableMap.of(0, 2L), ImmutableSet.of()),\n            new SeekableStreamEndSequenceNumbers<>(topic, ImmutableMap.of(0, 10L)),\n            kafkaServer.consumerProperties(),\n            KafkaSupervisorIOConfig.DEFAULT_POLL_TIMEOUT_MILLIS,\n            true,\n            null,\n            null\n        )\n    );\n\n    final ListenableFuture<TaskStatus> future = runTask(task);\n\n    TaskStatus status = future.get();\n\n    \r\n    Assert.assertEquals(TaskState.FAILED, status.getStatusCode());\n    IndexTaskTest.checkTaskStatusErrorMsgForParseExceptionsExceeded(status);\n\n    \r\n    Assert.assertEquals(3, task.getRunner().getRowIngestionMeters().getProcessed());\n    Assert.assertEquals(0, task.getRunner().getRowIngestionMeters().getProcessedWithError());\n    Assert.assertEquals(3, task.getRunner().getRowIngestionMeters().getUnparseable());\n    Assert.assertEquals(0, task.getRunner().getRowIngestionMeters().getThrownAway());\n\n    \r\n    Assert.assertEquals(ImmutableSet.of(), publishedDescriptors());\n    Assert.assertNull(metadataStorageCoordinator.getDataSourceMetadata(DATA_SCHEMA.getDataSource()));\n\n    IngestionStatsAndErrorsTaskReportData reportData = getTaskReportData();\n\n    Map<String, Object> expectedMetrics = ImmutableMap.of(\n        RowIngestionMeters.BUILD_SEGMENTS,\n        ImmutableMap.of(\n            RowIngestionMeters.PROCESSED, 3,\n            RowIngestionMeters.PROCESSED_WITH_ERROR, 0,\n            RowIngestionMeters.UNPARSEABLE, 3,\n            RowIngestionMeters.THROWN_AWAY, 0\n        )\n    );\n    Assert.assertEquals(expectedMetrics, reportData.getRowStats());\n\n    Map<String, Object> unparseableEvents = ImmutableMap.of(\n        RowIngestionMeters.BUILD_SEGMENTS,\n        Arrays.asList(\n            \"Unable to parse row [unparseable2]\",\n            \"Unable to parse row [unparseable]\"\n        )\n    );\n\n    Assert.assertEquals(unparseableEvents, reportData.getUnparseableEvents());\n  }\n","realPath":"extensions-core/kafka-indexing-service/src/test/java/org/apache/druid/indexing/kafka/KafkaIndexTaskTest.java","repoName":"druid","snippetEndLine":0,"snippetStartLine":0,"startLine":1369,"status":"N"}],"commitId":"f7bfe8f2697277f4216d90607d01bfd26e813ab3","commitMessage":"@@@Update mocking libraries for Java 11 support (#7596)\n\n* update easymock / powermock for to 4.0.2 / 2.0.2 for JDK11 support\n* update tests to use new easymock interfaces\n* fix tests failing due to easymock fixes\n* remove dependency on jmockit\n* fix race condition in ResourcePoolTest","date":"2019-05-07 03:28:56","modifiedFileCount":"7","status":"M","submitter":"Xavier L?aut?"},{"authorTime":"2019-03-22 04:12:22","codes":[{"authorDate":"2019-07-07 00:33:12","commitOrder":8,"curCode":"  public void testMultipleParseExceptionsFailure() throws Exception\n  {\n    reportParseExceptions = false;\n    maxParseExceptions = 2;\n    maxSavedParseExceptions = 2;\n\n    recordSupplier.assign(EasyMock.anyObject());\n    EasyMock.expectLastCall().anyTimes();\n\n    EasyMock.expect(recordSupplier.getEarliestSequenceNumber(EasyMock.anyObject())).andReturn(\"0\").anyTimes();\n\n    recordSupplier.seek(EasyMock.anyObject(), EasyMock.anyString());\n    EasyMock.expectLastCall().anyTimes();\n\n    EasyMock.expect(recordSupplier.poll(EasyMock.anyLong())).andReturn(records.subList(2, 13)).once();\n\n    recordSupplier.close();\n    EasyMock.expectLastCall().once();\n\n    replayAll();\n\n    final KinesisIndexTask task = createTask(\n        null,\n        new KinesisIndexTaskIOConfig(\n            0,\n            \"sequence0\",\n            new SeekableStreamStartSequenceNumbers<>(STREAM, ImmutableMap.of(SHARD_ID1, \"2\"), ImmutableSet.of()),\n            new SeekableStreamEndSequenceNumbers<>(STREAM, ImmutableMap.of(SHARD_ID1, \"9\")),\n            true,\n            null,\n            null,\n            \"awsEndpoint\",\n            null,\n            null,\n            null,\n            null,\n            false\n        )\n    );\n\n    final ListenableFuture<TaskStatus> future = runTask(task);\n\n    TaskStatus status = future.get();\n\n    \r\n    Assert.assertEquals(TaskState.FAILED, status.getStatusCode());\n    verifyAll();\n    IndexTaskTest.checkTaskStatusErrorMsgForParseExceptionsExceeded(status);\n\n    \r\n    Assert.assertEquals(3, task.getRunner().getRowIngestionMeters().getProcessed());\n    Assert.assertEquals(0, task.getRunner().getRowIngestionMeters().getProcessedWithError());\n    Assert.assertEquals(3, task.getRunner().getRowIngestionMeters().getUnparseable());\n    Assert.assertEquals(0, task.getRunner().getRowIngestionMeters().getThrownAway());\n\n    \r\n    Assert.assertEquals(ImmutableSet.of(), publishedDescriptors());\n    Assert.assertNull(metadataStorageCoordinator.getDataSourceMetadata(DATA_SCHEMA.getDataSource()));\n\n    IngestionStatsAndErrorsTaskReportData reportData = getTaskReportData();\n\n    Map<String, Object> expectedMetrics = ImmutableMap.of(\n        RowIngestionMeters.BUILD_SEGMENTS,\n        ImmutableMap.of(\n            RowIngestionMeters.PROCESSED, 3,\n            RowIngestionMeters.PROCESSED_WITH_ERROR, 0,\n            RowIngestionMeters.UNPARSEABLE, 3,\n            RowIngestionMeters.THROWN_AWAY, 0\n        )\n    );\n    Assert.assertEquals(expectedMetrics, reportData.getRowStats());\n\n    Map<String, Object> unparseableEvents = ImmutableMap.of(\n        RowIngestionMeters.BUILD_SEGMENTS,\n        Arrays.asList(\n            \"Unable to parse row [unparseable2]\",\n            \"Unable to parse row [unparseable]\"\n        )\n    );\n\n    Assert.assertEquals(unparseableEvents, reportData.getUnparseableEvents());\n  }\n","date":"2019-07-07 00:33:12","endLine":1430,"groupId":"14312","id":13,"instanceNumber":1,"isCurCommit":0,"methodName":"testMultipleParseExceptionsFailure","params":"()","path":"/mnt/clonedata/CloneManagementServer/ManagementServer/consistResult/result-druid-10-0.7/blobInfo/CC_OUT/blobs/d8/bf83ce2a3969f1243df5bd1dad6429b383e83e.src","preCode":"  public void testMultipleParseExceptionsFailure() throws Exception\n  {\n    reportParseExceptions = false;\n    maxParseExceptions = 2;\n    maxSavedParseExceptions = 2;\n\n    recordSupplier.assign(anyObject());\n    expectLastCall().anyTimes();\n\n    expect(recordSupplier.getEarliestSequenceNumber(anyObject())).andReturn(\"0\").anyTimes();\n\n    recordSupplier.seek(anyObject(), anyString());\n    expectLastCall().anyTimes();\n\n    expect(recordSupplier.poll(anyLong())).andReturn(records.subList(2, 13)).once();\n\n    recordSupplier.close();\n    expectLastCall().once();\n\n    replayAll();\n\n    final KinesisIndexTask task = createTask(\n        null,\n        new KinesisIndexTaskIOConfig(\n            0,\n            \"sequence0\",\n            new SeekableStreamStartSequenceNumbers<>(stream, ImmutableMap.of(shardId1, \"2\"), ImmutableSet.of()),\n            new SeekableStreamEndSequenceNumbers<>(stream, ImmutableMap.of(shardId1, \"9\")),\n            true,\n            null,\n            null,\n            \"awsEndpoint\",\n            null,\n            null,\n            null,\n            null,\n            false\n        )\n    );\n\n    final ListenableFuture<TaskStatus> future = runTask(task);\n\n    TaskStatus status = future.get();\n\n    \r\n    Assert.assertEquals(TaskState.FAILED, status.getStatusCode());\n    verifyAll();\n    IndexTaskTest.checkTaskStatusErrorMsgForParseExceptionsExceeded(status);\n\n    \r\n    Assert.assertEquals(3, task.getRunner().getRowIngestionMeters().getProcessed());\n    Assert.assertEquals(0, task.getRunner().getRowIngestionMeters().getProcessedWithError());\n    Assert.assertEquals(3, task.getRunner().getRowIngestionMeters().getUnparseable());\n    Assert.assertEquals(0, task.getRunner().getRowIngestionMeters().getThrownAway());\n\n    \r\n    Assert.assertEquals(ImmutableSet.of(), publishedDescriptors());\n    Assert.assertNull(metadataStorageCoordinator.getDataSourceMetadata(DATA_SCHEMA.getDataSource()));\n\n    IngestionStatsAndErrorsTaskReportData reportData = getTaskReportData();\n\n    Map<String, Object> expectedMetrics = ImmutableMap.of(\n        RowIngestionMeters.BUILD_SEGMENTS,\n        ImmutableMap.of(\n            RowIngestionMeters.PROCESSED, 3,\n            RowIngestionMeters.PROCESSED_WITH_ERROR, 0,\n            RowIngestionMeters.UNPARSEABLE, 3,\n            RowIngestionMeters.THROWN_AWAY, 0\n        )\n    );\n    Assert.assertEquals(expectedMetrics, reportData.getRowStats());\n\n    Map<String, Object> unparseableEvents = ImmutableMap.of(\n        RowIngestionMeters.BUILD_SEGMENTS,\n        Arrays.asList(\n            \"Unable to parse row [unparseable2]\",\n            \"Unable to parse row [unparseable]\"\n        )\n    );\n\n    Assert.assertEquals(unparseableEvents, reportData.getUnparseableEvents());\n  }\n","realPath":"extensions-core/kinesis-indexing-service/src/test/java/org/apache/druid/indexing/kinesis/KinesisIndexTaskTest.java","repoName":"druid","snippetEndLine":0,"snippetStartLine":0,"startLine":1349,"status":"M"},{"authorDate":"2019-03-22 04:12:22","commitOrder":8,"curCode":"  public void testMultipleParseExceptionsFailure() throws Exception\n  {\n    reportParseExceptions = false;\n    maxParseExceptions = 2;\n    maxSavedParseExceptions = 2;\n\n    \r\n    insertData();\n\n    final KafkaIndexTask task = createTask(\n        null,\n        new KafkaIndexTaskIOConfig(\n            0,\n            \"sequence0\",\n            new SeekableStreamStartSequenceNumbers<>(topic, ImmutableMap.of(0, 2L), ImmutableSet.of()),\n            new SeekableStreamEndSequenceNumbers<>(topic, ImmutableMap.of(0, 10L)),\n            kafkaServer.consumerProperties(),\n            KafkaSupervisorIOConfig.DEFAULT_POLL_TIMEOUT_MILLIS,\n            true,\n            null,\n            null\n        )\n    );\n\n    final ListenableFuture<TaskStatus> future = runTask(task);\n\n    TaskStatus status = future.get();\n\n    \r\n    Assert.assertEquals(TaskState.FAILED, status.getStatusCode());\n    IndexTaskTest.checkTaskStatusErrorMsgForParseExceptionsExceeded(status);\n\n    \r\n    Assert.assertEquals(3, task.getRunner().getRowIngestionMeters().getProcessed());\n    Assert.assertEquals(0, task.getRunner().getRowIngestionMeters().getProcessedWithError());\n    Assert.assertEquals(3, task.getRunner().getRowIngestionMeters().getUnparseable());\n    Assert.assertEquals(0, task.getRunner().getRowIngestionMeters().getThrownAway());\n\n    \r\n    Assert.assertEquals(ImmutableSet.of(), publishedDescriptors());\n    Assert.assertNull(metadataStorageCoordinator.getDataSourceMetadata(DATA_SCHEMA.getDataSource()));\n\n    IngestionStatsAndErrorsTaskReportData reportData = getTaskReportData();\n\n    Map<String, Object> expectedMetrics = ImmutableMap.of(\n        RowIngestionMeters.BUILD_SEGMENTS,\n        ImmutableMap.of(\n            RowIngestionMeters.PROCESSED, 3,\n            RowIngestionMeters.PROCESSED_WITH_ERROR, 0,\n            RowIngestionMeters.UNPARSEABLE, 3,\n            RowIngestionMeters.THROWN_AWAY, 0\n        )\n    );\n    Assert.assertEquals(expectedMetrics, reportData.getRowStats());\n\n    Map<String, Object> unparseableEvents = ImmutableMap.of(\n        RowIngestionMeters.BUILD_SEGMENTS,\n        Arrays.asList(\n            \"Unable to parse row [unparseable2]\",\n            \"Unable to parse row [unparseable]\"\n        )\n    );\n\n    Assert.assertEquals(unparseableEvents, reportData.getUnparseableEvents());\n  }\n","date":"2019-03-22 04:12:22","endLine":1433,"groupId":"12458","id":14,"instanceNumber":2,"isCurCommit":0,"methodName":"testMultipleParseExceptionsFailure","params":"()","path":"/mnt/clonedata/CloneManagementServer/ManagementServer/consistResult/result-druid-10-0.7/blobInfo/CC_OUT/blobs/eb/ef3fc4596a43b59efefcaf094105a8ae34ab5f.src","preCode":"  public void testMultipleParseExceptionsFailure() throws Exception\n  {\n    reportParseExceptions = false;\n    maxParseExceptions = 2;\n    maxSavedParseExceptions = 2;\n\n    \r\n    insertData();\n\n    final KafkaIndexTask task = createTask(\n        null,\n        new KafkaIndexTaskIOConfig(\n            0,\n            \"sequence0\",\n            new SeekableStreamStartSequenceNumbers<>(topic, ImmutableMap.of(0, 2L), ImmutableSet.of()),\n            new SeekableStreamEndSequenceNumbers<>(topic, ImmutableMap.of(0, 10L)),\n            kafkaServer.consumerProperties(),\n            KafkaSupervisorIOConfig.DEFAULT_POLL_TIMEOUT_MILLIS,\n            true,\n            null,\n            null\n        )\n    );\n\n    final ListenableFuture<TaskStatus> future = runTask(task);\n\n    TaskStatus status = future.get();\n\n    \r\n    Assert.assertEquals(TaskState.FAILED, status.getStatusCode());\n    IndexTaskTest.checkTaskStatusErrorMsgForParseExceptionsExceeded(status);\n\n    \r\n    Assert.assertEquals(3, task.getRunner().getRowIngestionMeters().getProcessed());\n    Assert.assertEquals(0, task.getRunner().getRowIngestionMeters().getProcessedWithError());\n    Assert.assertEquals(3, task.getRunner().getRowIngestionMeters().getUnparseable());\n    Assert.assertEquals(0, task.getRunner().getRowIngestionMeters().getThrownAway());\n\n    \r\n    Assert.assertEquals(ImmutableSet.of(), publishedDescriptors());\n    Assert.assertNull(metadataStorageCoordinator.getDataSourceMetadata(DATA_SCHEMA.getDataSource()));\n\n    IngestionStatsAndErrorsTaskReportData reportData = getTaskReportData();\n\n    Map<String, Object> expectedMetrics = ImmutableMap.of(\n        RowIngestionMeters.BUILD_SEGMENTS,\n        ImmutableMap.of(\n            RowIngestionMeters.PROCESSED, 3,\n            RowIngestionMeters.PROCESSED_WITH_ERROR, 0,\n            RowIngestionMeters.UNPARSEABLE, 3,\n            RowIngestionMeters.THROWN_AWAY, 0\n        )\n    );\n    Assert.assertEquals(expectedMetrics, reportData.getRowStats());\n\n    Map<String, Object> unparseableEvents = ImmutableMap.of(\n        RowIngestionMeters.BUILD_SEGMENTS,\n        Arrays.asList(\n            \"Unable to parse row [unparseable2]\",\n            \"Unable to parse row [unparseable]\"\n        )\n    );\n\n    Assert.assertEquals(unparseableEvents, reportData.getUnparseableEvents());\n  }\n","realPath":"extensions-core/kafka-indexing-service/src/test/java/org/apache/druid/indexing/kafka/KafkaIndexTaskTest.java","repoName":"druid","snippetEndLine":0,"snippetStartLine":0,"startLine":1369,"status":"N"}],"commitId":"1166bbcb75d432817715fdd429737f86730b5591","commitMessage":"@@@Remove static imports from tests (#8036)\n\nMake static imports forbidden in tests and remove all occurrences to be\nconsistent with the non-test code.\n\nAlso.  various changes to files affected by above:\n- Reformat to adhere to druid style guide\n- Fix various IntelliJ warnings\n- Fix various SonarLint warnings (e.g..  the expected/actual args to\n  Assert.assertEquals() were flipped)","date":"2019-07-07 00:33:12","modifiedFileCount":"98","status":"M","submitter":"Chi Cao Minh"},{"authorTime":"2019-07-25 08:35:46","codes":[{"authorDate":"2019-07-25 08:35:46","commitOrder":9,"curCode":"  public void testMultipleParseExceptionsFailure() throws Exception\n  {\n    reportParseExceptions = false;\n    maxParseExceptions = 2;\n    maxSavedParseExceptions = 2;\n\n    recordSupplier.assign(EasyMock.anyObject());\n    EasyMock.expectLastCall().anyTimes();\n\n    EasyMock.expect(recordSupplier.getEarliestSequenceNumber(EasyMock.anyObject())).andReturn(\"0\").anyTimes();\n\n    recordSupplier.seek(EasyMock.anyObject(), EasyMock.anyString());\n    EasyMock.expectLastCall().anyTimes();\n\n    EasyMock.expect(recordSupplier.poll(EasyMock.anyLong())).andReturn(records.subList(2, 13)).once();\n\n    recordSupplier.close();\n    EasyMock.expectLastCall().once();\n\n    replayAll();\n\n    final KinesisIndexTask task = createTask(\n        null,\n        new KinesisIndexTaskIOConfig(\n            0,\n            \"sequence0\",\n            new SeekableStreamStartSequenceNumbers<>(STREAM, ImmutableMap.of(SHARD_ID1, \"2\"), ImmutableSet.of()),\n            new SeekableStreamEndSequenceNumbers<>(STREAM, ImmutableMap.of(SHARD_ID1, \"9\")),\n            true,\n            null,\n            null,\n            \"awsEndpoint\",\n            null,\n            null,\n            null,\n            null,\n            false\n        )\n    );\n\n    final ListenableFuture<TaskStatus> future = runTask(task);\n\n    TaskStatus status = future.get();\n\n    \r\n    Assert.assertEquals(TaskState.FAILED, status.getStatusCode());\n    verifyAll();\n    IndexTaskTest.checkTaskStatusErrorMsgForParseExceptionsExceeded(status);\n\n    \r\n    Assert.assertEquals(3, task.getRunner().getRowIngestionMeters().getProcessed());\n    Assert.assertEquals(0, task.getRunner().getRowIngestionMeters().getProcessedWithError());\n    Assert.assertEquals(3, task.getRunner().getRowIngestionMeters().getUnparseable());\n    Assert.assertEquals(0, task.getRunner().getRowIngestionMeters().getThrownAway());\n\n    \r\n    Assert.assertEquals(ImmutableList.of(), publishedDescriptors());\n    Assert.assertNull(metadataStorageCoordinator.getDataSourceMetadata(DATA_SCHEMA.getDataSource()));\n\n    IngestionStatsAndErrorsTaskReportData reportData = getTaskReportData();\n\n    Map<String, Object> expectedMetrics = ImmutableMap.of(\n        RowIngestionMeters.BUILD_SEGMENTS,\n        ImmutableMap.of(\n            RowIngestionMeters.PROCESSED, 3,\n            RowIngestionMeters.PROCESSED_WITH_ERROR, 0,\n            RowIngestionMeters.UNPARSEABLE, 3,\n            RowIngestionMeters.THROWN_AWAY, 0\n        )\n    );\n    Assert.assertEquals(expectedMetrics, reportData.getRowStats());\n\n    Map<String, Object> unparseableEvents = ImmutableMap.of(\n        RowIngestionMeters.BUILD_SEGMENTS,\n        Arrays.asList(\n            \"Unable to parse row [unparseable2]\",\n            \"Unable to parse row [unparseable]\"\n        )\n    );\n\n    Assert.assertEquals(unparseableEvents, reportData.getUnparseableEvents());\n  }\n","date":"2019-07-25 08:35:46","endLine":1462,"groupId":"14312","id":15,"instanceNumber":1,"isCurCommit":0,"methodName":"testMultipleParseExceptionsFailure","params":"()","path":"/mnt/clonedata/CloneManagementServer/ManagementServer/consistResult/result-druid-10-0.7/blobInfo/CC_OUT/blobs/00/6677b0a28185be95d0eb677acfab7ede656120.src","preCode":"  public void testMultipleParseExceptionsFailure() throws Exception\n  {\n    reportParseExceptions = false;\n    maxParseExceptions = 2;\n    maxSavedParseExceptions = 2;\n\n    recordSupplier.assign(EasyMock.anyObject());\n    EasyMock.expectLastCall().anyTimes();\n\n    EasyMock.expect(recordSupplier.getEarliestSequenceNumber(EasyMock.anyObject())).andReturn(\"0\").anyTimes();\n\n    recordSupplier.seek(EasyMock.anyObject(), EasyMock.anyString());\n    EasyMock.expectLastCall().anyTimes();\n\n    EasyMock.expect(recordSupplier.poll(EasyMock.anyLong())).andReturn(records.subList(2, 13)).once();\n\n    recordSupplier.close();\n    EasyMock.expectLastCall().once();\n\n    replayAll();\n\n    final KinesisIndexTask task = createTask(\n        null,\n        new KinesisIndexTaskIOConfig(\n            0,\n            \"sequence0\",\n            new SeekableStreamStartSequenceNumbers<>(STREAM, ImmutableMap.of(SHARD_ID1, \"2\"), ImmutableSet.of()),\n            new SeekableStreamEndSequenceNumbers<>(STREAM, ImmutableMap.of(SHARD_ID1, \"9\")),\n            true,\n            null,\n            null,\n            \"awsEndpoint\",\n            null,\n            null,\n            null,\n            null,\n            false\n        )\n    );\n\n    final ListenableFuture<TaskStatus> future = runTask(task);\n\n    TaskStatus status = future.get();\n\n    \r\n    Assert.assertEquals(TaskState.FAILED, status.getStatusCode());\n    verifyAll();\n    IndexTaskTest.checkTaskStatusErrorMsgForParseExceptionsExceeded(status);\n\n    \r\n    Assert.assertEquals(3, task.getRunner().getRowIngestionMeters().getProcessed());\n    Assert.assertEquals(0, task.getRunner().getRowIngestionMeters().getProcessedWithError());\n    Assert.assertEquals(3, task.getRunner().getRowIngestionMeters().getUnparseable());\n    Assert.assertEquals(0, task.getRunner().getRowIngestionMeters().getThrownAway());\n\n    \r\n    Assert.assertEquals(ImmutableSet.of(), publishedDescriptors());\n    Assert.assertNull(metadataStorageCoordinator.getDataSourceMetadata(DATA_SCHEMA.getDataSource()));\n\n    IngestionStatsAndErrorsTaskReportData reportData = getTaskReportData();\n\n    Map<String, Object> expectedMetrics = ImmutableMap.of(\n        RowIngestionMeters.BUILD_SEGMENTS,\n        ImmutableMap.of(\n            RowIngestionMeters.PROCESSED, 3,\n            RowIngestionMeters.PROCESSED_WITH_ERROR, 0,\n            RowIngestionMeters.UNPARSEABLE, 3,\n            RowIngestionMeters.THROWN_AWAY, 0\n        )\n    );\n    Assert.assertEquals(expectedMetrics, reportData.getRowStats());\n\n    Map<String, Object> unparseableEvents = ImmutableMap.of(\n        RowIngestionMeters.BUILD_SEGMENTS,\n        Arrays.asList(\n            \"Unable to parse row [unparseable2]\",\n            \"Unable to parse row [unparseable]\"\n        )\n    );\n\n    Assert.assertEquals(unparseableEvents, reportData.getUnparseableEvents());\n  }\n","realPath":"extensions-core/kinesis-indexing-service/src/test/java/org/apache/druid/indexing/kinesis/KinesisIndexTaskTest.java","repoName":"druid","snippetEndLine":0,"snippetStartLine":0,"startLine":1381,"status":"M"},{"authorDate":"2019-07-25 08:35:46","commitOrder":9,"curCode":"  public void testMultipleParseExceptionsFailure() throws Exception\n  {\n    reportParseExceptions = false;\n    maxParseExceptions = 2;\n    maxSavedParseExceptions = 2;\n\n    \r\n    insertData();\n\n    final KafkaIndexTask task = createTask(\n        null,\n        new KafkaIndexTaskIOConfig(\n            0,\n            \"sequence0\",\n            new SeekableStreamStartSequenceNumbers<>(topic, ImmutableMap.of(0, 2L), ImmutableSet.of()),\n            new SeekableStreamEndSequenceNumbers<>(topic, ImmutableMap.of(0, 10L)),\n            kafkaServer.consumerProperties(),\n            KafkaSupervisorIOConfig.DEFAULT_POLL_TIMEOUT_MILLIS,\n            true,\n            null,\n            null\n        )\n    );\n\n    final ListenableFuture<TaskStatus> future = runTask(task);\n\n    TaskStatus status = future.get();\n\n    \r\n    Assert.assertEquals(TaskState.FAILED, status.getStatusCode());\n    IndexTaskTest.checkTaskStatusErrorMsgForParseExceptionsExceeded(status);\n\n    \r\n    Assert.assertEquals(3, task.getRunner().getRowIngestionMeters().getProcessed());\n    Assert.assertEquals(0, task.getRunner().getRowIngestionMeters().getProcessedWithError());\n    Assert.assertEquals(3, task.getRunner().getRowIngestionMeters().getUnparseable());\n    Assert.assertEquals(0, task.getRunner().getRowIngestionMeters().getThrownAway());\n\n    \r\n    Assert.assertEquals(ImmutableList.of(), publishedDescriptors());\n    Assert.assertNull(metadataStorageCoordinator.getDataSourceMetadata(DATA_SCHEMA.getDataSource()));\n\n    IngestionStatsAndErrorsTaskReportData reportData = getTaskReportData();\n\n    Map<String, Object> expectedMetrics = ImmutableMap.of(\n        RowIngestionMeters.BUILD_SEGMENTS,\n        ImmutableMap.of(\n            RowIngestionMeters.PROCESSED, 3,\n            RowIngestionMeters.PROCESSED_WITH_ERROR, 0,\n            RowIngestionMeters.UNPARSEABLE, 3,\n            RowIngestionMeters.THROWN_AWAY, 0\n        )\n    );\n    Assert.assertEquals(expectedMetrics, reportData.getRowStats());\n\n    Map<String, Object> unparseableEvents = ImmutableMap.of(\n        RowIngestionMeters.BUILD_SEGMENTS,\n        Arrays.asList(\n            \"Unable to parse row [unparseable2]\",\n            \"Unable to parse row [unparseable]\"\n        )\n    );\n\n    Assert.assertEquals(unparseableEvents, reportData.getUnparseableEvents());\n  }\n","date":"2019-07-25 08:35:46","endLine":1431,"groupId":"12458","id":16,"instanceNumber":2,"isCurCommit":0,"methodName":"testMultipleParseExceptionsFailure","params":"()","path":"/mnt/clonedata/CloneManagementServer/ManagementServer/consistResult/result-druid-10-0.7/blobInfo/CC_OUT/blobs/87/f522eae18ccef1c23a5fd9ae035dbbe3410dfc.src","preCode":"  public void testMultipleParseExceptionsFailure() throws Exception\n  {\n    reportParseExceptions = false;\n    maxParseExceptions = 2;\n    maxSavedParseExceptions = 2;\n\n    \r\n    insertData();\n\n    final KafkaIndexTask task = createTask(\n        null,\n        new KafkaIndexTaskIOConfig(\n            0,\n            \"sequence0\",\n            new SeekableStreamStartSequenceNumbers<>(topic, ImmutableMap.of(0, 2L), ImmutableSet.of()),\n            new SeekableStreamEndSequenceNumbers<>(topic, ImmutableMap.of(0, 10L)),\n            kafkaServer.consumerProperties(),\n            KafkaSupervisorIOConfig.DEFAULT_POLL_TIMEOUT_MILLIS,\n            true,\n            null,\n            null\n        )\n    );\n\n    final ListenableFuture<TaskStatus> future = runTask(task);\n\n    TaskStatus status = future.get();\n\n    \r\n    Assert.assertEquals(TaskState.FAILED, status.getStatusCode());\n    IndexTaskTest.checkTaskStatusErrorMsgForParseExceptionsExceeded(status);\n\n    \r\n    Assert.assertEquals(3, task.getRunner().getRowIngestionMeters().getProcessed());\n    Assert.assertEquals(0, task.getRunner().getRowIngestionMeters().getProcessedWithError());\n    Assert.assertEquals(3, task.getRunner().getRowIngestionMeters().getUnparseable());\n    Assert.assertEquals(0, task.getRunner().getRowIngestionMeters().getThrownAway());\n\n    \r\n    Assert.assertEquals(ImmutableSet.of(), publishedDescriptors());\n    Assert.assertNull(metadataStorageCoordinator.getDataSourceMetadata(DATA_SCHEMA.getDataSource()));\n\n    IngestionStatsAndErrorsTaskReportData reportData = getTaskReportData();\n\n    Map<String, Object> expectedMetrics = ImmutableMap.of(\n        RowIngestionMeters.BUILD_SEGMENTS,\n        ImmutableMap.of(\n            RowIngestionMeters.PROCESSED, 3,\n            RowIngestionMeters.PROCESSED_WITH_ERROR, 0,\n            RowIngestionMeters.UNPARSEABLE, 3,\n            RowIngestionMeters.THROWN_AWAY, 0\n        )\n    );\n    Assert.assertEquals(expectedMetrics, reportData.getRowStats());\n\n    Map<String, Object> unparseableEvents = ImmutableMap.of(\n        RowIngestionMeters.BUILD_SEGMENTS,\n        Arrays.asList(\n            \"Unable to parse row [unparseable2]\",\n            \"Unable to parse row [unparseable]\"\n        )\n    );\n\n    Assert.assertEquals(unparseableEvents, reportData.getUnparseableEvents());\n  }\n","realPath":"extensions-core/kafka-indexing-service/src/test/java/org/apache/druid/indexing/kafka/KafkaIndexTaskTest.java","repoName":"druid","snippetEndLine":0,"snippetStartLine":0,"startLine":1367,"status":"M"}],"commitId":"db149462073d59e7563f0d3834e69d44a2bb4011","commitMessage":"@@@Add support minor compaction with segment locking (#7547)\n\n* Segment locking\n\n* Allow both timeChunk and segment lock in the same gruop\n\n* fix it test\n\n* Fix adding same chunk to atomicUpdateGroup\n\n* resolving todos\n\n* Fix segments to lock\n\n* fix segments to lock\n\n* fix kill task\n\n* resolving todos\n\n* resolving todos\n\n* fix teamcity\n\n* remove unused class\n\n* fix single map\n\n* resolving todos\n\n* fix build\n\n* fix SQLMetadataSegmentManager\n\n* fix findInputSegments\n\n* adding more tests\n\n* fixing task lock checks\n\n* add SegmentTransactionalOverwriteAction\n\n* changing publisher\n\n* fixing something\n\n* fix for perfect rollup\n\n* fix test\n\n* adjust package-lock.json\n\n* fix test\n\n* fix style\n\n* adding javadocs\n\n* remove unused classes\n\n* add more javadocs\n\n* unused import\n\n* fix test\n\n* fix test\n\n* Support forceTimeChunk context and force timeChunk lock for parallel index task if intervals are missing\n\n* fix travis\n\n* fix travis\n\n* unused import\n\n* spotbug\n\n* revert getMaxVersion\n\n* address comments\n\n* fix tc\n\n* add missing error handling\n\n* fix backward compatibility\n\n* unused import\n\n* Fix perf of versionedIntervalTimeline\n\n* fix timeline\n\n* fix tc\n\n* remove remaining todos\n\n* add comment for parallel index\n\n* fix javadoc and typos\n\n* typo\n\n* address comments\n","date":"2019-07-25 08:35:46","modifiedFileCount":"130","status":"M","submitter":"Jihoon Son"},{"authorTime":"2019-11-21 06:51:25","codes":[{"authorDate":"2019-11-21 06:51:25","commitOrder":10,"curCode":"  public void testMultipleParseExceptionsFailure() throws Exception\n  {\n    reportParseExceptions = false;\n    maxParseExceptions = 2;\n    maxSavedParseExceptions = 2;\n\n    recordSupplier.assign(EasyMock.anyObject());\n    EasyMock.expectLastCall().anyTimes();\n\n    EasyMock.expect(recordSupplier.getEarliestSequenceNumber(EasyMock.anyObject())).andReturn(\"0\").anyTimes();\n\n    recordSupplier.seek(EasyMock.anyObject(), EasyMock.anyString());\n    EasyMock.expectLastCall().anyTimes();\n\n    EasyMock.expect(recordSupplier.poll(EasyMock.anyLong())).andReturn(records.subList(2, 13)).once();\n\n    recordSupplier.close();\n    EasyMock.expectLastCall().once();\n\n    replayAll();\n\n    final KinesisIndexTask task = createTask(\n        null,\n        new KinesisIndexTaskIOConfig(\n            0,\n            \"sequence0\",\n            new SeekableStreamStartSequenceNumbers<>(STREAM, ImmutableMap.of(SHARD_ID1, \"2\"), ImmutableSet.of()),\n            new SeekableStreamEndSequenceNumbers<>(STREAM, ImmutableMap.of(SHARD_ID1, \"9\")),\n            true,\n            null,\n            null,\n            INPUT_FORMAT,\n            \"awsEndpoint\",\n            null,\n            null,\n            null,\n            null,\n            false\n        )\n    );\n\n    final ListenableFuture<TaskStatus> future = runTask(task);\n\n    TaskStatus status = future.get();\n\n    \r\n    Assert.assertEquals(TaskState.FAILED, status.getStatusCode());\n    verifyAll();\n    IndexTaskTest.checkTaskStatusErrorMsgForParseExceptionsExceeded(status);\n\n    \r\n    Assert.assertEquals(3, task.getRunner().getRowIngestionMeters().getProcessed());\n    Assert.assertEquals(0, task.getRunner().getRowIngestionMeters().getProcessedWithError());\n    Assert.assertEquals(3, task.getRunner().getRowIngestionMeters().getUnparseable());\n    Assert.assertEquals(0, task.getRunner().getRowIngestionMeters().getThrownAway());\n\n    \r\n    Assert.assertEquals(ImmutableList.of(), publishedDescriptors());\n    Assert.assertNull(metadataStorageCoordinator.getDataSourceMetadata(NEW_DATA_SCHEMA.getDataSource()));\n\n    IngestionStatsAndErrorsTaskReportData reportData = getTaskReportData();\n\n    Map<String, Object> expectedMetrics = ImmutableMap.of(\n        RowIngestionMeters.BUILD_SEGMENTS,\n        ImmutableMap.of(\n            RowIngestionMeters.PROCESSED, 3,\n            RowIngestionMeters.PROCESSED_WITH_ERROR, 0,\n            RowIngestionMeters.UNPARSEABLE, 3,\n            RowIngestionMeters.THROWN_AWAY, 0\n        )\n    );\n    Assert.assertEquals(expectedMetrics, reportData.getRowStats());\n\n    Map<String, Object> unparseableEvents = ImmutableMap.of(\n        RowIngestionMeters.BUILD_SEGMENTS,\n        Arrays.asList(\n            \"Unable to parse row [unparseable2]\",\n            \"Unable to parse row [unparseable]\"\n        )\n    );\n\n    Assert.assertEquals(unparseableEvents, reportData.getUnparseableEvents());\n  }\n","date":"2019-11-21 06:51:25","endLine":1428,"groupId":"3035","id":17,"instanceNumber":1,"isCurCommit":0,"methodName":"testMultipleParseExceptionsFailure","params":"()","path":"/mnt/clonedata/CloneManagementServer/ManagementServer/consistResult/result-druid-10-0.7/blobInfo/CC_OUT/blobs/a3/34a790bdbace996db1007835d408c9c203e38c.src","preCode":"  public void testMultipleParseExceptionsFailure() throws Exception\n  {\n    reportParseExceptions = false;\n    maxParseExceptions = 2;\n    maxSavedParseExceptions = 2;\n\n    recordSupplier.assign(EasyMock.anyObject());\n    EasyMock.expectLastCall().anyTimes();\n\n    EasyMock.expect(recordSupplier.getEarliestSequenceNumber(EasyMock.anyObject())).andReturn(\"0\").anyTimes();\n\n    recordSupplier.seek(EasyMock.anyObject(), EasyMock.anyString());\n    EasyMock.expectLastCall().anyTimes();\n\n    EasyMock.expect(recordSupplier.poll(EasyMock.anyLong())).andReturn(records.subList(2, 13)).once();\n\n    recordSupplier.close();\n    EasyMock.expectLastCall().once();\n\n    replayAll();\n\n    final KinesisIndexTask task = createTask(\n        null,\n        new KinesisIndexTaskIOConfig(\n            0,\n            \"sequence0\",\n            new SeekableStreamStartSequenceNumbers<>(STREAM, ImmutableMap.of(SHARD_ID1, \"2\"), ImmutableSet.of()),\n            new SeekableStreamEndSequenceNumbers<>(STREAM, ImmutableMap.of(SHARD_ID1, \"9\")),\n            true,\n            null,\n            null,\n            \"awsEndpoint\",\n            null,\n            null,\n            null,\n            null,\n            false\n        )\n    );\n\n    final ListenableFuture<TaskStatus> future = runTask(task);\n\n    TaskStatus status = future.get();\n\n    \r\n    Assert.assertEquals(TaskState.FAILED, status.getStatusCode());\n    verifyAll();\n    IndexTaskTest.checkTaskStatusErrorMsgForParseExceptionsExceeded(status);\n\n    \r\n    Assert.assertEquals(3, task.getRunner().getRowIngestionMeters().getProcessed());\n    Assert.assertEquals(0, task.getRunner().getRowIngestionMeters().getProcessedWithError());\n    Assert.assertEquals(3, task.getRunner().getRowIngestionMeters().getUnparseable());\n    Assert.assertEquals(0, task.getRunner().getRowIngestionMeters().getThrownAway());\n\n    \r\n    Assert.assertEquals(ImmutableList.of(), publishedDescriptors());\n    Assert.assertNull(metadataStorageCoordinator.getDataSourceMetadata(DATA_SCHEMA.getDataSource()));\n\n    IngestionStatsAndErrorsTaskReportData reportData = getTaskReportData();\n\n    Map<String, Object> expectedMetrics = ImmutableMap.of(\n        RowIngestionMeters.BUILD_SEGMENTS,\n        ImmutableMap.of(\n            RowIngestionMeters.PROCESSED, 3,\n            RowIngestionMeters.PROCESSED_WITH_ERROR, 0,\n            RowIngestionMeters.UNPARSEABLE, 3,\n            RowIngestionMeters.THROWN_AWAY, 0\n        )\n    );\n    Assert.assertEquals(expectedMetrics, reportData.getRowStats());\n\n    Map<String, Object> unparseableEvents = ImmutableMap.of(\n        RowIngestionMeters.BUILD_SEGMENTS,\n        Arrays.asList(\n            \"Unable to parse row [unparseable2]\",\n            \"Unable to parse row [unparseable]\"\n        )\n    );\n\n    Assert.assertEquals(unparseableEvents, reportData.getUnparseableEvents());\n  }\n","realPath":"extensions-core/kinesis-indexing-service/src/test/java/org/apache/druid/indexing/kinesis/KinesisIndexTaskTest.java","repoName":"druid","snippetEndLine":0,"snippetStartLine":0,"startLine":1346,"status":"M"},{"authorDate":"2019-11-21 06:51:25","commitOrder":10,"curCode":"  public void testMultipleParseExceptionsFailure() throws Exception\n  {\n    reportParseExceptions = false;\n    maxParseExceptions = 2;\n    maxSavedParseExceptions = 2;\n\n    \r\n    insertData();\n\n    final KafkaIndexTask task = createTask(\n        null,\n        new KafkaIndexTaskIOConfig(\n            0,\n            \"sequence0\",\n            new SeekableStreamStartSequenceNumbers<>(topic, ImmutableMap.of(0, 2L), ImmutableSet.of()),\n            new SeekableStreamEndSequenceNumbers<>(topic, ImmutableMap.of(0, 10L)),\n            kafkaServer.consumerProperties(),\n            KafkaSupervisorIOConfig.DEFAULT_POLL_TIMEOUT_MILLIS,\n            true,\n            null,\n            null,\n            INPUT_FORMAT\n        )\n    );\n\n    final ListenableFuture<TaskStatus> future = runTask(task);\n\n    TaskStatus status = future.get();\n\n    \r\n    Assert.assertEquals(TaskState.FAILED, status.getStatusCode());\n    IndexTaskTest.checkTaskStatusErrorMsgForParseExceptionsExceeded(status);\n\n    \r\n    Assert.assertEquals(3, task.getRunner().getRowIngestionMeters().getProcessed());\n    Assert.assertEquals(0, task.getRunner().getRowIngestionMeters().getProcessedWithError());\n    Assert.assertEquals(3, task.getRunner().getRowIngestionMeters().getUnparseable());\n    Assert.assertEquals(0, task.getRunner().getRowIngestionMeters().getThrownAway());\n\n    \r\n    Assert.assertEquals(ImmutableList.of(), publishedDescriptors());\n    Assert.assertNull(metadataStorageCoordinator.getDataSourceMetadata(NEW_DATA_SCHEMA.getDataSource()));\n\n    IngestionStatsAndErrorsTaskReportData reportData = getTaskReportData();\n\n    Map<String, Object> expectedMetrics = ImmutableMap.of(\n        RowIngestionMeters.BUILD_SEGMENTS,\n        ImmutableMap.of(\n            RowIngestionMeters.PROCESSED, 3,\n            RowIngestionMeters.PROCESSED_WITH_ERROR, 0,\n            RowIngestionMeters.UNPARSEABLE, 3,\n            RowIngestionMeters.THROWN_AWAY, 0\n        )\n    );\n    Assert.assertEquals(expectedMetrics, reportData.getRowStats());\n\n    Map<String, Object> unparseableEvents = ImmutableMap.of(\n        RowIngestionMeters.BUILD_SEGMENTS,\n        Arrays.asList(\n            \"Unable to parse row [unparseable2]\",\n            \"Unable to parse row [unparseable]\"\n        )\n    );\n\n    Assert.assertEquals(unparseableEvents, reportData.getUnparseableEvents());\n  }\n","date":"2019-11-21 06:51:25","endLine":1429,"groupId":"12458","id":18,"instanceNumber":2,"isCurCommit":0,"methodName":"testMultipleParseExceptionsFailure","params":"()","path":"/mnt/clonedata/CloneManagementServer/ManagementServer/consistResult/result-druid-10-0.7/blobInfo/CC_OUT/blobs/e8/bde11469c6f5c09cd43b1c5d562e5b78e0f593.src","preCode":"  public void testMultipleParseExceptionsFailure() throws Exception\n  {\n    reportParseExceptions = false;\n    maxParseExceptions = 2;\n    maxSavedParseExceptions = 2;\n\n    \r\n    insertData();\n\n    final KafkaIndexTask task = createTask(\n        null,\n        new KafkaIndexTaskIOConfig(\n            0,\n            \"sequence0\",\n            new SeekableStreamStartSequenceNumbers<>(topic, ImmutableMap.of(0, 2L), ImmutableSet.of()),\n            new SeekableStreamEndSequenceNumbers<>(topic, ImmutableMap.of(0, 10L)),\n            kafkaServer.consumerProperties(),\n            KafkaSupervisorIOConfig.DEFAULT_POLL_TIMEOUT_MILLIS,\n            true,\n            null,\n            null\n        )\n    );\n\n    final ListenableFuture<TaskStatus> future = runTask(task);\n\n    TaskStatus status = future.get();\n\n    \r\n    Assert.assertEquals(TaskState.FAILED, status.getStatusCode());\n    IndexTaskTest.checkTaskStatusErrorMsgForParseExceptionsExceeded(status);\n\n    \r\n    Assert.assertEquals(3, task.getRunner().getRowIngestionMeters().getProcessed());\n    Assert.assertEquals(0, task.getRunner().getRowIngestionMeters().getProcessedWithError());\n    Assert.assertEquals(3, task.getRunner().getRowIngestionMeters().getUnparseable());\n    Assert.assertEquals(0, task.getRunner().getRowIngestionMeters().getThrownAway());\n\n    \r\n    Assert.assertEquals(ImmutableList.of(), publishedDescriptors());\n    Assert.assertNull(metadataStorageCoordinator.getDataSourceMetadata(DATA_SCHEMA.getDataSource()));\n\n    IngestionStatsAndErrorsTaskReportData reportData = getTaskReportData();\n\n    Map<String, Object> expectedMetrics = ImmutableMap.of(\n        RowIngestionMeters.BUILD_SEGMENTS,\n        ImmutableMap.of(\n            RowIngestionMeters.PROCESSED, 3,\n            RowIngestionMeters.PROCESSED_WITH_ERROR, 0,\n            RowIngestionMeters.UNPARSEABLE, 3,\n            RowIngestionMeters.THROWN_AWAY, 0\n        )\n    );\n    Assert.assertEquals(expectedMetrics, reportData.getRowStats());\n\n    Map<String, Object> unparseableEvents = ImmutableMap.of(\n        RowIngestionMeters.BUILD_SEGMENTS,\n        Arrays.asList(\n            \"Unable to parse row [unparseable2]\",\n            \"Unable to parse row [unparseable]\"\n        )\n    );\n\n    Assert.assertEquals(unparseableEvents, reportData.getUnparseableEvents());\n  }\n","realPath":"extensions-core/kafka-indexing-service/src/test/java/org/apache/druid/indexing/kafka/KafkaIndexTaskTest.java","repoName":"druid","snippetEndLine":0,"snippetStartLine":0,"startLine":1364,"status":"M"}],"commitId":"ac6d703814ccb5b258c586b63e0bc33d669e0f57","commitMessage":"@@@Support inputFormat and inputSource for sampler (#8901)\n\n* Support inputFormat and inputSource for sampler\n\n* Cleanup javadocs and names\n\n* fix style\n\n* fix timed shutoff input source reader\n\n* fix timed shutoff input source reader again\n\n* tidy up timed shutoff reader\n\n* unused imports\n\n* fix tc\n","date":"2019-11-21 06:51:25","modifiedFileCount":"66","status":"M","submitter":"Jihoon Son"},{"authorTime":"2020-01-28 03:24:29","codes":[{"authorDate":"2020-01-28 03:24:29","commitOrder":11,"curCode":"  public void testMultipleParseExceptionsFailure() throws Exception\n  {\n    reportParseExceptions = false;\n    maxParseExceptions = 2;\n    maxSavedParseExceptions = 2;\n\n    recordSupplier.assign(EasyMock.anyObject());\n    EasyMock.expectLastCall().anyTimes();\n\n    EasyMock.expect(recordSupplier.getEarliestSequenceNumber(EasyMock.anyObject())).andReturn(\"0\").anyTimes();\n\n    recordSupplier.seek(EasyMock.anyObject(), EasyMock.anyString());\n    EasyMock.expectLastCall().anyTimes();\n\n    EasyMock.expect(recordSupplier.poll(EasyMock.anyLong())).andReturn(records.subList(2, 13)).once();\n\n    recordSupplier.close();\n    EasyMock.expectLastCall().once();\n\n    replayAll();\n\n    final KinesisIndexTask task = createTask(\n        null,\n        new KinesisIndexTaskIOConfig(\n            0,\n            \"sequence0\",\n            new SeekableStreamStartSequenceNumbers<>(STREAM, ImmutableMap.of(SHARD_ID1, \"2\"), ImmutableSet.of()),\n            new SeekableStreamEndSequenceNumbers<>(STREAM, ImmutableMap.of(SHARD_ID1, \"9\")),\n            true,\n            null,\n            null,\n            INPUT_FORMAT,\n            \"awsEndpoint\",\n            null,\n            null,\n            null,\n            null,\n            false\n        )\n    );\n\n    final ListenableFuture<TaskStatus> future = runTask(task);\n\n    TaskStatus status = future.get();\n\n    \r\n    Assert.assertEquals(TaskState.FAILED, status.getStatusCode());\n    verifyAll();\n    IndexTaskTest.checkTaskStatusErrorMsgForParseExceptionsExceeded(status);\n\n    \r\n    Assert.assertEquals(3, task.getRunner().getRowIngestionMeters().getProcessed());\n    Assert.assertEquals(0, task.getRunner().getRowIngestionMeters().getProcessedWithError());\n    Assert.assertEquals(3, task.getRunner().getRowIngestionMeters().getUnparseable());\n    Assert.assertEquals(0, task.getRunner().getRowIngestionMeters().getThrownAway());\n\n    \r\n    Assert.assertEquals(ImmutableList.of(), publishedDescriptors());\n    Assert.assertNull(newDataSchemaMetadata());\n\n    IngestionStatsAndErrorsTaskReportData reportData = getTaskReportData();\n\n    Map<String, Object> expectedMetrics = ImmutableMap.of(\n        RowIngestionMeters.BUILD_SEGMENTS,\n        ImmutableMap.of(\n            RowIngestionMeters.PROCESSED, 3,\n            RowIngestionMeters.PROCESSED_WITH_ERROR, 0,\n            RowIngestionMeters.UNPARSEABLE, 3,\n            RowIngestionMeters.THROWN_AWAY, 0\n        )\n    );\n    Assert.assertEquals(expectedMetrics, reportData.getRowStats());\n\n    Map<String, Object> unparseableEvents = ImmutableMap.of(\n        RowIngestionMeters.BUILD_SEGMENTS,\n        Arrays.asList(\n            \"Unable to parse row [unparseable2]\",\n            \"Unable to parse row [unparseable]\"\n        )\n    );\n\n    Assert.assertEquals(unparseableEvents, reportData.getUnparseableEvents());\n  }\n","date":"2020-01-28 03:24:29","endLine":1431,"groupId":"3035","id":19,"instanceNumber":1,"isCurCommit":0,"methodName":"testMultipleParseExceptionsFailure","params":"()","path":"/mnt/clonedata/CloneManagementServer/ManagementServer/consistResult/result-druid-10-0.7/blobInfo/CC_OUT/blobs/78/67522a54ebcfbe41346f09fcd327a4d81923d3.src","preCode":"  public void testMultipleParseExceptionsFailure() throws Exception\n  {\n    reportParseExceptions = false;\n    maxParseExceptions = 2;\n    maxSavedParseExceptions = 2;\n\n    recordSupplier.assign(EasyMock.anyObject());\n    EasyMock.expectLastCall().anyTimes();\n\n    EasyMock.expect(recordSupplier.getEarliestSequenceNumber(EasyMock.anyObject())).andReturn(\"0\").anyTimes();\n\n    recordSupplier.seek(EasyMock.anyObject(), EasyMock.anyString());\n    EasyMock.expectLastCall().anyTimes();\n\n    EasyMock.expect(recordSupplier.poll(EasyMock.anyLong())).andReturn(records.subList(2, 13)).once();\n\n    recordSupplier.close();\n    EasyMock.expectLastCall().once();\n\n    replayAll();\n\n    final KinesisIndexTask task = createTask(\n        null,\n        new KinesisIndexTaskIOConfig(\n            0,\n            \"sequence0\",\n            new SeekableStreamStartSequenceNumbers<>(STREAM, ImmutableMap.of(SHARD_ID1, \"2\"), ImmutableSet.of()),\n            new SeekableStreamEndSequenceNumbers<>(STREAM, ImmutableMap.of(SHARD_ID1, \"9\")),\n            true,\n            null,\n            null,\n            INPUT_FORMAT,\n            \"awsEndpoint\",\n            null,\n            null,\n            null,\n            null,\n            false\n        )\n    );\n\n    final ListenableFuture<TaskStatus> future = runTask(task);\n\n    TaskStatus status = future.get();\n\n    \r\n    Assert.assertEquals(TaskState.FAILED, status.getStatusCode());\n    verifyAll();\n    IndexTaskTest.checkTaskStatusErrorMsgForParseExceptionsExceeded(status);\n\n    \r\n    Assert.assertEquals(3, task.getRunner().getRowIngestionMeters().getProcessed());\n    Assert.assertEquals(0, task.getRunner().getRowIngestionMeters().getProcessedWithError());\n    Assert.assertEquals(3, task.getRunner().getRowIngestionMeters().getUnparseable());\n    Assert.assertEquals(0, task.getRunner().getRowIngestionMeters().getThrownAway());\n\n    \r\n    Assert.assertEquals(ImmutableList.of(), publishedDescriptors());\n    Assert.assertNull(metadataStorageCoordinator.getDataSourceMetadata(NEW_DATA_SCHEMA.getDataSource()));\n\n    IngestionStatsAndErrorsTaskReportData reportData = getTaskReportData();\n\n    Map<String, Object> expectedMetrics = ImmutableMap.of(\n        RowIngestionMeters.BUILD_SEGMENTS,\n        ImmutableMap.of(\n            RowIngestionMeters.PROCESSED, 3,\n            RowIngestionMeters.PROCESSED_WITH_ERROR, 0,\n            RowIngestionMeters.UNPARSEABLE, 3,\n            RowIngestionMeters.THROWN_AWAY, 0\n        )\n    );\n    Assert.assertEquals(expectedMetrics, reportData.getRowStats());\n\n    Map<String, Object> unparseableEvents = ImmutableMap.of(\n        RowIngestionMeters.BUILD_SEGMENTS,\n        Arrays.asList(\n            \"Unable to parse row [unparseable2]\",\n            \"Unable to parse row [unparseable]\"\n        )\n    );\n\n    Assert.assertEquals(unparseableEvents, reportData.getUnparseableEvents());\n  }\n","realPath":"extensions-core/kinesis-indexing-service/src/test/java/org/apache/druid/indexing/kinesis/KinesisIndexTaskTest.java","repoName":"druid","snippetEndLine":0,"snippetStartLine":0,"startLine":1349,"status":"M"},{"authorDate":"2020-01-28 03:24:29","commitOrder":11,"curCode":"  public void testMultipleParseExceptionsFailure() throws Exception\n  {\n    reportParseExceptions = false;\n    maxParseExceptions = 2;\n    maxSavedParseExceptions = 2;\n\n    \r\n    insertData();\n\n    final KafkaIndexTask task = createTask(\n        null,\n        new KafkaIndexTaskIOConfig(\n            0,\n            \"sequence0\",\n            new SeekableStreamStartSequenceNumbers<>(topic, ImmutableMap.of(0, 2L), ImmutableSet.of()),\n            new SeekableStreamEndSequenceNumbers<>(topic, ImmutableMap.of(0, 10L)),\n            kafkaServer.consumerProperties(),\n            KafkaSupervisorIOConfig.DEFAULT_POLL_TIMEOUT_MILLIS,\n            true,\n            null,\n            null,\n            INPUT_FORMAT\n        )\n    );\n\n    final ListenableFuture<TaskStatus> future = runTask(task);\n\n    TaskStatus status = future.get();\n\n    \r\n    Assert.assertEquals(TaskState.FAILED, status.getStatusCode());\n    IndexTaskTest.checkTaskStatusErrorMsgForParseExceptionsExceeded(status);\n\n    \r\n    Assert.assertEquals(3, task.getRunner().getRowIngestionMeters().getProcessed());\n    Assert.assertEquals(0, task.getRunner().getRowIngestionMeters().getProcessedWithError());\n    Assert.assertEquals(3, task.getRunner().getRowIngestionMeters().getUnparseable());\n    Assert.assertEquals(0, task.getRunner().getRowIngestionMeters().getThrownAway());\n\n    \r\n    Assert.assertEquals(ImmutableList.of(), publishedDescriptors());\n    Assert.assertNull(newDataSchemaMetadata());\n\n    IngestionStatsAndErrorsTaskReportData reportData = getTaskReportData();\n\n    Map<String, Object> expectedMetrics = ImmutableMap.of(\n        RowIngestionMeters.BUILD_SEGMENTS,\n        ImmutableMap.of(\n            RowIngestionMeters.PROCESSED, 3,\n            RowIngestionMeters.PROCESSED_WITH_ERROR, 0,\n            RowIngestionMeters.UNPARSEABLE, 3,\n            RowIngestionMeters.THROWN_AWAY, 0\n        )\n    );\n    Assert.assertEquals(expectedMetrics, reportData.getRowStats());\n\n    Map<String, Object> unparseableEvents = ImmutableMap.of(\n        RowIngestionMeters.BUILD_SEGMENTS,\n        Arrays.asList(\n            \"Unable to parse row [unparseable2]\",\n            \"Unable to parse row [unparseable]\"\n        )\n    );\n\n    Assert.assertEquals(unparseableEvents, reportData.getUnparseableEvents());\n  }\n","date":"2020-01-28 03:24:29","endLine":1432,"groupId":"12458","id":20,"instanceNumber":2,"isCurCommit":0,"methodName":"testMultipleParseExceptionsFailure","params":"()","path":"/mnt/clonedata/CloneManagementServer/ManagementServer/consistResult/result-druid-10-0.7/blobInfo/CC_OUT/blobs/d4/468bd39e032386fe654402d0006ecea542747e.src","preCode":"  public void testMultipleParseExceptionsFailure() throws Exception\n  {\n    reportParseExceptions = false;\n    maxParseExceptions = 2;\n    maxSavedParseExceptions = 2;\n\n    \r\n    insertData();\n\n    final KafkaIndexTask task = createTask(\n        null,\n        new KafkaIndexTaskIOConfig(\n            0,\n            \"sequence0\",\n            new SeekableStreamStartSequenceNumbers<>(topic, ImmutableMap.of(0, 2L), ImmutableSet.of()),\n            new SeekableStreamEndSequenceNumbers<>(topic, ImmutableMap.of(0, 10L)),\n            kafkaServer.consumerProperties(),\n            KafkaSupervisorIOConfig.DEFAULT_POLL_TIMEOUT_MILLIS,\n            true,\n            null,\n            null,\n            INPUT_FORMAT\n        )\n    );\n\n    final ListenableFuture<TaskStatus> future = runTask(task);\n\n    TaskStatus status = future.get();\n\n    \r\n    Assert.assertEquals(TaskState.FAILED, status.getStatusCode());\n    IndexTaskTest.checkTaskStatusErrorMsgForParseExceptionsExceeded(status);\n\n    \r\n    Assert.assertEquals(3, task.getRunner().getRowIngestionMeters().getProcessed());\n    Assert.assertEquals(0, task.getRunner().getRowIngestionMeters().getProcessedWithError());\n    Assert.assertEquals(3, task.getRunner().getRowIngestionMeters().getUnparseable());\n    Assert.assertEquals(0, task.getRunner().getRowIngestionMeters().getThrownAway());\n\n    \r\n    Assert.assertEquals(ImmutableList.of(), publishedDescriptors());\n    Assert.assertNull(metadataStorageCoordinator.getDataSourceMetadata(NEW_DATA_SCHEMA.getDataSource()));\n\n    IngestionStatsAndErrorsTaskReportData reportData = getTaskReportData();\n\n    Map<String, Object> expectedMetrics = ImmutableMap.of(\n        RowIngestionMeters.BUILD_SEGMENTS,\n        ImmutableMap.of(\n            RowIngestionMeters.PROCESSED, 3,\n            RowIngestionMeters.PROCESSED_WITH_ERROR, 0,\n            RowIngestionMeters.UNPARSEABLE, 3,\n            RowIngestionMeters.THROWN_AWAY, 0\n        )\n    );\n    Assert.assertEquals(expectedMetrics, reportData.getRowStats());\n\n    Map<String, Object> unparseableEvents = ImmutableMap.of(\n        RowIngestionMeters.BUILD_SEGMENTS,\n        Arrays.asList(\n            \"Unable to parse row [unparseable2]\",\n            \"Unable to parse row [unparseable]\"\n        )\n    );\n\n    Assert.assertEquals(unparseableEvents, reportData.getUnparseableEvents());\n  }\n","realPath":"extensions-core/kafka-indexing-service/src/test/java/org/apache/druid/indexing/kafka/KafkaIndexTaskTest.java","repoName":"druid","snippetEndLine":0,"snippetStartLine":0,"startLine":1367,"status":"M"}],"commitId":"b9186f8f9ff2ff52aceda42bc5f24ffd47a7d17e","commitMessage":"@@@Reconcile terminology and method naming to 'used/unused segments'; Rename MetadataSegmentManager to MetadataSegmentsManager (#7306)\n\n* Reconcile terminology and method naming to 'used/unused segments'; Don't use terms 'enable/disable data source'; Rename MetadataSegmentManager to MetadataSegments; Make REST API methods which mark segments as used/unused to return server error instead of an empty response in case of error\n\n* Fix brace\n\n* Import order\n\n* Rename withKillDataSourceWhitelist to withSpecificDataSourcesToKill\n\n* Fix tests\n\n* Fix tests by adding proper methods without interval parameters to IndexerMetadataStorageCoordinator instead of hacking with Intervals.ETERNITY\n\n* More aligned names of DruidCoordinatorHelpers.  rename several CoordinatorDynamicConfig parameters\n\n* Rename ClientCompactTaskQuery to ClientCompactionTaskQuery for consistency with CompactionTask; ClientCompactQueryTuningConfig to ClientCompactionTaskQueryTuningConfig\n\n* More variable and method renames\n\n* Rename MetadataSegments to SegmentsMetadata\n\n* Javadoc update\n\n* Simplify SegmentsMetadata.getUnusedSegmentIntervals().  more javadocs\n\n* Update Javadoc of VersionedIntervalTimeline.iterateAllObjects()\n\n* Reorder imports\n\n* Rename SegmentsMetadata.tryMark... methods to mark... and make them to return boolean and the numbers of segments changed and relay exceptions to callers\n\n* Complete merge\n\n* Add CollectionUtils.newTreeSet(); Refactor DruidCoordinatorRuntimeParams creation in tests\n\n* Remove MetadataSegmentManager\n\n* Rename millisLagSinceCoordinatorBecomesLeaderBeforeCanMarkAsUnusedOvershadowedSegments to leadingTimeMillisBeforeCanMarkAsUnusedOvershadowedSegments\n\n* Fix tests.  refactor DruidCluster creation in tests into DruidClusterBuilder\n\n* Fix inspections\n\n* Fix SQLMetadataSegmentManagerEmptyTest and rename it to SqlSegmentsMetadataEmptyTest\n\n* Rename SegmentsAndMetadata to SegmentsAndCommitMetadata to reduce the similarity with SegmentsMetadata; Rename some methods\n\n* Rename DruidCoordinatorHelper to CoordinatorDuty.  refactor DruidCoordinator\n\n* Unused import\n\n* Optimize imports\n\n* Rename IndexerSQLMetadataStorageCoordinator.getDataSourceMetadata() to retrieveDataSourceMetadata()\n\n* Unused import\n\n* Update terminology in datasource-view.tsx\n\n* Fix label in datasource-view.spec.tsx.snap\n\n* Fix lint errors in datasource-view.tsx\n\n* Doc improvements\n\n* Another attempt to please TSLint\n\n* Another attempt to please TSLint\n\n* Style fixes\n\n* Fix IndexerSQLMetadataStorageCoordinator.createUsedSegmentsSqlQueryForIntervals() (wrong merge)\n\n* Try to fix docs build issue\n\n* Javadoc and spelling fixes\n\n* Rename SegmentsMetadata to SegmentsMetadataManager.  address other comments\n\n* Address more comments\n","date":"2020-01-28 03:24:29","modifiedFileCount":"127","status":"M","submitter":"Roman Leventov"},{"authorTime":"2020-01-28 03:24:29","codes":[{"authorDate":"2021-01-09 08:04:37","commitOrder":12,"curCode":"  public void testMultipleParseExceptionsFailure() throws Exception\n  {\n    reportParseExceptions = false;\n    maxParseExceptions = 2;\n    maxSavedParseExceptions = 2;\n\n    recordSupplier.assign(EasyMock.anyObject());\n    EasyMock.expectLastCall().anyTimes();\n\n    EasyMock.expect(recordSupplier.getEarliestSequenceNumber(EasyMock.anyObject())).andReturn(\"0\").anyTimes();\n\n    recordSupplier.seek(EasyMock.anyObject(), EasyMock.anyString());\n    EasyMock.expectLastCall().anyTimes();\n\n    EasyMock.expect(recordSupplier.poll(EasyMock.anyLong())).andReturn(generateRecords(2, 13)).once();\n\n    recordSupplier.close();\n    EasyMock.expectLastCall().once();\n\n    replayAll();\n\n    final KinesisIndexTask task = createTask(\n        null,\n        new KinesisIndexTaskIOConfig(\n            0,\n            \"sequence0\",\n            new SeekableStreamStartSequenceNumbers<>(STREAM, ImmutableMap.of(SHARD_ID1, \"2\"), ImmutableSet.of()),\n            new SeekableStreamEndSequenceNumbers<>(STREAM, ImmutableMap.of(SHARD_ID1, \"9\")),\n            true,\n            null,\n            null,\n            INPUT_FORMAT,\n            \"awsEndpoint\",\n            null,\n            null,\n            null,\n            null,\n            false\n        )\n    );\n\n    final ListenableFuture<TaskStatus> future = runTask(task);\n\n    TaskStatus status = future.get();\n\n    \r\n    Assert.assertEquals(TaskState.FAILED, status.getStatusCode());\n    verifyAll();\n    IndexTaskTest.checkTaskStatusErrorMsgForParseExceptionsExceeded(status);\n\n    \r\n    Assert.assertEquals(3, task.getRunner().getRowIngestionMeters().getProcessed());\n    Assert.assertEquals(0, task.getRunner().getRowIngestionMeters().getProcessedWithError());\n    Assert.assertEquals(3, task.getRunner().getRowIngestionMeters().getUnparseable());\n    Assert.assertEquals(0, task.getRunner().getRowIngestionMeters().getThrownAway());\n\n    \r\n    Assert.assertEquals(ImmutableList.of(), publishedDescriptors());\n    Assert.assertNull(newDataSchemaMetadata());\n\n    IngestionStatsAndErrorsTaskReportData reportData = getTaskReportData();\n\n    Map<String, Object> expectedMetrics = ImmutableMap.of(\n        RowIngestionMeters.BUILD_SEGMENTS,\n        ImmutableMap.of(\n            RowIngestionMeters.PROCESSED, 3,\n            RowIngestionMeters.PROCESSED_WITH_ERROR, 0,\n            RowIngestionMeters.UNPARSEABLE, 3,\n            RowIngestionMeters.THROWN_AWAY, 0\n        )\n    );\n    Assert.assertEquals(expectedMetrics, reportData.getRowStats());\n\n    Map<String, Object> unparseableEvents = ImmutableMap.of(\n        RowIngestionMeters.BUILD_SEGMENTS,\n        Arrays.asList(\n            \"Unable to parse row [unparseable2]\",\n            \"Unable to parse row [unparseable]\"\n        )\n    );\n\n    Assert.assertEquals(unparseableEvents, reportData.getUnparseableEvents());\n  }\n","date":"2021-01-09 08:04:37","endLine":1443,"groupId":"3035","id":21,"instanceNumber":1,"isCurCommit":0,"methodName":"testMultipleParseExceptionsFailure","params":"()","path":"/mnt/clonedata/CloneManagementServer/ManagementServer/consistResult/result-druid-10-0.7/blobInfo/CC_OUT/blobs/37/d699a39eb40765e6e0d5b39c1e64d358746164.src","preCode":"  public void testMultipleParseExceptionsFailure() throws Exception\n  {\n    reportParseExceptions = false;\n    maxParseExceptions = 2;\n    maxSavedParseExceptions = 2;\n\n    recordSupplier.assign(EasyMock.anyObject());\n    EasyMock.expectLastCall().anyTimes();\n\n    EasyMock.expect(recordSupplier.getEarliestSequenceNumber(EasyMock.anyObject())).andReturn(\"0\").anyTimes();\n\n    recordSupplier.seek(EasyMock.anyObject(), EasyMock.anyString());\n    EasyMock.expectLastCall().anyTimes();\n\n    EasyMock.expect(recordSupplier.poll(EasyMock.anyLong())).andReturn(records.subList(2, 13)).once();\n\n    recordSupplier.close();\n    EasyMock.expectLastCall().once();\n\n    replayAll();\n\n    final KinesisIndexTask task = createTask(\n        null,\n        new KinesisIndexTaskIOConfig(\n            0,\n            \"sequence0\",\n            new SeekableStreamStartSequenceNumbers<>(STREAM, ImmutableMap.of(SHARD_ID1, \"2\"), ImmutableSet.of()),\n            new SeekableStreamEndSequenceNumbers<>(STREAM, ImmutableMap.of(SHARD_ID1, \"9\")),\n            true,\n            null,\n            null,\n            INPUT_FORMAT,\n            \"awsEndpoint\",\n            null,\n            null,\n            null,\n            null,\n            false\n        )\n    );\n\n    final ListenableFuture<TaskStatus> future = runTask(task);\n\n    TaskStatus status = future.get();\n\n    \r\n    Assert.assertEquals(TaskState.FAILED, status.getStatusCode());\n    verifyAll();\n    IndexTaskTest.checkTaskStatusErrorMsgForParseExceptionsExceeded(status);\n\n    \r\n    Assert.assertEquals(3, task.getRunner().getRowIngestionMeters().getProcessed());\n    Assert.assertEquals(0, task.getRunner().getRowIngestionMeters().getProcessedWithError());\n    Assert.assertEquals(3, task.getRunner().getRowIngestionMeters().getUnparseable());\n    Assert.assertEquals(0, task.getRunner().getRowIngestionMeters().getThrownAway());\n\n    \r\n    Assert.assertEquals(ImmutableList.of(), publishedDescriptors());\n    Assert.assertNull(newDataSchemaMetadata());\n\n    IngestionStatsAndErrorsTaskReportData reportData = getTaskReportData();\n\n    Map<String, Object> expectedMetrics = ImmutableMap.of(\n        RowIngestionMeters.BUILD_SEGMENTS,\n        ImmutableMap.of(\n            RowIngestionMeters.PROCESSED, 3,\n            RowIngestionMeters.PROCESSED_WITH_ERROR, 0,\n            RowIngestionMeters.UNPARSEABLE, 3,\n            RowIngestionMeters.THROWN_AWAY, 0\n        )\n    );\n    Assert.assertEquals(expectedMetrics, reportData.getRowStats());\n\n    Map<String, Object> unparseableEvents = ImmutableMap.of(\n        RowIngestionMeters.BUILD_SEGMENTS,\n        Arrays.asList(\n            \"Unable to parse row [unparseable2]\",\n            \"Unable to parse row [unparseable]\"\n        )\n    );\n\n    Assert.assertEquals(unparseableEvents, reportData.getUnparseableEvents());\n  }\n","realPath":"extensions-core/kinesis-indexing-service/src/test/java/org/apache/druid/indexing/kinesis/KinesisIndexTaskTest.java","repoName":"druid","snippetEndLine":0,"snippetStartLine":0,"startLine":1361,"status":"M"},{"authorDate":"2020-01-28 03:24:29","commitOrder":12,"curCode":"  public void testMultipleParseExceptionsFailure() throws Exception\n  {\n    reportParseExceptions = false;\n    maxParseExceptions = 2;\n    maxSavedParseExceptions = 2;\n\n    \r\n    insertData();\n\n    final KafkaIndexTask task = createTask(\n        null,\n        new KafkaIndexTaskIOConfig(\n            0,\n            \"sequence0\",\n            new SeekableStreamStartSequenceNumbers<>(topic, ImmutableMap.of(0, 2L), ImmutableSet.of()),\n            new SeekableStreamEndSequenceNumbers<>(topic, ImmutableMap.of(0, 10L)),\n            kafkaServer.consumerProperties(),\n            KafkaSupervisorIOConfig.DEFAULT_POLL_TIMEOUT_MILLIS,\n            true,\n            null,\n            null,\n            INPUT_FORMAT\n        )\n    );\n\n    final ListenableFuture<TaskStatus> future = runTask(task);\n\n    TaskStatus status = future.get();\n\n    \r\n    Assert.assertEquals(TaskState.FAILED, status.getStatusCode());\n    IndexTaskTest.checkTaskStatusErrorMsgForParseExceptionsExceeded(status);\n\n    \r\n    Assert.assertEquals(3, task.getRunner().getRowIngestionMeters().getProcessed());\n    Assert.assertEquals(0, task.getRunner().getRowIngestionMeters().getProcessedWithError());\n    Assert.assertEquals(3, task.getRunner().getRowIngestionMeters().getUnparseable());\n    Assert.assertEquals(0, task.getRunner().getRowIngestionMeters().getThrownAway());\n\n    \r\n    Assert.assertEquals(ImmutableList.of(), publishedDescriptors());\n    Assert.assertNull(newDataSchemaMetadata());\n\n    IngestionStatsAndErrorsTaskReportData reportData = getTaskReportData();\n\n    Map<String, Object> expectedMetrics = ImmutableMap.of(\n        RowIngestionMeters.BUILD_SEGMENTS,\n        ImmutableMap.of(\n            RowIngestionMeters.PROCESSED, 3,\n            RowIngestionMeters.PROCESSED_WITH_ERROR, 0,\n            RowIngestionMeters.UNPARSEABLE, 3,\n            RowIngestionMeters.THROWN_AWAY, 0\n        )\n    );\n    Assert.assertEquals(expectedMetrics, reportData.getRowStats());\n\n    Map<String, Object> unparseableEvents = ImmutableMap.of(\n        RowIngestionMeters.BUILD_SEGMENTS,\n        Arrays.asList(\n            \"Unable to parse row [unparseable2]\",\n            \"Unable to parse row [unparseable]\"\n        )\n    );\n\n    Assert.assertEquals(unparseableEvents, reportData.getUnparseableEvents());\n  }\n","date":"2020-01-28 03:24:29","endLine":1432,"groupId":"12458","id":22,"instanceNumber":2,"isCurCommit":0,"methodName":"testMultipleParseExceptionsFailure","params":"()","path":"/mnt/clonedata/CloneManagementServer/ManagementServer/consistResult/result-druid-10-0.7/blobInfo/CC_OUT/blobs/d4/468bd39e032386fe654402d0006ecea542747e.src","preCode":"  public void testMultipleParseExceptionsFailure() throws Exception\n  {\n    reportParseExceptions = false;\n    maxParseExceptions = 2;\n    maxSavedParseExceptions = 2;\n\n    \r\n    insertData();\n\n    final KafkaIndexTask task = createTask(\n        null,\n        new KafkaIndexTaskIOConfig(\n            0,\n            \"sequence0\",\n            new SeekableStreamStartSequenceNumbers<>(topic, ImmutableMap.of(0, 2L), ImmutableSet.of()),\n            new SeekableStreamEndSequenceNumbers<>(topic, ImmutableMap.of(0, 10L)),\n            kafkaServer.consumerProperties(),\n            KafkaSupervisorIOConfig.DEFAULT_POLL_TIMEOUT_MILLIS,\n            true,\n            null,\n            null,\n            INPUT_FORMAT\n        )\n    );\n\n    final ListenableFuture<TaskStatus> future = runTask(task);\n\n    TaskStatus status = future.get();\n\n    \r\n    Assert.assertEquals(TaskState.FAILED, status.getStatusCode());\n    IndexTaskTest.checkTaskStatusErrorMsgForParseExceptionsExceeded(status);\n\n    \r\n    Assert.assertEquals(3, task.getRunner().getRowIngestionMeters().getProcessed());\n    Assert.assertEquals(0, task.getRunner().getRowIngestionMeters().getProcessedWithError());\n    Assert.assertEquals(3, task.getRunner().getRowIngestionMeters().getUnparseable());\n    Assert.assertEquals(0, task.getRunner().getRowIngestionMeters().getThrownAway());\n\n    \r\n    Assert.assertEquals(ImmutableList.of(), publishedDescriptors());\n    Assert.assertNull(newDataSchemaMetadata());\n\n    IngestionStatsAndErrorsTaskReportData reportData = getTaskReportData();\n\n    Map<String, Object> expectedMetrics = ImmutableMap.of(\n        RowIngestionMeters.BUILD_SEGMENTS,\n        ImmutableMap.of(\n            RowIngestionMeters.PROCESSED, 3,\n            RowIngestionMeters.PROCESSED_WITH_ERROR, 0,\n            RowIngestionMeters.UNPARSEABLE, 3,\n            RowIngestionMeters.THROWN_AWAY, 0\n        )\n    );\n    Assert.assertEquals(expectedMetrics, reportData.getRowStats());\n\n    Map<String, Object> unparseableEvents = ImmutableMap.of(\n        RowIngestionMeters.BUILD_SEGMENTS,\n        Arrays.asList(\n            \"Unable to parse row [unparseable2]\",\n            \"Unable to parse row [unparseable]\"\n        )\n    );\n\n    Assert.assertEquals(unparseableEvents, reportData.getUnparseableEvents());\n  }\n","realPath":"extensions-core/kafka-indexing-service/src/test/java/org/apache/druid/indexing/kafka/KafkaIndexTaskTest.java","repoName":"druid","snippetEndLine":0,"snippetStartLine":0,"startLine":1367,"status":"N"}],"commitId":"118b50195e5c2989e04e0f5290aa72cae114db39","commitMessage":"@@@Introduce KafkaRecordEntity to support Kafka headers in InputFormats (#10730)\n\nToday Kafka message support in streaming indexing tasks is limited to\nmessage values.  and does not provide a way to expose Kafka headers. \ntimestamps.  or keys.  which may be of interest to more specialized\nDruid input formats. For instance.  Kafka headers may be used to indicate\npayload format/encoding or additional metadata.  and timestamps are often\nomitted from values in Kafka streams applications.  since they are\nincluded in the record.\n\nThis change proposes to introduce KafkaRecordEntity as InputEntity. \nwhich would give input formats full access to the underlying Kafka record. \nincluding headers.  key.  timestamps. It would also open access to low-level\ninformation such as topic.  partition.  offset if needed.\n\nKafkaEntity is a subclass of ByteEntity for backwards compatibility with\nexisting input formats.  and to avoid introducing unnecessary complexity\nfor Kinesis indexing tasks.","date":"2021-01-09 08:04:37","modifiedFileCount":"30","status":"M","submitter":"Xavier L?aut?"},{"authorTime":"2021-03-10 04:11:58","codes":[{"authorDate":"2021-03-10 04:11:58","commitOrder":13,"curCode":"  public void testMultipleParseExceptionsFailure() throws Exception\n  {\n    reportParseExceptions = false;\n    maxParseExceptions = 2;\n    maxSavedParseExceptions = 2;\n\n    recordSupplier.assign(EasyMock.anyObject());\n    EasyMock.expectLastCall().anyTimes();\n\n    EasyMock.expect(recordSupplier.getEarliestSequenceNumber(EasyMock.anyObject())).andReturn(\"0\").anyTimes();\n\n    recordSupplier.seek(EasyMock.anyObject(), EasyMock.anyString());\n    EasyMock.expectLastCall().anyTimes();\n\n    EasyMock.expect(recordSupplier.poll(EasyMock.anyLong())).andReturn(generateRecords(2, 13)).once();\n\n    recordSupplier.close();\n    EasyMock.expectLastCall().once();\n\n    replayAll();\n\n    final KinesisIndexTask task = createTask(\n        null,\n        new KinesisIndexTaskIOConfig(\n            0,\n            \"sequence0\",\n            new SeekableStreamStartSequenceNumbers<>(STREAM, ImmutableMap.of(SHARD_ID1, \"2\"), ImmutableSet.of()),\n            new SeekableStreamEndSequenceNumbers<>(STREAM, ImmutableMap.of(SHARD_ID1, \"9\")),\n            true,\n            null,\n            null,\n            INPUT_FORMAT,\n            \"awsEndpoint\",\n            null,\n            null,\n            null,\n            null,\n            false\n        )\n    );\n\n    final ListenableFuture<TaskStatus> future = runTask(task);\n\n    TaskStatus status = future.get();\n\n    \r\n    Assert.assertEquals(TaskState.FAILED, status.getStatusCode());\n    verifyAll();\n    IndexTaskTest.checkTaskStatusErrorMsgForParseExceptionsExceeded(status);\n\n    \r\n    Assert.assertEquals(3, task.getRunner().getRowIngestionMeters().getProcessed());\n    Assert.assertEquals(0, task.getRunner().getRowIngestionMeters().getProcessedWithError());\n    Assert.assertEquals(3, task.getRunner().getRowIngestionMeters().getUnparseable());\n    Assert.assertEquals(0, task.getRunner().getRowIngestionMeters().getThrownAway());\n\n    \r\n    Assert.assertEquals(ImmutableList.of(), publishedDescriptors());\n    Assert.assertNull(newDataSchemaMetadata());\n\n    IngestionStatsAndErrorsTaskReportData reportData = getTaskReportData();\n\n    Map<String, Object> expectedMetrics = ImmutableMap.of(\n        RowIngestionMeters.BUILD_SEGMENTS,\n        ImmutableMap.of(\n            RowIngestionMeters.PROCESSED, 3,\n            RowIngestionMeters.PROCESSED_WITH_ERROR, 0,\n            RowIngestionMeters.UNPARSEABLE, 3,\n            RowIngestionMeters.THROWN_AWAY, 0\n        )\n    );\n    Assert.assertEquals(expectedMetrics, reportData.getRowStats());\n\n    Map<String, Object> unparseableEvents = ImmutableMap.of(\n        RowIngestionMeters.BUILD_SEGMENTS,\n        Arrays.asList(\n            \"Unable to parse [] as the intermediateRow resulted in empty input row\",\n            \"Unable to parse row [unparseable]\"\n        )\n    );\n\n    Assert.assertEquals(unparseableEvents, reportData.getUnparseableEvents());\n  }\n","date":"2021-03-10 04:11:58","endLine":1443,"groupId":"102360","id":23,"instanceNumber":1,"isCurCommit":0,"methodName":"testMultipleParseExceptionsFailure","params":"()","path":"/mnt/clonedata/CloneManagementServer/ManagementServer/consistResult/result-druid-10-0.7/blobInfo/CC_OUT/blobs/74/5375d96ba21727e83da99bb27d40c8d0c8a4dc.src","preCode":"  public void testMultipleParseExceptionsFailure() throws Exception\n  {\n    reportParseExceptions = false;\n    maxParseExceptions = 2;\n    maxSavedParseExceptions = 2;\n\n    recordSupplier.assign(EasyMock.anyObject());\n    EasyMock.expectLastCall().anyTimes();\n\n    EasyMock.expect(recordSupplier.getEarliestSequenceNumber(EasyMock.anyObject())).andReturn(\"0\").anyTimes();\n\n    recordSupplier.seek(EasyMock.anyObject(), EasyMock.anyString());\n    EasyMock.expectLastCall().anyTimes();\n\n    EasyMock.expect(recordSupplier.poll(EasyMock.anyLong())).andReturn(generateRecords(2, 13)).once();\n\n    recordSupplier.close();\n    EasyMock.expectLastCall().once();\n\n    replayAll();\n\n    final KinesisIndexTask task = createTask(\n        null,\n        new KinesisIndexTaskIOConfig(\n            0,\n            \"sequence0\",\n            new SeekableStreamStartSequenceNumbers<>(STREAM, ImmutableMap.of(SHARD_ID1, \"2\"), ImmutableSet.of()),\n            new SeekableStreamEndSequenceNumbers<>(STREAM, ImmutableMap.of(SHARD_ID1, \"9\")),\n            true,\n            null,\n            null,\n            INPUT_FORMAT,\n            \"awsEndpoint\",\n            null,\n            null,\n            null,\n            null,\n            false\n        )\n    );\n\n    final ListenableFuture<TaskStatus> future = runTask(task);\n\n    TaskStatus status = future.get();\n\n    \r\n    Assert.assertEquals(TaskState.FAILED, status.getStatusCode());\n    verifyAll();\n    IndexTaskTest.checkTaskStatusErrorMsgForParseExceptionsExceeded(status);\n\n    \r\n    Assert.assertEquals(3, task.getRunner().getRowIngestionMeters().getProcessed());\n    Assert.assertEquals(0, task.getRunner().getRowIngestionMeters().getProcessedWithError());\n    Assert.assertEquals(3, task.getRunner().getRowIngestionMeters().getUnparseable());\n    Assert.assertEquals(0, task.getRunner().getRowIngestionMeters().getThrownAway());\n\n    \r\n    Assert.assertEquals(ImmutableList.of(), publishedDescriptors());\n    Assert.assertNull(newDataSchemaMetadata());\n\n    IngestionStatsAndErrorsTaskReportData reportData = getTaskReportData();\n\n    Map<String, Object> expectedMetrics = ImmutableMap.of(\n        RowIngestionMeters.BUILD_SEGMENTS,\n        ImmutableMap.of(\n            RowIngestionMeters.PROCESSED, 3,\n            RowIngestionMeters.PROCESSED_WITH_ERROR, 0,\n            RowIngestionMeters.UNPARSEABLE, 3,\n            RowIngestionMeters.THROWN_AWAY, 0\n        )\n    );\n    Assert.assertEquals(expectedMetrics, reportData.getRowStats());\n\n    Map<String, Object> unparseableEvents = ImmutableMap.of(\n        RowIngestionMeters.BUILD_SEGMENTS,\n        Arrays.asList(\n            \"Unable to parse row [unparseable2]\",\n            \"Unable to parse row [unparseable]\"\n        )\n    );\n\n    Assert.assertEquals(unparseableEvents, reportData.getUnparseableEvents());\n  }\n","realPath":"extensions-core/kinesis-indexing-service/src/test/java/org/apache/druid/indexing/kinesis/KinesisIndexTaskTest.java","repoName":"druid","snippetEndLine":0,"snippetStartLine":0,"startLine":1361,"status":"M"},{"authorDate":"2021-03-10 04:11:58","commitOrder":13,"curCode":"  public void testMultipleParseExceptionsFailure() throws Exception\n  {\n    reportParseExceptions = false;\n    maxParseExceptions = 2;\n    maxSavedParseExceptions = 2;\n\n    \r\n    insertData();\n\n    final KafkaIndexTask task = createTask(\n        null,\n        new KafkaIndexTaskIOConfig(\n            0,\n            \"sequence0\",\n            new SeekableStreamStartSequenceNumbers<>(topic, ImmutableMap.of(0, 2L), ImmutableSet.of()),\n            new SeekableStreamEndSequenceNumbers<>(topic, ImmutableMap.of(0, 10L)),\n            kafkaServer.consumerProperties(),\n            KafkaSupervisorIOConfig.DEFAULT_POLL_TIMEOUT_MILLIS,\n            true,\n            null,\n            null,\n            INPUT_FORMAT\n        )\n    );\n\n    final ListenableFuture<TaskStatus> future = runTask(task);\n\n    TaskStatus status = future.get();\n\n    \r\n    Assert.assertEquals(TaskState.FAILED, status.getStatusCode());\n    IndexTaskTest.checkTaskStatusErrorMsgForParseExceptionsExceeded(status);\n\n    \r\n    Assert.assertEquals(3, task.getRunner().getRowIngestionMeters().getProcessed());\n    Assert.assertEquals(0, task.getRunner().getRowIngestionMeters().getProcessedWithError());\n    Assert.assertEquals(3, task.getRunner().getRowIngestionMeters().getUnparseable());\n    Assert.assertEquals(0, task.getRunner().getRowIngestionMeters().getThrownAway());\n\n    \r\n    Assert.assertEquals(ImmutableList.of(), publishedDescriptors());\n    Assert.assertNull(newDataSchemaMetadata());\n\n    IngestionStatsAndErrorsTaskReportData reportData = getTaskReportData();\n\n    Map<String, Object> expectedMetrics = ImmutableMap.of(\n        RowIngestionMeters.BUILD_SEGMENTS,\n        ImmutableMap.of(\n            RowIngestionMeters.PROCESSED, 3,\n            RowIngestionMeters.PROCESSED_WITH_ERROR, 0,\n            RowIngestionMeters.UNPARSEABLE, 3,\n            RowIngestionMeters.THROWN_AWAY, 0\n        )\n    );\n    Assert.assertEquals(expectedMetrics, reportData.getRowStats());\n\n    Map<String, Object> unparseableEvents = ImmutableMap.of(\n        RowIngestionMeters.BUILD_SEGMENTS,\n        Arrays.asList(\n            \"Unable to parse [] as the intermediateRow resulted in empty input row\",\n            \"Unable to parse row [unparseable]\"\n        )\n    );\n\n    Assert.assertEquals(unparseableEvents, reportData.getUnparseableEvents());\n  }\n","date":"2021-03-10 04:11:58","endLine":1569,"groupId":"102360","id":24,"instanceNumber":2,"isCurCommit":0,"methodName":"testMultipleParseExceptionsFailure","params":"()","path":"/mnt/clonedata/CloneManagementServer/ManagementServer/consistResult/result-druid-10-0.7/blobInfo/CC_OUT/blobs/99/2bb19b62652802e4984a32abc3bed41f383096.src","preCode":"  public void testMultipleParseExceptionsFailure() throws Exception\n  {\n    reportParseExceptions = false;\n    maxParseExceptions = 2;\n    maxSavedParseExceptions = 2;\n\n    \r\n    insertData();\n\n    final KafkaIndexTask task = createTask(\n        null,\n        new KafkaIndexTaskIOConfig(\n            0,\n            \"sequence0\",\n            new SeekableStreamStartSequenceNumbers<>(topic, ImmutableMap.of(0, 2L), ImmutableSet.of()),\n            new SeekableStreamEndSequenceNumbers<>(topic, ImmutableMap.of(0, 10L)),\n            kafkaServer.consumerProperties(),\n            KafkaSupervisorIOConfig.DEFAULT_POLL_TIMEOUT_MILLIS,\n            true,\n            null,\n            null,\n            INPUT_FORMAT\n        )\n    );\n\n    final ListenableFuture<TaskStatus> future = runTask(task);\n\n    TaskStatus status = future.get();\n\n    \r\n    Assert.assertEquals(TaskState.FAILED, status.getStatusCode());\n    IndexTaskTest.checkTaskStatusErrorMsgForParseExceptionsExceeded(status);\n\n    \r\n    Assert.assertEquals(3, task.getRunner().getRowIngestionMeters().getProcessed());\n    Assert.assertEquals(0, task.getRunner().getRowIngestionMeters().getProcessedWithError());\n    Assert.assertEquals(3, task.getRunner().getRowIngestionMeters().getUnparseable());\n    Assert.assertEquals(0, task.getRunner().getRowIngestionMeters().getThrownAway());\n\n    \r\n    Assert.assertEquals(ImmutableList.of(), publishedDescriptors());\n    Assert.assertNull(newDataSchemaMetadata());\n\n    IngestionStatsAndErrorsTaskReportData reportData = getTaskReportData();\n\n    Map<String, Object> expectedMetrics = ImmutableMap.of(\n        RowIngestionMeters.BUILD_SEGMENTS,\n        ImmutableMap.of(\n            RowIngestionMeters.PROCESSED, 3,\n            RowIngestionMeters.PROCESSED_WITH_ERROR, 0,\n            RowIngestionMeters.UNPARSEABLE, 3,\n            RowIngestionMeters.THROWN_AWAY, 0\n        )\n    );\n    Assert.assertEquals(expectedMetrics, reportData.getRowStats());\n\n    Map<String, Object> unparseableEvents = ImmutableMap.of(\n        RowIngestionMeters.BUILD_SEGMENTS,\n        Arrays.asList(\n            \"Unable to parse row [unparseable2]\",\n            \"Unable to parse row [unparseable]\"\n        )\n    );\n\n    Assert.assertEquals(unparseableEvents, reportData.getUnparseableEvents());\n  }\n","realPath":"extensions-core/kafka-indexing-service/src/test/java/org/apache/druid/indexing/kafka/KafkaIndexTaskTest.java","repoName":"druid","snippetEndLine":0,"snippetStartLine":0,"startLine":1504,"status":"M"}],"commitId":"4dd22a850b2e5acc8d9732513303fe6e3560d3d0","commitMessage":"@@@Fix streaming ingestion fails if it encounters empty rows (Regression) (#10962)\n\n* Fix streaming ingestion fails and halt if it  encounters empty rows\n\n* address comments","date":"2021-03-10 04:11:58","modifiedFileCount":"4","status":"M","submitter":"Maytas Monsereenusorn"}]
