[{"authorTime":"2018-08-31 00:56:26","codes":[{"authorDate":"2018-08-31 00:56:26","commitOrder":1,"curCode":"  private void doTestMergeResultsWithValidLimit(final int limit)\n  {\n    GroupByQuery.Builder builder = GroupByQuery\n        .builder()\n        .setDataSource(QueryRunnerTestHelper.dataSource)\n        .setInterval(\"2011-04-02/2011-04-04\")\n        .setDimensions(new DefaultDimensionSpec(\"quality\", \"alias\"))\n        .setAggregatorSpecs(QueryRunnerTestHelper.rowsCount, new LongSumAggregatorFactory(\"idx\", \"index\"))\n        .setGranularity(new PeriodGranularity(new Period(\"P1M\"), null, null))\n        .setLimit(limit);\n\n    final GroupByQuery fullQuery = builder.build();\n\n    List<Row> expectedResults = Arrays.asList(\n        GroupByQueryRunnerTestHelper.createExpectedRow(\"2011-04-01\", \"alias\", \"automotive\", \"rows\", 2L, \"idx\", 269L),\n        GroupByQueryRunnerTestHelper.createExpectedRow(\"2011-04-01\", \"alias\", \"business\", \"rows\", 2L, \"idx\", 217L),\n        GroupByQueryRunnerTestHelper.createExpectedRow(\"2011-04-01\", \"alias\", \"entertainment\", \"rows\", 2L, \"idx\", 319L),\n        GroupByQueryRunnerTestHelper.createExpectedRow(\"2011-04-01\", \"alias\", \"health\", \"rows\", 2L, \"idx\", 216L),\n        GroupByQueryRunnerTestHelper.createExpectedRow(\"2011-04-01\", \"alias\", \"mezzanine\", \"rows\", 6L, \"idx\", 4420L),\n        GroupByQueryRunnerTestHelper.createExpectedRow(\"2011-04-01\", \"alias\", \"news\", \"rows\", 2L, \"idx\", 221L),\n        GroupByQueryRunnerTestHelper.createExpectedRow(\"2011-04-01\", \"alias\", \"premium\", \"rows\", 6L, \"idx\", 4416L),\n        GroupByQueryRunnerTestHelper.createExpectedRow(\"2011-04-01\", \"alias\", \"technology\", \"rows\", 2L, \"idx\", 177L),\n        GroupByQueryRunnerTestHelper.createExpectedRow(\"2011-04-01\", \"alias\", \"travel\", \"rows\", 2L, \"idx\", 243L)\n    );\n\n    QueryRunner<Row> mergeRunner = factory.getToolchest().mergeResults(runner);\n\n    Map<String, Object> context = Maps.newHashMap();\n    TestHelper.assertExpectedObjects(\n        Iterables.limit(expectedResults, limit),\n        mergeRunner.run(QueryPlus.wrap(fullQuery), context),\n        StringUtils.format(\"limit: %d\", limit)\n    );\n  }\n","date":"2018-08-31 00:56:26","endLine":2448,"groupId":"22359","id":1,"instanceNumber":1,"isCurCommit":0,"methodName":"doTestMergeResultsWithValidLimit","params":"(finalintlimit)","path":"/mnt/clonedata/CloneManagementServer/ManagementServer/consistResult/result-druid-10-0.7/blobInfo/CC_OUT/blobs/76/97506dd7d87c44ca1fada8cf624753ca12a2ea.src","preCode":"  private void doTestMergeResultsWithValidLimit(final int limit)\n  {\n    GroupByQuery.Builder builder = GroupByQuery\n        .builder()\n        .setDataSource(QueryRunnerTestHelper.dataSource)\n        .setInterval(\"2011-04-02/2011-04-04\")\n        .setDimensions(new DefaultDimensionSpec(\"quality\", \"alias\"))\n        .setAggregatorSpecs(QueryRunnerTestHelper.rowsCount, new LongSumAggregatorFactory(\"idx\", \"index\"))\n        .setGranularity(new PeriodGranularity(new Period(\"P1M\"), null, null))\n        .setLimit(limit);\n\n    final GroupByQuery fullQuery = builder.build();\n\n    List<Row> expectedResults = Arrays.asList(\n        GroupByQueryRunnerTestHelper.createExpectedRow(\"2011-04-01\", \"alias\", \"automotive\", \"rows\", 2L, \"idx\", 269L),\n        GroupByQueryRunnerTestHelper.createExpectedRow(\"2011-04-01\", \"alias\", \"business\", \"rows\", 2L, \"idx\", 217L),\n        GroupByQueryRunnerTestHelper.createExpectedRow(\"2011-04-01\", \"alias\", \"entertainment\", \"rows\", 2L, \"idx\", 319L),\n        GroupByQueryRunnerTestHelper.createExpectedRow(\"2011-04-01\", \"alias\", \"health\", \"rows\", 2L, \"idx\", 216L),\n        GroupByQueryRunnerTestHelper.createExpectedRow(\"2011-04-01\", \"alias\", \"mezzanine\", \"rows\", 6L, \"idx\", 4420L),\n        GroupByQueryRunnerTestHelper.createExpectedRow(\"2011-04-01\", \"alias\", \"news\", \"rows\", 2L, \"idx\", 221L),\n        GroupByQueryRunnerTestHelper.createExpectedRow(\"2011-04-01\", \"alias\", \"premium\", \"rows\", 6L, \"idx\", 4416L),\n        GroupByQueryRunnerTestHelper.createExpectedRow(\"2011-04-01\", \"alias\", \"technology\", \"rows\", 2L, \"idx\", 177L),\n        GroupByQueryRunnerTestHelper.createExpectedRow(\"2011-04-01\", \"alias\", \"travel\", \"rows\", 2L, \"idx\", 243L)\n    );\n\n    QueryRunner<Row> mergeRunner = factory.getToolchest().mergeResults(runner);\n\n    Map<String, Object> context = Maps.newHashMap();\n    TestHelper.assertExpectedObjects(\n        Iterables.limit(expectedResults, limit),\n        mergeRunner.run(QueryPlus.wrap(fullQuery), context),\n        StringUtils.format(\"limit: %d\", limit)\n    );\n  }\n","realPath":"processing/src/test/java/org/apache/druid/query/groupby/GroupByQueryRunnerTest.java","repoName":"druid","snippetEndLine":0,"snippetStartLine":0,"startLine":2415,"status":"B"},{"authorDate":"2018-08-31 00:56:26","commitOrder":1,"curCode":"  public void testBySegmentResultsUnOptimizedDimextraction()\n  {\n    int segmentCount = 32;\n    Result<BySegmentResultValue> singleSegmentResult = new Result<BySegmentResultValue>(\n        DateTimes.of(\"2011-01-12T00:00:00.000Z\"),\n        new BySegmentResultValueClass(\n            Collections.singletonList(\n                GroupByQueryRunnerTestHelper.createExpectedRow(\n                    \"2011-04-01\",\n                    \"alias\",\n                    \"mezzanine0\",\n                    \"rows\",\n                    6L,\n                    \"idx\",\n                    4420L\n                )\n            ), \"testSegment\", Intervals.of(\"2011-04-02T00:00:00.000Z/2011-04-04T00:00:00.000Z\")\n        )\n    );\n    List<Result> bySegmentResults = Lists.newArrayList();\n    for (int i = 0; i < segmentCount; i++) {\n      bySegmentResults.add(singleSegmentResult);\n    }\n    GroupByQuery.Builder builder = GroupByQuery\n        .builder()\n        .setDataSource(QueryRunnerTestHelper.dataSource)\n        .setInterval(\"2011-04-02/2011-04-04\").setDimensions(new ExtractionDimensionSpec(\n            \"quality\",\n            \"alias\",\n            new LookupExtractionFn(\n                new MapLookupExtractor(ImmutableMap.of(\"mezzanine\", \"mezzanine0\"), false),\n                false,\n                null,\n                false,\n                false\n            )\n        )).setAggregatorSpecs(QueryRunnerTestHelper.rowsCount, new LongSumAggregatorFactory(\"idx\", \"index\"))\n        .setGranularity(new PeriodGranularity(new Period(\"P1M\"), null, null))\n        .setDimFilter(new SelectorDimFilter(\"quality\", \"mezzanine\", null))\n        .setContext(ImmutableMap.of(\"bySegment\", true));\n    final GroupByQuery fullQuery = builder.build();\n    QueryToolChest toolChest = factory.getToolchest();\n\n    List<QueryRunner<Row>> singleSegmentRunners = Lists.newArrayList();\n    for (int i = 0; i < segmentCount; i++) {\n      singleSegmentRunners.add(toolChest.preMergeQueryDecoration(runner));\n    }\n    ExecutorService exec = Executors.newCachedThreadPool();\n    QueryRunner theRunner = toolChest.postMergeQueryDecoration(\n        new FinalizeResultsQueryRunner<>(\n            toolChest.mergeResults(factory.mergeRunners(Executors.newCachedThreadPool(), singleSegmentRunners)),\n            toolChest\n        )\n    );\n\n    TestHelper.assertExpectedObjects(bySegmentResults, theRunner.run(QueryPlus.wrap(fullQuery), Maps.newHashMap()), \"\");\n    exec.shutdownNow();\n  }\n","date":"2018-08-31 00:56:26","endLine":6384,"groupId":"15456","id":2,"instanceNumber":2,"isCurCommit":0,"methodName":"testBySegmentResultsUnOptimizedDimextraction","params":"()","path":"/mnt/clonedata/CloneManagementServer/ManagementServer/consistResult/result-druid-10-0.7/blobInfo/CC_OUT/blobs/76/97506dd7d87c44ca1fada8cf624753ca12a2ea.src","preCode":"  public void testBySegmentResultsUnOptimizedDimextraction()\n  {\n    int segmentCount = 32;\n    Result<BySegmentResultValue> singleSegmentResult = new Result<BySegmentResultValue>(\n        DateTimes.of(\"2011-01-12T00:00:00.000Z\"),\n        new BySegmentResultValueClass(\n            Collections.singletonList(\n                GroupByQueryRunnerTestHelper.createExpectedRow(\n                    \"2011-04-01\",\n                    \"alias\",\n                    \"mezzanine0\",\n                    \"rows\",\n                    6L,\n                    \"idx\",\n                    4420L\n                )\n            ), \"testSegment\", Intervals.of(\"2011-04-02T00:00:00.000Z/2011-04-04T00:00:00.000Z\")\n        )\n    );\n    List<Result> bySegmentResults = Lists.newArrayList();\n    for (int i = 0; i < segmentCount; i++) {\n      bySegmentResults.add(singleSegmentResult);\n    }\n    GroupByQuery.Builder builder = GroupByQuery\n        .builder()\n        .setDataSource(QueryRunnerTestHelper.dataSource)\n        .setInterval(\"2011-04-02/2011-04-04\").setDimensions(new ExtractionDimensionSpec(\n            \"quality\",\n            \"alias\",\n            new LookupExtractionFn(\n                new MapLookupExtractor(ImmutableMap.of(\"mezzanine\", \"mezzanine0\"), false),\n                false,\n                null,\n                false,\n                false\n            )\n        )).setAggregatorSpecs(QueryRunnerTestHelper.rowsCount, new LongSumAggregatorFactory(\"idx\", \"index\"))\n        .setGranularity(new PeriodGranularity(new Period(\"P1M\"), null, null))\n        .setDimFilter(new SelectorDimFilter(\"quality\", \"mezzanine\", null))\n        .setContext(ImmutableMap.of(\"bySegment\", true));\n    final GroupByQuery fullQuery = builder.build();\n    QueryToolChest toolChest = factory.getToolchest();\n\n    List<QueryRunner<Row>> singleSegmentRunners = Lists.newArrayList();\n    for (int i = 0; i < segmentCount; i++) {\n      singleSegmentRunners.add(toolChest.preMergeQueryDecoration(runner));\n    }\n    ExecutorService exec = Executors.newCachedThreadPool();\n    QueryRunner theRunner = toolChest.postMergeQueryDecoration(\n        new FinalizeResultsQueryRunner<>(\n            toolChest.mergeResults(factory.mergeRunners(Executors.newCachedThreadPool(), singleSegmentRunners)),\n            toolChest\n        )\n    );\n\n    TestHelper.assertExpectedObjects(bySegmentResults, theRunner.run(QueryPlus.wrap(fullQuery), Maps.newHashMap()), \"\");\n    exec.shutdownNow();\n  }\n","realPath":"processing/src/test/java/org/apache/druid/query/groupby/GroupByQueryRunnerTest.java","repoName":"druid","snippetEndLine":0,"snippetStartLine":0,"startLine":6327,"status":"B"}],"commitId":"431d3d8497f9079857c3baa7ae7ab4cb44a22355","commitMessage":"@@@Rename io.druid to org.apache.druid. (#6266)\n\n* Rename io.druid to org.apache.druid.\n\n* Fix META-INF files and remove some benchmark results.\n\n* MonitorsConfig update for metrics package migration.\n\n* Reorder some dimensions in inner queries for some reason.\n\n* Fix protobuf tests.\n","date":"2018-08-31 00:56:26","modifiedFileCount":"5","status":"B","submitter":"Gian Merlino"},{"authorTime":"2018-10-29 20:02:43","codes":[{"authorDate":"2018-10-29 20:02:43","commitOrder":2,"curCode":"  private void doTestMergeResultsWithValidLimit(final int limit)\n  {\n    GroupByQuery.Builder builder = GroupByQuery\n        .builder()\n        .setDataSource(QueryRunnerTestHelper.dataSource)\n        .setInterval(\"2011-04-02/2011-04-04\")\n        .setDimensions(new DefaultDimensionSpec(\"quality\", \"alias\"))\n        .setAggregatorSpecs(QueryRunnerTestHelper.rowsCount, new LongSumAggregatorFactory(\"idx\", \"index\"))\n        .setGranularity(new PeriodGranularity(new Period(\"P1M\"), null, null))\n        .setLimit(limit);\n\n    final GroupByQuery fullQuery = builder.build();\n\n    List<Row> expectedResults = Arrays.asList(\n        GroupByQueryRunnerTestHelper.createExpectedRow(\"2011-04-01\", \"alias\", \"automotive\", \"rows\", 2L, \"idx\", 269L),\n        GroupByQueryRunnerTestHelper.createExpectedRow(\"2011-04-01\", \"alias\", \"business\", \"rows\", 2L, \"idx\", 217L),\n        GroupByQueryRunnerTestHelper.createExpectedRow(\"2011-04-01\", \"alias\", \"entertainment\", \"rows\", 2L, \"idx\", 319L),\n        GroupByQueryRunnerTestHelper.createExpectedRow(\"2011-04-01\", \"alias\", \"health\", \"rows\", 2L, \"idx\", 216L),\n        GroupByQueryRunnerTestHelper.createExpectedRow(\"2011-04-01\", \"alias\", \"mezzanine\", \"rows\", 6L, \"idx\", 4420L),\n        GroupByQueryRunnerTestHelper.createExpectedRow(\"2011-04-01\", \"alias\", \"news\", \"rows\", 2L, \"idx\", 221L),\n        GroupByQueryRunnerTestHelper.createExpectedRow(\"2011-04-01\", \"alias\", \"premium\", \"rows\", 6L, \"idx\", 4416L),\n        GroupByQueryRunnerTestHelper.createExpectedRow(\"2011-04-01\", \"alias\", \"technology\", \"rows\", 2L, \"idx\", 177L),\n        GroupByQueryRunnerTestHelper.createExpectedRow(\"2011-04-01\", \"alias\", \"travel\", \"rows\", 2L, \"idx\", 243L)\n    );\n\n    QueryRunner<Row> mergeRunner = factory.getToolchest().mergeResults(runner);\n\n    Map<String, Object> context = new HashMap<>();\n    TestHelper.assertExpectedObjects(\n        Iterables.limit(expectedResults, limit),\n        mergeRunner.run(QueryPlus.wrap(fullQuery), context),\n        StringUtils.format(\"limit: %d\", limit)\n    );\n  }\n","date":"2018-10-29 20:02:43","endLine":2445,"groupId":"22359","id":3,"instanceNumber":1,"isCurCommit":0,"methodName":"doTestMergeResultsWithValidLimit","params":"(finalintlimit)","path":"/mnt/clonedata/CloneManagementServer/ManagementServer/consistResult/result-druid-10-0.7/blobInfo/CC_OUT/blobs/99/a1aea429912fd6c96752f3d7781eed422e6878.src","preCode":"  private void doTestMergeResultsWithValidLimit(final int limit)\n  {\n    GroupByQuery.Builder builder = GroupByQuery\n        .builder()\n        .setDataSource(QueryRunnerTestHelper.dataSource)\n        .setInterval(\"2011-04-02/2011-04-04\")\n        .setDimensions(new DefaultDimensionSpec(\"quality\", \"alias\"))\n        .setAggregatorSpecs(QueryRunnerTestHelper.rowsCount, new LongSumAggregatorFactory(\"idx\", \"index\"))\n        .setGranularity(new PeriodGranularity(new Period(\"P1M\"), null, null))\n        .setLimit(limit);\n\n    final GroupByQuery fullQuery = builder.build();\n\n    List<Row> expectedResults = Arrays.asList(\n        GroupByQueryRunnerTestHelper.createExpectedRow(\"2011-04-01\", \"alias\", \"automotive\", \"rows\", 2L, \"idx\", 269L),\n        GroupByQueryRunnerTestHelper.createExpectedRow(\"2011-04-01\", \"alias\", \"business\", \"rows\", 2L, \"idx\", 217L),\n        GroupByQueryRunnerTestHelper.createExpectedRow(\"2011-04-01\", \"alias\", \"entertainment\", \"rows\", 2L, \"idx\", 319L),\n        GroupByQueryRunnerTestHelper.createExpectedRow(\"2011-04-01\", \"alias\", \"health\", \"rows\", 2L, \"idx\", 216L),\n        GroupByQueryRunnerTestHelper.createExpectedRow(\"2011-04-01\", \"alias\", \"mezzanine\", \"rows\", 6L, \"idx\", 4420L),\n        GroupByQueryRunnerTestHelper.createExpectedRow(\"2011-04-01\", \"alias\", \"news\", \"rows\", 2L, \"idx\", 221L),\n        GroupByQueryRunnerTestHelper.createExpectedRow(\"2011-04-01\", \"alias\", \"premium\", \"rows\", 6L, \"idx\", 4416L),\n        GroupByQueryRunnerTestHelper.createExpectedRow(\"2011-04-01\", \"alias\", \"technology\", \"rows\", 2L, \"idx\", 177L),\n        GroupByQueryRunnerTestHelper.createExpectedRow(\"2011-04-01\", \"alias\", \"travel\", \"rows\", 2L, \"idx\", 243L)\n    );\n\n    QueryRunner<Row> mergeRunner = factory.getToolchest().mergeResults(runner);\n\n    Map<String, Object> context = Maps.newHashMap();\n    TestHelper.assertExpectedObjects(\n        Iterables.limit(expectedResults, limit),\n        mergeRunner.run(QueryPlus.wrap(fullQuery), context),\n        StringUtils.format(\"limit: %d\", limit)\n    );\n  }\n","realPath":"processing/src/test/java/org/apache/druid/query/groupby/GroupByQueryRunnerTest.java","repoName":"druid","snippetEndLine":0,"snippetStartLine":0,"startLine":2412,"status":"M"},{"authorDate":"2018-10-29 20:02:43","commitOrder":2,"curCode":"  public void testBySegmentResultsUnOptimizedDimextraction()\n  {\n    int segmentCount = 32;\n    Result<BySegmentResultValue> singleSegmentResult = new Result<BySegmentResultValue>(\n        DateTimes.of(\"2011-01-12T00:00:00.000Z\"),\n        new BySegmentResultValueClass(\n            Collections.singletonList(\n                GroupByQueryRunnerTestHelper.createExpectedRow(\n                    \"2011-04-01\",\n                    \"alias\",\n                    \"mezzanine0\",\n                    \"rows\",\n                    6L,\n                    \"idx\",\n                    4420L\n                )\n            ), \"testSegment\", Intervals.of(\"2011-04-02T00:00:00.000Z/2011-04-04T00:00:00.000Z\")\n        )\n    );\n    List<Result> bySegmentResults = new ArrayList<>();\n    for (int i = 0; i < segmentCount; i++) {\n      bySegmentResults.add(singleSegmentResult);\n    }\n    GroupByQuery.Builder builder = GroupByQuery\n        .builder()\n        .setDataSource(QueryRunnerTestHelper.dataSource)\n        .setInterval(\"2011-04-02/2011-04-04\").setDimensions(new ExtractionDimensionSpec(\n            \"quality\",\n            \"alias\",\n            new LookupExtractionFn(\n                new MapLookupExtractor(ImmutableMap.of(\"mezzanine\", \"mezzanine0\"), false),\n                false,\n                null,\n                false,\n                false\n            )\n        )).setAggregatorSpecs(QueryRunnerTestHelper.rowsCount, new LongSumAggregatorFactory(\"idx\", \"index\"))\n        .setGranularity(new PeriodGranularity(new Period(\"P1M\"), null, null))\n        .setDimFilter(new SelectorDimFilter(\"quality\", \"mezzanine\", null))\n        .setContext(ImmutableMap.of(\"bySegment\", true));\n    final GroupByQuery fullQuery = builder.build();\n    QueryToolChest toolChest = factory.getToolchest();\n\n    List<QueryRunner<Row>> singleSegmentRunners = new ArrayList<>();\n    for (int i = 0; i < segmentCount; i++) {\n      singleSegmentRunners.add(toolChest.preMergeQueryDecoration(runner));\n    }\n    ExecutorService exec = Executors.newCachedThreadPool();\n    QueryRunner theRunner = toolChest.postMergeQueryDecoration(\n        new FinalizeResultsQueryRunner<>(\n            toolChest.mergeResults(factory.mergeRunners(Executors.newCachedThreadPool(), singleSegmentRunners)),\n            toolChest\n        )\n    );\n\n    TestHelper.assertExpectedObjects(bySegmentResults, theRunner.run(QueryPlus.wrap(fullQuery), new HashMap<>()), \"\");\n    exec.shutdownNow();\n  }\n","date":"2018-10-29 20:02:43","endLine":6382,"groupId":"17113","id":4,"instanceNumber":2,"isCurCommit":0,"methodName":"testBySegmentResultsUnOptimizedDimextraction","params":"()","path":"/mnt/clonedata/CloneManagementServer/ManagementServer/consistResult/result-druid-10-0.7/blobInfo/CC_OUT/blobs/99/a1aea429912fd6c96752f3d7781eed422e6878.src","preCode":"  public void testBySegmentResultsUnOptimizedDimextraction()\n  {\n    int segmentCount = 32;\n    Result<BySegmentResultValue> singleSegmentResult = new Result<BySegmentResultValue>(\n        DateTimes.of(\"2011-01-12T00:00:00.000Z\"),\n        new BySegmentResultValueClass(\n            Collections.singletonList(\n                GroupByQueryRunnerTestHelper.createExpectedRow(\n                    \"2011-04-01\",\n                    \"alias\",\n                    \"mezzanine0\",\n                    \"rows\",\n                    6L,\n                    \"idx\",\n                    4420L\n                )\n            ), \"testSegment\", Intervals.of(\"2011-04-02T00:00:00.000Z/2011-04-04T00:00:00.000Z\")\n        )\n    );\n    List<Result> bySegmentResults = Lists.newArrayList();\n    for (int i = 0; i < segmentCount; i++) {\n      bySegmentResults.add(singleSegmentResult);\n    }\n    GroupByQuery.Builder builder = GroupByQuery\n        .builder()\n        .setDataSource(QueryRunnerTestHelper.dataSource)\n        .setInterval(\"2011-04-02/2011-04-04\").setDimensions(new ExtractionDimensionSpec(\n            \"quality\",\n            \"alias\",\n            new LookupExtractionFn(\n                new MapLookupExtractor(ImmutableMap.of(\"mezzanine\", \"mezzanine0\"), false),\n                false,\n                null,\n                false,\n                false\n            )\n        )).setAggregatorSpecs(QueryRunnerTestHelper.rowsCount, new LongSumAggregatorFactory(\"idx\", \"index\"))\n        .setGranularity(new PeriodGranularity(new Period(\"P1M\"), null, null))\n        .setDimFilter(new SelectorDimFilter(\"quality\", \"mezzanine\", null))\n        .setContext(ImmutableMap.of(\"bySegment\", true));\n    final GroupByQuery fullQuery = builder.build();\n    QueryToolChest toolChest = factory.getToolchest();\n\n    List<QueryRunner<Row>> singleSegmentRunners = Lists.newArrayList();\n    for (int i = 0; i < segmentCount; i++) {\n      singleSegmentRunners.add(toolChest.preMergeQueryDecoration(runner));\n    }\n    ExecutorService exec = Executors.newCachedThreadPool();\n    QueryRunner theRunner = toolChest.postMergeQueryDecoration(\n        new FinalizeResultsQueryRunner<>(\n            toolChest.mergeResults(factory.mergeRunners(Executors.newCachedThreadPool(), singleSegmentRunners)),\n            toolChest\n        )\n    );\n\n    TestHelper.assertExpectedObjects(bySegmentResults, theRunner.run(QueryPlus.wrap(fullQuery), Maps.newHashMap()), \"\");\n    exec.shutdownNow();\n  }\n","realPath":"processing/src/test/java/org/apache/druid/query/groupby/GroupByQueryRunnerTest.java","repoName":"druid","snippetEndLine":0,"snippetStartLine":0,"startLine":6325,"status":"M"}],"commitId":"676f5e6d7f184101b8763e4249b18b237bbe0ec7","commitMessage":"@@@Prohibit some guava collection APIs and use JDK collection APIs directly (#6511)\n\n* Prohibit some guava collection APIs and use JDK APIs directly\n\n* reset files that changed by accident\n\n* sort codestyle/druid-forbidden-apis.txt alphabetically\n","date":"2018-10-29 20:02:43","modifiedFileCount":"427","status":"M","submitter":"QiuMM"},{"authorTime":"2018-12-12 00:05:50","codes":[{"authorDate":"2018-10-29 20:02:43","commitOrder":3,"curCode":"  private void doTestMergeResultsWithValidLimit(final int limit)\n  {\n    GroupByQuery.Builder builder = GroupByQuery\n        .builder()\n        .setDataSource(QueryRunnerTestHelper.dataSource)\n        .setInterval(\"2011-04-02/2011-04-04\")\n        .setDimensions(new DefaultDimensionSpec(\"quality\", \"alias\"))\n        .setAggregatorSpecs(QueryRunnerTestHelper.rowsCount, new LongSumAggregatorFactory(\"idx\", \"index\"))\n        .setGranularity(new PeriodGranularity(new Period(\"P1M\"), null, null))\n        .setLimit(limit);\n\n    final GroupByQuery fullQuery = builder.build();\n\n    List<Row> expectedResults = Arrays.asList(\n        GroupByQueryRunnerTestHelper.createExpectedRow(\"2011-04-01\", \"alias\", \"automotive\", \"rows\", 2L, \"idx\", 269L),\n        GroupByQueryRunnerTestHelper.createExpectedRow(\"2011-04-01\", \"alias\", \"business\", \"rows\", 2L, \"idx\", 217L),\n        GroupByQueryRunnerTestHelper.createExpectedRow(\"2011-04-01\", \"alias\", \"entertainment\", \"rows\", 2L, \"idx\", 319L),\n        GroupByQueryRunnerTestHelper.createExpectedRow(\"2011-04-01\", \"alias\", \"health\", \"rows\", 2L, \"idx\", 216L),\n        GroupByQueryRunnerTestHelper.createExpectedRow(\"2011-04-01\", \"alias\", \"mezzanine\", \"rows\", 6L, \"idx\", 4420L),\n        GroupByQueryRunnerTestHelper.createExpectedRow(\"2011-04-01\", \"alias\", \"news\", \"rows\", 2L, \"idx\", 221L),\n        GroupByQueryRunnerTestHelper.createExpectedRow(\"2011-04-01\", \"alias\", \"premium\", \"rows\", 6L, \"idx\", 4416L),\n        GroupByQueryRunnerTestHelper.createExpectedRow(\"2011-04-01\", \"alias\", \"technology\", \"rows\", 2L, \"idx\", 177L),\n        GroupByQueryRunnerTestHelper.createExpectedRow(\"2011-04-01\", \"alias\", \"travel\", \"rows\", 2L, \"idx\", 243L)\n    );\n\n    QueryRunner<Row> mergeRunner = factory.getToolchest().mergeResults(runner);\n\n    Map<String, Object> context = new HashMap<>();\n    TestHelper.assertExpectedObjects(\n        Iterables.limit(expectedResults, limit),\n        mergeRunner.run(QueryPlus.wrap(fullQuery), context),\n        StringUtils.format(\"limit: %d\", limit)\n    );\n  }\n","date":"2018-10-29 20:02:43","endLine":2445,"groupId":"22359","id":5,"instanceNumber":1,"isCurCommit":0,"methodName":"doTestMergeResultsWithValidLimit","params":"(finalintlimit)","path":"/mnt/clonedata/CloneManagementServer/ManagementServer/consistResult/result-druid-10-0.7/blobInfo/CC_OUT/blobs/99/a1aea429912fd6c96752f3d7781eed422e6878.src","preCode":"  private void doTestMergeResultsWithValidLimit(final int limit)\n  {\n    GroupByQuery.Builder builder = GroupByQuery\n        .builder()\n        .setDataSource(QueryRunnerTestHelper.dataSource)\n        .setInterval(\"2011-04-02/2011-04-04\")\n        .setDimensions(new DefaultDimensionSpec(\"quality\", \"alias\"))\n        .setAggregatorSpecs(QueryRunnerTestHelper.rowsCount, new LongSumAggregatorFactory(\"idx\", \"index\"))\n        .setGranularity(new PeriodGranularity(new Period(\"P1M\"), null, null))\n        .setLimit(limit);\n\n    final GroupByQuery fullQuery = builder.build();\n\n    List<Row> expectedResults = Arrays.asList(\n        GroupByQueryRunnerTestHelper.createExpectedRow(\"2011-04-01\", \"alias\", \"automotive\", \"rows\", 2L, \"idx\", 269L),\n        GroupByQueryRunnerTestHelper.createExpectedRow(\"2011-04-01\", \"alias\", \"business\", \"rows\", 2L, \"idx\", 217L),\n        GroupByQueryRunnerTestHelper.createExpectedRow(\"2011-04-01\", \"alias\", \"entertainment\", \"rows\", 2L, \"idx\", 319L),\n        GroupByQueryRunnerTestHelper.createExpectedRow(\"2011-04-01\", \"alias\", \"health\", \"rows\", 2L, \"idx\", 216L),\n        GroupByQueryRunnerTestHelper.createExpectedRow(\"2011-04-01\", \"alias\", \"mezzanine\", \"rows\", 6L, \"idx\", 4420L),\n        GroupByQueryRunnerTestHelper.createExpectedRow(\"2011-04-01\", \"alias\", \"news\", \"rows\", 2L, \"idx\", 221L),\n        GroupByQueryRunnerTestHelper.createExpectedRow(\"2011-04-01\", \"alias\", \"premium\", \"rows\", 6L, \"idx\", 4416L),\n        GroupByQueryRunnerTestHelper.createExpectedRow(\"2011-04-01\", \"alias\", \"technology\", \"rows\", 2L, \"idx\", 177L),\n        GroupByQueryRunnerTestHelper.createExpectedRow(\"2011-04-01\", \"alias\", \"travel\", \"rows\", 2L, \"idx\", 243L)\n    );\n\n    QueryRunner<Row> mergeRunner = factory.getToolchest().mergeResults(runner);\n\n    Map<String, Object> context = new HashMap<>();\n    TestHelper.assertExpectedObjects(\n        Iterables.limit(expectedResults, limit),\n        mergeRunner.run(QueryPlus.wrap(fullQuery), context),\n        StringUtils.format(\"limit: %d\", limit)\n    );\n  }\n","realPath":"processing/src/test/java/org/apache/druid/query/groupby/GroupByQueryRunnerTest.java","repoName":"druid","snippetEndLine":0,"snippetStartLine":0,"startLine":2412,"status":"N"},{"authorDate":"2018-12-12 00:05:50","commitOrder":3,"curCode":"  public void testBySegmentResultsUnOptimizedDimextraction()\n  {\n    int segmentCount = 32;\n    Result<BySegmentResultValue> singleSegmentResult = new Result<BySegmentResultValue>(\n        DateTimes.of(\"2011-01-12T00:00:00.000Z\"),\n        new BySegmentResultValueClass(\n            Collections.singletonList(\n                GroupByQueryRunnerTestHelper.createExpectedRow(\n                    \"2011-04-01\",\n                    \"alias\",\n                    \"mezzanine0\",\n                    \"rows\",\n                    6L,\n                    \"idx\",\n                    4420L\n                )\n            ), \"testSegment\", Intervals.of(\"2011-04-02T00:00:00.000Z/2011-04-04T00:00:00.000Z\")\n        )\n    );\n    List<Result> bySegmentResults = new ArrayList<>();\n    for (int i = 0; i < segmentCount; i++) {\n      bySegmentResults.add(singleSegmentResult);\n    }\n    GroupByQuery.Builder builder = GroupByQuery\n        .builder()\n        .setDataSource(QueryRunnerTestHelper.dataSource)\n        .setInterval(\"2011-04-02/2011-04-04\").setDimensions(new ExtractionDimensionSpec(\n            \"quality\",\n            \"alias\",\n            new LookupExtractionFn(\n                new MapLookupExtractor(ImmutableMap.of(\"mezzanine\", \"mezzanine0\"), false),\n                false,\n                null,\n                false,\n                false\n            )\n        )).setAggregatorSpecs(QueryRunnerTestHelper.rowsCount, new LongSumAggregatorFactory(\"idx\", \"index\"))\n        .setGranularity(new PeriodGranularity(new Period(\"P1M\"), null, null))\n        .setDimFilter(new SelectorDimFilter(\"quality\", \"mezzanine\", null))\n        .setContext(ImmutableMap.of(\"bySegment\", true));\n    final GroupByQuery fullQuery = builder.build();\n    QueryToolChest toolChest = factory.getToolchest();\n\n    List<QueryRunner<Row>> singleSegmentRunners = new ArrayList<>();\n    for (int i = 0; i < segmentCount; i++) {\n      singleSegmentRunners.add(toolChest.preMergeQueryDecoration(runner));\n    }\n    ExecutorService exec = Executors.newCachedThreadPool();\n    QueryRunner theRunner = toolChest.postMergeQueryDecoration(\n        new FinalizeResultsQueryRunner<>(\n            toolChest.mergeResults(factory.mergeRunners(Executors.newCachedThreadPool(), singleSegmentRunners)),\n            toolChest\n        )\n    );\n\n    TestHelper.assertExpectedObjects(bySegmentResults, theRunner.run(QueryPlus.wrap(fullQuery), new HashMap<>()), \"bySegment\");\n    exec.shutdownNow();\n  }\n","date":"2018-12-12 00:05:50","endLine":6489,"groupId":"17113","id":6,"instanceNumber":2,"isCurCommit":0,"methodName":"testBySegmentResultsUnOptimizedDimextraction","params":"()","path":"/mnt/clonedata/CloneManagementServer/ManagementServer/consistResult/result-druid-10-0.7/blobInfo/CC_OUT/blobs/64/6e2d33a4e2dcc89f87c320ca62cdd241ff9479.src","preCode":"  public void testBySegmentResultsUnOptimizedDimextraction()\n  {\n    int segmentCount = 32;\n    Result<BySegmentResultValue> singleSegmentResult = new Result<BySegmentResultValue>(\n        DateTimes.of(\"2011-01-12T00:00:00.000Z\"),\n        new BySegmentResultValueClass(\n            Collections.singletonList(\n                GroupByQueryRunnerTestHelper.createExpectedRow(\n                    \"2011-04-01\",\n                    \"alias\",\n                    \"mezzanine0\",\n                    \"rows\",\n                    6L,\n                    \"idx\",\n                    4420L\n                )\n            ), \"testSegment\", Intervals.of(\"2011-04-02T00:00:00.000Z/2011-04-04T00:00:00.000Z\")\n        )\n    );\n    List<Result> bySegmentResults = new ArrayList<>();\n    for (int i = 0; i < segmentCount; i++) {\n      bySegmentResults.add(singleSegmentResult);\n    }\n    GroupByQuery.Builder builder = GroupByQuery\n        .builder()\n        .setDataSource(QueryRunnerTestHelper.dataSource)\n        .setInterval(\"2011-04-02/2011-04-04\").setDimensions(new ExtractionDimensionSpec(\n            \"quality\",\n            \"alias\",\n            new LookupExtractionFn(\n                new MapLookupExtractor(ImmutableMap.of(\"mezzanine\", \"mezzanine0\"), false),\n                false,\n                null,\n                false,\n                false\n            )\n        )).setAggregatorSpecs(QueryRunnerTestHelper.rowsCount, new LongSumAggregatorFactory(\"idx\", \"index\"))\n        .setGranularity(new PeriodGranularity(new Period(\"P1M\"), null, null))\n        .setDimFilter(new SelectorDimFilter(\"quality\", \"mezzanine\", null))\n        .setContext(ImmutableMap.of(\"bySegment\", true));\n    final GroupByQuery fullQuery = builder.build();\n    QueryToolChest toolChest = factory.getToolchest();\n\n    List<QueryRunner<Row>> singleSegmentRunners = new ArrayList<>();\n    for (int i = 0; i < segmentCount; i++) {\n      singleSegmentRunners.add(toolChest.preMergeQueryDecoration(runner));\n    }\n    ExecutorService exec = Executors.newCachedThreadPool();\n    QueryRunner theRunner = toolChest.postMergeQueryDecoration(\n        new FinalizeResultsQueryRunner<>(\n            toolChest.mergeResults(factory.mergeRunners(Executors.newCachedThreadPool(), singleSegmentRunners)),\n            toolChest\n        )\n    );\n\n    TestHelper.assertExpectedObjects(bySegmentResults, theRunner.run(QueryPlus.wrap(fullQuery), new HashMap<>()), \"\");\n    exec.shutdownNow();\n  }\n","realPath":"processing/src/test/java/org/apache/druid/query/groupby/GroupByQueryRunnerTest.java","repoName":"druid","snippetEndLine":0,"snippetStartLine":0,"startLine":6432,"status":"M"}],"commitId":"86e3ae5b48bca7cbb9bf05e9e66d013f34dabf16","commitMessage":"@@@Add fail message (#6720)\n\n","date":"2018-12-12 00:05:50","modifiedFileCount":"4","status":"M","submitter":"Atul Mohan"},{"authorTime":"2019-01-22 03:11:10","codes":[{"authorDate":"2018-10-29 20:02:43","commitOrder":4,"curCode":"  private void doTestMergeResultsWithValidLimit(final int limit)\n  {\n    GroupByQuery.Builder builder = GroupByQuery\n        .builder()\n        .setDataSource(QueryRunnerTestHelper.dataSource)\n        .setInterval(\"2011-04-02/2011-04-04\")\n        .setDimensions(new DefaultDimensionSpec(\"quality\", \"alias\"))\n        .setAggregatorSpecs(QueryRunnerTestHelper.rowsCount, new LongSumAggregatorFactory(\"idx\", \"index\"))\n        .setGranularity(new PeriodGranularity(new Period(\"P1M\"), null, null))\n        .setLimit(limit);\n\n    final GroupByQuery fullQuery = builder.build();\n\n    List<Row> expectedResults = Arrays.asList(\n        GroupByQueryRunnerTestHelper.createExpectedRow(\"2011-04-01\", \"alias\", \"automotive\", \"rows\", 2L, \"idx\", 269L),\n        GroupByQueryRunnerTestHelper.createExpectedRow(\"2011-04-01\", \"alias\", \"business\", \"rows\", 2L, \"idx\", 217L),\n        GroupByQueryRunnerTestHelper.createExpectedRow(\"2011-04-01\", \"alias\", \"entertainment\", \"rows\", 2L, \"idx\", 319L),\n        GroupByQueryRunnerTestHelper.createExpectedRow(\"2011-04-01\", \"alias\", \"health\", \"rows\", 2L, \"idx\", 216L),\n        GroupByQueryRunnerTestHelper.createExpectedRow(\"2011-04-01\", \"alias\", \"mezzanine\", \"rows\", 6L, \"idx\", 4420L),\n        GroupByQueryRunnerTestHelper.createExpectedRow(\"2011-04-01\", \"alias\", \"news\", \"rows\", 2L, \"idx\", 221L),\n        GroupByQueryRunnerTestHelper.createExpectedRow(\"2011-04-01\", \"alias\", \"premium\", \"rows\", 6L, \"idx\", 4416L),\n        GroupByQueryRunnerTestHelper.createExpectedRow(\"2011-04-01\", \"alias\", \"technology\", \"rows\", 2L, \"idx\", 177L),\n        GroupByQueryRunnerTestHelper.createExpectedRow(\"2011-04-01\", \"alias\", \"travel\", \"rows\", 2L, \"idx\", 243L)\n    );\n\n    QueryRunner<Row> mergeRunner = factory.getToolchest().mergeResults(runner);\n\n    Map<String, Object> context = new HashMap<>();\n    TestHelper.assertExpectedObjects(\n        Iterables.limit(expectedResults, limit),\n        mergeRunner.run(QueryPlus.wrap(fullQuery), context),\n        StringUtils.format(\"limit: %d\", limit)\n    );\n  }\n","date":"2018-10-29 20:02:43","endLine":2445,"groupId":"22359","id":7,"instanceNumber":1,"isCurCommit":0,"methodName":"doTestMergeResultsWithValidLimit","params":"(finalintlimit)","path":"/mnt/clonedata/CloneManagementServer/ManagementServer/consistResult/result-druid-10-0.7/blobInfo/CC_OUT/blobs/99/a1aea429912fd6c96752f3d7781eed422e6878.src","preCode":"  private void doTestMergeResultsWithValidLimit(final int limit)\n  {\n    GroupByQuery.Builder builder = GroupByQuery\n        .builder()\n        .setDataSource(QueryRunnerTestHelper.dataSource)\n        .setInterval(\"2011-04-02/2011-04-04\")\n        .setDimensions(new DefaultDimensionSpec(\"quality\", \"alias\"))\n        .setAggregatorSpecs(QueryRunnerTestHelper.rowsCount, new LongSumAggregatorFactory(\"idx\", \"index\"))\n        .setGranularity(new PeriodGranularity(new Period(\"P1M\"), null, null))\n        .setLimit(limit);\n\n    final GroupByQuery fullQuery = builder.build();\n\n    List<Row> expectedResults = Arrays.asList(\n        GroupByQueryRunnerTestHelper.createExpectedRow(\"2011-04-01\", \"alias\", \"automotive\", \"rows\", 2L, \"idx\", 269L),\n        GroupByQueryRunnerTestHelper.createExpectedRow(\"2011-04-01\", \"alias\", \"business\", \"rows\", 2L, \"idx\", 217L),\n        GroupByQueryRunnerTestHelper.createExpectedRow(\"2011-04-01\", \"alias\", \"entertainment\", \"rows\", 2L, \"idx\", 319L),\n        GroupByQueryRunnerTestHelper.createExpectedRow(\"2011-04-01\", \"alias\", \"health\", \"rows\", 2L, \"idx\", 216L),\n        GroupByQueryRunnerTestHelper.createExpectedRow(\"2011-04-01\", \"alias\", \"mezzanine\", \"rows\", 6L, \"idx\", 4420L),\n        GroupByQueryRunnerTestHelper.createExpectedRow(\"2011-04-01\", \"alias\", \"news\", \"rows\", 2L, \"idx\", 221L),\n        GroupByQueryRunnerTestHelper.createExpectedRow(\"2011-04-01\", \"alias\", \"premium\", \"rows\", 6L, \"idx\", 4416L),\n        GroupByQueryRunnerTestHelper.createExpectedRow(\"2011-04-01\", \"alias\", \"technology\", \"rows\", 2L, \"idx\", 177L),\n        GroupByQueryRunnerTestHelper.createExpectedRow(\"2011-04-01\", \"alias\", \"travel\", \"rows\", 2L, \"idx\", 243L)\n    );\n\n    QueryRunner<Row> mergeRunner = factory.getToolchest().mergeResults(runner);\n\n    Map<String, Object> context = new HashMap<>();\n    TestHelper.assertExpectedObjects(\n        Iterables.limit(expectedResults, limit),\n        mergeRunner.run(QueryPlus.wrap(fullQuery), context),\n        StringUtils.format(\"limit: %d\", limit)\n    );\n  }\n","realPath":"processing/src/test/java/org/apache/druid/query/groupby/GroupByQueryRunnerTest.java","repoName":"druid","snippetEndLine":0,"snippetStartLine":0,"startLine":2412,"status":"N"},{"authorDate":"2019-01-22 03:11:10","commitOrder":4,"curCode":"  public void testBySegmentResultsUnOptimizedDimextraction()\n  {\n    int segmentCount = 32;\n    Result<BySegmentResultValue> singleSegmentResult = new Result<>(\n        DateTimes.of(\"2011-01-12T00:00:00.000Z\"),\n        new BySegmentResultValueClass<>(\n            Collections.singletonList(\n                GroupByQueryRunnerTestHelper.createExpectedRow(\n                    \"2011-04-01\",\n                    \"alias\",\n                    \"mezzanine0\",\n                    \"rows\",\n                    6L,\n                    \"idx\",\n                    4420L\n                )\n            ),\n            QueryRunnerTestHelper.segmentId.toString(),\n            Intervals.of(\"2011-04-02T00:00:00.000Z/2011-04-04T00:00:00.000Z\")\n        )\n    );\n    List<Result> bySegmentResults = new ArrayList<>();\n    for (int i = 0; i < segmentCount; i++) {\n      bySegmentResults.add(singleSegmentResult);\n    }\n    GroupByQuery.Builder builder = GroupByQuery\n        .builder()\n        .setDataSource(QueryRunnerTestHelper.dataSource)\n        .setInterval(\"2011-04-02/2011-04-04\").setDimensions(new ExtractionDimensionSpec(\n            \"quality\",\n            \"alias\",\n            new LookupExtractionFn(\n                new MapLookupExtractor(ImmutableMap.of(\"mezzanine\", \"mezzanine0\"), false),\n                false,\n                null,\n                false,\n                false\n            )\n        )).setAggregatorSpecs(QueryRunnerTestHelper.rowsCount, new LongSumAggregatorFactory(\"idx\", \"index\"))\n        .setGranularity(new PeriodGranularity(new Period(\"P1M\"), null, null))\n        .setDimFilter(new SelectorDimFilter(\"quality\", \"mezzanine\", null))\n        .setContext(ImmutableMap.of(\"bySegment\", true));\n    final GroupByQuery fullQuery = builder.build();\n    QueryToolChest toolChest = factory.getToolchest();\n\n    List<QueryRunner<Row>> singleSegmentRunners = new ArrayList<>();\n    for (int i = 0; i < segmentCount; i++) {\n      singleSegmentRunners.add(toolChest.preMergeQueryDecoration(runner));\n    }\n    ExecutorService exec = Executors.newCachedThreadPool();\n    QueryRunner theRunner = toolChest.postMergeQueryDecoration(\n        new FinalizeResultsQueryRunner<>(\n            toolChest.mergeResults(factory.mergeRunners(Executors.newCachedThreadPool(), singleSegmentRunners)),\n            toolChest\n        )\n    );\n\n    TestHelper.assertExpectedObjects(\n        bySegmentResults,\n        theRunner.run(QueryPlus.wrap(fullQuery), new HashMap<>()),\n        \"bySegment\"\n    );\n    exec.shutdownNow();\n  }\n","date":"2019-01-22 03:11:10","endLine":7963,"groupId":"17113","id":8,"instanceNumber":2,"isCurCommit":0,"methodName":"testBySegmentResultsUnOptimizedDimextraction","params":"()","path":"/mnt/clonedata/CloneManagementServer/ManagementServer/consistResult/result-druid-10-0.7/blobInfo/CC_OUT/blobs/3a/2d5324e53bb1e6db9aa94f99f59fd95c69d21f.src","preCode":"  public void testBySegmentResultsUnOptimizedDimextraction()\n  {\n    int segmentCount = 32;\n    Result<BySegmentResultValue> singleSegmentResult = new Result<BySegmentResultValue>(\n        DateTimes.of(\"2011-01-12T00:00:00.000Z\"),\n        new BySegmentResultValueClass(\n            Collections.singletonList(\n                GroupByQueryRunnerTestHelper.createExpectedRow(\n                    \"2011-04-01\",\n                    \"alias\",\n                    \"mezzanine0\",\n                    \"rows\",\n                    6L,\n                    \"idx\",\n                    4420L\n                )\n            ), \"testSegment\", Intervals.of(\"2011-04-02T00:00:00.000Z/2011-04-04T00:00:00.000Z\")\n        )\n    );\n    List<Result> bySegmentResults = new ArrayList<>();\n    for (int i = 0; i < segmentCount; i++) {\n      bySegmentResults.add(singleSegmentResult);\n    }\n    GroupByQuery.Builder builder = GroupByQuery\n        .builder()\n        .setDataSource(QueryRunnerTestHelper.dataSource)\n        .setInterval(\"2011-04-02/2011-04-04\").setDimensions(new ExtractionDimensionSpec(\n            \"quality\",\n            \"alias\",\n            new LookupExtractionFn(\n                new MapLookupExtractor(ImmutableMap.of(\"mezzanine\", \"mezzanine0\"), false),\n                false,\n                null,\n                false,\n                false\n            )\n        )).setAggregatorSpecs(QueryRunnerTestHelper.rowsCount, new LongSumAggregatorFactory(\"idx\", \"index\"))\n        .setGranularity(new PeriodGranularity(new Period(\"P1M\"), null, null))\n        .setDimFilter(new SelectorDimFilter(\"quality\", \"mezzanine\", null))\n        .setContext(ImmutableMap.of(\"bySegment\", true));\n    final GroupByQuery fullQuery = builder.build();\n    QueryToolChest toolChest = factory.getToolchest();\n\n    List<QueryRunner<Row>> singleSegmentRunners = new ArrayList<>();\n    for (int i = 0; i < segmentCount; i++) {\n      singleSegmentRunners.add(toolChest.preMergeQueryDecoration(runner));\n    }\n    ExecutorService exec = Executors.newCachedThreadPool();\n    QueryRunner theRunner = toolChest.postMergeQueryDecoration(\n        new FinalizeResultsQueryRunner<>(\n            toolChest.mergeResults(factory.mergeRunners(Executors.newCachedThreadPool(), singleSegmentRunners)),\n            toolChest\n        )\n    );\n\n    TestHelper.assertExpectedObjects(\n        bySegmentResults,\n        theRunner.run(QueryPlus.wrap(fullQuery), new HashMap<>()),\n        \"bySegment\"\n    );\n    exec.shutdownNow();\n  }\n","realPath":"processing/src/test/java/org/apache/druid/query/groupby/GroupByQueryRunnerTest.java","repoName":"druid","snippetEndLine":0,"snippetStartLine":0,"startLine":7900,"status":"M"}],"commitId":"8eae26fd4e7572060d112864dd3d5f6a865b9c89","commitMessage":"@@@Introduce SegmentId class (#6370)\n\n* Introduce SegmentId class\n\n* tmp\n\n* Fix SelectQueryRunnerTest\n\n* Fix indentation\n\n* Fixes\n\n* Remove Comparators.inverse() tests\n\n* Refinements\n\n* Fix tests\n\n* Fix more tests\n\n* Remove duplicate DataSegmentTest.  fixes #6064\n\n* SegmentDescriptor doc\n\n* Fix SQLMetadataStorageUpdaterJobHandler\n\n* Fix DataSegment deserialization for ignoring id\n\n* Add comments\n\n* More comments\n\n* Address more comments\n\n* Fix compilation\n\n* Restore segment2 in SystemSchemaTest according to a comment\n\n* Fix style\n\n* fix testServerSegmentsTable\n\n* Fix compilation\n\n* Add comments about why SegmentId and SegmentIdWithShardSpec are separate classes\n\n* Fix SystemSchemaTest\n\n* Fix style\n\n* Compare SegmentDescriptor with SegmentId in Javadoc and comments rather than with DataSegment\n\n* Remove a link.  see https://youtrack.jetbrains.com/issue/IDEA-205164\n\n* Fix compilation\n","date":"2019-01-22 03:11:10","modifiedFileCount":"308","status":"M","submitter":"Roman Leventov"},{"authorTime":"2019-07-13 03:54:07","codes":[{"authorDate":"2019-07-13 03:54:07","commitOrder":5,"curCode":"  private void doTestMergeResultsWithValidLimit(final int limit)\n  {\n    GroupByQuery.Builder builder = makeQueryBuilder()\n        .setDataSource(QueryRunnerTestHelper.dataSource)\n        .setInterval(\"2011-04-02/2011-04-04\")\n        .setDimensions(new DefaultDimensionSpec(\"quality\", \"alias\"))\n        .setAggregatorSpecs(QueryRunnerTestHelper.rowsCount, new LongSumAggregatorFactory(\"idx\", \"index\"))\n        .setGranularity(new PeriodGranularity(new Period(\"P1M\"), null, null))\n        .setLimit(limit);\n\n    final GroupByQuery fullQuery = builder.build();\n\n    List<Row> expectedResults = Arrays.asList(\n        GroupByQueryRunnerTestHelper.createExpectedRow(\"2011-04-01\", \"alias\", \"automotive\", \"rows\", 2L, \"idx\", 269L),\n        GroupByQueryRunnerTestHelper.createExpectedRow(\"2011-04-01\", \"alias\", \"business\", \"rows\", 2L, \"idx\", 217L),\n        GroupByQueryRunnerTestHelper.createExpectedRow(\"2011-04-01\", \"alias\", \"entertainment\", \"rows\", 2L, \"idx\", 319L),\n        GroupByQueryRunnerTestHelper.createExpectedRow(\"2011-04-01\", \"alias\", \"health\", \"rows\", 2L, \"idx\", 216L),\n        GroupByQueryRunnerTestHelper.createExpectedRow(\"2011-04-01\", \"alias\", \"mezzanine\", \"rows\", 6L, \"idx\", 4420L),\n        GroupByQueryRunnerTestHelper.createExpectedRow(\"2011-04-01\", \"alias\", \"news\", \"rows\", 2L, \"idx\", 221L),\n        GroupByQueryRunnerTestHelper.createExpectedRow(\"2011-04-01\", \"alias\", \"premium\", \"rows\", 6L, \"idx\", 4416L),\n        GroupByQueryRunnerTestHelper.createExpectedRow(\"2011-04-01\", \"alias\", \"technology\", \"rows\", 2L, \"idx\", 177L),\n        GroupByQueryRunnerTestHelper.createExpectedRow(\"2011-04-01\", \"alias\", \"travel\", \"rows\", 2L, \"idx\", 243L)\n    );\n\n    QueryRunner<Row> mergeRunner = factory.getToolchest().mergeResults(runner);\n\n    Map<String, Object> context = new HashMap<>();\n    TestHelper.assertExpectedObjects(\n        Iterables.limit(expectedResults, limit),\n        mergeRunner.run(QueryPlus.wrap(fullQuery), context),\n        StringUtils.format(\"limit: %d\", limit)\n    );\n  }\n","date":"2019-07-13 03:54:07","endLine":3018,"groupId":"22359","id":9,"instanceNumber":1,"isCurCommit":0,"methodName":"doTestMergeResultsWithValidLimit","params":"(finalintlimit)","path":"/mnt/clonedata/CloneManagementServer/ManagementServer/consistResult/result-druid-10-0.7/blobInfo/CC_OUT/blobs/3e/b69c0dda419ff55b78ba723f929d5a15f00f96.src","preCode":"  private void doTestMergeResultsWithValidLimit(final int limit)\n  {\n    GroupByQuery.Builder builder = GroupByQuery\n        .builder()\n        .setDataSource(QueryRunnerTestHelper.dataSource)\n        .setInterval(\"2011-04-02/2011-04-04\")\n        .setDimensions(new DefaultDimensionSpec(\"quality\", \"alias\"))\n        .setAggregatorSpecs(QueryRunnerTestHelper.rowsCount, new LongSumAggregatorFactory(\"idx\", \"index\"))\n        .setGranularity(new PeriodGranularity(new Period(\"P1M\"), null, null))\n        .setLimit(limit);\n\n    final GroupByQuery fullQuery = builder.build();\n\n    List<Row> expectedResults = Arrays.asList(\n        GroupByQueryRunnerTestHelper.createExpectedRow(\"2011-04-01\", \"alias\", \"automotive\", \"rows\", 2L, \"idx\", 269L),\n        GroupByQueryRunnerTestHelper.createExpectedRow(\"2011-04-01\", \"alias\", \"business\", \"rows\", 2L, \"idx\", 217L),\n        GroupByQueryRunnerTestHelper.createExpectedRow(\"2011-04-01\", \"alias\", \"entertainment\", \"rows\", 2L, \"idx\", 319L),\n        GroupByQueryRunnerTestHelper.createExpectedRow(\"2011-04-01\", \"alias\", \"health\", \"rows\", 2L, \"idx\", 216L),\n        GroupByQueryRunnerTestHelper.createExpectedRow(\"2011-04-01\", \"alias\", \"mezzanine\", \"rows\", 6L, \"idx\", 4420L),\n        GroupByQueryRunnerTestHelper.createExpectedRow(\"2011-04-01\", \"alias\", \"news\", \"rows\", 2L, \"idx\", 221L),\n        GroupByQueryRunnerTestHelper.createExpectedRow(\"2011-04-01\", \"alias\", \"premium\", \"rows\", 6L, \"idx\", 4416L),\n        GroupByQueryRunnerTestHelper.createExpectedRow(\"2011-04-01\", \"alias\", \"technology\", \"rows\", 2L, \"idx\", 177L),\n        GroupByQueryRunnerTestHelper.createExpectedRow(\"2011-04-01\", \"alias\", \"travel\", \"rows\", 2L, \"idx\", 243L)\n    );\n\n    QueryRunner<Row> mergeRunner = factory.getToolchest().mergeResults(runner);\n\n    Map<String, Object> context = new HashMap<>();\n    TestHelper.assertExpectedObjects(\n        Iterables.limit(expectedResults, limit),\n        mergeRunner.run(QueryPlus.wrap(fullQuery), context),\n        StringUtils.format(\"limit: %d\", limit)\n    );\n  }\n","realPath":"processing/src/test/java/org/apache/druid/query/groupby/GroupByQueryRunnerTest.java","repoName":"druid","snippetEndLine":0,"snippetStartLine":0,"startLine":2986,"status":"M"},{"authorDate":"2019-07-13 03:54:07","commitOrder":5,"curCode":"  public void testBySegmentResultsUnOptimizedDimextraction()\n  {\n    \r\n    cannotVectorize();\n\n    int segmentCount = 32;\n    Result<BySegmentResultValue> singleSegmentResult = new Result<>(\n        DateTimes.of(\"2011-01-12T00:00:00.000Z\"),\n        new BySegmentResultValueClass<>(\n            Collections.singletonList(\n                GroupByQueryRunnerTestHelper.createExpectedRow(\n                    \"2011-04-01\",\n                    \"alias\",\n                    \"mezzanine0\",\n                    \"rows\",\n                    6L,\n                    \"idx\",\n                    4420L\n                )\n            ),\n            QueryRunnerTestHelper.segmentId.toString(),\n            Intervals.of(\"2011-04-02T00:00:00.000Z/2011-04-04T00:00:00.000Z\")\n        )\n    );\n    List<Result> bySegmentResults = new ArrayList<>();\n    for (int i = 0; i < segmentCount; i++) {\n      bySegmentResults.add(singleSegmentResult);\n    }\n    GroupByQuery.Builder builder = makeQueryBuilder()\n        .setDataSource(QueryRunnerTestHelper.dataSource)\n        .setInterval(\"2011-04-02/2011-04-04\").setDimensions(new ExtractionDimensionSpec(\n            \"quality\",\n            \"alias\",\n            new LookupExtractionFn(\n                new MapLookupExtractor(ImmutableMap.of(\"mezzanine\", \"mezzanine0\"), false),\n                false,\n                null,\n                false,\n                false\n            )\n        )).setAggregatorSpecs(QueryRunnerTestHelper.rowsCount, new LongSumAggregatorFactory(\"idx\", \"index\"))\n        .setGranularity(new PeriodGranularity(new Period(\"P1M\"), null, null))\n        .setDimFilter(new SelectorDimFilter(\"quality\", \"mezzanine\", null))\n        .overrideContext(ImmutableMap.of(\"bySegment\", true));\n    final GroupByQuery fullQuery = builder.build();\n    QueryToolChest toolChest = factory.getToolchest();\n\n    List<QueryRunner<Row>> singleSegmentRunners = new ArrayList<>();\n    for (int i = 0; i < segmentCount; i++) {\n      singleSegmentRunners.add(toolChest.preMergeQueryDecoration(runner));\n    }\n    ExecutorService exec = Executors.newCachedThreadPool();\n    QueryRunner theRunner = toolChest.postMergeQueryDecoration(\n        new FinalizeResultsQueryRunner<>(\n            toolChest.mergeResults(factory.mergeRunners(Executors.newCachedThreadPool(), singleSegmentRunners)),\n            toolChest\n        )\n    );\n\n    TestHelper.assertExpectedObjects(\n        bySegmentResults,\n        theRunner.run(QueryPlus.wrap(fullQuery), new HashMap<>()),\n        \"bySegment\"\n    );\n    exec.shutdownNow();\n  }\n","date":"2019-07-13 03:54:07","endLine":8036,"groupId":"20575","id":10,"instanceNumber":2,"isCurCommit":0,"methodName":"testBySegmentResultsUnOptimizedDimextraction","params":"()","path":"/mnt/clonedata/CloneManagementServer/ManagementServer/consistResult/result-druid-10-0.7/blobInfo/CC_OUT/blobs/3e/b69c0dda419ff55b78ba723f929d5a15f00f96.src","preCode":"  public void testBySegmentResultsUnOptimizedDimextraction()\n  {\n    int segmentCount = 32;\n    Result<BySegmentResultValue> singleSegmentResult = new Result<>(\n        DateTimes.of(\"2011-01-12T00:00:00.000Z\"),\n        new BySegmentResultValueClass<>(\n            Collections.singletonList(\n                GroupByQueryRunnerTestHelper.createExpectedRow(\n                    \"2011-04-01\",\n                    \"alias\",\n                    \"mezzanine0\",\n                    \"rows\",\n                    6L,\n                    \"idx\",\n                    4420L\n                )\n            ),\n            QueryRunnerTestHelper.segmentId.toString(),\n            Intervals.of(\"2011-04-02T00:00:00.000Z/2011-04-04T00:00:00.000Z\")\n        )\n    );\n    List<Result> bySegmentResults = new ArrayList<>();\n    for (int i = 0; i < segmentCount; i++) {\n      bySegmentResults.add(singleSegmentResult);\n    }\n    GroupByQuery.Builder builder = GroupByQuery\n        .builder()\n        .setDataSource(QueryRunnerTestHelper.dataSource)\n        .setInterval(\"2011-04-02/2011-04-04\").setDimensions(new ExtractionDimensionSpec(\n            \"quality\",\n            \"alias\",\n            new LookupExtractionFn(\n                new MapLookupExtractor(ImmutableMap.of(\"mezzanine\", \"mezzanine0\"), false),\n                false,\n                null,\n                false,\n                false\n            )\n        )).setAggregatorSpecs(QueryRunnerTestHelper.rowsCount, new LongSumAggregatorFactory(\"idx\", \"index\"))\n        .setGranularity(new PeriodGranularity(new Period(\"P1M\"), null, null))\n        .setDimFilter(new SelectorDimFilter(\"quality\", \"mezzanine\", null))\n        .setContext(ImmutableMap.of(\"bySegment\", true));\n    final GroupByQuery fullQuery = builder.build();\n    QueryToolChest toolChest = factory.getToolchest();\n\n    List<QueryRunner<Row>> singleSegmentRunners = new ArrayList<>();\n    for (int i = 0; i < segmentCount; i++) {\n      singleSegmentRunners.add(toolChest.preMergeQueryDecoration(runner));\n    }\n    ExecutorService exec = Executors.newCachedThreadPool();\n    QueryRunner theRunner = toolChest.postMergeQueryDecoration(\n        new FinalizeResultsQueryRunner<>(\n            toolChest.mergeResults(factory.mergeRunners(Executors.newCachedThreadPool(), singleSegmentRunners)),\n            toolChest\n        )\n    );\n\n    TestHelper.assertExpectedObjects(\n        bySegmentResults,\n        theRunner.run(QueryPlus.wrap(fullQuery), new HashMap<>()),\n        \"bySegment\"\n    );\n    exec.shutdownNow();\n  }\n","realPath":"processing/src/test/java/org/apache/druid/query/groupby/GroupByQueryRunnerTest.java","repoName":"druid","snippetEndLine":0,"snippetStartLine":0,"startLine":7971,"status":"M"}],"commitId":"ffa25b78321a56b210a8ad5b6c9ca61b3b737153","commitMessage":"@@@Query vectorization. (#6794)\n\n* Benchmarks: New SqlBenchmark.  add caching & vectorization to some others.\n\n- Introduce a new SqlBenchmark geared towards benchmarking a wide\n  variety of SQL queries. Rename the old SqlBenchmark to\n  SqlVsNativeBenchmark.\n- Add (optional) caching to SegmentGenerator to enable easier\n  benchmarking of larger segments.\n- Add vectorization to FilteredAggregatorBenchmark and GroupByBenchmark.\n\n* Query vectorization.\n\nThis patch includes vectorized timeseries and groupBy engines.  as well\nas some analogs of your favorite Druid classes:\n\n- VectorCursor is like Cursor. (It comes from StorageAdapter.makeVectorCursor.)\n- VectorColumnSelectorFactory is like ColumnSelectorFactory.  and it has\n  methods to create analogs of the column selectors you know and love.\n- VectorOffset and ReadableVectorOffset are like Offset and ReadableOffset.\n- VectorAggregator is like BufferAggregator.\n- VectorValueMatcher is like ValueMatcher.\n\nThere are some noticeable differences between vectorized and regular\nexecution:\n\n- Unlike regular cursors.  vector cursors do not understand time\n  granularity. They expect query engines to handle this on their own. \n  which a new VectorCursorGranularizer class helps with. This is to\n  avoid too much batch-splitting and to respect the fact that vector\n  selectors are somewhat more heavyweight than regular selectors.\n- Unlike FilteredOffset.  FilteredVectorOffset does not leverage indexes\n  for filters that might partially support them (like an OR of one\n  filter that supports indexing and another that doesn't). I'm not sure\n  that this behavior is desirable anyway (it is potentially too eager)\n  but.  at any rate.  it'd be better to harmonize it between the two\n  classes. Potentially they should both do some different thing that\n  is smarter than what either of them is doing right now.\n- When vector cursors are created by QueryableIndexCursorSequenceBuilder. \n  they use a morphing binary-then-linear search to find their start and\n  end rows.  rather than linear search.\n\nLimitations in this patch are:\n\n- Only timeseries and groupBy have vectorized engines.\n- GroupBy doesn't handle multi-value dimensions yet.\n- Vector cursors cannot handle virtual columns or descending order.\n- Only some filters have vectorized matchers: \"selector\".  \"bound\".  \"in\". \n  \"like\".  \"regex\".  \"search\".  \"and\".  \"or\".  and \"not\".\n- Only some aggregators have vectorized implementations: \"count\". \n  \"doubleSum\".  \"floatSum\".  \"longSum\".  \"hyperUnique\".  and \"filtered\".\n- Dimension specs other than \"default\" don't work yet (no extraction\n  functions or filtered dimension specs).\n\nCurrently.  the testing strategy includes adding vectorization-enabled\ntests to TimeseriesQueryRunnerTest.  GroupByQueryRunnerTest. \nGroupByTimeseriesQueryRunnerTest.  CalciteQueryTest.  and all of the\nfiltering tests that extend BaseFilterTest. In all of those classes. \nthere are some test cases that don't support vectorization. They are\nmarked by special function calls like \"cannotVectorize\" or \"skipVectorize\"\nthat tell the test harness to either expect an exception or to skip the\ntest case.\n\nTesting should be expanded in the future -- a project in and of itself.\n\nRelated to #3011.\n\n* WIP\n\n* Adjustments for unused things.\n\n* Adjust javadocs.\n\n* DimensionDictionarySelector adjustments.\n\n* Add \"clone\" to BatchIteratorAdapter.\n\n* ValueMatcher javadocs.\n\n* Fix benchmark.\n\n* Fixups post-merge.\n\n* Expect exception on testGroupByWithStringVirtualColumn for IncrementalIndex.\n\n* BloomDimFilterSqlTest: Tag two non-vectorizable tests.\n\n* Minor adjustments.\n\n* Update surefire.  bump up Xmx in Travis.\n\n* Some more adjustments.\n\n* Javadoc adjustments\n\n* AggregatorAdapters adjustments.\n\n* Additional comments.\n\n* Remove switching search.\n\n* Only missiles.\n","date":"2019-07-13 03:54:07","modifiedFileCount":"143","status":"M","submitter":"Gian Merlino"},{"authorTime":"2019-07-24 23:29:03","codes":[{"authorDate":"2019-07-24 23:29:03","commitOrder":6,"curCode":"  private void doTestMergeResultsWithValidLimit(final int limit)\n  {\n    GroupByQuery.Builder builder = makeQueryBuilder()\n        .setDataSource(QueryRunnerTestHelper.dataSource)\n        .setInterval(\"2011-04-02/2011-04-04\")\n        .setDimensions(new DefaultDimensionSpec(\"quality\", \"alias\"))\n        .setAggregatorSpecs(QueryRunnerTestHelper.rowsCount, new LongSumAggregatorFactory(\"idx\", \"index\"))\n        .setGranularity(new PeriodGranularity(new Period(\"P1M\"), null, null))\n        .setLimit(limit);\n\n    final GroupByQuery fullQuery = builder.build();\n\n    List<Row> expectedResults = Arrays.asList(\n        GroupByQueryRunnerTestHelper.createExpectedRow(\"2011-04-01\", \"alias\", \"automotive\", \"rows\", 2L, \"idx\", 269L),\n        GroupByQueryRunnerTestHelper.createExpectedRow(\"2011-04-01\", \"alias\", \"business\", \"rows\", 2L, \"idx\", 217L),\n        GroupByQueryRunnerTestHelper.createExpectedRow(\"2011-04-01\", \"alias\", \"entertainment\", \"rows\", 2L, \"idx\", 319L),\n        GroupByQueryRunnerTestHelper.createExpectedRow(\"2011-04-01\", \"alias\", \"health\", \"rows\", 2L, \"idx\", 216L),\n        GroupByQueryRunnerTestHelper.createExpectedRow(\"2011-04-01\", \"alias\", \"mezzanine\", \"rows\", 6L, \"idx\", 4420L),\n        GroupByQueryRunnerTestHelper.createExpectedRow(\"2011-04-01\", \"alias\", \"news\", \"rows\", 2L, \"idx\", 221L),\n        GroupByQueryRunnerTestHelper.createExpectedRow(\"2011-04-01\", \"alias\", \"premium\", \"rows\", 6L, \"idx\", 4416L),\n        GroupByQueryRunnerTestHelper.createExpectedRow(\"2011-04-01\", \"alias\", \"technology\", \"rows\", 2L, \"idx\", 177L),\n        GroupByQueryRunnerTestHelper.createExpectedRow(\"2011-04-01\", \"alias\", \"travel\", \"rows\", 2L, \"idx\", 243L)\n    );\n\n    QueryRunner<Row> mergeRunner = factory.getToolchest().mergeResults(runner);\n\n    TestHelper.assertExpectedObjects(\n        Iterables.limit(expectedResults, limit),\n        mergeRunner.run(QueryPlus.wrap(fullQuery)),\n        StringUtils.format(\"limit: %d\", limit)\n    );\n  }\n","date":"2019-07-24 23:29:03","endLine":3018,"groupId":"22359","id":11,"instanceNumber":1,"isCurCommit":0,"methodName":"doTestMergeResultsWithValidLimit","params":"(finalintlimit)","path":"/mnt/clonedata/CloneManagementServer/ManagementServer/consistResult/result-druid-10-0.7/blobInfo/CC_OUT/blobs/c0/fd03e1879220a73e7b2a23965aca56ff1c9d70.src","preCode":"  private void doTestMergeResultsWithValidLimit(final int limit)\n  {\n    GroupByQuery.Builder builder = makeQueryBuilder()\n        .setDataSource(QueryRunnerTestHelper.dataSource)\n        .setInterval(\"2011-04-02/2011-04-04\")\n        .setDimensions(new DefaultDimensionSpec(\"quality\", \"alias\"))\n        .setAggregatorSpecs(QueryRunnerTestHelper.rowsCount, new LongSumAggregatorFactory(\"idx\", \"index\"))\n        .setGranularity(new PeriodGranularity(new Period(\"P1M\"), null, null))\n        .setLimit(limit);\n\n    final GroupByQuery fullQuery = builder.build();\n\n    List<Row> expectedResults = Arrays.asList(\n        GroupByQueryRunnerTestHelper.createExpectedRow(\"2011-04-01\", \"alias\", \"automotive\", \"rows\", 2L, \"idx\", 269L),\n        GroupByQueryRunnerTestHelper.createExpectedRow(\"2011-04-01\", \"alias\", \"business\", \"rows\", 2L, \"idx\", 217L),\n        GroupByQueryRunnerTestHelper.createExpectedRow(\"2011-04-01\", \"alias\", \"entertainment\", \"rows\", 2L, \"idx\", 319L),\n        GroupByQueryRunnerTestHelper.createExpectedRow(\"2011-04-01\", \"alias\", \"health\", \"rows\", 2L, \"idx\", 216L),\n        GroupByQueryRunnerTestHelper.createExpectedRow(\"2011-04-01\", \"alias\", \"mezzanine\", \"rows\", 6L, \"idx\", 4420L),\n        GroupByQueryRunnerTestHelper.createExpectedRow(\"2011-04-01\", \"alias\", \"news\", \"rows\", 2L, \"idx\", 221L),\n        GroupByQueryRunnerTestHelper.createExpectedRow(\"2011-04-01\", \"alias\", \"premium\", \"rows\", 6L, \"idx\", 4416L),\n        GroupByQueryRunnerTestHelper.createExpectedRow(\"2011-04-01\", \"alias\", \"technology\", \"rows\", 2L, \"idx\", 177L),\n        GroupByQueryRunnerTestHelper.createExpectedRow(\"2011-04-01\", \"alias\", \"travel\", \"rows\", 2L, \"idx\", 243L)\n    );\n\n    QueryRunner<Row> mergeRunner = factory.getToolchest().mergeResults(runner);\n\n    Map<String, Object> context = new HashMap<>();\n    TestHelper.assertExpectedObjects(\n        Iterables.limit(expectedResults, limit),\n        mergeRunner.run(QueryPlus.wrap(fullQuery), context),\n        StringUtils.format(\"limit: %d\", limit)\n    );\n  }\n","realPath":"processing/src/test/java/org/apache/druid/query/groupby/GroupByQueryRunnerTest.java","repoName":"druid","snippetEndLine":0,"snippetStartLine":0,"startLine":2987,"status":"M"},{"authorDate":"2019-07-24 23:29:03","commitOrder":6,"curCode":"  public void testBySegmentResultsUnOptimizedDimextraction()\n  {\n    \r\n    cannotVectorize();\n\n    int segmentCount = 32;\n    Result<BySegmentResultValue> singleSegmentResult = new Result<>(\n        DateTimes.of(\"2011-01-12T00:00:00.000Z\"),\n        new BySegmentResultValueClass<>(\n            Collections.singletonList(\n                GroupByQueryRunnerTestHelper.createExpectedRow(\n                    \"2011-04-01\",\n                    \"alias\",\n                    \"mezzanine0\",\n                    \"rows\",\n                    6L,\n                    \"idx\",\n                    4420L\n                )\n            ),\n            QueryRunnerTestHelper.segmentId.toString(),\n            Intervals.of(\"2011-04-02T00:00:00.000Z/2011-04-04T00:00:00.000Z\")\n        )\n    );\n    List<Result> bySegmentResults = new ArrayList<>();\n    for (int i = 0; i < segmentCount; i++) {\n      bySegmentResults.add(singleSegmentResult);\n    }\n    GroupByQuery.Builder builder = makeQueryBuilder()\n        .setDataSource(QueryRunnerTestHelper.dataSource)\n        .setInterval(\"2011-04-02/2011-04-04\").setDimensions(new ExtractionDimensionSpec(\n            \"quality\",\n            \"alias\",\n            new LookupExtractionFn(\n                new MapLookupExtractor(ImmutableMap.of(\"mezzanine\", \"mezzanine0\"), false),\n                false,\n                null,\n                false,\n                false\n            )\n        )).setAggregatorSpecs(QueryRunnerTestHelper.rowsCount, new LongSumAggregatorFactory(\"idx\", \"index\"))\n        .setGranularity(new PeriodGranularity(new Period(\"P1M\"), null, null))\n        .setDimFilter(new SelectorDimFilter(\"quality\", \"mezzanine\", null))\n        .overrideContext(ImmutableMap.of(\"bySegment\", true));\n    final GroupByQuery fullQuery = builder.build();\n    QueryToolChest toolChest = factory.getToolchest();\n\n    List<QueryRunner<Row>> singleSegmentRunners = new ArrayList<>();\n    for (int i = 0; i < segmentCount; i++) {\n      singleSegmentRunners.add(toolChest.preMergeQueryDecoration(runner));\n    }\n    ExecutorService exec = Executors.newCachedThreadPool();\n    QueryRunner theRunner = toolChest.postMergeQueryDecoration(\n        new FinalizeResultsQueryRunner<>(\n            toolChest.mergeResults(factory.mergeRunners(Executors.newCachedThreadPool(), singleSegmentRunners)),\n            toolChest\n        )\n    );\n\n    TestHelper.assertExpectedObjects(\n        bySegmentResults,\n        theRunner.run(QueryPlus.wrap(fullQuery)),\n        \"bySegment\"\n    );\n    exec.shutdownNow();\n  }\n","date":"2019-07-24 23:29:03","endLine":8029,"groupId":"20575","id":12,"instanceNumber":2,"isCurCommit":0,"methodName":"testBySegmentResultsUnOptimizedDimextraction","params":"()","path":"/mnt/clonedata/CloneManagementServer/ManagementServer/consistResult/result-druid-10-0.7/blobInfo/CC_OUT/blobs/c0/fd03e1879220a73e7b2a23965aca56ff1c9d70.src","preCode":"  public void testBySegmentResultsUnOptimizedDimextraction()\n  {\n    \r\n    cannotVectorize();\n\n    int segmentCount = 32;\n    Result<BySegmentResultValue> singleSegmentResult = new Result<>(\n        DateTimes.of(\"2011-01-12T00:00:00.000Z\"),\n        new BySegmentResultValueClass<>(\n            Collections.singletonList(\n                GroupByQueryRunnerTestHelper.createExpectedRow(\n                    \"2011-04-01\",\n                    \"alias\",\n                    \"mezzanine0\",\n                    \"rows\",\n                    6L,\n                    \"idx\",\n                    4420L\n                )\n            ),\n            QueryRunnerTestHelper.segmentId.toString(),\n            Intervals.of(\"2011-04-02T00:00:00.000Z/2011-04-04T00:00:00.000Z\")\n        )\n    );\n    List<Result> bySegmentResults = new ArrayList<>();\n    for (int i = 0; i < segmentCount; i++) {\n      bySegmentResults.add(singleSegmentResult);\n    }\n    GroupByQuery.Builder builder = makeQueryBuilder()\n        .setDataSource(QueryRunnerTestHelper.dataSource)\n        .setInterval(\"2011-04-02/2011-04-04\").setDimensions(new ExtractionDimensionSpec(\n            \"quality\",\n            \"alias\",\n            new LookupExtractionFn(\n                new MapLookupExtractor(ImmutableMap.of(\"mezzanine\", \"mezzanine0\"), false),\n                false,\n                null,\n                false,\n                false\n            )\n        )).setAggregatorSpecs(QueryRunnerTestHelper.rowsCount, new LongSumAggregatorFactory(\"idx\", \"index\"))\n        .setGranularity(new PeriodGranularity(new Period(\"P1M\"), null, null))\n        .setDimFilter(new SelectorDimFilter(\"quality\", \"mezzanine\", null))\n        .overrideContext(ImmutableMap.of(\"bySegment\", true));\n    final GroupByQuery fullQuery = builder.build();\n    QueryToolChest toolChest = factory.getToolchest();\n\n    List<QueryRunner<Row>> singleSegmentRunners = new ArrayList<>();\n    for (int i = 0; i < segmentCount; i++) {\n      singleSegmentRunners.add(toolChest.preMergeQueryDecoration(runner));\n    }\n    ExecutorService exec = Executors.newCachedThreadPool();\n    QueryRunner theRunner = toolChest.postMergeQueryDecoration(\n        new FinalizeResultsQueryRunner<>(\n            toolChest.mergeResults(factory.mergeRunners(Executors.newCachedThreadPool(), singleSegmentRunners)),\n            toolChest\n        )\n    );\n\n    TestHelper.assertExpectedObjects(\n        bySegmentResults,\n        theRunner.run(QueryPlus.wrap(fullQuery), new HashMap<>()),\n        \"bySegment\"\n    );\n    exec.shutdownNow();\n  }\n","realPath":"processing/src/test/java/org/apache/druid/query/groupby/GroupByQueryRunnerTest.java","repoName":"druid","snippetEndLine":0,"snippetStartLine":0,"startLine":7964,"status":"M"}],"commitId":"799d20249fe6333ea86b020f6d09c91fa4d3f998","commitMessage":"@@@Response context refactoring (#8110)\n\n* Response context refactoring\n\n* Serialization/Deserialization of ResponseContext\n\n* Added java doc comments\n\n* Renamed vars related to ResponseContext\n\n* Renamed empty() methods to createEmpty()\n\n* Fixed ResponseContext usage\n\n* Renamed multiple ResponseContext static fields\n\n* Added PublicApi annotations\n\n* Renamed QueryResponseContext class to ResourceIOReaderWriter\n\n* Moved the protected method below public static constants\n\n* Added createEmpty method to ResponseContext with DefaultResponseContext creation\n\n* Fixed inspection error\n\n* Added comments to the ResponseContext length limit and ResponseContext\nhttp header name\n\n* Added a comment of possible future refactoring\n\n* Removed .gitignore file of indexing-service\n\n* Removed a never-used method\n\n* VisibleForTesting method reducing boilerplate\n\nCo-Authored-By: Clint Wylie <cjwylie@gmail.com>\n\n* Reduced boilerplate\n\n* Renamed the method serialize to serializeWith\n\n* Removed unused import\n\n* Fixed incorrectly refactored test method\n\n* Added comments for ResponseContext keys\n\n* Fixed incorrectly refactored test method\n\n* Fixed IntervalChunkingQueryRunnerTest mocks\n","date":"2019-07-24 23:29:03","modifiedFileCount":"142","status":"M","submitter":"Eugene Sevastianov"},{"authorTime":"2019-08-01 07:15:12","codes":[{"authorDate":"2019-08-01 07:15:12","commitOrder":7,"curCode":"  private void doTestMergeResultsWithValidLimit(final int limit)\n  {\n    GroupByQuery.Builder builder = makeQueryBuilder()\n        .setDataSource(QueryRunnerTestHelper.dataSource)\n        .setInterval(\"2011-04-02/2011-04-04\")\n        .setDimensions(new DefaultDimensionSpec(\"quality\", \"alias\"))\n        .setAggregatorSpecs(QueryRunnerTestHelper.rowsCount, new LongSumAggregatorFactory(\"idx\", \"index\"))\n        .setGranularity(new PeriodGranularity(new Period(\"P1M\"), null, null))\n        .setLimit(limit);\n\n    final GroupByQuery fullQuery = builder.build();\n\n    List<ResultRow> expectedResults = Arrays.asList(\n        makeRow(\n            fullQuery,\n            \"2011-04-01\",\n            \"alias\",\n            \"automotive\",\n            \"rows\",\n            2L,\n            \"idx\",\n            269L\n        ),\n        makeRow(\n            fullQuery,\n            \"2011-04-01\",\n            \"alias\",\n            \"business\",\n            \"rows\",\n            2L,\n            \"idx\",\n            217L\n        ),\n        makeRow(\n            fullQuery,\n            \"2011-04-01\",\n            \"alias\",\n            \"entertainment\",\n            \"rows\",\n            2L,\n            \"idx\",\n            319L\n        ),\n        makeRow(\n            fullQuery,\n            \"2011-04-01\",\n            \"alias\",\n            \"health\",\n            \"rows\",\n            2L,\n            \"idx\",\n            216L\n        ),\n        makeRow(\n            fullQuery,\n            \"2011-04-01\",\n            \"alias\",\n            \"mezzanine\",\n            \"rows\",\n            6L,\n            \"idx\",\n            4420L\n        ),\n        makeRow(\n            fullQuery,\n            \"2011-04-01\",\n            \"alias\",\n            \"news\",\n            \"rows\",\n            2L,\n            \"idx\",\n            221L\n        ),\n        makeRow(\n            fullQuery,\n            \"2011-04-01\",\n            \"alias\",\n            \"premium\",\n            \"rows\",\n            6L,\n            \"idx\",\n            4416L\n        ),\n        makeRow(\n            fullQuery,\n            \"2011-04-01\",\n            \"alias\",\n            \"technology\",\n            \"rows\",\n            2L,\n            \"idx\",\n            177L\n        ),\n        makeRow(\n            fullQuery,\n            \"2011-04-01\",\n            \"alias\",\n            \"travel\",\n            \"rows\",\n            2L,\n            \"idx\",\n            243L\n        )\n    );\n\n    QueryRunner<ResultRow> mergeRunner = factory.getToolchest().mergeResults(runner);\n\n    TestHelper.assertExpectedObjects(\n        Iterables.limit(expectedResults, limit),\n        mergeRunner.run(QueryPlus.wrap(fullQuery)),\n        StringUtils.format(\"limit: %d\", limit)\n    );\n  }\n","date":"2019-08-01 07:15:12","endLine":3191,"groupId":"6621","id":13,"instanceNumber":1,"isCurCommit":0,"methodName":"doTestMergeResultsWithValidLimit","params":"(finalintlimit)","path":"/mnt/clonedata/CloneManagementServer/ManagementServer/consistResult/result-druid-10-0.7/blobInfo/CC_OUT/blobs/d1/574885a60d20d8c6aaa2ff4f2d57758cf04e59.src","preCode":"  private void doTestMergeResultsWithValidLimit(final int limit)\n  {\n    GroupByQuery.Builder builder = makeQueryBuilder()\n        .setDataSource(QueryRunnerTestHelper.dataSource)\n        .setInterval(\"2011-04-02/2011-04-04\")\n        .setDimensions(new DefaultDimensionSpec(\"quality\", \"alias\"))\n        .setAggregatorSpecs(QueryRunnerTestHelper.rowsCount, new LongSumAggregatorFactory(\"idx\", \"index\"))\n        .setGranularity(new PeriodGranularity(new Period(\"P1M\"), null, null))\n        .setLimit(limit);\n\n    final GroupByQuery fullQuery = builder.build();\n\n    List<Row> expectedResults = Arrays.asList(\n        GroupByQueryRunnerTestHelper.createExpectedRow(\"2011-04-01\", \"alias\", \"automotive\", \"rows\", 2L, \"idx\", 269L),\n        GroupByQueryRunnerTestHelper.createExpectedRow(\"2011-04-01\", \"alias\", \"business\", \"rows\", 2L, \"idx\", 217L),\n        GroupByQueryRunnerTestHelper.createExpectedRow(\"2011-04-01\", \"alias\", \"entertainment\", \"rows\", 2L, \"idx\", 319L),\n        GroupByQueryRunnerTestHelper.createExpectedRow(\"2011-04-01\", \"alias\", \"health\", \"rows\", 2L, \"idx\", 216L),\n        GroupByQueryRunnerTestHelper.createExpectedRow(\"2011-04-01\", \"alias\", \"mezzanine\", \"rows\", 6L, \"idx\", 4420L),\n        GroupByQueryRunnerTestHelper.createExpectedRow(\"2011-04-01\", \"alias\", \"news\", \"rows\", 2L, \"idx\", 221L),\n        GroupByQueryRunnerTestHelper.createExpectedRow(\"2011-04-01\", \"alias\", \"premium\", \"rows\", 6L, \"idx\", 4416L),\n        GroupByQueryRunnerTestHelper.createExpectedRow(\"2011-04-01\", \"alias\", \"technology\", \"rows\", 2L, \"idx\", 177L),\n        GroupByQueryRunnerTestHelper.createExpectedRow(\"2011-04-01\", \"alias\", \"travel\", \"rows\", 2L, \"idx\", 243L)\n    );\n\n    QueryRunner<Row> mergeRunner = factory.getToolchest().mergeResults(runner);\n\n    TestHelper.assertExpectedObjects(\n        Iterables.limit(expectedResults, limit),\n        mergeRunner.run(QueryPlus.wrap(fullQuery)),\n        StringUtils.format(\"limit: %d\", limit)\n    );\n  }\n","realPath":"processing/src/test/java/org/apache/druid/query/groupby/GroupByQueryRunnerTest.java","repoName":"druid","snippetEndLine":0,"snippetStartLine":0,"startLine":3079,"status":"M"},{"authorDate":"2019-08-01 07:15:12","commitOrder":7,"curCode":"  public void testBySegmentResultsUnOptimizedDimextraction()\n  {\n    GroupByQuery.Builder builder = makeQueryBuilder()\n        .setDataSource(QueryRunnerTestHelper.dataSource)\n        .setInterval(\"2011-04-02/2011-04-04\").setDimensions(new ExtractionDimensionSpec(\n            \"quality\",\n            \"alias\",\n            new LookupExtractionFn(\n                new MapLookupExtractor(ImmutableMap.of(\"mezzanine\", \"mezzanine0\"), false),\n                false,\n                null,\n                false,\n                false\n            )\n        )).setAggregatorSpecs(QueryRunnerTestHelper.rowsCount, new LongSumAggregatorFactory(\"idx\", \"index\"))\n        .setGranularity(new PeriodGranularity(new Period(\"P1M\"), null, null))\n        .setDimFilter(new SelectorDimFilter(\"quality\", \"mezzanine\", null))\n        .setContext(ImmutableMap.of(\"bySegment\", true));\n    final GroupByQuery fullQuery = builder.build();\n\n    int segmentCount = 32;\n    Result<BySegmentResultValue> singleSegmentResult = new Result<>(\n        DateTimes.of(\"2011-01-12T00:00:00.000Z\"),\n        new BySegmentResultValueClass<>(\n            Collections.singletonList(\n                makeRow(\n                    fullQuery,\n                    \"2011-04-01\",\n                    \"alias\",\n                    \"mezzanine0\",\n                    \"rows\",\n                    6L,\n                    \"idx\",\n                    4420L\n                )\n            ),\n            QueryRunnerTestHelper.segmentId.toString(),\n            Intervals.of(\"2011-04-02T00:00:00.000Z/2011-04-04T00:00:00.000Z\")\n        )\n    );\n    List<Result> bySegmentResults = new ArrayList<>();\n    for (int i = 0; i < segmentCount; i++) {\n      bySegmentResults.add(singleSegmentResult);\n    }\n    QueryToolChest toolChest = factory.getToolchest();\n\n    List<QueryRunner<ResultRow>> singleSegmentRunners = new ArrayList<>();\n    for (int i = 0; i < segmentCount; i++) {\n      singleSegmentRunners.add(toolChest.preMergeQueryDecoration(runner));\n    }\n    ExecutorService exec = Executors.newCachedThreadPool();\n    QueryRunner theRunner = toolChest.postMergeQueryDecoration(\n        new FinalizeResultsQueryRunner<>(\n            toolChest.mergeResults(factory.mergeRunners(Executors.newCachedThreadPool(), singleSegmentRunners)),\n            toolChest\n        )\n    );\n\n    TestHelper.assertExpectedObjects(\n        bySegmentResults,\n        theRunner.run(QueryPlus.wrap(fullQuery)),\n        \"bySegment\"\n    );\n    exec.shutdownNow();\n  }\n","date":"2019-08-01 07:15:12","endLine":8260,"groupId":"16812","id":14,"instanceNumber":2,"isCurCommit":0,"methodName":"testBySegmentResultsUnOptimizedDimextraction","params":"()","path":"/mnt/clonedata/CloneManagementServer/ManagementServer/consistResult/result-druid-10-0.7/blobInfo/CC_OUT/blobs/d1/574885a60d20d8c6aaa2ff4f2d57758cf04e59.src","preCode":"  public void testBySegmentResultsUnOptimizedDimextraction()\n  {\n    \r\n    cannotVectorize();\n\n    int segmentCount = 32;\n    Result<BySegmentResultValue> singleSegmentResult = new Result<>(\n        DateTimes.of(\"2011-01-12T00:00:00.000Z\"),\n        new BySegmentResultValueClass<>(\n            Collections.singletonList(\n                GroupByQueryRunnerTestHelper.createExpectedRow(\n                    \"2011-04-01\",\n                    \"alias\",\n                    \"mezzanine0\",\n                    \"rows\",\n                    6L,\n                    \"idx\",\n                    4420L\n                )\n            ),\n            QueryRunnerTestHelper.segmentId.toString(),\n            Intervals.of(\"2011-04-02T00:00:00.000Z/2011-04-04T00:00:00.000Z\")\n        )\n    );\n    List<Result> bySegmentResults = new ArrayList<>();\n    for (int i = 0; i < segmentCount; i++) {\n      bySegmentResults.add(singleSegmentResult);\n    }\n    GroupByQuery.Builder builder = makeQueryBuilder()\n        .setDataSource(QueryRunnerTestHelper.dataSource)\n        .setInterval(\"2011-04-02/2011-04-04\").setDimensions(new ExtractionDimensionSpec(\n            \"quality\",\n            \"alias\",\n            new LookupExtractionFn(\n                new MapLookupExtractor(ImmutableMap.of(\"mezzanine\", \"mezzanine0\"), false),\n                false,\n                null,\n                false,\n                false\n            )\n        )).setAggregatorSpecs(QueryRunnerTestHelper.rowsCount, new LongSumAggregatorFactory(\"idx\", \"index\"))\n        .setGranularity(new PeriodGranularity(new Period(\"P1M\"), null, null))\n        .setDimFilter(new SelectorDimFilter(\"quality\", \"mezzanine\", null))\n        .overrideContext(ImmutableMap.of(\"bySegment\", true));\n    final GroupByQuery fullQuery = builder.build();\n    QueryToolChest toolChest = factory.getToolchest();\n\n    List<QueryRunner<Row>> singleSegmentRunners = new ArrayList<>();\n    for (int i = 0; i < segmentCount; i++) {\n      singleSegmentRunners.add(toolChest.preMergeQueryDecoration(runner));\n    }\n    ExecutorService exec = Executors.newCachedThreadPool();\n    QueryRunner theRunner = toolChest.postMergeQueryDecoration(\n        new FinalizeResultsQueryRunner<>(\n            toolChest.mergeResults(factory.mergeRunners(Executors.newCachedThreadPool(), singleSegmentRunners)),\n            toolChest\n        )\n    );\n\n    TestHelper.assertExpectedObjects(\n        bySegmentResults,\n        theRunner.run(QueryPlus.wrap(fullQuery)),\n        \"bySegment\"\n    );\n    exec.shutdownNow();\n  }\n","realPath":"processing/src/test/java/org/apache/druid/query/groupby/GroupByQueryRunnerTest.java","repoName":"druid","snippetEndLine":0,"snippetStartLine":0,"startLine":8196,"status":"M"}],"commitId":"77297f4e6f2e9d617c96cd46852bb5a772961e85","commitMessage":"@@@GroupBy array-based result rows. (#8196)\n\n* GroupBy array-based result rows.\n\nFixes #8118; see that proposal for details.\n\nOther than the GroupBy changes.  the main other \"interesting\" classes are:\n\n- ResultRow: The array-based result type.\n- BaseQuery: T is no longer required to be Comparable.\n- QueryToolChest: Adds \"decorateObjectMapper\" to enable query-aware serialization\n  and deserialization of result rows (necessary due to their positional nature).\n- QueryResource: Uses the new decoration functionality.\n- DirectDruidClient: Also uses the new decoration functionality.\n- QueryMaker (in Druid SQL): Modifications to read ResultRows.\n\nThese classes weren't changed.  but got some new javadocs:\n\n- BySegmentQueryRunner\n- FinalizeResultsQueryRunner\n- Query\n\n* Adjustments for TC stuff.\n","date":"2019-08-01 07:15:12","modifiedFileCount":"111","status":"M","submitter":"Gian Merlino"},{"authorTime":"2019-08-23 18:13:54","codes":[{"authorDate":"2019-08-23 18:13:54","commitOrder":8,"curCode":"  private void doTestMergeResultsWithValidLimit(final int limit)\n  {\n    GroupByQuery.Builder builder = makeQueryBuilder()\n        .setDataSource(QueryRunnerTestHelper.DATA_SOURCE)\n        .setInterval(\"2011-04-02/2011-04-04\")\n        .setDimensions(new DefaultDimensionSpec(\"quality\", \"alias\"))\n        .setAggregatorSpecs(QueryRunnerTestHelper.ROWS_COUNT, new LongSumAggregatorFactory(\"idx\", \"index\"))\n        .setGranularity(new PeriodGranularity(new Period(\"P1M\"), null, null))\n        .setLimit(limit);\n\n    final GroupByQuery fullQuery = builder.build();\n\n    List<ResultRow> expectedResults = Arrays.asList(\n        makeRow(\n            fullQuery,\n            \"2011-04-01\",\n            \"alias\",\n            \"automotive\",\n            \"rows\",\n            2L,\n            \"idx\",\n            269L\n        ),\n        makeRow(\n            fullQuery,\n            \"2011-04-01\",\n            \"alias\",\n            \"business\",\n            \"rows\",\n            2L,\n            \"idx\",\n            217L\n        ),\n        makeRow(\n            fullQuery,\n            \"2011-04-01\",\n            \"alias\",\n            \"entertainment\",\n            \"rows\",\n            2L,\n            \"idx\",\n            319L\n        ),\n        makeRow(\n            fullQuery,\n            \"2011-04-01\",\n            \"alias\",\n            \"health\",\n            \"rows\",\n            2L,\n            \"idx\",\n            216L\n        ),\n        makeRow(\n            fullQuery,\n            \"2011-04-01\",\n            \"alias\",\n            \"mezzanine\",\n            \"rows\",\n            6L,\n            \"idx\",\n            4420L\n        ),\n        makeRow(\n            fullQuery,\n            \"2011-04-01\",\n            \"alias\",\n            \"news\",\n            \"rows\",\n            2L,\n            \"idx\",\n            221L\n        ),\n        makeRow(\n            fullQuery,\n            \"2011-04-01\",\n            \"alias\",\n            \"premium\",\n            \"rows\",\n            6L,\n            \"idx\",\n            4416L\n        ),\n        makeRow(\n            fullQuery,\n            \"2011-04-01\",\n            \"alias\",\n            \"technology\",\n            \"rows\",\n            2L,\n            \"idx\",\n            177L\n        ),\n        makeRow(\n            fullQuery,\n            \"2011-04-01\",\n            \"alias\",\n            \"travel\",\n            \"rows\",\n            2L,\n            \"idx\",\n            243L\n        )\n    );\n\n    QueryRunner<ResultRow> mergeRunner = factory.getToolchest().mergeResults(runner);\n\n    TestHelper.assertExpectedObjects(\n        Iterables.limit(expectedResults, limit),\n        mergeRunner.run(QueryPlus.wrap(fullQuery)),\n        StringUtils.format(\"limit: %d\", limit)\n    );\n  }\n","date":"2019-08-23 18:13:54","endLine":3184,"groupId":"6621","id":15,"instanceNumber":1,"isCurCommit":0,"methodName":"doTestMergeResultsWithValidLimit","params":"(finalintlimit)","path":"/mnt/clonedata/CloneManagementServer/ManagementServer/consistResult/result-druid-10-0.7/blobInfo/CC_OUT/blobs/62/70f6bf822e9df520831f224e31e00bb9d44650.src","preCode":"  private void doTestMergeResultsWithValidLimit(final int limit)\n  {\n    GroupByQuery.Builder builder = makeQueryBuilder()\n        .setDataSource(QueryRunnerTestHelper.dataSource)\n        .setInterval(\"2011-04-02/2011-04-04\")\n        .setDimensions(new DefaultDimensionSpec(\"quality\", \"alias\"))\n        .setAggregatorSpecs(QueryRunnerTestHelper.rowsCount, new LongSumAggregatorFactory(\"idx\", \"index\"))\n        .setGranularity(new PeriodGranularity(new Period(\"P1M\"), null, null))\n        .setLimit(limit);\n\n    final GroupByQuery fullQuery = builder.build();\n\n    List<ResultRow> expectedResults = Arrays.asList(\n        makeRow(\n            fullQuery,\n            \"2011-04-01\",\n            \"alias\",\n            \"automotive\",\n            \"rows\",\n            2L,\n            \"idx\",\n            269L\n        ),\n        makeRow(\n            fullQuery,\n            \"2011-04-01\",\n            \"alias\",\n            \"business\",\n            \"rows\",\n            2L,\n            \"idx\",\n            217L\n        ),\n        makeRow(\n            fullQuery,\n            \"2011-04-01\",\n            \"alias\",\n            \"entertainment\",\n            \"rows\",\n            2L,\n            \"idx\",\n            319L\n        ),\n        makeRow(\n            fullQuery,\n            \"2011-04-01\",\n            \"alias\",\n            \"health\",\n            \"rows\",\n            2L,\n            \"idx\",\n            216L\n        ),\n        makeRow(\n            fullQuery,\n            \"2011-04-01\",\n            \"alias\",\n            \"mezzanine\",\n            \"rows\",\n            6L,\n            \"idx\",\n            4420L\n        ),\n        makeRow(\n            fullQuery,\n            \"2011-04-01\",\n            \"alias\",\n            \"news\",\n            \"rows\",\n            2L,\n            \"idx\",\n            221L\n        ),\n        makeRow(\n            fullQuery,\n            \"2011-04-01\",\n            \"alias\",\n            \"premium\",\n            \"rows\",\n            6L,\n            \"idx\",\n            4416L\n        ),\n        makeRow(\n            fullQuery,\n            \"2011-04-01\",\n            \"alias\",\n            \"technology\",\n            \"rows\",\n            2L,\n            \"idx\",\n            177L\n        ),\n        makeRow(\n            fullQuery,\n            \"2011-04-01\",\n            \"alias\",\n            \"travel\",\n            \"rows\",\n            2L,\n            \"idx\",\n            243L\n        )\n    );\n\n    QueryRunner<ResultRow> mergeRunner = factory.getToolchest().mergeResults(runner);\n\n    TestHelper.assertExpectedObjects(\n        Iterables.limit(expectedResults, limit),\n        mergeRunner.run(QueryPlus.wrap(fullQuery)),\n        StringUtils.format(\"limit: %d\", limit)\n    );\n  }\n","realPath":"processing/src/test/java/org/apache/druid/query/groupby/GroupByQueryRunnerTest.java","repoName":"druid","snippetEndLine":0,"snippetStartLine":0,"startLine":3072,"status":"M"},{"authorDate":"2019-08-23 18:13:54","commitOrder":8,"curCode":"  public void testBySegmentResultsUnOptimizedDimextraction()\n  {\n    GroupByQuery.Builder builder = makeQueryBuilder()\n        .setDataSource(QueryRunnerTestHelper.DATA_SOURCE)\n        .setInterval(\"2011-04-02/2011-04-04\").setDimensions(new ExtractionDimensionSpec(\n            \"quality\",\n            \"alias\",\n            new LookupExtractionFn(\n                new MapLookupExtractor(ImmutableMap.of(\"mezzanine\", \"mezzanine0\"), false),\n                false,\n                null,\n                false,\n                false\n            )\n        )).setAggregatorSpecs(QueryRunnerTestHelper.ROWS_COUNT, new LongSumAggregatorFactory(\"idx\", \"index\"))\n        .setGranularity(new PeriodGranularity(new Period(\"P1M\"), null, null))\n        .setDimFilter(new SelectorDimFilter(\"quality\", \"mezzanine\", null))\n        .setContext(ImmutableMap.of(\"bySegment\", true));\n    final GroupByQuery fullQuery = builder.build();\n\n    int segmentCount = 32;\n    Result<BySegmentResultValue> singleSegmentResult = new Result<>(\n        DateTimes.of(\"2011-01-12T00:00:00.000Z\"),\n        new BySegmentResultValueClass<>(\n            Collections.singletonList(\n                makeRow(\n                    fullQuery,\n                    \"2011-04-01\",\n                    \"alias\",\n                    \"mezzanine0\",\n                    \"rows\",\n                    6L,\n                    \"idx\",\n                    4420L\n                )\n            ),\n            QueryRunnerTestHelper.SEGMENT_ID.toString(),\n            Intervals.of(\"2011-04-02T00:00:00.000Z/2011-04-04T00:00:00.000Z\")\n        )\n    );\n    List<Result> bySegmentResults = new ArrayList<>();\n    for (int i = 0; i < segmentCount; i++) {\n      bySegmentResults.add(singleSegmentResult);\n    }\n    QueryToolChest toolChest = factory.getToolchest();\n\n    List<QueryRunner<ResultRow>> singleSegmentRunners = new ArrayList<>();\n    for (int i = 0; i < segmentCount; i++) {\n      singleSegmentRunners.add(toolChest.preMergeQueryDecoration(runner));\n    }\n    ExecutorService exec = Executors.newCachedThreadPool();\n    QueryRunner theRunner = toolChest.postMergeQueryDecoration(\n        new FinalizeResultsQueryRunner<>(\n            toolChest.mergeResults(factory.mergeRunners(Executors.newCachedThreadPool(), singleSegmentRunners)),\n            toolChest\n        )\n    );\n\n    TestHelper.assertExpectedObjects(\n        bySegmentResults,\n        theRunner.run(QueryPlus.wrap(fullQuery)),\n        \"bySegment\"\n    );\n    exec.shutdownNow();\n  }\n","date":"2019-08-23 18:13:54","endLine":7968,"groupId":"16812","id":16,"instanceNumber":2,"isCurCommit":0,"methodName":"testBySegmentResultsUnOptimizedDimextraction","params":"()","path":"/mnt/clonedata/CloneManagementServer/ManagementServer/consistResult/result-druid-10-0.7/blobInfo/CC_OUT/blobs/62/70f6bf822e9df520831f224e31e00bb9d44650.src","preCode":"  public void testBySegmentResultsUnOptimizedDimextraction()\n  {\n    GroupByQuery.Builder builder = makeQueryBuilder()\n        .setDataSource(QueryRunnerTestHelper.dataSource)\n        .setInterval(\"2011-04-02/2011-04-04\").setDimensions(new ExtractionDimensionSpec(\n            \"quality\",\n            \"alias\",\n            new LookupExtractionFn(\n                new MapLookupExtractor(ImmutableMap.of(\"mezzanine\", \"mezzanine0\"), false),\n                false,\n                null,\n                false,\n                false\n            )\n        )).setAggregatorSpecs(QueryRunnerTestHelper.rowsCount, new LongSumAggregatorFactory(\"idx\", \"index\"))\n        .setGranularity(new PeriodGranularity(new Period(\"P1M\"), null, null))\n        .setDimFilter(new SelectorDimFilter(\"quality\", \"mezzanine\", null))\n        .setContext(ImmutableMap.of(\"bySegment\", true));\n    final GroupByQuery fullQuery = builder.build();\n\n    int segmentCount = 32;\n    Result<BySegmentResultValue> singleSegmentResult = new Result<>(\n        DateTimes.of(\"2011-01-12T00:00:00.000Z\"),\n        new BySegmentResultValueClass<>(\n            Collections.singletonList(\n                makeRow(\n                    fullQuery,\n                    \"2011-04-01\",\n                    \"alias\",\n                    \"mezzanine0\",\n                    \"rows\",\n                    6L,\n                    \"idx\",\n                    4420L\n                )\n            ),\n            QueryRunnerTestHelper.segmentId.toString(),\n            Intervals.of(\"2011-04-02T00:00:00.000Z/2011-04-04T00:00:00.000Z\")\n        )\n    );\n    List<Result> bySegmentResults = new ArrayList<>();\n    for (int i = 0; i < segmentCount; i++) {\n      bySegmentResults.add(singleSegmentResult);\n    }\n    QueryToolChest toolChest = factory.getToolchest();\n\n    List<QueryRunner<ResultRow>> singleSegmentRunners = new ArrayList<>();\n    for (int i = 0; i < segmentCount; i++) {\n      singleSegmentRunners.add(toolChest.preMergeQueryDecoration(runner));\n    }\n    ExecutorService exec = Executors.newCachedThreadPool();\n    QueryRunner theRunner = toolChest.postMergeQueryDecoration(\n        new FinalizeResultsQueryRunner<>(\n            toolChest.mergeResults(factory.mergeRunners(Executors.newCachedThreadPool(), singleSegmentRunners)),\n            toolChest\n        )\n    );\n\n    TestHelper.assertExpectedObjects(\n        bySegmentResults,\n        theRunner.run(QueryPlus.wrap(fullQuery)),\n        \"bySegment\"\n    );\n    exec.shutdownNow();\n  }\n","realPath":"processing/src/test/java/org/apache/druid/query/groupby/GroupByQueryRunnerTest.java","repoName":"druid","snippetEndLine":0,"snippetStartLine":0,"startLine":7904,"status":"M"}],"commitId":"33f0753a70361e7d345a488034f76a889f7c3682","commitMessage":"@@@Add Checkstyle for constant name static final (#8060)\n\n* check ctyle for constant field name\n\n* check ctyle for constant field name\n\n* check ctyle for constant field name\n\n* check ctyle for constant field name\n\n* check ctyle for constant field name\n\n* check ctyle for constant field name\n\n* check ctyle for constant field name\n\n* check ctyle for constant field name\n\n* check ctyle for constant field name\n\n* merging with upstream\n\n* review-1\n\n* unknow changes\n\n* unknow changes\n\n* review-2\n\n* merging with master\n\n* review-2 1 changes\n\n* review changes-2 2\n\n* bug fix\n","date":"2019-08-23 18:13:54","modifiedFileCount":"298","status":"M","submitter":"SandishKumarHN"},{"authorTime":"2019-08-23 18:13:54","codes":[{"authorDate":"2020-08-06 06:39:58","commitOrder":9,"curCode":"  private void doTestMergeResultsWithValidLimit(final int limit, final int offset)\n  {\n    GroupByQuery.Builder builder = makeQueryBuilder()\n        .setDataSource(QueryRunnerTestHelper.DATA_SOURCE)\n        .setInterval(\"2011-04-02/2011-04-04\")\n        .setDimensions(new DefaultDimensionSpec(\"quality\", \"alias\"))\n        .setAggregatorSpecs(QueryRunnerTestHelper.ROWS_COUNT, new LongSumAggregatorFactory(\"idx\", \"index\"))\n        .setGranularity(new PeriodGranularity(new Period(\"P1M\"), null, null))\n        .setLimitSpec(DefaultLimitSpec.builder().limit(limit).offset(offset).build());\n\n    final GroupByQuery fullQuery = builder.build();\n\n    List<ResultRow> expectedResults = Arrays.asList(\n        makeRow(\n            fullQuery,\n            \"2011-04-01\",\n            \"alias\",\n            \"automotive\",\n            \"rows\",\n            2L,\n            \"idx\",\n            269L\n        ),\n        makeRow(\n            fullQuery,\n            \"2011-04-01\",\n            \"alias\",\n            \"business\",\n            \"rows\",\n            2L,\n            \"idx\",\n            217L\n        ),\n        makeRow(\n            fullQuery,\n            \"2011-04-01\",\n            \"alias\",\n            \"entertainment\",\n            \"rows\",\n            2L,\n            \"idx\",\n            319L\n        ),\n        makeRow(\n            fullQuery,\n            \"2011-04-01\",\n            \"alias\",\n            \"health\",\n            \"rows\",\n            2L,\n            \"idx\",\n            216L\n        ),\n        makeRow(\n            fullQuery,\n            \"2011-04-01\",\n            \"alias\",\n            \"mezzanine\",\n            \"rows\",\n            6L,\n            \"idx\",\n            4420L\n        ),\n        makeRow(\n            fullQuery,\n            \"2011-04-01\",\n            \"alias\",\n            \"news\",\n            \"rows\",\n            2L,\n            \"idx\",\n            221L\n        ),\n        makeRow(\n            fullQuery,\n            \"2011-04-01\",\n            \"alias\",\n            \"premium\",\n            \"rows\",\n            6L,\n            \"idx\",\n            4416L\n        ),\n        makeRow(\n            fullQuery,\n            \"2011-04-01\",\n            \"alias\",\n            \"technology\",\n            \"rows\",\n            2L,\n            \"idx\",\n            177L\n        ),\n        makeRow(\n            fullQuery,\n            \"2011-04-01\",\n            \"alias\",\n            \"travel\",\n            \"rows\",\n            2L,\n            \"idx\",\n            243L\n        )\n    );\n\n    QueryRunner<ResultRow> mergeRunner = factory.getToolchest().mergeResults(runner);\n\n    TestHelper.assertExpectedObjects(\n        Iterables.limit(Iterables.skip(expectedResults, offset), limit),\n        mergeRunner.run(QueryPlus.wrap(fullQuery)),\n        StringUtils.format(\"limit: %d\", limit)\n    );\n  }\n","date":"2020-08-06 06:39:58","endLine":3167,"groupId":"6621","id":17,"instanceNumber":1,"isCurCommit":0,"methodName":"doTestMergeResultsWithValidLimit","params":"(finalintlimit@finalintoffset)","path":"/mnt/clonedata/CloneManagementServer/ManagementServer/consistResult/result-druid-10-0.7/blobInfo/CC_OUT/blobs/39/3c558d87951cea5b49e52a9d66a40358506720.src","preCode":"  private void doTestMergeResultsWithValidLimit(final int limit)\n  {\n    GroupByQuery.Builder builder = makeQueryBuilder()\n        .setDataSource(QueryRunnerTestHelper.DATA_SOURCE)\n        .setInterval(\"2011-04-02/2011-04-04\")\n        .setDimensions(new DefaultDimensionSpec(\"quality\", \"alias\"))\n        .setAggregatorSpecs(QueryRunnerTestHelper.ROWS_COUNT, new LongSumAggregatorFactory(\"idx\", \"index\"))\n        .setGranularity(new PeriodGranularity(new Period(\"P1M\"), null, null))\n        .setLimit(limit);\n\n    final GroupByQuery fullQuery = builder.build();\n\n    List<ResultRow> expectedResults = Arrays.asList(\n        makeRow(\n            fullQuery,\n            \"2011-04-01\",\n            \"alias\",\n            \"automotive\",\n            \"rows\",\n            2L,\n            \"idx\",\n            269L\n        ),\n        makeRow(\n            fullQuery,\n            \"2011-04-01\",\n            \"alias\",\n            \"business\",\n            \"rows\",\n            2L,\n            \"idx\",\n            217L\n        ),\n        makeRow(\n            fullQuery,\n            \"2011-04-01\",\n            \"alias\",\n            \"entertainment\",\n            \"rows\",\n            2L,\n            \"idx\",\n            319L\n        ),\n        makeRow(\n            fullQuery,\n            \"2011-04-01\",\n            \"alias\",\n            \"health\",\n            \"rows\",\n            2L,\n            \"idx\",\n            216L\n        ),\n        makeRow(\n            fullQuery,\n            \"2011-04-01\",\n            \"alias\",\n            \"mezzanine\",\n            \"rows\",\n            6L,\n            \"idx\",\n            4420L\n        ),\n        makeRow(\n            fullQuery,\n            \"2011-04-01\",\n            \"alias\",\n            \"news\",\n            \"rows\",\n            2L,\n            \"idx\",\n            221L\n        ),\n        makeRow(\n            fullQuery,\n            \"2011-04-01\",\n            \"alias\",\n            \"premium\",\n            \"rows\",\n            6L,\n            \"idx\",\n            4416L\n        ),\n        makeRow(\n            fullQuery,\n            \"2011-04-01\",\n            \"alias\",\n            \"technology\",\n            \"rows\",\n            2L,\n            \"idx\",\n            177L\n        ),\n        makeRow(\n            fullQuery,\n            \"2011-04-01\",\n            \"alias\",\n            \"travel\",\n            \"rows\",\n            2L,\n            \"idx\",\n            243L\n        )\n    );\n\n    QueryRunner<ResultRow> mergeRunner = factory.getToolchest().mergeResults(runner);\n\n    TestHelper.assertExpectedObjects(\n        Iterables.limit(expectedResults, limit),\n        mergeRunner.run(QueryPlus.wrap(fullQuery)),\n        StringUtils.format(\"limit: %d\", limit)\n    );\n  }\n","realPath":"processing/src/test/java/org/apache/druid/query/groupby/GroupByQueryRunnerTest.java","repoName":"druid","snippetEndLine":0,"snippetStartLine":0,"startLine":3055,"status":"M"},{"authorDate":"2019-08-23 18:13:54","commitOrder":9,"curCode":"  public void testBySegmentResultsUnOptimizedDimextraction()\n  {\n    GroupByQuery.Builder builder = makeQueryBuilder()\n        .setDataSource(QueryRunnerTestHelper.DATA_SOURCE)\n        .setInterval(\"2011-04-02/2011-04-04\").setDimensions(new ExtractionDimensionSpec(\n            \"quality\",\n            \"alias\",\n            new LookupExtractionFn(\n                new MapLookupExtractor(ImmutableMap.of(\"mezzanine\", \"mezzanine0\"), false),\n                false,\n                null,\n                false,\n                false\n            )\n        )).setAggregatorSpecs(QueryRunnerTestHelper.ROWS_COUNT, new LongSumAggregatorFactory(\"idx\", \"index\"))\n        .setGranularity(new PeriodGranularity(new Period(\"P1M\"), null, null))\n        .setDimFilter(new SelectorDimFilter(\"quality\", \"mezzanine\", null))\n        .setContext(ImmutableMap.of(\"bySegment\", true));\n    final GroupByQuery fullQuery = builder.build();\n\n    int segmentCount = 32;\n    Result<BySegmentResultValue> singleSegmentResult = new Result<>(\n        DateTimes.of(\"2011-01-12T00:00:00.000Z\"),\n        new BySegmentResultValueClass<>(\n            Collections.singletonList(\n                makeRow(\n                    fullQuery,\n                    \"2011-04-01\",\n                    \"alias\",\n                    \"mezzanine0\",\n                    \"rows\",\n                    6L,\n                    \"idx\",\n                    4420L\n                )\n            ),\n            QueryRunnerTestHelper.SEGMENT_ID.toString(),\n            Intervals.of(\"2011-04-02T00:00:00.000Z/2011-04-04T00:00:00.000Z\")\n        )\n    );\n    List<Result> bySegmentResults = new ArrayList<>();\n    for (int i = 0; i < segmentCount; i++) {\n      bySegmentResults.add(singleSegmentResult);\n    }\n    QueryToolChest toolChest = factory.getToolchest();\n\n    List<QueryRunner<ResultRow>> singleSegmentRunners = new ArrayList<>();\n    for (int i = 0; i < segmentCount; i++) {\n      singleSegmentRunners.add(toolChest.preMergeQueryDecoration(runner));\n    }\n    ExecutorService exec = Executors.newCachedThreadPool();\n    QueryRunner theRunner = toolChest.postMergeQueryDecoration(\n        new FinalizeResultsQueryRunner<>(\n            toolChest.mergeResults(factory.mergeRunners(Executors.newCachedThreadPool(), singleSegmentRunners)),\n            toolChest\n        )\n    );\n\n    TestHelper.assertExpectedObjects(\n        bySegmentResults,\n        theRunner.run(QueryPlus.wrap(fullQuery)),\n        \"bySegment\"\n    );\n    exec.shutdownNow();\n  }\n","date":"2019-08-23 18:13:54","endLine":7968,"groupId":"16812","id":18,"instanceNumber":2,"isCurCommit":0,"methodName":"testBySegmentResultsUnOptimizedDimextraction","params":"()","path":"/mnt/clonedata/CloneManagementServer/ManagementServer/consistResult/result-druid-10-0.7/blobInfo/CC_OUT/blobs/62/70f6bf822e9df520831f224e31e00bb9d44650.src","preCode":"  public void testBySegmentResultsUnOptimizedDimextraction()\n  {\n    GroupByQuery.Builder builder = makeQueryBuilder()\n        .setDataSource(QueryRunnerTestHelper.DATA_SOURCE)\n        .setInterval(\"2011-04-02/2011-04-04\").setDimensions(new ExtractionDimensionSpec(\n            \"quality\",\n            \"alias\",\n            new LookupExtractionFn(\n                new MapLookupExtractor(ImmutableMap.of(\"mezzanine\", \"mezzanine0\"), false),\n                false,\n                null,\n                false,\n                false\n            )\n        )).setAggregatorSpecs(QueryRunnerTestHelper.ROWS_COUNT, new LongSumAggregatorFactory(\"idx\", \"index\"))\n        .setGranularity(new PeriodGranularity(new Period(\"P1M\"), null, null))\n        .setDimFilter(new SelectorDimFilter(\"quality\", \"mezzanine\", null))\n        .setContext(ImmutableMap.of(\"bySegment\", true));\n    final GroupByQuery fullQuery = builder.build();\n\n    int segmentCount = 32;\n    Result<BySegmentResultValue> singleSegmentResult = new Result<>(\n        DateTimes.of(\"2011-01-12T00:00:00.000Z\"),\n        new BySegmentResultValueClass<>(\n            Collections.singletonList(\n                makeRow(\n                    fullQuery,\n                    \"2011-04-01\",\n                    \"alias\",\n                    \"mezzanine0\",\n                    \"rows\",\n                    6L,\n                    \"idx\",\n                    4420L\n                )\n            ),\n            QueryRunnerTestHelper.SEGMENT_ID.toString(),\n            Intervals.of(\"2011-04-02T00:00:00.000Z/2011-04-04T00:00:00.000Z\")\n        )\n    );\n    List<Result> bySegmentResults = new ArrayList<>();\n    for (int i = 0; i < segmentCount; i++) {\n      bySegmentResults.add(singleSegmentResult);\n    }\n    QueryToolChest toolChest = factory.getToolchest();\n\n    List<QueryRunner<ResultRow>> singleSegmentRunners = new ArrayList<>();\n    for (int i = 0; i < segmentCount; i++) {\n      singleSegmentRunners.add(toolChest.preMergeQueryDecoration(runner));\n    }\n    ExecutorService exec = Executors.newCachedThreadPool();\n    QueryRunner theRunner = toolChest.postMergeQueryDecoration(\n        new FinalizeResultsQueryRunner<>(\n            toolChest.mergeResults(factory.mergeRunners(Executors.newCachedThreadPool(), singleSegmentRunners)),\n            toolChest\n        )\n    );\n\n    TestHelper.assertExpectedObjects(\n        bySegmentResults,\n        theRunner.run(QueryPlus.wrap(fullQuery)),\n        \"bySegment\"\n    );\n    exec.shutdownNow();\n  }\n","realPath":"processing/src/test/java/org/apache/druid/query/groupby/GroupByQueryRunnerTest.java","repoName":"druid","snippetEndLine":0,"snippetStartLine":0,"startLine":7904,"status":"N"}],"commitId":"b6aaf59e8cdc4b2965ec9f54d8b824a51baaa594","commitMessage":"@@@Add \"offset\" parameter to GroupBy query. (#10235)\n\n* Add \"offset\" parameter to GroupBy query.\n\nIt works by doing the query as normal and then throwing away the first\n\"offset\" number of rows on the broker.\n\n* Stabilize GroupBy sorts.\n\n* Fix inspections.\n\n* Fix suppression.\n\n* Fixups.\n\n* Move TopNSequence to druid-core.\n\n* Addl comments.\n\n* NumberedElement equals verification.\n\n* Changes from review.","date":"2020-08-06 06:39:58","modifiedFileCount":"12","status":"M","submitter":"Gian Merlino"},{"authorTime":"2021-06-12 04:49:03","codes":[{"authorDate":"2020-08-06 06:39:58","commitOrder":10,"curCode":"  private void doTestMergeResultsWithValidLimit(final int limit, final int offset)\n  {\n    GroupByQuery.Builder builder = makeQueryBuilder()\n        .setDataSource(QueryRunnerTestHelper.DATA_SOURCE)\n        .setInterval(\"2011-04-02/2011-04-04\")\n        .setDimensions(new DefaultDimensionSpec(\"quality\", \"alias\"))\n        .setAggregatorSpecs(QueryRunnerTestHelper.ROWS_COUNT, new LongSumAggregatorFactory(\"idx\", \"index\"))\n        .setGranularity(new PeriodGranularity(new Period(\"P1M\"), null, null))\n        .setLimitSpec(DefaultLimitSpec.builder().limit(limit).offset(offset).build());\n\n    final GroupByQuery fullQuery = builder.build();\n\n    List<ResultRow> expectedResults = Arrays.asList(\n        makeRow(\n            fullQuery,\n            \"2011-04-01\",\n            \"alias\",\n            \"automotive\",\n            \"rows\",\n            2L,\n            \"idx\",\n            269L\n        ),\n        makeRow(\n            fullQuery,\n            \"2011-04-01\",\n            \"alias\",\n            \"business\",\n            \"rows\",\n            2L,\n            \"idx\",\n            217L\n        ),\n        makeRow(\n            fullQuery,\n            \"2011-04-01\",\n            \"alias\",\n            \"entertainment\",\n            \"rows\",\n            2L,\n            \"idx\",\n            319L\n        ),\n        makeRow(\n            fullQuery,\n            \"2011-04-01\",\n            \"alias\",\n            \"health\",\n            \"rows\",\n            2L,\n            \"idx\",\n            216L\n        ),\n        makeRow(\n            fullQuery,\n            \"2011-04-01\",\n            \"alias\",\n            \"mezzanine\",\n            \"rows\",\n            6L,\n            \"idx\",\n            4420L\n        ),\n        makeRow(\n            fullQuery,\n            \"2011-04-01\",\n            \"alias\",\n            \"news\",\n            \"rows\",\n            2L,\n            \"idx\",\n            221L\n        ),\n        makeRow(\n            fullQuery,\n            \"2011-04-01\",\n            \"alias\",\n            \"premium\",\n            \"rows\",\n            6L,\n            \"idx\",\n            4416L\n        ),\n        makeRow(\n            fullQuery,\n            \"2011-04-01\",\n            \"alias\",\n            \"technology\",\n            \"rows\",\n            2L,\n            \"idx\",\n            177L\n        ),\n        makeRow(\n            fullQuery,\n            \"2011-04-01\",\n            \"alias\",\n            \"travel\",\n            \"rows\",\n            2L,\n            \"idx\",\n            243L\n        )\n    );\n\n    QueryRunner<ResultRow> mergeRunner = factory.getToolchest().mergeResults(runner);\n\n    TestHelper.assertExpectedObjects(\n        Iterables.limit(Iterables.skip(expectedResults, offset), limit),\n        mergeRunner.run(QueryPlus.wrap(fullQuery)),\n        StringUtils.format(\"limit: %d\", limit)\n    );\n  }\n","date":"2020-08-06 06:39:58","endLine":3167,"groupId":"106463","id":19,"instanceNumber":1,"isCurCommit":0,"methodName":"doTestMergeResultsWithValidLimit","params":"(finalintlimit@finalintoffset)","path":"/mnt/clonedata/CloneManagementServer/ManagementServer/consistResult/result-druid-10-0.7/blobInfo/CC_OUT/blobs/39/3c558d87951cea5b49e52a9d66a40358506720.src","preCode":"  private void doTestMergeResultsWithValidLimit(final int limit, final int offset)\n  {\n    GroupByQuery.Builder builder = makeQueryBuilder()\n        .setDataSource(QueryRunnerTestHelper.DATA_SOURCE)\n        .setInterval(\"2011-04-02/2011-04-04\")\n        .setDimensions(new DefaultDimensionSpec(\"quality\", \"alias\"))\n        .setAggregatorSpecs(QueryRunnerTestHelper.ROWS_COUNT, new LongSumAggregatorFactory(\"idx\", \"index\"))\n        .setGranularity(new PeriodGranularity(new Period(\"P1M\"), null, null))\n        .setLimitSpec(DefaultLimitSpec.builder().limit(limit).offset(offset).build());\n\n    final GroupByQuery fullQuery = builder.build();\n\n    List<ResultRow> expectedResults = Arrays.asList(\n        makeRow(\n            fullQuery,\n            \"2011-04-01\",\n            \"alias\",\n            \"automotive\",\n            \"rows\",\n            2L,\n            \"idx\",\n            269L\n        ),\n        makeRow(\n            fullQuery,\n            \"2011-04-01\",\n            \"alias\",\n            \"business\",\n            \"rows\",\n            2L,\n            \"idx\",\n            217L\n        ),\n        makeRow(\n            fullQuery,\n            \"2011-04-01\",\n            \"alias\",\n            \"entertainment\",\n            \"rows\",\n            2L,\n            \"idx\",\n            319L\n        ),\n        makeRow(\n            fullQuery,\n            \"2011-04-01\",\n            \"alias\",\n            \"health\",\n            \"rows\",\n            2L,\n            \"idx\",\n            216L\n        ),\n        makeRow(\n            fullQuery,\n            \"2011-04-01\",\n            \"alias\",\n            \"mezzanine\",\n            \"rows\",\n            6L,\n            \"idx\",\n            4420L\n        ),\n        makeRow(\n            fullQuery,\n            \"2011-04-01\",\n            \"alias\",\n            \"news\",\n            \"rows\",\n            2L,\n            \"idx\",\n            221L\n        ),\n        makeRow(\n            fullQuery,\n            \"2011-04-01\",\n            \"alias\",\n            \"premium\",\n            \"rows\",\n            6L,\n            \"idx\",\n            4416L\n        ),\n        makeRow(\n            fullQuery,\n            \"2011-04-01\",\n            \"alias\",\n            \"technology\",\n            \"rows\",\n            2L,\n            \"idx\",\n            177L\n        ),\n        makeRow(\n            fullQuery,\n            \"2011-04-01\",\n            \"alias\",\n            \"travel\",\n            \"rows\",\n            2L,\n            \"idx\",\n            243L\n        )\n    );\n\n    QueryRunner<ResultRow> mergeRunner = factory.getToolchest().mergeResults(runner);\n\n    TestHelper.assertExpectedObjects(\n        Iterables.limit(Iterables.skip(expectedResults, offset), limit),\n        mergeRunner.run(QueryPlus.wrap(fullQuery)),\n        StringUtils.format(\"limit: %d\", limit)\n    );\n  }\n","realPath":"processing/src/test/java/org/apache/druid/query/groupby/GroupByQueryRunnerTest.java","repoName":"druid","snippetEndLine":0,"snippetStartLine":0,"startLine":3055,"status":"N"},{"authorDate":"2021-06-12 04:49:03","commitOrder":10,"curCode":"  public void testBySegmentResultsUnOptimizedDimextraction()\n  {\n    GroupByQuery.Builder builder = makeQueryBuilder()\n        .setDataSource(QueryRunnerTestHelper.DATA_SOURCE)\n        .setInterval(\"2011-04-02/2011-04-04\").setDimensions(new ExtractionDimensionSpec(\n            \"quality\",\n            \"alias\",\n            new LookupExtractionFn(\n                new MapLookupExtractor(ImmutableMap.of(\"mezzanine\", \"mezzanine0\"), false),\n                false,\n                null,\n                false,\n                false\n            )\n        )).setAggregatorSpecs(QueryRunnerTestHelper.ROWS_COUNT, new LongSumAggregatorFactory(\"idx\", \"index\"))\n        .setGranularity(new PeriodGranularity(new Period(\"P1M\"), null, null))\n        .setDimFilter(new SelectorDimFilter(\"quality\", \"mezzanine\", null))\n        .setContext(ImmutableMap.of(QueryContexts.BY_SEGMENT_KEY, true));\n    final GroupByQuery fullQuery = builder.build();\n\n    int segmentCount = 32;\n    Result<BySegmentResultValue> singleSegmentResult = new Result<>(\n        DateTimes.of(\"2011-01-12T00:00:00.000Z\"),\n        new BySegmentResultValueClass<>(\n            Collections.singletonList(\n                makeRow(\n                    fullQuery,\n                    \"2011-04-01\",\n                    \"alias\",\n                    \"mezzanine0\",\n                    \"rows\",\n                    6L,\n                    \"idx\",\n                    4420L\n                )\n            ),\n            QueryRunnerTestHelper.SEGMENT_ID.toString(),\n            Intervals.of(\"2011-04-02T00:00:00.000Z/2011-04-04T00:00:00.000Z\")\n        )\n    );\n    List<Result> bySegmentResults = new ArrayList<>();\n    for (int i = 0; i < segmentCount; i++) {\n      bySegmentResults.add(singleSegmentResult);\n    }\n    QueryToolChest toolChest = factory.getToolchest();\n\n    List<QueryRunner<ResultRow>> singleSegmentRunners = new ArrayList<>();\n    for (int i = 0; i < segmentCount; i++) {\n      singleSegmentRunners.add(toolChest.preMergeQueryDecoration(runner));\n    }\n    ExecutorService exec = Executors.newCachedThreadPool();\n    QueryRunner theRunner = toolChest.postMergeQueryDecoration(\n        new FinalizeResultsQueryRunner<>(\n            toolChest.mergeResults(factory.mergeRunners(Executors.newCachedThreadPool(), singleSegmentRunners)),\n            toolChest\n        )\n    );\n\n    TestHelper.assertExpectedObjects(\n        bySegmentResults,\n        theRunner.run(QueryPlus.wrap(fullQuery)),\n        \"bySegment\"\n    );\n    exec.shutdownNow();\n  }\n","date":"2021-06-12 04:49:03","endLine":8034,"groupId":"106463","id":20,"instanceNumber":2,"isCurCommit":0,"methodName":"testBySegmentResultsUnOptimizedDimextraction","params":"()","path":"/mnt/clonedata/CloneManagementServer/ManagementServer/consistResult/result-druid-10-0.7/blobInfo/CC_OUT/blobs/bf/d40ce593532de8e6a8201b841866c97cf184d9.src","preCode":"  public void testBySegmentResultsUnOptimizedDimextraction()\n  {\n    GroupByQuery.Builder builder = makeQueryBuilder()\n        .setDataSource(QueryRunnerTestHelper.DATA_SOURCE)\n        .setInterval(\"2011-04-02/2011-04-04\").setDimensions(new ExtractionDimensionSpec(\n            \"quality\",\n            \"alias\",\n            new LookupExtractionFn(\n                new MapLookupExtractor(ImmutableMap.of(\"mezzanine\", \"mezzanine0\"), false),\n                false,\n                null,\n                false,\n                false\n            )\n        )).setAggregatorSpecs(QueryRunnerTestHelper.ROWS_COUNT, new LongSumAggregatorFactory(\"idx\", \"index\"))\n        .setGranularity(new PeriodGranularity(new Period(\"P1M\"), null, null))\n        .setDimFilter(new SelectorDimFilter(\"quality\", \"mezzanine\", null))\n        .setContext(ImmutableMap.of(\"bySegment\", true));\n    final GroupByQuery fullQuery = builder.build();\n\n    int segmentCount = 32;\n    Result<BySegmentResultValue> singleSegmentResult = new Result<>(\n        DateTimes.of(\"2011-01-12T00:00:00.000Z\"),\n        new BySegmentResultValueClass<>(\n            Collections.singletonList(\n                makeRow(\n                    fullQuery,\n                    \"2011-04-01\",\n                    \"alias\",\n                    \"mezzanine0\",\n                    \"rows\",\n                    6L,\n                    \"idx\",\n                    4420L\n                )\n            ),\n            QueryRunnerTestHelper.SEGMENT_ID.toString(),\n            Intervals.of(\"2011-04-02T00:00:00.000Z/2011-04-04T00:00:00.000Z\")\n        )\n    );\n    List<Result> bySegmentResults = new ArrayList<>();\n    for (int i = 0; i < segmentCount; i++) {\n      bySegmentResults.add(singleSegmentResult);\n    }\n    QueryToolChest toolChest = factory.getToolchest();\n\n    List<QueryRunner<ResultRow>> singleSegmentRunners = new ArrayList<>();\n    for (int i = 0; i < segmentCount; i++) {\n      singleSegmentRunners.add(toolChest.preMergeQueryDecoration(runner));\n    }\n    ExecutorService exec = Executors.newCachedThreadPool();\n    QueryRunner theRunner = toolChest.postMergeQueryDecoration(\n        new FinalizeResultsQueryRunner<>(\n            toolChest.mergeResults(factory.mergeRunners(Executors.newCachedThreadPool(), singleSegmentRunners)),\n            toolChest\n        )\n    );\n\n    TestHelper.assertExpectedObjects(\n        bySegmentResults,\n        theRunner.run(QueryPlus.wrap(fullQuery)),\n        \"bySegment\"\n    );\n    exec.shutdownNow();\n  }\n","realPath":"processing/src/test/java/org/apache/druid/query/groupby/GroupByQueryRunnerTest.java","repoName":"druid","snippetEndLine":0,"snippetStartLine":0,"startLine":7970,"status":"M"}],"commitId":"50327b8f6334f9bb89ab60e262ed47983a7711ee","commitMessage":"@@@ignore bySegment query context for SQL queries (#11352)\n\n* ignore bySegment query context for SQL queries\n\n* revert unintended change","date":"2021-06-12 04:49:03","modifiedFileCount":"9","status":"M","submitter":"Clint Wylie"}]
